BLOCKS:  {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3}}, 'b3': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 3, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.25, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10}, 'pool': None, 'activation': None}}
CL:  True
{'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 100, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True}
CL:  False
{'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 100, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': False}
CL:  False
{'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 100, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 50, 'validation': False, 'continual_learning': False}
SEED:  0
block 0, size : 96 16 16
range = 2.886751345948129
block 1, size : 384 8 8
range = 0.8505172717997146
block 2, size : 1536 4 4
range = 0.4252586358998573
range = 0.11048543456039805
The device used will be: 
True
cuda:0
CONFIG: {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CONFIG: {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CONFIG: {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CONFIG: {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.11048543456039805, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 100, 'in_channels': 24576, 'old_channels': 1536, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}

 Model CIFAR100_CIFAR10_CL loaded successfuly with best perf



 ----- Architecture Block BatchNorm2dSK3962(5, 5)1.0reflect, number 0 -----
- BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK963842(3, 3)0.6499999761581421reflect, number 1 -----
- BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.4)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK38415362(3, 3)0.25reflect, number 2 -----
- BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 3 -----
- Flatten(start_dim=1, end_dim=-1)
- Dropout(p=0.5, inplace=False)
- Linear(in_features=24576, out_features=100, bias=True)

 ********** Hebbian Unsupervised learning of blocks [0] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
INDICES:  50000
IMAGE SIZE: torch.Size([10, 3, 32, 32])
SAVING FOLDER FOR UNSUP:  CIFAR100_CIFAR10_CL
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 1.40e-01	time: 00:00:26	Acc_train 0.00	Acc_test 0.00	convergence: 2.40e+01	R1: 0	Info MB:0.000e+00/SB:0.000e+00/MW:8.512e-03/SW:5.669e-01/MR:4.551e+00/SR:1.852e+00/MeD:1.458e+00/MaD:3.897e+00/MW:0.621/MAW:0.379
|       0 |       1 |          2 |       3 |       4 |       5 |         6 |          7 |       8 |       9 |      10 |       11 |      12 |     13 |      14 |      15 |      16 |      17 |      18 |      19 |     20 |      21 |      22 |      23 |      24 |     25 |      26 |      27 |      28 |     29 |
|---------+---------+------------+---------+---------+---------+-----------+------------+---------+---------+---------+----------+---------+--------+---------+---------+---------+---------+---------+---------+--------+---------+---------+---------+---------+--------+---------+---------+---------+--------|
|   0.168 |   0.189 |   0.000265 |   0.144 |   0.216 |   0.099 |   0.00143 |   0.000275 |   0.105 |   0.145 |   0.178 |   0.0963 |   0.168 |   0.19 |   0.155 |   0.169 |   0.162 |   0.183 |   0.133 |   0.162 |   0.17 |   0.169 |   0.141 |   0.185 |   0.149 |   0.18 |   0.162 |   0.155 |   0.197 |   0.17 |
|   5.4   |   6.56  |   1        |   4.25  |   8.27  |   2.53  |   1       |   1        |   2.73  |   4.29  |   5.94  |   2.45   |   5.41  |   6.62 |   4.78  |   5.46  |   5.09  |   6.23  |   3.77  |   5.09  |   5.53 |   5.46  |   4.09  |   6.33  |   4.49  |   6.04 |   5.09  |   4.77  |   7.05  |   5.5  |
|   0.46  |   0.48  |   1.43     |   0.32  |   0.42  |   0.43  |   2.09    |   1.41     |   0.43  |   0.36  |   0.36  |   0.47   |   0.47  |   0.32 |   0.35  |   0.46  |   0.35  |   0.47  |   0.35  |   0.31  |   0.46 |   0.46  |   0.43  |   0.32  |   0.34  |   0.35 |   0.48  |   0.46  |   0.46  |   0.31 |
| nan     | nan     | nan        | nan     | nan     | nan     | nan       | nan        | nan     | nan     | nan     | nan      | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan    |
| nan     | nan     | nan        | nan     | nan     | nan     | nan       | nan        | nan     | nan     | nan     | nan      | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan    |
| nan     | nan     | nan        | nan     | nan     | nan     | nan       | nan        | nan     | nan     | nan     | nan      | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan    |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/g100_work/EIRI_E_POLIMI/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/CIFAR100_CIFAR10_CL/models

 ********** Hebbian Unsupervised learning of blocks [1] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
INDICES:  50000
IMAGE SIZE: torch.Size([10, 3, 32, 32])
SAVING FOLDER FOR UNSUP:  CIFAR100_CIFAR10_CL
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 1.40e-01	time: 00:00:43	Acc_train 0.00	Acc_test 0.00	convergence: 2.40e+01	R1: 0	Info MB:0.000e+00/SB:0.000e+00/MW:6.380e-03/SW:1.262e-01/MR:3.492e+00/SR:1.270e+00/MeD:1.007e+00/MaD:3.762e+00/MW:0.629/MAW:0.371
|         0 |         1 |         2 |         3 |        4 |         5 |         6 |         7 |         8 |        9 |        10 |        11 |        12 |        13 |        14 |        15 |        16 |         17 |        18 |        19 |       20 |        21 |       22 |       23 |        24 |        25 |        26 |        27 |        28 |        29 |
|-----------+-----------+-----------+-----------+----------+-----------+-----------+-----------+-----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+-----------+-----------+----------+-----------+----------+----------+-----------+-----------+-----------+-----------+-----------+-----------|
|   0.00979 |   0.00614 |   0.00685 |   0.00713 |   0.0101 |   0.00841 |   0.00789 |   0.00778 |   0.00789 |   0.0019 |   0.00305 |   0.00887 |   0.00894 |   9.3e-06 |   0.00907 |   0.00898 |   0.00777 |   3.65e-05 |   0.00927 |   0.00865 |   0.0083 |   0.00979 |   0.0102 |   0.0102 |   0.00982 |   0.00711 |   0.00742 |   0.00963 |   0.00948 |   0.00911 |
|   4.83    |   2.51    |   2.88    |   3.04    |   5.1    |   3.83    |   3.49    |   3.42    |   3.49    |   1.14   |   1.37    |   4.15    |   4.2     |   1       |   4.29    |   4.23    |   3.42    |   1        |   4.44    |   3.99    |   3.75   |   4.83    |   5.14   |   5.14   |   4.86    |   3.02    |   3.21    |   4.71    |   4.59    |   4.32    |
|   0.12    |   0.1     |   0.06    |   0.13    |   0.16   |   0.1     |   0.1     |   0.11    |   0.07    |   0.12   |   0.15    |   0.12    |   0.13    |  13.46    |   0.14    |   0.09    |   0.12    |   3.12     |   0.17    |   0.1     |   0.12   |   0.04    |   0.15   |   0.14   |   0.06    |   0.03    |   0.12    |   0.13    |   0.11    |   0.14    |
| nan       | nan       | nan       | nan       | nan      | nan       | nan       | nan       | nan       | nan      | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan        | nan       | nan       | nan      | nan       | nan      | nan      | nan       | nan       | nan       | nan       | nan       | nan       |
| nan       | nan       | nan       | nan       | nan      | nan       | nan       | nan       | nan       | nan      | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan        | nan       | nan       | nan      | nan       | nan      | nan      | nan       | nan       | nan       | nan       | nan       | nan       |
| nan       | nan       | nan       | nan       | nan      | nan       | nan       | nan       | nan       | nan      | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan        | nan       | nan       | nan      | nan       | nan      | nan      | nan       | nan       | nan       | nan       | nan       | nan       |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/g100_work/EIRI_E_POLIMI/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/CIFAR100_CIFAR10_CL/models

 ********** Hebbian Unsupervised learning of blocks [2] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
INDICES:  50000
IMAGE SIZE: torch.Size([10, 3, 32, 32])
SAVING FOLDER FOR UNSUP:  CIFAR100_CIFAR10_CL
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 3.75e-02	time: 00:01:07	Acc_train 0.00	Acc_test 0.00	convergence: 1.41e+01	R1: 1	Info MB:0.000e+00/SB:0.000e+00/MW:1.782e-02/SW:2.590e-01/MR:1.515e+01/SR:1.870e+00/MeD:1.497e+00/MaD:1.415e+01/MW:0.467/MAW:0.533
|        0 |        1 |        2 |        3 |       4 |        5 |        6 |        7 |        8 |       9 |       10 |       11 |       12 |       13 |       14 |       15 |      16 |       17 |       18 |       19 |       20 |      21 |       22 |       23 |      24 |       25 |       26 |       27 |       28 |       29 |
|----------+----------+----------+----------+---------+----------+----------+----------+----------+---------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------+---------+----------+----------+---------+----------+----------+----------+----------+----------|
|   0.0375 |   0.0377 |   0.0406 |   0.0335 |   0.038 |   0.0384 |   0.0406 |   0.0353 |   0.0392 |   0.035 |   0.0383 |   0.0319 |   0.0348 |   0.0392 |   0.0386 |   0.0347 |   0.033 |   0.0408 |   0.0378 |   0.0357 |   0.0368 |   0.033 |   0.0378 |   0.0341 |   0.038 |   0.0341 |   0.0352 |   0.0396 |   0.0377 |   0.0384 |
|  15.04   |  15.19   |  17.47   |  12.2    |  15.44  |  15.76   |  17.48   |  13.48   |  16.39   |  13.28  |  15.64   |  11.17   |  13.1    |  16.38   |  15.87   |  13.02   |  11.86  |  17.62   |  15.26   |  13.72   |  14.56   |  11.89  |  15.31   |  12.6    |  15.47  |  12.63   |  13.36   |  16.65   |  15.25   |  15.78   |
|   0.07   |   0.06   |   0.07   |   0.07   |   0.07  |   0.06   |   0.05   |   0.08   |   0.07   |   0.06  |   0.04   |   0.07   |   0.03   |   0.08   |   0.06   |   0.04   |   0.04  |   0.05   |   0.04   |   0.06   |   0.06   |   0.06  |   0.09   |   0.04   |   0.05  |   0.06   |   0.03   |   0.07   |   0.05   |   0.06   |
| nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/g100_work/EIRI_E_POLIMI/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/CIFAR100_CIFAR10_CL/models
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
INDICES:  50000
IMAGE SIZE: torch.Size([64, 3, 32, 32])
Accuracy of the network on the 1st dataset: 42.890 %
Test loss on the 1st dataset: 0.081
CL:  True
{'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True}
CL:  True
{'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': True}
SEED:  0
block 0, size : 96 16 16
range = 2.886751345948129
block 1, size : 384 8 8
range = 0.8505172717997146
block 2, size : 1536 4 4
range = 0.4252586358998573
range = 0.11048543456039805
The device used will be: 
True
cuda:0
CONFIG: {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CONFIG: {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CONFIG: {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CONFIG: {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.11048543456039805, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 100, 'in_channels': 24576, 'old_channels': 1536, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}

 Model CIFAR100_CIFAR10_CL loaded successfuly with best perf



 ----- Architecture Block BatchNorm2dSK3962(5, 5)1.0reflect, number 0 -----
- BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK963842(3, 3)0.6499999761581421reflect, number 1 -----
- BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.4)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK38415362(3, 3)0.25reflect, number 2 -----
- BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 3 -----
- Flatten(start_dim=1, end_dim=-1)
- Dropout(p=0.5, inplace=False)
- Linear(in_features=24576, out_features=100, bias=True)

 ********** Hebbian Unsupervised learning of blocks [0] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
INDICES:  50000
IMAGE SIZE: torch.Size([10, 3, 32, 32])
SAVING FOLDER FOR UNSUP:  CIFAR100_CIFAR10_CL
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 1.55e-01	time: 00:00:23	Acc_train 0.00	Acc_test 0.00	convergence: 2.40e+01	R1: 0	Info MB:0.000e+00/SB:0.000e+00/MW:1.419e-02/SW:6.712e-01/MR:5.323e+00/SR:2.351e+00/MeD:1.837e+00/MaD:6.816e+00/MW:0.611/MAW:0.389
|       0 |       1 |          2 |       3 |       4 |       5 |        6 |          7 |       8 |       9 |      10 |      11 |     12 |      13 |      14 |      15 |     16 |      17 |      18 |      19 |      20 |      21 |      22 |      23 |      24 |     25 |      26 |      27 |      28 |      29 |
|---------+---------+------------+---------+---------+---------+----------+------------+---------+---------+---------+---------+--------+---------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+--------+---------+---------+---------+---------|
|   0.153 |   0.186 |   0.000241 |   0.136 |   0.238 |   0.109 |   0.0396 |   0.000247 |   0.111 |   0.175 |   0.197 |   0.105 |   0.18 |   0.237 |   0.156 |   0.183 |   0.19 |   0.221 |   0.147 |   0.158 |   0.267 |   0.168 |   0.161 |   0.204 |   0.166 |   0.18 |   0.168 |   0.175 |   0.195 |   0.173 |
|   4.64  |   6.39  |   1        |   3.88  |   9.83  |   2.87  |   1.25   |   1        |   2.93  |   5.76  |   7.08  |   2.71  |   6.09 |   9.77  |   4.81  |   6.24  |   6.65 |   8.63  |   4.37  |   4.88  |  12.14  |   5.43  |   5.05  |   7.53  |   5.31  |   6.06 |   5.4   |   5.79  |   6.93  |   5.7   |
|   0.5   |   0.52  |   1.63     |   0.33  |   0.36  |   0.46  |   0.9    |   1.56     |   0.46  |   0.38  |   0.4   |   0.5   |   0.51 |   0.35  |   0.33  |   0.51  |   0.38 |   0.5   |   0.36  |   0.33  |   0.5   |   0.5   |   0.48  |   0.34  |   0.37  |   0.36 |   0.52  |   0.51  |   0.5   |   0.33  |
| nan     | nan     | nan        | nan     | nan     | nan     | nan      | nan        | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     |
| nan     | nan     | nan        | nan     | nan     | nan     | nan      | nan        | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     |
| nan     | nan     | nan        | nan     | nan     | nan     | nan      | nan        | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/g100_work/EIRI_E_POLIMI/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/CIFAR100_CIFAR10_CL/models

 ********** Hebbian Unsupervised learning of blocks [1] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
INDICES:  50000
IMAGE SIZE: torch.Size([10, 3, 32, 32])
SAVING FOLDER FOR UNSUP:  CIFAR100_CIFAR10_CL
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 1.55e-01	time: 00:00:46	Acc_train 0.00	Acc_test 0.00	convergence: 2.40e+01	R1: 0	Info MB:0.000e+00/SB:0.000e+00/MW:5.601e-03/SW:1.256e-01/MR:3.461e+00/SR:1.298e+00/MeD:1.023e+00/MaD:4.834e+00/MW:0.622/MAW:0.378
|         0 |         1 |         2 |         3 |         4 |         5 |         6 |         7 |         8 |         9 |        10 |        11 |        12 |         13 |        14 |        15 |        16 |         17 |        18 |        19 |       20 |        21 |        22 |        23 |        24 |        25 |        26 |        27 |       28 |        29 |
|-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+-----------+-----------+-----------+------------+-----------+-----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+----------+-----------|
|   0.00811 |   0.00826 |   0.00936 |   0.00738 |   0.00882 |   0.00856 |   0.00838 |   0.00863 |   0.00959 |   0.00225 |   0.00381 |   0.00869 |   0.00932 |   9.92e-06 |   0.00954 |   0.00863 |   0.00834 |   3.66e-05 |   0.00865 |   0.00775 |   0.0084 |   0.00589 |   0.00805 |   0.00978 |   0.00909 |   0.00913 |   0.00847 |   0.00757 |   0.0082 |   0.00845 |
|   3.63    |   3.73    |   4.51    |   3.18    |   4.11    |   3.93    |   3.81    |   3.98    |   4.68    |   1.2     |   1.58    |   4.02    |   4.47    |   1        |   4.64    |   3.98    |   3.78    |   1        |   3.99    |   3.4     |   3.82   |   2.39    |   3.59    |   4.83    |   4.3     |   4.34    |   3.87    |   3.29    |   3.69   |   3.85    |
|   0.09    |   0.07    |   0.03    |   0.06    |   0.13    |   0.14    |   0.1     |   0.11    |   0.06    |   0.05    |   0.04    |   0.1     |   0.12    |  14.08     |   0.11    |   0.1     |   0.08    |   3.12     |   0.12    |   0.07    |   0.1    |   0.12    |   0.13    |   0.12    |   0.15    |   0.06    |   0.11    |   0.11    |   0.1    |   0.12    |
| nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan        | nan       | nan       | nan       | nan        | nan       | nan       | nan      | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan      | nan       |
| nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan        | nan       | nan       | nan       | nan        | nan       | nan       | nan      | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan      | nan       |
| nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan        | nan       | nan       | nan       | nan        | nan       | nan       | nan      | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan      | nan       |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/g100_work/EIRI_E_POLIMI/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/CIFAR100_CIFAR10_CL/models

 ********** Hebbian Unsupervised learning of blocks [2] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
INDICES:  50000
IMAGE SIZE: torch.Size([10, 3, 32, 32])
SAVING FOLDER FOR UNSUP:  CIFAR100_CIFAR10_CL
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 3.75e-02	time: 00:01:15	Acc_train 0.00	Acc_test 0.00	convergence: 1.41e+01	R1: 1	Info MB:0.000e+00/SB:0.000e+00/MW:2.292e-02/SW:2.576e-01/MR:1.512e+01/SR:1.631e+00/MeD:1.280e+00/MaD:1.412e+01/MW:0.471/MAW:0.529
|        0 |       1 |        2 |        3 |        4 |        5 |        6 |        7 |        8 |        9 |       10 |       11 |      12 |       13 |       14 |       15 |       16 |       17 |       18 |       19 |       20 |       21 |       22 |       23 |       24 |       25 |       26 |       27 |       28 |       29 |
|----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------|
|   0.0387 |   0.037 |   0.0406 |   0.0334 |   0.0379 |   0.0378 |   0.0405 |   0.0368 |   0.0383 |   0.0352 |   0.0378 |   0.0325 |   0.033 |   0.0399 |   0.0367 |   0.0371 |   0.0376 |   0.0396 |   0.0373 |   0.0347 |   0.0374 |   0.0346 |   0.0378 |   0.0331 |   0.0359 |   0.0351 |   0.0363 |   0.0386 |   0.0385 |   0.0375 |
|  16      |  14.7   |  17.45   |  12.14   |  15.35   |  15.3    |  17.41   |  14.54   |  15.65   |  13.39   |  15.28   |  11.56   |  11.87  |  16.9    |  14.46   |  14.78   |  15.16   |  16.7    |  14.94   |  13.02   |  14.98   |  12.94   |  15.32   |  11.94   |  13.85   |  13.29   |  14.2    |  15.87   |  15.83   |  15.08   |
|   0.06   |   0.06  |   0.06   |   0.06   |   0.06   |   0.04   |   0.04   |   0.07   |   0.06   |   0.04   |   0.04   |   0.03   |   0.02  |   0.06   |   0.07   |   0.04   |   0.03   |   0.04   |   0.05   |   0.06   |   0.05   |   0.05   |   0.06   |   0.04   |   0.05   |   0.05   |   0.04   |   0.09   |   0.06   |   0.06   |
| nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |
| nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |
| nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/g100_work/EIRI_E_POLIMI/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/CIFAR100_CIFAR10_CL/models

 ********** Supervised learning of blocks [3] **********
SAVING FOLDER FOR SUP:  CIFAR100_CIFAR10_CL
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
INDICES:  50000
IMAGE SIZE: torch.Size([64, 3, 32, 32])
Epoch: [1/50]	lr: 1.00e-03	time: 00:01:28	Loss_train 1.36111	Acc_train 48.21	/	Loss_test 0.04129	Acc_test 65.07
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/g100_work/EIRI_E_POLIMI/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/CIFAR100_CIFAR10_CL/models
Epoch: [10/50]	lr: 1.00e-03	time: 00:02:01	Loss_train 0.54973	Acc_train 70.26	/	Loss_test 0.02812	Acc_test 74.43
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/g100_work/EIRI_E_POLIMI/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/CIFAR100_CIFAR10_CL/models
Epoch: [20/50]	lr: 2.50e-04	time: 00:02:38	Loss_train 0.30268	Acc_train 78.19	/	Loss_test 0.01983	Acc_test 77.80
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/g100_work/EIRI_E_POLIMI/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/CIFAR100_CIFAR10_CL/models
Epoch: [30/50]	lr: 1.25e-04	time: 00:03:15	Loss_train 0.22043	Acc_train 80.79	/	Loss_test 0.01821	Acc_test 78.20
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/g100_work/EIRI_E_POLIMI/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/CIFAR100_CIFAR10_CL/models
Epoch: [40/50]	lr: 3.13e-05	time: 00:03:51	Loss_train 0.18691	Acc_train 82.00	/	Loss_test 0.01703	Acc_test 78.54
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/g100_work/EIRI_E_POLIMI/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/CIFAR100_CIFAR10_CL/models
Epoch: [50/50]	lr: 7.81e-06	time: 00:04:28	Loss_train 0.17674	Acc_train 82.40	/	Loss_test 0.01679	Acc_test 78.64
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/g100_work/EIRI_E_POLIMI/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/CIFAR100_CIFAR10_CL/models
RESULT:  {'train_loss': 0.17673709988594055, 'train_acc': 82.39820003509521, 'test_loss': 0.01678629033267498, 'test_acc': 78.63999938964844, 'convergence': 14.115509033203125, 'R1': 1, 'dataset_sup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32}, 'dataset_unsup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32}, 'train_config': {'t0': {'blocks': [0], 'mode': 'unsupervised', 'lr': 0.08, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't2': {'blocks': [1], 'mode': 'unsupervised', 'lr': 0.005, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't4': {'blocks': [2], 'mode': 'unsupervised', 'lr': 0.01, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't6': {'blocks': [3], 'mode': 'supervised', 'lr': 0.001, 'nb_epoch': 50, 'batch_size': 64, 'print_freq': 10}}}
IN R1:  {'eval_1': {'test_loss': 0.08102434128522873, 'test_acc': 42.88999938964844, 'convergence': 14.147342681884766, 'R1': 1, 'dataset_sup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 100, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': False}, 'dataset_unsup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 100, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 50, 'validation': False, 'continual_learning': False}}, 'R1': {'train_loss': 0.17673709988594055, 'train_acc': 82.39820003509521, 'test_loss': 0.01678629033267498, 'test_acc': 78.63999938964844, 'convergence': 14.115509033203125, 'R1': 1, 'dataset_sup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32}, 'dataset_unsup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32}, 'train_config': {'t0': {'blocks': [0], 'mode': 'unsupervised', 'lr': 0.08, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't2': {'blocks': [1], 'mode': 'unsupervised', 'lr': 0.005, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't4': {'blocks': [2], 'mode': 'unsupervised', 'lr': 0.01, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't6': {'blocks': [3], 'mode': 'supervised', 'lr': 0.001, 'nb_epoch': 50, 'batch_size': 64, 'print_freq': 10}}}}
SEED:  0
block 0, size : 96 16 16
range = 2.886751345948129
block 1, size : 384 8 8
range = 0.8505172717997146
block 2, size : 1536 4 4
range = 0.4252586358998573
range = 0.11048543456039805
The device used will be: 
True
cuda:0
CONFIG: {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CONFIG: {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CONFIG: {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CONFIG: {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.11048543456039805, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 100, 'in_channels': 24576, 'old_channels': 1536, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}

 Model CIFAR100_CIFAR10_CL loaded successfuly with best perf



 ----- Architecture Block BatchNorm2dSK3962(5, 5)1.0reflect, number 0 -----
- BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK963842(3, 3)0.6499999761581421reflect, number 1 -----
- BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.4)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK38415362(3, 3)0.25reflect, number 2 -----
- BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 3 -----
- Flatten(start_dim=1, end_dim=-1)
- Dropout(p=0.5, inplace=False)
- Linear(in_features=24576, out_features=100, bias=True)

 ********** Hebbian Unsupervised learning of blocks [0] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
INDICES:  50000
IMAGE SIZE: torch.Size([10, 3, 32, 32])
SAVING FOLDER FOR UNSUP:  CIFAR100_CIFAR10_CL
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 1.53e-01	time: 00:00:17	Acc_train 0.00	Acc_test 0.00	convergence: 2.40e+01	R1: 0	Info MB:0.000e+00/SB:0.000e+00/MW:8.542e-03/SW:6.561e-01/MR:5.194e+00/SR:2.316e+00/MeD:1.800e+00/MaD:5.834e+00/MW:0.632/MAW:0.368
|       0 |       1 |          2 |       3 |       4 |        5 |       6 |          7 |       8 |       9 |      10 |       11 |     12 |      13 |     14 |      15 |      16 |      17 |      18 |      19 |      20 |      21 |      22 |      23 |      24 |      25 |      26 |     27 |      28 |      29 |
|---------+---------+------------+---------+---------+----------+---------+------------+---------+---------+---------+----------+--------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------+---------+---------|
|   0.146 |   0.177 |   0.000246 |   0.156 |   0.232 |   0.0877 |   0.109 |   0.000252 |   0.107 |   0.148 |   0.191 |   0.0939 |   0.16 |   0.189 |   0.17 |   0.178 |   0.174 |   0.226 |   0.142 |   0.174 |   0.231 |   0.159 |   0.238 |   0.206 |   0.148 |   0.168 |   0.161 |   0.17 |   0.204 |   0.167 |
|   4.33  |   5.91  |   1        |   4.81  |   9.39  |   2.2    |   2.84  |   1        |   2.78  |   4.42  |   6.7   |   2.38   |   5.01 |   6.57  |   5.51 |   5.95  |   5.76  |   8.95  |   4.16  |   5.73  |   9.34  |   4.93  |   9.84  |   7.64  |   4.44  |   5.41  |   5.07  |   5.54 |   7.52  |   5.37  |
|   0.49  |   0.52  |   1.62     |   0.34  |   0.36  |   0.45   |   0.39  |   1.61     |   0.44  |   0.39  |   0.39  |   0.48   |   0.5  |   0.35  |   0.35 |   0.49  |   0.38  |   0.49  |   0.34  |   0.34  |   0.35  |   0.47  |   0.43  |   0.34  |   0.37  |   0.35  |   0.5   |   0.49 |   0.49  |   0.33  |
| nan     | nan     | nan        | nan     | nan     | nan      | nan     | nan        | nan     | nan     | nan     | nan      | nan    | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     |
| nan     | nan     | nan        | nan     | nan     | nan      | nan     | nan        | nan     | nan     | nan     | nan      | nan    | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     |
| nan     | nan     | nan        | nan     | nan     | nan      | nan     | nan        | nan     | nan     | nan     | nan      | nan    | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/g100_work/EIRI_E_POLIMI/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/CIFAR100_CIFAR10_CL/models

 ********** Hebbian Unsupervised learning of blocks [1] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
INDICES:  50000
IMAGE SIZE: torch.Size([10, 3, 32, 32])
SAVING FOLDER FOR UNSUP:  CIFAR100_CIFAR10_CL
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 1.53e-01	time: 00:00:34	Acc_train 0.00	Acc_test 0.00	convergence: 2.40e+01	R1: 0	Info MB:0.000e+00/SB:0.000e+00/MW:6.009e-03/SW:1.075e-01/MR:2.970e+00/SR:1.095e+00/MeD:9.031e-01/MaD:3.223e+00/MW:0.647/MAW:0.353
|         0 |         1 |         2 |         3 |         4 |        5 |        6 |         7 |         8 |         9 |        10 |        11 |       12 |         13 |        14 |        15 |        16 |         17 |        18 |        19 |        20 |        21 |        22 |        23 |        24 |        25 |        26 |        27 |        28 |        29 |
|-----------+-----------+-----------+-----------+-----------+----------+----------+-----------+-----------+-----------+-----------+-----------+----------+------------+-----------+-----------+-----------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------|
|   0.00716 |   0.00634 |   0.00708 |   0.00572 |   0.00861 |   0.0072 |   0.0077 |   0.00769 |   0.00758 |   0.00253 |   0.00404 |   0.00868 |   0.0083 |   9.92e-06 |   0.00903 |   0.00931 |   0.00797 |   4.35e-05 |   0.00725 |   0.00807 |   0.00765 |   0.00706 |   0.00708 |   0.00978 |   0.00915 |   0.00557 |   0.00608 |   0.00892 |   0.00853 |   0.00879 |
|   3.05    |   2.61    |   3.01    |   2.31    |   3.97    |   3.07   |   3.37   |   3.36    |   3.3     |   1.26    |   1.65    |   4.02    |   3.76   |   1        |   4.26    |   4.46    |   3.54    |   1        |   3.1     |   3.6     |   3.34    |   2.99    |   3       |   4.83    |   4.35    |   2.24    |   2.48    |   4.18    |   3.91    |   4.09    |
|   0.05    |   0.07    |   0.06    |   0.06    |   0.11    |   0.11   |   0.1    |   0.09    |   0.06    |   0.03    |   0.02    |   0.08    |   0.1    |  14.35     |   0.09    |   0.07    |   0.09    |   3        |   0.09    |   0.05    |   0.1     |   0.05    |   0.08    |   0.11    |   0.11    |   0.06    |   0.11    |   0.08    |   0.09    |   0.09    |
| nan       | nan       | nan       | nan       | nan       | nan      | nan      | nan       | nan       | nan       | nan       | nan       | nan      | nan        | nan       | nan       | nan       | nan        | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       |
| nan       | nan       | nan       | nan       | nan       | nan      | nan      | nan       | nan       | nan       | nan       | nan       | nan      | nan        | nan       | nan       | nan       | nan        | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       |
| nan       | nan       | nan       | nan       | nan       | nan      | nan      | nan       | nan       | nan       | nan       | nan       | nan      | nan        | nan       | nan       | nan       | nan        | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/g100_work/EIRI_E_POLIMI/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/CIFAR100_CIFAR10_CL/models

 ********** Hebbian Unsupervised learning of blocks [2] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
INDICES:  50000
IMAGE SIZE: torch.Size([10, 3, 32, 32])
SAVING FOLDER FOR UNSUP:  CIFAR100_CIFAR10_CL
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 3.56e-02	time: 00:00:58	Acc_train 0.00	Acc_test 0.00	convergence: 1.28e+01	R1: 1	Info MB:0.000e+00/SB:0.000e+00/MW:2.457e-02/SW:2.346e-01/MR:1.375e+01/SR:1.810e+00/MeD:1.446e+00/MaD:1.275e+01/MW:0.507/MAW:0.493
|        0 |        1 |        2 |        3 |        4 |       5 |        6 |        7 |        8 |        9 |       10 |       11 |       12 |      13 |       14 |       15 |       16 |       17 |       18 |       19 |       20 |       21 |       22 |       23 |       24 |       25 |       26 |       27 |      28 |       29 |
|----------+----------+----------+----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------|
|   0.0372 |   0.0361 |   0.0383 |   0.0336 |   0.0341 |   0.036 |   0.0386 |   0.0365 |   0.0379 |   0.0342 |   0.0359 |   0.0304 |   0.0319 |   0.039 |   0.0374 |   0.0321 |   0.0315 |   0.0387 |   0.0357 |   0.0323 |   0.0349 |   0.0329 |   0.0359 |   0.0308 |   0.0358 |   0.0324 |   0.0361 |   0.0378 |   0.038 |   0.0367 |
|  14.81   |  14      |  15.65   |  12.29   |  12.64   |  13.94  |  15.89   |  14.33   |  15.39   |  12.66   |  13.88   |  10.22   |  11.19   |  16.22  |  15      |  11.33   |  10.93   |  15.98   |  13.77   |  11.41   |  13.18   |  11.83   |  13.9    |  10.47   |  13.81   |  11.48   |  14.03   |  15.31   |  15.46  |  14.46   |
|   0.06   |   0.06   |   0.05   |   0.04   |   0.04   |   0.05  |   0.05   |   0.06   |   0.05   |   0.04   |   0.04   |   0.03   |   0.02   |   0.06  |   0.06   |   0.04   |   0.03   |   0.04   |   0.04   |   0.04   |   0.03   |   0.05   |   0.06   |   0.03   |   0.03   |   0.03   |   0.04   |   0.08   |   0.05  |   0.06   |
| nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      |
| nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      |
| nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/g100_work/EIRI_E_POLIMI/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/CIFAR100_CIFAR10_CL/models
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
INDICES:  50000
IMAGE SIZE: torch.Size([64, 3, 32, 32])
Accuracy of the network on the 1st dataset: 1.120 %
Test loss on the 1st dataset: 0.689


--------------- /leonardo/prod/opt/modulefiles/deeplrn/libraries ---------------
cineca-ai/3.0.0  cineca-ai/4.0.0  cineca-ai/4.1.1(default)  
cineca-ai/3.0.1  cineca-ai/4.1.0  cineca-ai/4.3.0           

Key:
(symbolic-version)  
The device used will be: 
True
cuda:0
BLOCKS:  {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.25, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10}, 'pool': None, 'activation': None}}
[[0,3],[5,7]]


 ########### WARNING ############


 Invalid combination of parameters, provide either: [--classes, --dataset-sup, --dataset-unsup] or [--dataset-sup-1, --dataset-unsup-1, --dataset-sup-2, --dataset-unsup-2]
The continual learning is implemented per tasks where each task is made up of different classes 
 of the same dataset, so only one dataset will be considered.


 ################################


{'training_mode': 'consecutive', 'cf_sol': False, 'head_sol': False, 'top_k': 0.15, 'topk_lock': False, 'high_lr': 0.15, 'low_lr': 1.0, 't_criteria': 'activations', 'delta_w_interval': 50.0, 'heads_basis_t': 0.9, 'n_tasks': 2}
CL:  True
{'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 5000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': True, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 100, 'print_freq': 10, 'validation': False, 'continual_learning': True}
CL:  False
{'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 5000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': True, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 100, 'print_freq': 10, 'validation': False, 'continual_learning': False}
CL:  False
{'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 100000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 32, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'unlabeled', 'nb_epoch': 1, 'print_freq': 25, 'validation': False, 'continual_learning': False}
SEED:  0
block 0, size : 96 48 48
96
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.25, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10}, 'pool': None, 'activation': None}}
range = 2.886751345948129
block 1, size : 384 24 24
384
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.25, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10}, 'pool': None, 'activation': None}}
range = 0.8505172717997146
block 2, size : 1536 12 12
1536
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.25, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10}, 'pool': None, 'activation': None}}
range = 0.4252586358998573
block 3, size : 6144 6 6
6144
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.25, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10}, 'pool': None, 'activation': None}}
range = 0.21262931794992865
NOWWWW
6 6
10
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.25, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 221184, 'old_channels': 6144}, 'pool': None, 'activation': None}}
blocks['b%s' % 4] 221184 221184
range = 0.036828478186799345
train_layer_order: consecutive {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 32, 'nb_epoch': 1, 'print_freq': 25, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 100, 'print_freq': 10, 'lr': 0.001}}
The device used will be: 
True
cuda:0
avg_deltas:  {}
topk_layer:  None
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}

 ----- Architecture Block BatchNorm2dSK3962(5, 5)1.0reflect, number 0 -----
- BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- AvgPool2d(kernel_size=4, stride=2, padding=1)

 ----- Architecture Block BatchNorm2dSK963842(3, 3)0.6499999761581421reflect, number 1 -----
- BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK38415362(3, 3)0.44999998807907104reflect, number 2 -----
- BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.4)
- AvgPool2d(kernel_size=4, stride=2, padding=1)

 ----- Architecture Block BatchNorm2dSK153661442(3, 3)0.25reflect, number 3 -----
- BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 4 -----
- Flatten(start_dim=1, end_dim=-1)
- Dropout(p=0.5, inplace=False)
- Linear(in_features=221184, out_features=10, bias=True)
model.heads:  []
[Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=1.4)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=221184, out_features=10, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)]
LAYER NAME:  Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=1.4)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=221184, out_features=10, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)
LAYER CHILDREN:  [BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
  (activation): Triangle(power=1.4)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1)
), BasicBlock(
  (operations): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
  )
  (layer): Linear(in_features=221184, out_features=10, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)]
subsubl NAME:  Sequential(
  (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=4, stride=2, padding=1)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=4, stride=2, padding=1)
subsubl NAME:  Triangle(power=1.4)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1)
subsubl NAME:  Sequential(
  (0): Flatten(start_dim=1, end_dim=-1)
)
subsubl NAME:  Linear(in_features=221184, out_features=10, bias=True)
subsubl NAME:  Dropout(p=0.5, inplace=False)
CONFIG MODE:  unsupervised

 ********** Hebbian Unsupervised learning of blocks [0, 1, 2, 3] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
INDICES:  5000
IMAGE SIZE: torch.Size([32, 3, 96, 96])
SAVING FOLDER FOR UNSUP:  STL10_CIFAR10_CLe4977f8a-d515-45b7-aaf1-e9dd54f7b531
prev_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
activations_sum.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
DEPTH:  4
WTA IN delta_weight:  tensor([[[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         ...,
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00]],

        [[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         ...,
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00]],

        [[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         ...,
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00]],

        ...,

        [[-9.4754e-16, -9.4754e-16, -9.4754e-16,  ..., -9.4754e-16,
          -9.4754e-16, -9.4754e-16],
         [-9.4754e-16, -9.4754e-16, -9.4754e-16,  ..., -9.4754e-16,
          -9.4754e-16, -9.4754e-16],
         [-9.4754e-16, -9.4754e-16, -9.4754e-16,  ..., -9.4754e-16,
          -9.4754e-16, -9.4754e-16],
         ...,
         [-9.4754e-16, -9.4754e-16, -9.4754e-16,  ..., -9.4754e-16,
          -9.4754e-16, -9.4754e-16],
         [-9.4754e-16, -9.4754e-16, -9.4754e-16,  ..., -9.4754e-16,
          -9.4754e-16, -9.4754e-16],
         [-9.4754e-16, -9.4754e-16, -9.4754e-16,  ..., -9.4754e-16,
          -9.4754e-16, -9.4754e-16]],

        [[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         ...,
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00]],

        [[-5.5351e-43, -5.5351e-43, -5.5351e-43,  ..., -5.5351e-43,
          -5.5351e-43, -5.5351e-43],
         [-5.5351e-43, -5.5351e-43, -5.5351e-43,  ..., -5.5351e-43,
          -5.5351e-43, -5.5351e-43],
         [-5.5351e-43, -5.5351e-43, -5.5351e-43,  ..., -5.5351e-43,
          -5.5351e-43, -5.5351e-43],
         ...,
         [-5.5351e-43, -5.5351e-43, -5.5351e-43,  ..., -5.5351e-43,
          -5.5351e-43, -5.5351e-43],
         [-5.5351e-43, -5.5351e-43, -5.5351e-43,  ..., -5.5351e-43,
          -5.5351e-43, -5.5351e-43],
         [-5.5351e-43, -5.5351e-43, -5.5351e-43,  ..., -5.5351e-43,
          -5.5351e-43, -5.5351e-43]]], device='cuda:0')
self.avg_deltas_layer:  <class 'NoneType'>
self.topk_layer:  <class 'NoneType'>
self.avg_deltas_layer:  <class 'NoneType'>
self.topk_layer:  <class 'NoneType'>
self.avg_deltas_layer:  <class 'NoneType'>
self.topk_layer:  <class 'NoneType'>
self.avg_deltas_layer:  <class 'NoneType'>
self.topk_layer:  <class 'NoneType'>
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
activations_sum[k] 96
activations_sum[k]:  [68, 25, 59, 30, 77, 93, 69, 55, 35, 14]
topk_kernels len:  1
topk_kernels keys:  ['conv0']
topk_kernels:  {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38]}
activations_sum[k] len:  96
activations_sum[k] 384
activations_sum[k]:  [345, 365, 75, 354, 190, 100, 56, 180, 191, 105]
topk_kernels len:  2
topk_kernels keys:  ['conv0', 'conv1']
topk_kernels:  {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286]}
activations_sum[k] len:  384
activations_sum[k] 1536
activations_sum[k]:  [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35]
topk_kernels len:  3
topk_kernels keys:  ['conv0', 'conv1', 'conv2']
topk_kernels:  {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276]}
activations_sum[k] len:  1536
activations_sum[k] 6144
activations_sum[k]:  [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822]
topk_kernels len:  4
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3']
topk_kernels:  {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
activations_sum[k] len:  6144
['conv0', 'conv1', 'conv2', 'conv3']
delta_weights INFO: 
NUM OF TRACKED COV LAYERS:  4
NUM OF TRACKED WEIGHTS CHANGES PER LAYER:  4
avg_deltas INFO:  <class 'dict'>
avg_deltas keys:  ['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight']
################################################
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 4.52e-02	time: 00:00:34	Acc_train 0.00	Acc_test 0.00	convergence: 2.37e+01	R1: 1	Info MB:0.000e+00/SB:0.000e+00/MW:4.460e-06/SW:2.112e-01/MR:2.472e+01/SR:2.331e+00/MeD:1.064e+00/MaD:2.372e+01/MW:0.543/MAW:0.457
|       0 |       1 |        2 |        3 |        4 |        5 |        6 |        7 |        8 |        9 |       10 |       11 |       12 |       13 |       14 |       15 |       16 |       17 |       18 |       19 |       20 |       21 |       22 |       23 |       24 |       25 |       26 |      27 |       28 |       29 |
|---------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------|
|   0.049 |   0.049 |   0.0496 |   0.0495 |   0.0494 |   0.0489 |   0.0501 |   0.0491 |   0.0384 |   0.0488 |   0.0495 |   0.0491 |   0.0489 |   0.0495 |   0.0484 |   0.0497 |   0.0494 |   0.0495 |   0.0492 |   0.0493 |   0.0492 |   0.0463 |   0.0491 |   0.0487 |   0.0484 |   0.0491 |   0.0499 |   0.049 |   0.0495 |   0.0496 |
|  24.97  |  24.96  |  25.6    |  25.52   |  25.43   |  24.9    |  26.1    |  25.07   |  15.72   |  24.85   |  25.52   |  25.09   |  24.91   |  25.5    |  24.4    |  25.67   |  25.36   |  25.47   |  25.17   |  25.29   |  25.17   |  22.48   |  25.11   |  24.76   |  24.47   |  25.11   |  25.91   |  25.05  |  25.49   |  25.62   |
|   0.01  |   0.01  |   0      |   0      |   0.01   |   0.02   |   0.01   |   0      |   0.12   |   0      |   0.01   |   0.01   |   0      |   0      |   0.02   |   0      |   0      |   0      |   0      |   0      |   0      |   0.04   |   0      |   0.01   |   0.01   |   0.01   |   0.02   |   0     |   0.01   |   0      |
| nan     | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      |
| nan     | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      |
| nan     | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:1.522e-04/SW:3.711e-01/MR:2.156e+01/SR:3.332e+00/MeD:2.742e+00/MaD:1.700e+01/MW:0.596/MAW:0.404
|        0 |        1 |        2 |        3 |        4 |        5 |        6 |        7 |        8 |       9 |       10 |       11 |       12 |       13 |       14 |       15 |       16 |       17 |       18 |       19 |      20 |       21 |       22 |       23 |       24 |       25 |       26 |       27 |       28 |       29 |
|----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------|
|   0.0491 |   0.0483 |   0.0451 |   0.0462 |   0.0428 |   0.0428 |   0.0492 |   0.0405 |   0.0453 |   0.049 |   0.0398 |   0.0475 |   0.0434 |   0.0357 |   0.0474 |   0.0448 |   0.0387 |   0.0497 |   0.0469 |   0.0492 |   0.041 |   0.0467 |   0.0404 |   0.0478 |   0.0432 |   0.0429 |   0.0488 |   0.0457 |   0.0397 |   0.0486 |
|  25.08   |  24.36   |  21.36   |  22.34   |  19.33   |  19.35   |  25.22   |  17.37   |  21.51   |  25.05  |  16.81   |  23.55   |  19.85   |  13.75   |  23.48   |  21.08   |  16.01   |  25.68   |  22.97   |  25.2    |  17.82  |  22.81   |  17.32   |  23.87   |  19.69   |  19.39   |  24.77   |  21.84   |  16.78   |  24.63   |
|   0      |   0.01   |   0.06   |   0.06   |   0.06   |   0.1    |   0      |   0.07   |   0.06   |   0     |   0.1    |   0.03   |   0.07   |   0.21   |   0.03   |   0.04   |   0.22   |   0      |   0.02   |   0      |   0.14  |   0.03   |   0.2    |   0.02   |   0.05   |   0.05   |   0      |   0.04   |   0.11   |   0      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:-3.013e-03/SW:7.925e-01/MR:2.315e+01/SR:2.575e+00/MeD:1.984e+00/MaD:1.146e+01/MW:0.695/MAW:0.305
|        0 |        1 |        2 |        3 |        4 |        5 |        6 |        7 |        8 |        9 |       10 |       11 |       12 |       13 |       14 |       15 |       16 |       17 |       18 |       19 |       20 |       21 |       22 |       23 |       24 |       25 |       26 |      27 |      28 |       29 |
|----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+---------+----------|
|   0.0245 |   0.0246 |   0.0242 |   0.0241 |   0.0199 |   0.0235 |   0.0244 |   0.0249 |   0.0244 |   0.0234 |   0.0248 |   0.0239 |   0.0242 |   0.0242 |   0.0242 |   0.0241 |   0.0241 |   0.0219 |   0.0243 |   0.0237 |   0.0231 |   0.0248 |   0.0244 |   0.0247 |   0.0233 |   0.0226 |   0.0213 |   0.024 |   0.024 |   0.0251 |
|  25.05   |  25.12   |  24.39   |  24.27   |  16.82   |  23.03   |  24.87   |  25.72   |  24.84   |  22.85   |  25.62   |  23.77   |  24.33   |  24.47   |  24.36   |  24.18   |  24.22   |  20.12   |  24.69   |  23.54   |  22.28   |  25.7    |  24.78   |  25.37   |  22.75   |  21.47   |  19.17   |  24.11  |  24.04  |  26.12   |
|   0.04   |   0      |   0.02   |   0.01   |   1.6    |   0.19   |   0.02   |   0.05   |   0.01   |   0.2    |   0      |   0.07   |   0.16   |   0.12   |   0.02   |   0.06   |   0      |   0.66   |   0.01   |   0.16   |   0.16   |   0      |   0.12   |   0.11   |   0.28   |   0.49   |   1.15   |   0.03  |   0.07  |   0.02   |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan     | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan     | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan     | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:-1.311e-02/SW:1.508e+00/MR:1.280e+01/SR:2.610e+00/MeD:2.095e+00/MaD:7.090e+00/MW:0.547/MAW:0.453
|       0 |      1 |       2 |      3 |      4 |       5 |       6 |       7 |       8 |       9 |      10 |      11 |      12 |      13 |      14 |      15 |      16 |      17 |      18 |     19 |      20 |     21 |      22 |      23 |      24 |      25 |      26 |      27 |      28 |      29 |
|---------+--------+---------+--------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------|
|   0.256 |   0.33 |   0.295 |   0.31 |   0.24 |   0.219 |   0.277 |   0.329 |   0.245 |   0.348 |   0.254 |   0.272 |   0.289 |   0.305 |   0.269 |   0.309 |   0.203 |   0.276 |   0.275 |   0.26 |   0.303 |   0.27 |   0.287 |   0.324 |   0.305 |   0.236 |   0.303 |   0.292 |   0.276 |   0.328 |
|  11.22  |  17.99 |  14.58  |  16.02 |  10.03 |   8.5   |  12.95  |  17.88  |  10.42  |  19.89  |  11.08  |  12.58  |  14.04  |  15.56  |  12.34  |  15.91  |   7.44  |  12.89  |  12.84  |  11.56 |  15.33  |  12.4  |  13.85  |  17.37  |  15.56  |   9.67  |  15.39  |  14.35  |  12.92  |  17.79  |
|   0.64  |   0.18 |   0.17  |   0.26 |   1.98 |   3.7   |   0.76  |   0.33  |   1.3   |   0.13  |   1.05  |   0.49  |   0.54  |   0.25  |   1.82  |   0.22  |   3.77  |   0.5   |   0.66  |   1.12 |   0.45  |   0.84 |   0.42  |   0.27  |   0.39  |   2.91  |   0.37  |   0.58  |   0.34  |   0.18  |
| nan     | nan    | nan     | nan    | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     |
| nan     | nan    | nan     | nan    | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     |
| nan     | nan    | nan     | nan    | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLe4977f8a-d515-45b7-aaf1-e9dd54f7b531/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
CONFIG MODE:  supervised

 ********** Supervised learning of blocks [4] **********
SAVING FOLDER FOR SUP:  STL10_CIFAR10_CLe4977f8a-d515-45b7-aaf1-e9dd54f7b531
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
INDICES:  5000
IMAGE SIZE: torch.Size([64, 3, 96, 96])
num_blocks:  5
Epoch: [1/100]	lr: 1.00e-03	time: 00:01:01	Loss_train 7.66232	Acc_train 36.62	/	Loss_test 3.46408	Acc_test 46.84
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLe4977f8a-d515-45b7-aaf1-e9dd54f7b531/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [10/100]	lr: 1.00e-03	time: 00:01:40	Loss_train 2.96399	Acc_train 61.89	/	Loss_test 4.18236	Acc_test 58.64
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLe4977f8a-d515-45b7-aaf1-e9dd54f7b531/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [20/100]	lr: 1.00e-03	time: 00:02:23	Loss_train 1.76923	Acc_train 76.53	/	Loss_test 5.84089	Acc_test 55.47
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLe4977f8a-d515-45b7-aaf1-e9dd54f7b531/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [30/100]	lr: 5.00e-04	time: 00:03:05	Loss_train 0.56039	Acc_train 88.94	/	Loss_test 3.81810	Acc_test 62.55
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLe4977f8a-d515-45b7-aaf1-e9dd54f7b531/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [40/100]	lr: 2.50e-04	time: 00:03:48	Loss_train 0.33715	Acc_train 92.10	/	Loss_test 3.39348	Acc_test 64.84
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLe4977f8a-d515-45b7-aaf1-e9dd54f7b531/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [50/100]	lr: 2.50e-04	time: 00:04:31	Loss_train 0.18524	Acc_train 94.79	/	Loss_test 3.65853	Acc_test 64.54
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLe4977f8a-d515-45b7-aaf1-e9dd54f7b531/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [60/100]	lr: 1.25e-04	time: 00:05:13	Loss_train 0.10238	Acc_train 96.51	/	Loss_test 3.27746	Acc_test 65.28
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLe4977f8a-d515-45b7-aaf1-e9dd54f7b531/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [70/100]	lr: 6.25e-05	time: 00:05:56	Loss_train 0.06835	Acc_train 97.37	/	Loss_test 3.15753	Acc_test 66.12
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLe4977f8a-d515-45b7-aaf1-e9dd54f7b531/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [80/100]	lr: 3.13e-05	time: 00:06:38	Loss_train 0.05019	Acc_train 97.93	/	Loss_test 3.11265	Acc_test 66.12
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLe4977f8a-d515-45b7-aaf1-e9dd54f7b531/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [90/100]	lr: 1.56e-05	time: 00:07:21	Loss_train 0.04391	Acc_train 98.04	/	Loss_test 3.07585	Acc_test 66.49
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLe4977f8a-d515-45b7-aaf1-e9dd54f7b531/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [100/100]	lr: 7.81e-06	time: 00:08:04	Loss_train 0.03978	Acc_train 98.28	/	Loss_test 3.06802	Acc_test 66.56
new_head:  {'blocks.4.layer.bias': tensor([ 0.0016, -0.0015,  0.0004, -0.0005,  0.0020,  0.0031,  0.0054, -0.0016,
         0.0026,  0.0032], device='cuda:0'), 'blocks.4.layer.weight': tensor([[ 0.0400,  0.0110,  0.0159,  ..., -0.0202, -0.0415, -0.0304],
        [ 0.0107, -0.0007,  0.0315,  ..., -0.0158,  0.0113,  0.0158],
        [ 0.0364, -0.0160,  0.0407,  ...,  0.0141,  0.0031,  0.0035],
        ...,
        [ 0.0219,  0.0111,  0.0037,  ...,  0.0494,  0.0133, -0.0125],
        [-0.0186,  0.0274, -0.0178,  ..., -0.0034, -0.0105,  0.0038],
        [-0.0212, -0.0315, -0.0387,  ..., -0.0497,  0.0010, -0.0065]],
       device='cuda:0')}
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLe4977f8a-d515-45b7-aaf1-e9dd54f7b531/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
RESULT:  {'train_loss': 0.03978398069739342, 'train_acc': 98.27600121498108, 'test_loss': 3.0680172443389893, 'test_acc': 66.5625, 'convergence': 23.72207260131836, 'R1': 1, 'dataset_sup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 5000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 4, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': True, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 100, 'print_freq': 10, 'validation': False, 'continual_learning': False}, 'dataset_unsup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 100000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 32, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'unlabeled', 'nb_epoch': 1, 'print_freq': 25, 'validation': False, 'continual_learning': False}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 32, 'nb_epoch': 1, 'print_freq': 25, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 100, 'print_freq': 10, 'lr': 0.001}}}
IN R0:  {'count': 0, 'R0': {'train_loss': 0.03978398069739342, 'train_acc': 98.27600121498108, 'test_loss': 3.0680172443389893, 'test_acc': 66.5625, 'convergence': 23.72207260131836, 'R1': 1, 'dataset_sup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 5000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 4, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': True, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 100, 'print_freq': 10, 'validation': False, 'continual_learning': False}, 'dataset_unsup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 100000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 32, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'unlabeled', 'nb_epoch': 1, 'print_freq': 25, 'validation': False, 'continual_learning': False}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 32, 'nb_epoch': 1, 'print_freq': 25, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 100, 'print_freq': 10, 'lr': 0.001}}}}
CL:  True
{'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True}
CL:  True
{'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': True}
SEED:  0
block 0, size : 96 16 16
96
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.036828478186799345, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 221184, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 2.886751345948129
block 1, size : 384 8 8
384
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.036828478186799345, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 221184, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.8505172717997146
block 2, size : 1536 4 4
1536
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.036828478186799345, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 221184, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.4252586358998573
block 3, size : 6144 2 2
6144
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.036828478186799345, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 221184, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.21262931794992865
NOWWWW
2 2
10
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.036828478186799345, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 24576, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
blocks['b%s' % 4] 24576 24576
range = 0.11048543456039805
train_layer_order: consecutive {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 10, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}
The device used will be: 
True
cuda:0
PARAMSSSS IN LOAD:  {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.036828478186799345, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 221184, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
avg_deltas:  {'blocks.0.layer.weight': tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')}
topk_layer:  None
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.0.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')}
topk_layer inside model after retrieval:  [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.1.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')}
topk_layer inside model after retrieval:  [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.2.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')}
topk_layer inside model after retrieval:  [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.3.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')}
topk_layer inside model after retrieval:  [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}

 Model STL10_CIFAR10_CLe4977f8a-d515-45b7-aaf1-e9dd54f7b531 loaded successfuly with best perf



 ----- Architecture Block BatchNorm2dSK3962(5, 5)1.0reflect, number 0 -----
- BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- AvgPool2d(kernel_size=4, stride=2, padding=1)

 ----- Architecture Block BatchNorm2dSK963842(3, 3)0.6499999761581421reflect, number 1 -----
- BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK38415362(3, 3)0.44999998807907104reflect, number 2 -----
- BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.4)
- AvgPool2d(kernel_size=4, stride=2, padding=1)

 ----- Architecture Block BatchNorm2dSK153661442(3, 3)0.25reflect, number 3 -----
- BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 4 -----
- Flatten(start_dim=1, end_dim=-1)
- Dropout(p=0.5, inplace=False)
- Linear(in_features=221184, out_features=10, bias=True)
model.heads:  [{'blocks.4.layer.bias': tensor([ 0.0016, -0.0015,  0.0004, -0.0005,  0.0020,  0.0031,  0.0054, -0.0016,
         0.0026,  0.0032], device='cuda:0'), 'blocks.4.layer.weight': tensor([[ 0.0400,  0.0110,  0.0159,  ..., -0.0202, -0.0415, -0.0304],
        [ 0.0107, -0.0007,  0.0315,  ..., -0.0158,  0.0113,  0.0158],
        [ 0.0364, -0.0160,  0.0407,  ...,  0.0141,  0.0031,  0.0035],
        ...,
        [ 0.0219,  0.0111,  0.0037,  ...,  0.0494,  0.0133, -0.0125],
        [-0.0186,  0.0274, -0.0178,  ..., -0.0034, -0.0105,  0.0038],
        [-0.0212, -0.0315, -0.0387,  ..., -0.0497,  0.0010, -0.0065]],
       device='cuda:0')}]
[Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=1.4)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=221184, out_features=10, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)]
LAYER NAME:  Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=1.4)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=221184, out_features=10, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)
LAYER CHILDREN:  [BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
  (activation): Triangle(power=1.4)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1)
), BasicBlock(
  (operations): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
  )
  (layer): Linear(in_features=221184, out_features=10, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)]
subsubl NAME:  Sequential(
  (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=4, stride=2, padding=1)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=4, stride=2, padding=1)
subsubl NAME:  Triangle(power=1.4)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1)
subsubl NAME:  Sequential(
  (0): Flatten(start_dim=1, end_dim=-1)
)
subsubl NAME:  Linear(in_features=221184, out_features=10, bias=True)
subsubl NAME:  Dropout(p=0.5, inplace=False)
CONFIG MODE:  unsupervised

 ********** Hebbian Unsupervised learning of blocks [0, 1, 2, 3] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
INDICES:  50000
IMAGE SIZE: torch.Size([10, 3, 96, 96])
SAVING FOLDER FOR UNSUP:  STL10_CIFAR10_CLe4977f8a-d515-45b7-aaf1-e9dd54f7b531
prev_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
activations_sum.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
DEPTH:  4
ACTIVATIONS SHAPE:  tensor([[ 7.0837e+00, -1.7342e+02, -5.2790e+01,  ..., -3.0243e+02,
         -3.1626e+02, -8.0347e+01],
        [-8.1204e+01, -2.1264e+02, -1.1070e+02,  ..., -3.5674e+02,
         -3.7442e+02, -1.3079e+02],
        [-1.0510e+01, -2.5627e+02, -1.4349e+02,  ..., -2.6999e+02,
         -3.1838e+02, -2.5646e+01],
        ...,
        [ 2.0937e+02,  2.8768e-01,  5.9503e+01,  ..., -6.2056e+01,
         -1.4965e+02,  1.2563e+02],
        [ 4.0981e+02,  2.2179e+02,  2.8451e+02,  ...,  1.7146e+02,
          3.1691e+01,  3.1062e+02],
        [ 1.0649e+02, -1.2042e+02, -1.1781e+02,  ..., -1.8817e+02,
         -2.5834e+02,  1.3789e+00]], device='cuda:0')
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.topk_layer:  <class 'list'>
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.topk_layer:  <class 'list'>
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.topk_layer:  <class 'list'>
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.topk_layer:  <class 'list'>
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
activations_sum[k] 96
activations_sum[k]:  [25, 16, 59, 53, 30, 42, 77, 68, 69, 93]
topk_kernels len:  4
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3']
topk_kernels:  {'conv0': [25, 16, 59, 53, 30, 42, 77, 68, 69, 93, 75, 51, 34, 38, 70], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
activations_sum[k] len:  96
activations_sum[k] 384
activations_sum[k]:  [23, 75, 345, 61, 349, 92, 330, 197, 100, 33]
topk_kernels len:  4
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3']
topk_kernels:  {'conv0': [25, 16, 59, 53, 30, 42, 77, 68, 69, 93, 75, 51, 34, 38, 70], 'conv1': [23, 75, 345, 61, 349, 92, 330, 197, 100, 33, 69, 190, 296, 41, 121, 157, 76, 166, 247, 133, 39, 336, 266, 362, 184, 17, 63, 158, 50, 116, 344, 354, 201, 98, 292, 156, 130, 335, 291, 365, 332, 366, 146, 382, 186, 287, 159, 104, 217, 139, 219, 28, 99, 260, 180, 264, 227, 273, 379], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
activations_sum[k] len:  384
activations_sum[k] 1536
activations_sum[k]:  [1006, 307, 576, 276, 733, 972, 632, 1411, 320, 301]
topk_kernels len:  4
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3']
topk_kernels:  {'conv0': [25, 16, 59, 53, 30, 42, 77, 68, 69, 93, 75, 51, 34, 38, 70], 'conv1': [23, 75, 345, 61, 349, 92, 330, 197, 100, 33, 69, 190, 296, 41, 121, 157, 76, 166, 247, 133, 39, 336, 266, 362, 184, 17, 63, 158, 50, 116, 344, 354, 201, 98, 292, 156, 130, 335, 291, 365, 332, 366, 146, 382, 186, 287, 159, 104, 217, 139, 219, 28, 99, 260, 180, 264, 227, 273, 379], 'conv2': [1006, 307, 576, 276, 733, 972, 632, 1411, 320, 301, 350, 1284, 915, 1211, 49, 1502, 503, 1385, 272, 1265, 549, 185, 904, 734, 1413, 326, 984, 468, 764, 186, 551, 122, 969, 347, 1050, 1293, 89, 994, 1185, 697, 261, 1500, 1090, 1532, 930, 1047, 879, 1427, 1290, 519, 973, 167, 1030, 1154, 577, 1086, 786, 1453, 334, 1033, 188, 813, 1400, 294, 241, 796, 627, 1042, 1176, 619, 868, 445, 482, 135, 427, 238, 573, 70, 1239, 953, 726, 138, 1271, 985, 240, 268, 710, 1113, 1326, 1394, 823, 938, 601, 195, 205, 988, 1475, 1184, 1371, 1085, 747, 760, 1465, 617, 377, 1494, 73, 289, 737, 1409, 952, 863, 992, 660, 222, 557, 252, 43, 926, 801, 998, 159, 1493, 1251, 1263, 964, 1029, 844, 539, 1091, 1167, 1066, 177, 108, 1458, 477, 923, 766, 458, 1000, 676, 650, 310, 869, 705, 1129, 13, 269, 260, 75, 1428, 1060, 1195, 571, 562, 1214, 430, 659, 851, 854, 1344, 535, 1213, 777, 369, 187, 1242, 775, 1529, 802, 1225, 97, 779, 126, 640, 833, 174, 595, 158, 194, 1068, 946, 1503, 995, 592, 170, 812, 471, 1307, 486, 1462, 1365, 914, 373, 610, 417, 1327, 128, 1230, 133, 346, 1238, 1253, 910, 44, 1076, 950, 423, 90, 1430, 1324, 348, 1150, 1531, 193, 1412, 171, 1084, 1021, 1032, 1522, 1299, 405, 1471, 958, 1122, 580, 509, 446, 408, 715], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
activations_sum[k] len:  1536
activations_sum[k] 6144
activations_sum[k]:  [5017, 6047, 1653, 3916, 170, 5041, 2613, 4323, 6098, 2691]
topk_kernels len:  4
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3']
topk_kernels:  {'conv0': [25, 16, 59, 53, 30, 42, 77, 68, 69, 93, 75, 51, 34, 38, 70], 'conv1': [23, 75, 345, 61, 349, 92, 330, 197, 100, 33, 69, 190, 296, 41, 121, 157, 76, 166, 247, 133, 39, 336, 266, 362, 184, 17, 63, 158, 50, 116, 344, 354, 201, 98, 292, 156, 130, 335, 291, 365, 332, 366, 146, 382, 186, 287, 159, 104, 217, 139, 219, 28, 99, 260, 180, 264, 227, 273, 379], 'conv2': [1006, 307, 576, 276, 733, 972, 632, 1411, 320, 301, 350, 1284, 915, 1211, 49, 1502, 503, 1385, 272, 1265, 549, 185, 904, 734, 1413, 326, 984, 468, 764, 186, 551, 122, 969, 347, 1050, 1293, 89, 994, 1185, 697, 261, 1500, 1090, 1532, 930, 1047, 879, 1427, 1290, 519, 973, 167, 1030, 1154, 577, 1086, 786, 1453, 334, 1033, 188, 813, 1400, 294, 241, 796, 627, 1042, 1176, 619, 868, 445, 482, 135, 427, 238, 573, 70, 1239, 953, 726, 138, 1271, 985, 240, 268, 710, 1113, 1326, 1394, 823, 938, 601, 195, 205, 988, 1475, 1184, 1371, 1085, 747, 760, 1465, 617, 377, 1494, 73, 289, 737, 1409, 952, 863, 992, 660, 222, 557, 252, 43, 926, 801, 998, 159, 1493, 1251, 1263, 964, 1029, 844, 539, 1091, 1167, 1066, 177, 108, 1458, 477, 923, 766, 458, 1000, 676, 650, 310, 869, 705, 1129, 13, 269, 260, 75, 1428, 1060, 1195, 571, 562, 1214, 430, 659, 851, 854, 1344, 535, 1213, 777, 369, 187, 1242, 775, 1529, 802, 1225, 97, 779, 126, 640, 833, 174, 595, 158, 194, 1068, 946, 1503, 995, 592, 170, 812, 471, 1307, 486, 1462, 1365, 914, 373, 610, 417, 1327, 128, 1230, 133, 346, 1238, 1253, 910, 44, 1076, 950, 423, 90, 1430, 1324, 348, 1150, 1531, 193, 1412, 171, 1084, 1021, 1032, 1522, 1299, 405, 1471, 958, 1122, 580, 509, 446, 408, 715], 'conv3': [5017, 6047, 1653, 3916, 170, 5041, 2613, 4323, 6098, 2691, 5433, 3067, 4958, 4900, 3563, 1730, 62, 2026, 3435, 2075, 2778, 5255, 4087, 4574, 4983, 634, 5795, 1366, 948, 4473, 4359, 3906, 547, 750, 1229, 5651, 4738, 3197, 5004, 785, 514, 1599, 4502, 2051, 5968, 3718, 1680, 5558, 3938, 353, 5984, 5349, 4768, 815, 5956, 1310, 3516, 4415, 2483, 5292, 2021, 4333, 5515, 4090, 3988, 1149, 2470, 1856, 2392, 549, 4350, 1022, 1921, 999, 4682, 3986, 2462, 921, 3227, 265, 706, 1055, 5541, 4429, 680, 3972, 3828, 1635, 5175, 1587, 1685, 2966, 2696, 1318, 4262, 2037, 4163, 4746, 4699, 2299, 3481, 2398, 3740, 4029, 2413, 5314, 3181, 2804, 837, 3511, 5918, 1575, 3807, 1983, 1778, 63, 3486, 51, 3669, 4924, 1526, 817, 3179, 5498, 6, 1385, 1066, 534, 5733, 3210, 1086, 4357, 2176, 1331, 1670, 4408, 117, 2344, 1181, 3222, 1911, 2427, 2979, 1304, 5752, 2453, 5283, 5188, 745, 5591, 4968, 1344, 3603, 261, 1585, 5, 2786, 5406, 8, 300, 2300, 4905, 2714, 3431, 2661, 5571, 4360, 4556, 3750, 771, 211, 3888, 4486, 813, 2049, 4025, 6114, 1294, 845, 3414, 5312, 5001, 2404, 2769, 4094, 4624, 5065, 4496, 455, 3090, 6070, 3101, 1760, 1037, 1913, 1831, 1036, 1117, 3449, 5919, 418, 5455, 5299, 485, 905, 4733, 5091, 3959, 4739, 183, 2078, 1009, 1043, 4134, 2085, 5717, 3543, 3430, 552, 3830, 1512, 1822, 2914, 5705, 822, 3983, 5860, 688, 4687, 6134, 2201, 5222, 1342, 784, 1112, 1198, 2938, 2041, 4862, 3321, 5579, 5931, 526, 2608, 1872, 1163, 2780, 4218, 4256, 872, 122, 4396, 551, 1437, 5473, 4513, 2584, 1883, 3495, 2136, 5477, 5977, 3159, 1703, 1118, 4538, 1346, 2698, 4397, 3941, 2047, 5531, 2488, 3581, 132, 339, 3502, 2850, 1792, 2418, 5346, 2354, 6054, 3001, 4227, 1790, 4598, 2056, 4100, 3725, 909, 3909, 4451, 4072, 4660, 2635, 1102, 3656, 2581, 176, 1001, 2152, 2444, 3448, 685, 497, 2920, 652, 621, 4390, 3186, 4221, 1097, 2069, 4177, 1620, 4118, 4377, 178, 1615, 1950, 3010, 2092, 2793, 4694, 2103, 5834, 6004, 3905, 632, 844, 4618, 4131, 4063, 1403, 583, 1351, 5469, 1753, 1514, 4602, 4173, 2835, 2750, 3215, 2274, 5457, 3507, 5482, 5981, 3062, 5135, 2847, 2010, 4223, 2662, 3824, 2447, 5847, 165, 5276, 2210, 5493, 129, 198, 5780, 4853, 1544, 2511, 1806, 5910, 4382, 956, 2426, 1443, 1622, 4906, 5146, 4159, 5214, 668, 4876, 1284, 194, 227, 2980, 1187, 6090, 49, 4036, 436, 3470, 6058, 3488, 1594, 5322, 1283, 1864, 4539, 860, 3420, 3138, 818, 1861, 257, 2700, 2139, 2695, 3351, 2379, 4544, 1176, 2342, 2128, 1343, 2514, 4394, 686, 5212, 3895, 3540, 4110, 109, 3312, 893, 5542, 6053, 3473, 4301, 5196, 4937, 1337, 5407, 4219, 2972, 208, 2812, 4266, 4646, 5027, 4938, 3719, 1075, 295, 5436, 78, 5243, 4804, 638, 1564, 1894, 4980, 5929, 5853, 3640, 4964, 5821, 5297, 5201, 398, 5909, 59, 2542, 495, 6119, 4097, 2292, 1560, 2666, 2627, 1873, 3912, 4636, 3436, 26, 1453, 1394, 4947, 5216, 4089, 4166, 1415, 4954, 2083, 1834, 4212, 3809, 4635, 4102, 3866, 2533, 5838, 5412, 221, 6116, 2367, 3944, 1988, 1132, 1380, 1454, 4264, 1499, 5786, 1316, 4306, 3584, 171, 2675, 2667, 5205, 2530, 5108, 5045, 3793, 2703, 1684, 4421, 1191, 294, 2008, 41, 3760, 4280, 2923, 13, 4651, 1309, 1185, 1435, 6122, 2945, 2755, 4114, 1371, 2808, 4319, 2679, 5520, 4897, 6142, 2089, 2563, 5630, 292, 3151, 2668, 4553, 2927, 2156, 5076, 3193, 313, 3670, 3696, 709, 252, 2779, 1108, 1312, 2236, 2098, 2058, 772, 4257, 3330, 5791, 2957, 3569, 65, 5056, 3602, 5739, 1641, 5318, 722, 5445, 3755, 24, 5396, 675, 760, 3873, 673, 926, 3017, 1168, 5354, 4546, 2314, 5607, 259, 3658, 1608, 2007, 4289, 4777, 888, 3434, 4736, 4445, 4107, 1164, 2173, 4431, 576, 3802, 4120, 1879, 3191, 4220, 4076, 4961, 5839, 3150, 5206, 3377, 3096, 255, 5087, 2048, 2393, 1956, 3039, 1162, 4603, 5161, 5268, 491, 2580, 5423, 795, 2597, 463, 5043, 4236, 4291, 1230, 1882, 1978, 1511, 1866, 4728, 5942, 1362, 4879, 2348, 2845, 796, 5582, 4919, 2834, 4402, 6109, 1634, 3735, 6008, 3914, 1960, 5172, 625, 4270, 3556, 4541, 2976, 6089, 2860, 2962, 2082, 3479, 5596, 4688, 5321, 4762, 2711, 3775, 3553, 2251, 4885, 2887, 3657, 1096, 5401, 4640, 5921, 5204, 2033, 2206, 2802, 4412, 2928, 441, 1578, 951, 287, 1357, 3761, 4176, 2101, 14, 4299, 2310, 5936, 2073, 2538, 2077, 4628, 4104, 870, 2318, 4447, 2227, 5932, 2736, 5386, 4162, 2918, 5164, 2859, 3804, 1161, 6102, 4881, 246, 5503, 707, 4840, 2917, 1941, 1836, 2607, 763, 5454, 2759, 1115, 530, 1936, 426, 1755, 980, 620, 846, 1949, 1537, 1846, 865, 223, 3686, 4785, 5697, 1021, 4028, 1887, 2100, 2268, 4449, 3133, 3538, 5626, 826, 5008, 1569, 1457, 780, 1194, 2889, 5103, 229, 2460, 2798, 3918, 5680, 89, 493, 5818, 4658, 3594, 2445, 1151, 1535, 1273, 1169, 1422, 1264, 4239, 1543, 834, 5015, 2203, 3425, 4365, 6125, 3170, 1368, 4752, 1820, 3834, 2680, 2112, 4778, 3965, 5116, 1028, 3027, 2715, 4300, 2443, 1574, 2519, 3947, 1649, 144, 1088, 5854, 2792, 5885, 2382, 3163, 1912, 765, 3023, 4013, 2022, 527, 5980, 4482, 3636, 3283, 2067, 2106, 2071, 3242, 1999, 2449, 5893, 1376, 3025, 966, 1204, 5245, 1470, 4185, 4410, 5693, 4614, 3943, 4034, 3384, 4460, 5199, 3759, 1116, 3902, 2861, 3836, 2585, 179, 1697, 5994, 1702, 2817, 4519, 4330, 4480, 4882, 555, 5081, 1279, 3429, 545, 4154, 590, 4135, 5887, 970, 363, 3032, 2419, 4956, 5642, 1345, 1845, 1438, 4775, 6096, 878, 1600, 425, 1335, 1914, 2016, 5523, 3277, 4051, 6021, 1980, 5862, 4566, 450, 3350, 175, 1665, 1969, 524, 1095, 4363, 3118, 3097, 4669, 5460, 3126, 4242, 5702, 5552, 52, 3733, 4142, 2547, 1839, 271, 674, 511, 4248]}
activations_sum[k] len:  6144
['conv0', 'conv1', 'conv2', 'conv3']
delta_weights INFO: 
NUM OF TRACKED COV LAYERS:  4
NUM OF TRACKED WEIGHTS CHANGES PER LAYER:  100
avg_deltas INFO:  <class 'dict'>
avg_deltas keys:  ['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight']
################################################
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 2.66e-02	time: 00:04:25	Acc_train 0.00	Acc_test 0.00	convergence: 1.76e+01	R1: 49	Info MB:0.000e+00/SB:0.000e+00/MW:1.723e-03/SW:1.630e-01/MR:1.855e+01/SR:4.787e+00/MeD:3.463e+00/MaD:1.755e+01/MW:0.489/MAW:0.511
|       0 |        1 |        2 |        3 |        4 |        5 |        6 |        7 |        8 |        9 |       10 |       11 |       12 |       13 |       14 |       15 |      16 |       17 |       18 |       19 |       20 |       21 |       22 |       23 |       24 |       25 |       26 |      27 |       28 |       29 |
|---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------|
|   0.044 |   0.0458 |   0.0412 |   0.0401 |   0.0392 |   0.0396 |   0.0377 |   0.0449 |   0.0224 |   0.0471 |   0.0472 |   0.0472 |   0.0442 |   0.0435 |   0.0384 |   0.0505 |   0.027 |   0.0395 |   0.0466 |   0.0457 |   0.0368 |   0.0477 |   0.0395 |   0.0433 |   0.0446 |   0.0428 |   0.0464 |   0.043 |   0.0473 |   0.0483 |
|  20.35  |  22.02   |  17.97   |  17.04   |  16.36   |  16.65   |  15.22   |  21.14   |   6      |  23.16   |  23.25   |  23.28   |  20.55   |  19.93   |  15.72   |  26.55   |   8.28  |  16.57   |  22.68   |  21.92   |  14.51   |  23.73   |  16.59   |  19.77   |  20.9    |  19.35   |  22.54   |  19.45  |  23.42   |  24.28   |
|   0.01  |   0.02   |   0.03   |   0.02   |   0.01   |   0.01   |   0.02   |   0.01   |   0.01   |   0.01   |   0.01   |   0.01   |   0      |   0.02   |   0.01   |   0.01   |   0.01  |   0.02   |   0.01   |   0.01   |   0.01   |   0.01   |   0.02   |   0.01   |   0.01   |   0.01   |   0.02   |   0     |   0.02   |   0.01   |
| nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      |
| nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      |
| nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:2.754e-03/SW:1.704e-01/MR:9.165e+00/SR:4.044e+00/MeD:3.238e+00/MaD:8.165e+00/MW:0.585/MAW:0.415
|         0 |        1 |        2 |        3 |        4 |        5 |        6 |        7 |       8 |        9 |     10 |       11 |       12 |       13 |       14 |       15 |       16 |       17 |       18 |      19 |       20 |       21 |       22 |       23 |       24 |       25 |        26 |       27 |       28 |       29 |
|-----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+--------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------+----------+----------+-----------+----------+----------+----------|
|   0.00934 |   0.0316 |   0.0244 |   0.0312 |   0.0327 |   0.0314 |   0.0331 |   0.0298 |   0.014 |   0.0286 |   0.02 |   0.0256 |   0.0365 |   0.0311 |   0.0232 |   0.0366 |   0.0341 |   0.0356 |   0.0111 |   0.036 |   0.0312 |   0.0336 |   0.0318 |   0.0248 |   0.0359 |   0.0331 |   0.00699 |   0.0264 |   0.0274 |   0.0299 |
|   1.87    |  10.99   |   6.93   |  10.74   |  11.71   |  10.83   |  11.95   |   9.91   |   2.96  |   9.21   |   5.01 |   7.57   |  14.3    |  10.68   |   6.39   |  14.37   |  12.63   |  13.69   |   2.23   |  13.99  |  10.75   |  12.28   |  11.13   |   7.16   |  13.92   |  11.94   |   1.49    |   7.95   |   8.49   |   9.95   |
|   0.04    |   0.01   |   0.03   |   0.05   |   0.03   |   0.03   |   0.04   |   0.04   |   0.03  |   0.02   |   0.03 |   0.03   |   0.04   |   0.03   |   0.06   |   0.03   |   0.06   |   0.04   |   0.04   |   0.03  |   0.05   |   0.04   |   0.05   |   0.03   |   0.05   |   0.02   |   0.02    |   0.03   |   0.02   |   0.03   |
| nan       | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan    | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan       | nan      | nan      | nan      |
| nan       | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan    | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan       | nan      | nan      | nan      |
| nan       | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan    | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan       | nan      | nan      | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:1.597e-02/SW:1.467e-01/MR:3.808e+00/SR:2.081e+00/MeD:1.769e+00/MaD:6.026e+00/MW:0.584/MAW:0.416
|        0 |        1 |        2 |         3 |        4 |         5 |         6 |        7 |         8 |         9 |        10 |        11 |        12 |       13 |       14 |        15 |        16 |        17 |       18 |        19 |        20 |         21 |        22 |        23 |        24 |       25 |        26 |        27 |        28 |        29 |
|----------+----------+----------+-----------+----------+-----------+-----------+----------+-----------+-----------+-----------+-----------+-----------+----------+----------+-----------+-----------+-----------+----------+-----------+-----------+------------+-----------+-----------+-----------+----------+-----------+-----------+-----------+-----------|
|   0.0112 |   0.0107 |   0.0103 |   0.00923 |   0.0017 |   0.00783 |   0.00536 |   0.0066 |   0.00818 |   0.00725 |   0.00868 |   0.00942 |   0.00104 |   0.0051 |   0.0115 |   0.00982 |   0.00971 |   0.00598 |   0.0026 |   0.00195 |   0.00714 |   0.000184 |   0.00546 |   0.00658 |   0.00156 |   0.0111 |   0.00937 |   0.00858 |   0.00481 |   0.00481 |
|   6.04   |   5.6    |   5.25   |   4.41    |   1.12   |   3.45    |   2.15    |   2.74   |   3.68    |   3.1     |   4.01    |   4.55    |   1.04    |   2.04   |   6.25   |   4.86    |   4.77    |   2.43    |   1.27   |   1.15    |   3.04    |   1        |   2.19    |   2.73    |   1.1     |   5.94   |   4.51    |   3.94    |   1.92    |   1.92    |
|   0.19   |   0.21   |   0.18   |   0.15    |   0.49   |   0.11    |   0.15    |   0.13   |   0.16    |   0.17    |   0.09    |   0.21    |   0.36    |   0.26   |   0.2    |   0.16    |   0.23    |   0.29    |   0.15   |   0.19    |   0.16    |   1.83     |   0.39    |   0.31    |   0.31    |   0.32   |   0.2     |   0.11    |   0.2     |   0.24    |
| nan      | nan      | nan      | nan       | nan      | nan       | nan       | nan      | nan       | nan       | nan       | nan       | nan       | nan      | nan      | nan       | nan       | nan       | nan      | nan       | nan       | nan        | nan       | nan       | nan       | nan      | nan       | nan       | nan       | nan       |
| nan      | nan      | nan      | nan       | nan      | nan       | nan       | nan      | nan       | nan       | nan       | nan       | nan       | nan      | nan      | nan       | nan       | nan       | nan      | nan       | nan       | nan        | nan       | nan       | nan       | nan      | nan       | nan       | nan       | nan       |
| nan      | nan      | nan      | nan       | nan      | nan       | nan       | nan      | nan       | nan       | nan       | nan       | nan       | nan      | nan      | nan       | nan       | nan       | nan      | nan       | nan       | nan        | nan       | nan       | nan       | nan      | nan       | nan       | nan       | nan       |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:1.179e-02/SW:2.138e+00/MR:1.694e+01/SR:7.510e+00/MeD:6.027e+00/MaD:1.730e+01/MW:0.598/MAW:0.402
|      0 |       1 |       2 |       3 |       4 |       5 |       6 |       7 |       8 |      9 |        10 |      11 |      12 |      13 |      14 |      15 |        16 |      17 |      18 |      19 |      20 |      21 |      22 |      23 |      24 |        25 |      26 |     27 |      28 |      29 |
|--------+---------+---------+---------+---------+---------+---------+---------+---------+--------+-----------+---------+---------+---------+---------+---------+-----------+---------+---------+---------+---------+---------+---------+---------+---------+-----------+---------+--------+---------+---------|
|   0.31 |   0.404 |   0.381 |   0.356 |   0.319 |   0.323 |   0.326 |   0.461 |   0.314 |   0.39 |   0.00415 |   0.331 |   0.371 |   0.353 |   0.314 |   0.409 |   0.00115 |   0.311 |   0.363 |   0.373 |   0.375 |   0.332 |   0.331 |   0.423 |   0.406 |   0.00117 |   0.367 |   0.2  |   0.253 |   0.381 |
|  16.04 |  26.44  |  23.64  |  20.78  |  16.89  |  17.34  |  17.66  |  34.24  |  16.39  |  24.77 |   1       |  18.11  |  22.52  |  20.48  |  16.44  |  27.1   |   1       |  16.08  |  21.58  |  22.78  |  23.03  |  18.19  |  18.13  |  28.99  |  26.74  |   1       |  22.04  |   7.28 |  11     |  23.66  |
|   0.48 |   0.6   |   0.4   |   0.53  |   0.77  |   0.92  |   0.56  |   0.74  |   0.64  |   0.28 |   1.32    |   0.6   |   0.58  |   0.45  |   0.73  |   0.44  |  14.01    |   0.47  |   0.41  |   0.92  |   0.61  |   0.68  |   0.45  |   0.51  |   0.43  |  17.09    |   0.55  |   0.94 |   0.45  |   0.43  |
| nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan       | nan     | nan     | nan     | nan     | nan     | nan       | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan       | nan     | nan    | nan     | nan     |
| nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan       | nan     | nan     | nan     | nan     | nan     | nan       | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan       | nan     | nan    | nan     | nan     |
| nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan       | nan     | nan     | nan     | nan     | nan     | nan       | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan       | nan     | nan    | nan     | nan     |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLe4977f8a-d515-45b7-aaf1-e9dd54f7b531/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [25, 16, 59, 53, 30, 42, 77, 68, 69, 93, 75, 51, 34, 38, 70], 'conv1': [23, 75, 345, 61, 349, 92, 330, 197, 100, 33, 69, 190, 296, 41, 121, 157, 76, 166, 247, 133, 39, 336, 266, 362, 184, 17, 63, 158, 50, 116, 344, 354, 201, 98, 292, 156, 130, 335, 291, 365, 332, 366, 146, 382, 186, 287, 159, 104, 217, 139, 219, 28, 99, 260, 180, 264, 227, 273, 379], 'conv2': [1006, 307, 576, 276, 733, 972, 632, 1411, 320, 301, 350, 1284, 915, 1211, 49, 1502, 503, 1385, 272, 1265, 549, 185, 904, 734, 1413, 326, 984, 468, 764, 186, 551, 122, 969, 347, 1050, 1293, 89, 994, 1185, 697, 261, 1500, 1090, 1532, 930, 1047, 879, 1427, 1290, 519, 973, 167, 1030, 1154, 577, 1086, 786, 1453, 334, 1033, 188, 813, 1400, 294, 241, 796, 627, 1042, 1176, 619, 868, 445, 482, 135, 427, 238, 573, 70, 1239, 953, 726, 138, 1271, 985, 240, 268, 710, 1113, 1326, 1394, 823, 938, 601, 195, 205, 988, 1475, 1184, 1371, 1085, 747, 760, 1465, 617, 377, 1494, 73, 289, 737, 1409, 952, 863, 992, 660, 222, 557, 252, 43, 926, 801, 998, 159, 1493, 1251, 1263, 964, 1029, 844, 539, 1091, 1167, 1066, 177, 108, 1458, 477, 923, 766, 458, 1000, 676, 650, 310, 869, 705, 1129, 13, 269, 260, 75, 1428, 1060, 1195, 571, 562, 1214, 430, 659, 851, 854, 1344, 535, 1213, 777, 369, 187, 1242, 775, 1529, 802, 1225, 97, 779, 126, 640, 833, 174, 595, 158, 194, 1068, 946, 1503, 995, 592, 170, 812, 471, 1307, 486, 1462, 1365, 914, 373, 610, 417, 1327, 128, 1230, 133, 346, 1238, 1253, 910, 44, 1076, 950, 423, 90, 1430, 1324, 348, 1150, 1531, 193, 1412, 171, 1084, 1021, 1032, 1522, 1299, 405, 1471, 958, 1122, 580, 509, 446, 408, 715], 'conv3': [5017, 6047, 1653, 3916, 170, 5041, 2613, 4323, 6098, 2691, 5433, 3067, 4958, 4900, 3563, 1730, 62, 2026, 3435, 2075, 2778, 5255, 4087, 4574, 4983, 634, 5795, 1366, 948, 4473, 4359, 3906, 547, 750, 1229, 5651, 4738, 3197, 5004, 785, 514, 1599, 4502, 2051, 5968, 3718, 1680, 5558, 3938, 353, 5984, 5349, 4768, 815, 5956, 1310, 3516, 4415, 2483, 5292, 2021, 4333, 5515, 4090, 3988, 1149, 2470, 1856, 2392, 549, 4350, 1022, 1921, 999, 4682, 3986, 2462, 921, 3227, 265, 706, 1055, 5541, 4429, 680, 3972, 3828, 1635, 5175, 1587, 1685, 2966, 2696, 1318, 4262, 2037, 4163, 4746, 4699, 2299, 3481, 2398, 3740, 4029, 2413, 5314, 3181, 2804, 837, 3511, 5918, 1575, 3807, 1983, 1778, 63, 3486, 51, 3669, 4924, 1526, 817, 3179, 5498, 6, 1385, 1066, 534, 5733, 3210, 1086, 4357, 2176, 1331, 1670, 4408, 117, 2344, 1181, 3222, 1911, 2427, 2979, 1304, 5752, 2453, 5283, 5188, 745, 5591, 4968, 1344, 3603, 261, 1585, 5, 2786, 5406, 8, 300, 2300, 4905, 2714, 3431, 2661, 5571, 4360, 4556, 3750, 771, 211, 3888, 4486, 813, 2049, 4025, 6114, 1294, 845, 3414, 5312, 5001, 2404, 2769, 4094, 4624, 5065, 4496, 455, 3090, 6070, 3101, 1760, 1037, 1913, 1831, 1036, 1117, 3449, 5919, 418, 5455, 5299, 485, 905, 4733, 5091, 3959, 4739, 183, 2078, 1009, 1043, 4134, 2085, 5717, 3543, 3430, 552, 3830, 1512, 1822, 2914, 5705, 822, 3983, 5860, 688, 4687, 6134, 2201, 5222, 1342, 784, 1112, 1198, 2938, 2041, 4862, 3321, 5579, 5931, 526, 2608, 1872, 1163, 2780, 4218, 4256, 872, 122, 4396, 551, 1437, 5473, 4513, 2584, 1883, 3495, 2136, 5477, 5977, 3159, 1703, 1118, 4538, 1346, 2698, 4397, 3941, 2047, 5531, 2488, 3581, 132, 339, 3502, 2850, 1792, 2418, 5346, 2354, 6054, 3001, 4227, 1790, 4598, 2056, 4100, 3725, 909, 3909, 4451, 4072, 4660, 2635, 1102, 3656, 2581, 176, 1001, 2152, 2444, 3448, 685, 497, 2920, 652, 621, 4390, 3186, 4221, 1097, 2069, 4177, 1620, 4118, 4377, 178, 1615, 1950, 3010, 2092, 2793, 4694, 2103, 5834, 6004, 3905, 632, 844, 4618, 4131, 4063, 1403, 583, 1351, 5469, 1753, 1514, 4602, 4173, 2835, 2750, 3215, 2274, 5457, 3507, 5482, 5981, 3062, 5135, 2847, 2010, 4223, 2662, 3824, 2447, 5847, 165, 5276, 2210, 5493, 129, 198, 5780, 4853, 1544, 2511, 1806, 5910, 4382, 956, 2426, 1443, 1622, 4906, 5146, 4159, 5214, 668, 4876, 1284, 194, 227, 2980, 1187, 6090, 49, 4036, 436, 3470, 6058, 3488, 1594, 5322, 1283, 1864, 4539, 860, 3420, 3138, 818, 1861, 257, 2700, 2139, 2695, 3351, 2379, 4544, 1176, 2342, 2128, 1343, 2514, 4394, 686, 5212, 3895, 3540, 4110, 109, 3312, 893, 5542, 6053, 3473, 4301, 5196, 4937, 1337, 5407, 4219, 2972, 208, 2812, 4266, 4646, 5027, 4938, 3719, 1075, 295, 5436, 78, 5243, 4804, 638, 1564, 1894, 4980, 5929, 5853, 3640, 4964, 5821, 5297, 5201, 398, 5909, 59, 2542, 495, 6119, 4097, 2292, 1560, 2666, 2627, 1873, 3912, 4636, 3436, 26, 1453, 1394, 4947, 5216, 4089, 4166, 1415, 4954, 2083, 1834, 4212, 3809, 4635, 4102, 3866, 2533, 5838, 5412, 221, 6116, 2367, 3944, 1988, 1132, 1380, 1454, 4264, 1499, 5786, 1316, 4306, 3584, 171, 2675, 2667, 5205, 2530, 5108, 5045, 3793, 2703, 1684, 4421, 1191, 294, 2008, 41, 3760, 4280, 2923, 13, 4651, 1309, 1185, 1435, 6122, 2945, 2755, 4114, 1371, 2808, 4319, 2679, 5520, 4897, 6142, 2089, 2563, 5630, 292, 3151, 2668, 4553, 2927, 2156, 5076, 3193, 313, 3670, 3696, 709, 252, 2779, 1108, 1312, 2236, 2098, 2058, 772, 4257, 3330, 5791, 2957, 3569, 65, 5056, 3602, 5739, 1641, 5318, 722, 5445, 3755, 24, 5396, 675, 760, 3873, 673, 926, 3017, 1168, 5354, 4546, 2314, 5607, 259, 3658, 1608, 2007, 4289, 4777, 888, 3434, 4736, 4445, 4107, 1164, 2173, 4431, 576, 3802, 4120, 1879, 3191, 4220, 4076, 4961, 5839, 3150, 5206, 3377, 3096, 255, 5087, 2048, 2393, 1956, 3039, 1162, 4603, 5161, 5268, 491, 2580, 5423, 795, 2597, 463, 5043, 4236, 4291, 1230, 1882, 1978, 1511, 1866, 4728, 5942, 1362, 4879, 2348, 2845, 796, 5582, 4919, 2834, 4402, 6109, 1634, 3735, 6008, 3914, 1960, 5172, 625, 4270, 3556, 4541, 2976, 6089, 2860, 2962, 2082, 3479, 5596, 4688, 5321, 4762, 2711, 3775, 3553, 2251, 4885, 2887, 3657, 1096, 5401, 4640, 5921, 5204, 2033, 2206, 2802, 4412, 2928, 441, 1578, 951, 287, 1357, 3761, 4176, 2101, 14, 4299, 2310, 5936, 2073, 2538, 2077, 4628, 4104, 870, 2318, 4447, 2227, 5932, 2736, 5386, 4162, 2918, 5164, 2859, 3804, 1161, 6102, 4881, 246, 5503, 707, 4840, 2917, 1941, 1836, 2607, 763, 5454, 2759, 1115, 530, 1936, 426, 1755, 980, 620, 846, 1949, 1537, 1846, 865, 223, 3686, 4785, 5697, 1021, 4028, 1887, 2100, 2268, 4449, 3133, 3538, 5626, 826, 5008, 1569, 1457, 780, 1194, 2889, 5103, 229, 2460, 2798, 3918, 5680, 89, 493, 5818, 4658, 3594, 2445, 1151, 1535, 1273, 1169, 1422, 1264, 4239, 1543, 834, 5015, 2203, 3425, 4365, 6125, 3170, 1368, 4752, 1820, 3834, 2680, 2112, 4778, 3965, 5116, 1028, 3027, 2715, 4300, 2443, 1574, 2519, 3947, 1649, 144, 1088, 5854, 2792, 5885, 2382, 3163, 1912, 765, 3023, 4013, 2022, 527, 5980, 4482, 3636, 3283, 2067, 2106, 2071, 3242, 1999, 2449, 5893, 1376, 3025, 966, 1204, 5245, 1470, 4185, 4410, 5693, 4614, 3943, 4034, 3384, 4460, 5199, 3759, 1116, 3902, 2861, 3836, 2585, 179, 1697, 5994, 1702, 2817, 4519, 4330, 4480, 4882, 555, 5081, 1279, 3429, 545, 4154, 590, 4135, 5887, 970, 363, 3032, 2419, 4956, 5642, 1345, 1845, 1438, 4775, 6096, 878, 1600, 425, 1335, 1914, 2016, 5523, 3277, 4051, 6021, 1980, 5862, 4566, 450, 3350, 175, 1665, 1969, 524, 1095, 4363, 3118, 3097, 4669, 5460, 3126, 4242, 5702, 5552, 52, 3733, 4142, 2547, 1839, 271, 674, 511, 4248]}
SAVED model.heads_thresh 0.9
CONFIG MODE:  supervised

 ********** Supervised learning of blocks [4] **********
SAVING FOLDER FOR SUP:  STL10_CIFAR10_CLe4977f8a-d515-45b7-aaf1-e9dd54f7b531
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
INDICES:  50000
IMAGE SIZE: torch.Size([64, 3, 96, 96])
num_blocks:  5
Epoch: [1/50]	lr: 1.00e-03	time: 00:05:04	Loss_train 3.99276	Acc_train 58.52	/	Loss_test 0.20008	Acc_test 66.18
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLe4977f8a-d515-45b7-aaf1-e9dd54f7b531/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [25, 16, 59, 53, 30, 42, 77, 68, 69, 93, 75, 51, 34, 38, 70], 'conv1': [23, 75, 345, 61, 349, 92, 330, 197, 100, 33, 69, 190, 296, 41, 121, 157, 76, 166, 247, 133, 39, 336, 266, 362, 184, 17, 63, 158, 50, 116, 344, 354, 201, 98, 292, 156, 130, 335, 291, 365, 332, 366, 146, 382, 186, 287, 159, 104, 217, 139, 219, 28, 99, 260, 180, 264, 227, 273, 379], 'conv2': [1006, 307, 576, 276, 733, 972, 632, 1411, 320, 301, 350, 1284, 915, 1211, 49, 1502, 503, 1385, 272, 1265, 549, 185, 904, 734, 1413, 326, 984, 468, 764, 186, 551, 122, 969, 347, 1050, 1293, 89, 994, 1185, 697, 261, 1500, 1090, 1532, 930, 1047, 879, 1427, 1290, 519, 973, 167, 1030, 1154, 577, 1086, 786, 1453, 334, 1033, 188, 813, 1400, 294, 241, 796, 627, 1042, 1176, 619, 868, 445, 482, 135, 427, 238, 573, 70, 1239, 953, 726, 138, 1271, 985, 240, 268, 710, 1113, 1326, 1394, 823, 938, 601, 195, 205, 988, 1475, 1184, 1371, 1085, 747, 760, 1465, 617, 377, 1494, 73, 289, 737, 1409, 952, 863, 992, 660, 222, 557, 252, 43, 926, 801, 998, 159, 1493, 1251, 1263, 964, 1029, 844, 539, 1091, 1167, 1066, 177, 108, 1458, 477, 923, 766, 458, 1000, 676, 650, 310, 869, 705, 1129, 13, 269, 260, 75, 1428, 1060, 1195, 571, 562, 1214, 430, 659, 851, 854, 1344, 535, 1213, 777, 369, 187, 1242, 775, 1529, 802, 1225, 97, 779, 126, 640, 833, 174, 595, 158, 194, 1068, 946, 1503, 995, 592, 170, 812, 471, 1307, 486, 1462, 1365, 914, 373, 610, 417, 1327, 128, 1230, 133, 346, 1238, 1253, 910, 44, 1076, 950, 423, 90, 1430, 1324, 348, 1150, 1531, 193, 1412, 171, 1084, 1021, 1032, 1522, 1299, 405, 1471, 958, 1122, 580, 509, 446, 408, 715], 'conv3': [5017, 6047, 1653, 3916, 170, 5041, 2613, 4323, 6098, 2691, 5433, 3067, 4958, 4900, 3563, 1730, 62, 2026, 3435, 2075, 2778, 5255, 4087, 4574, 4983, 634, 5795, 1366, 948, 4473, 4359, 3906, 547, 750, 1229, 5651, 4738, 3197, 5004, 785, 514, 1599, 4502, 2051, 5968, 3718, 1680, 5558, 3938, 353, 5984, 5349, 4768, 815, 5956, 1310, 3516, 4415, 2483, 5292, 2021, 4333, 5515, 4090, 3988, 1149, 2470, 1856, 2392, 549, 4350, 1022, 1921, 999, 4682, 3986, 2462, 921, 3227, 265, 706, 1055, 5541, 4429, 680, 3972, 3828, 1635, 5175, 1587, 1685, 2966, 2696, 1318, 4262, 2037, 4163, 4746, 4699, 2299, 3481, 2398, 3740, 4029, 2413, 5314, 3181, 2804, 837, 3511, 5918, 1575, 3807, 1983, 1778, 63, 3486, 51, 3669, 4924, 1526, 817, 3179, 5498, 6, 1385, 1066, 534, 5733, 3210, 1086, 4357, 2176, 1331, 1670, 4408, 117, 2344, 1181, 3222, 1911, 2427, 2979, 1304, 5752, 2453, 5283, 5188, 745, 5591, 4968, 1344, 3603, 261, 1585, 5, 2786, 5406, 8, 300, 2300, 4905, 2714, 3431, 2661, 5571, 4360, 4556, 3750, 771, 211, 3888, 4486, 813, 2049, 4025, 6114, 1294, 845, 3414, 5312, 5001, 2404, 2769, 4094, 4624, 5065, 4496, 455, 3090, 6070, 3101, 1760, 1037, 1913, 1831, 1036, 1117, 3449, 5919, 418, 5455, 5299, 485, 905, 4733, 5091, 3959, 4739, 183, 2078, 1009, 1043, 4134, 2085, 5717, 3543, 3430, 552, 3830, 1512, 1822, 2914, 5705, 822, 3983, 5860, 688, 4687, 6134, 2201, 5222, 1342, 784, 1112, 1198, 2938, 2041, 4862, 3321, 5579, 5931, 526, 2608, 1872, 1163, 2780, 4218, 4256, 872, 122, 4396, 551, 1437, 5473, 4513, 2584, 1883, 3495, 2136, 5477, 5977, 3159, 1703, 1118, 4538, 1346, 2698, 4397, 3941, 2047, 5531, 2488, 3581, 132, 339, 3502, 2850, 1792, 2418, 5346, 2354, 6054, 3001, 4227, 1790, 4598, 2056, 4100, 3725, 909, 3909, 4451, 4072, 4660, 2635, 1102, 3656, 2581, 176, 1001, 2152, 2444, 3448, 685, 497, 2920, 652, 621, 4390, 3186, 4221, 1097, 2069, 4177, 1620, 4118, 4377, 178, 1615, 1950, 3010, 2092, 2793, 4694, 2103, 5834, 6004, 3905, 632, 844, 4618, 4131, 4063, 1403, 583, 1351, 5469, 1753, 1514, 4602, 4173, 2835, 2750, 3215, 2274, 5457, 3507, 5482, 5981, 3062, 5135, 2847, 2010, 4223, 2662, 3824, 2447, 5847, 165, 5276, 2210, 5493, 129, 198, 5780, 4853, 1544, 2511, 1806, 5910, 4382, 956, 2426, 1443, 1622, 4906, 5146, 4159, 5214, 668, 4876, 1284, 194, 227, 2980, 1187, 6090, 49, 4036, 436, 3470, 6058, 3488, 1594, 5322, 1283, 1864, 4539, 860, 3420, 3138, 818, 1861, 257, 2700, 2139, 2695, 3351, 2379, 4544, 1176, 2342, 2128, 1343, 2514, 4394, 686, 5212, 3895, 3540, 4110, 109, 3312, 893, 5542, 6053, 3473, 4301, 5196, 4937, 1337, 5407, 4219, 2972, 208, 2812, 4266, 4646, 5027, 4938, 3719, 1075, 295, 5436, 78, 5243, 4804, 638, 1564, 1894, 4980, 5929, 5853, 3640, 4964, 5821, 5297, 5201, 398, 5909, 59, 2542, 495, 6119, 4097, 2292, 1560, 2666, 2627, 1873, 3912, 4636, 3436, 26, 1453, 1394, 4947, 5216, 4089, 4166, 1415, 4954, 2083, 1834, 4212, 3809, 4635, 4102, 3866, 2533, 5838, 5412, 221, 6116, 2367, 3944, 1988, 1132, 1380, 1454, 4264, 1499, 5786, 1316, 4306, 3584, 171, 2675, 2667, 5205, 2530, 5108, 5045, 3793, 2703, 1684, 4421, 1191, 294, 2008, 41, 3760, 4280, 2923, 13, 4651, 1309, 1185, 1435, 6122, 2945, 2755, 4114, 1371, 2808, 4319, 2679, 5520, 4897, 6142, 2089, 2563, 5630, 292, 3151, 2668, 4553, 2927, 2156, 5076, 3193, 313, 3670, 3696, 709, 252, 2779, 1108, 1312, 2236, 2098, 2058, 772, 4257, 3330, 5791, 2957, 3569, 65, 5056, 3602, 5739, 1641, 5318, 722, 5445, 3755, 24, 5396, 675, 760, 3873, 673, 926, 3017, 1168, 5354, 4546, 2314, 5607, 259, 3658, 1608, 2007, 4289, 4777, 888, 3434, 4736, 4445, 4107, 1164, 2173, 4431, 576, 3802, 4120, 1879, 3191, 4220, 4076, 4961, 5839, 3150, 5206, 3377, 3096, 255, 5087, 2048, 2393, 1956, 3039, 1162, 4603, 5161, 5268, 491, 2580, 5423, 795, 2597, 463, 5043, 4236, 4291, 1230, 1882, 1978, 1511, 1866, 4728, 5942, 1362, 4879, 2348, 2845, 796, 5582, 4919, 2834, 4402, 6109, 1634, 3735, 6008, 3914, 1960, 5172, 625, 4270, 3556, 4541, 2976, 6089, 2860, 2962, 2082, 3479, 5596, 4688, 5321, 4762, 2711, 3775, 3553, 2251, 4885, 2887, 3657, 1096, 5401, 4640, 5921, 5204, 2033, 2206, 2802, 4412, 2928, 441, 1578, 951, 287, 1357, 3761, 4176, 2101, 14, 4299, 2310, 5936, 2073, 2538, 2077, 4628, 4104, 870, 2318, 4447, 2227, 5932, 2736, 5386, 4162, 2918, 5164, 2859, 3804, 1161, 6102, 4881, 246, 5503, 707, 4840, 2917, 1941, 1836, 2607, 763, 5454, 2759, 1115, 530, 1936, 426, 1755, 980, 620, 846, 1949, 1537, 1846, 865, 223, 3686, 4785, 5697, 1021, 4028, 1887, 2100, 2268, 4449, 3133, 3538, 5626, 826, 5008, 1569, 1457, 780, 1194, 2889, 5103, 229, 2460, 2798, 3918, 5680, 89, 493, 5818, 4658, 3594, 2445, 1151, 1535, 1273, 1169, 1422, 1264, 4239, 1543, 834, 5015, 2203, 3425, 4365, 6125, 3170, 1368, 4752, 1820, 3834, 2680, 2112, 4778, 3965, 5116, 1028, 3027, 2715, 4300, 2443, 1574, 2519, 3947, 1649, 144, 1088, 5854, 2792, 5885, 2382, 3163, 1912, 765, 3023, 4013, 2022, 527, 5980, 4482, 3636, 3283, 2067, 2106, 2071, 3242, 1999, 2449, 5893, 1376, 3025, 966, 1204, 5245, 1470, 4185, 4410, 5693, 4614, 3943, 4034, 3384, 4460, 5199, 3759, 1116, 3902, 2861, 3836, 2585, 179, 1697, 5994, 1702, 2817, 4519, 4330, 4480, 4882, 555, 5081, 1279, 3429, 545, 4154, 590, 4135, 5887, 970, 363, 3032, 2419, 4956, 5642, 1345, 1845, 1438, 4775, 6096, 878, 1600, 425, 1335, 1914, 2016, 5523, 3277, 4051, 6021, 1980, 5862, 4566, 450, 3350, 175, 1665, 1969, 524, 1095, 4363, 3118, 3097, 4669, 5460, 3126, 4242, 5702, 5552, 52, 3733, 4142, 2547, 1839, 271, 674, 511, 4248]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [10/50]	lr: 1.00e-03	time: 00:08:46	Loss_train 1.86903	Acc_train 79.95	/	Loss_test 0.21385	Acc_test 76.52
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLe4977f8a-d515-45b7-aaf1-e9dd54f7b531/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [25, 16, 59, 53, 30, 42, 77, 68, 69, 93, 75, 51, 34, 38, 70], 'conv1': [23, 75, 345, 61, 349, 92, 330, 197, 100, 33, 69, 190, 296, 41, 121, 157, 76, 166, 247, 133, 39, 336, 266, 362, 184, 17, 63, 158, 50, 116, 344, 354, 201, 98, 292, 156, 130, 335, 291, 365, 332, 366, 146, 382, 186, 287, 159, 104, 217, 139, 219, 28, 99, 260, 180, 264, 227, 273, 379], 'conv2': [1006, 307, 576, 276, 733, 972, 632, 1411, 320, 301, 350, 1284, 915, 1211, 49, 1502, 503, 1385, 272, 1265, 549, 185, 904, 734, 1413, 326, 984, 468, 764, 186, 551, 122, 969, 347, 1050, 1293, 89, 994, 1185, 697, 261, 1500, 1090, 1532, 930, 1047, 879, 1427, 1290, 519, 973, 167, 1030, 1154, 577, 1086, 786, 1453, 334, 1033, 188, 813, 1400, 294, 241, 796, 627, 1042, 1176, 619, 868, 445, 482, 135, 427, 238, 573, 70, 1239, 953, 726, 138, 1271, 985, 240, 268, 710, 1113, 1326, 1394, 823, 938, 601, 195, 205, 988, 1475, 1184, 1371, 1085, 747, 760, 1465, 617, 377, 1494, 73, 289, 737, 1409, 952, 863, 992, 660, 222, 557, 252, 43, 926, 801, 998, 159, 1493, 1251, 1263, 964, 1029, 844, 539, 1091, 1167, 1066, 177, 108, 1458, 477, 923, 766, 458, 1000, 676, 650, 310, 869, 705, 1129, 13, 269, 260, 75, 1428, 1060, 1195, 571, 562, 1214, 430, 659, 851, 854, 1344, 535, 1213, 777, 369, 187, 1242, 775, 1529, 802, 1225, 97, 779, 126, 640, 833, 174, 595, 158, 194, 1068, 946, 1503, 995, 592, 170, 812, 471, 1307, 486, 1462, 1365, 914, 373, 610, 417, 1327, 128, 1230, 133, 346, 1238, 1253, 910, 44, 1076, 950, 423, 90, 1430, 1324, 348, 1150, 1531, 193, 1412, 171, 1084, 1021, 1032, 1522, 1299, 405, 1471, 958, 1122, 580, 509, 446, 408, 715], 'conv3': [5017, 6047, 1653, 3916, 170, 5041, 2613, 4323, 6098, 2691, 5433, 3067, 4958, 4900, 3563, 1730, 62, 2026, 3435, 2075, 2778, 5255, 4087, 4574, 4983, 634, 5795, 1366, 948, 4473, 4359, 3906, 547, 750, 1229, 5651, 4738, 3197, 5004, 785, 514, 1599, 4502, 2051, 5968, 3718, 1680, 5558, 3938, 353, 5984, 5349, 4768, 815, 5956, 1310, 3516, 4415, 2483, 5292, 2021, 4333, 5515, 4090, 3988, 1149, 2470, 1856, 2392, 549, 4350, 1022, 1921, 999, 4682, 3986, 2462, 921, 3227, 265, 706, 1055, 5541, 4429, 680, 3972, 3828, 1635, 5175, 1587, 1685, 2966, 2696, 1318, 4262, 2037, 4163, 4746, 4699, 2299, 3481, 2398, 3740, 4029, 2413, 5314, 3181, 2804, 837, 3511, 5918, 1575, 3807, 1983, 1778, 63, 3486, 51, 3669, 4924, 1526, 817, 3179, 5498, 6, 1385, 1066, 534, 5733, 3210, 1086, 4357, 2176, 1331, 1670, 4408, 117, 2344, 1181, 3222, 1911, 2427, 2979, 1304, 5752, 2453, 5283, 5188, 745, 5591, 4968, 1344, 3603, 261, 1585, 5, 2786, 5406, 8, 300, 2300, 4905, 2714, 3431, 2661, 5571, 4360, 4556, 3750, 771, 211, 3888, 4486, 813, 2049, 4025, 6114, 1294, 845, 3414, 5312, 5001, 2404, 2769, 4094, 4624, 5065, 4496, 455, 3090, 6070, 3101, 1760, 1037, 1913, 1831, 1036, 1117, 3449, 5919, 418, 5455, 5299, 485, 905, 4733, 5091, 3959, 4739, 183, 2078, 1009, 1043, 4134, 2085, 5717, 3543, 3430, 552, 3830, 1512, 1822, 2914, 5705, 822, 3983, 5860, 688, 4687, 6134, 2201, 5222, 1342, 784, 1112, 1198, 2938, 2041, 4862, 3321, 5579, 5931, 526, 2608, 1872, 1163, 2780, 4218, 4256, 872, 122, 4396, 551, 1437, 5473, 4513, 2584, 1883, 3495, 2136, 5477, 5977, 3159, 1703, 1118, 4538, 1346, 2698, 4397, 3941, 2047, 5531, 2488, 3581, 132, 339, 3502, 2850, 1792, 2418, 5346, 2354, 6054, 3001, 4227, 1790, 4598, 2056, 4100, 3725, 909, 3909, 4451, 4072, 4660, 2635, 1102, 3656, 2581, 176, 1001, 2152, 2444, 3448, 685, 497, 2920, 652, 621, 4390, 3186, 4221, 1097, 2069, 4177, 1620, 4118, 4377, 178, 1615, 1950, 3010, 2092, 2793, 4694, 2103, 5834, 6004, 3905, 632, 844, 4618, 4131, 4063, 1403, 583, 1351, 5469, 1753, 1514, 4602, 4173, 2835, 2750, 3215, 2274, 5457, 3507, 5482, 5981, 3062, 5135, 2847, 2010, 4223, 2662, 3824, 2447, 5847, 165, 5276, 2210, 5493, 129, 198, 5780, 4853, 1544, 2511, 1806, 5910, 4382, 956, 2426, 1443, 1622, 4906, 5146, 4159, 5214, 668, 4876, 1284, 194, 227, 2980, 1187, 6090, 49, 4036, 436, 3470, 6058, 3488, 1594, 5322, 1283, 1864, 4539, 860, 3420, 3138, 818, 1861, 257, 2700, 2139, 2695, 3351, 2379, 4544, 1176, 2342, 2128, 1343, 2514, 4394, 686, 5212, 3895, 3540, 4110, 109, 3312, 893, 5542, 6053, 3473, 4301, 5196, 4937, 1337, 5407, 4219, 2972, 208, 2812, 4266, 4646, 5027, 4938, 3719, 1075, 295, 5436, 78, 5243, 4804, 638, 1564, 1894, 4980, 5929, 5853, 3640, 4964, 5821, 5297, 5201, 398, 5909, 59, 2542, 495, 6119, 4097, 2292, 1560, 2666, 2627, 1873, 3912, 4636, 3436, 26, 1453, 1394, 4947, 5216, 4089, 4166, 1415, 4954, 2083, 1834, 4212, 3809, 4635, 4102, 3866, 2533, 5838, 5412, 221, 6116, 2367, 3944, 1988, 1132, 1380, 1454, 4264, 1499, 5786, 1316, 4306, 3584, 171, 2675, 2667, 5205, 2530, 5108, 5045, 3793, 2703, 1684, 4421, 1191, 294, 2008, 41, 3760, 4280, 2923, 13, 4651, 1309, 1185, 1435, 6122, 2945, 2755, 4114, 1371, 2808, 4319, 2679, 5520, 4897, 6142, 2089, 2563, 5630, 292, 3151, 2668, 4553, 2927, 2156, 5076, 3193, 313, 3670, 3696, 709, 252, 2779, 1108, 1312, 2236, 2098, 2058, 772, 4257, 3330, 5791, 2957, 3569, 65, 5056, 3602, 5739, 1641, 5318, 722, 5445, 3755, 24, 5396, 675, 760, 3873, 673, 926, 3017, 1168, 5354, 4546, 2314, 5607, 259, 3658, 1608, 2007, 4289, 4777, 888, 3434, 4736, 4445, 4107, 1164, 2173, 4431, 576, 3802, 4120, 1879, 3191, 4220, 4076, 4961, 5839, 3150, 5206, 3377, 3096, 255, 5087, 2048, 2393, 1956, 3039, 1162, 4603, 5161, 5268, 491, 2580, 5423, 795, 2597, 463, 5043, 4236, 4291, 1230, 1882, 1978, 1511, 1866, 4728, 5942, 1362, 4879, 2348, 2845, 796, 5582, 4919, 2834, 4402, 6109, 1634, 3735, 6008, 3914, 1960, 5172, 625, 4270, 3556, 4541, 2976, 6089, 2860, 2962, 2082, 3479, 5596, 4688, 5321, 4762, 2711, 3775, 3553, 2251, 4885, 2887, 3657, 1096, 5401, 4640, 5921, 5204, 2033, 2206, 2802, 4412, 2928, 441, 1578, 951, 287, 1357, 3761, 4176, 2101, 14, 4299, 2310, 5936, 2073, 2538, 2077, 4628, 4104, 870, 2318, 4447, 2227, 5932, 2736, 5386, 4162, 2918, 5164, 2859, 3804, 1161, 6102, 4881, 246, 5503, 707, 4840, 2917, 1941, 1836, 2607, 763, 5454, 2759, 1115, 530, 1936, 426, 1755, 980, 620, 846, 1949, 1537, 1846, 865, 223, 3686, 4785, 5697, 1021, 4028, 1887, 2100, 2268, 4449, 3133, 3538, 5626, 826, 5008, 1569, 1457, 780, 1194, 2889, 5103, 229, 2460, 2798, 3918, 5680, 89, 493, 5818, 4658, 3594, 2445, 1151, 1535, 1273, 1169, 1422, 1264, 4239, 1543, 834, 5015, 2203, 3425, 4365, 6125, 3170, 1368, 4752, 1820, 3834, 2680, 2112, 4778, 3965, 5116, 1028, 3027, 2715, 4300, 2443, 1574, 2519, 3947, 1649, 144, 1088, 5854, 2792, 5885, 2382, 3163, 1912, 765, 3023, 4013, 2022, 527, 5980, 4482, 3636, 3283, 2067, 2106, 2071, 3242, 1999, 2449, 5893, 1376, 3025, 966, 1204, 5245, 1470, 4185, 4410, 5693, 4614, 3943, 4034, 3384, 4460, 5199, 3759, 1116, 3902, 2861, 3836, 2585, 179, 1697, 5994, 1702, 2817, 4519, 4330, 4480, 4882, 555, 5081, 1279, 3429, 545, 4154, 590, 4135, 5887, 970, 363, 3032, 2419, 4956, 5642, 1345, 1845, 1438, 4775, 6096, 878, 1600, 425, 1335, 1914, 2016, 5523, 3277, 4051, 6021, 1980, 5862, 4566, 450, 3350, 175, 1665, 1969, 524, 1095, 4363, 3118, 3097, 4669, 5460, 3126, 4242, 5702, 5552, 52, 3733, 4142, 2547, 1839, 271, 674, 511, 4248]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [20/50]	lr: 2.50e-04	time: 00:12:51	Loss_train 0.57331	Acc_train 91.90	/	Loss_test 0.15852	Acc_test 80.43
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLe4977f8a-d515-45b7-aaf1-e9dd54f7b531/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [25, 16, 59, 53, 30, 42, 77, 68, 69, 93, 75, 51, 34, 38, 70], 'conv1': [23, 75, 345, 61, 349, 92, 330, 197, 100, 33, 69, 190, 296, 41, 121, 157, 76, 166, 247, 133, 39, 336, 266, 362, 184, 17, 63, 158, 50, 116, 344, 354, 201, 98, 292, 156, 130, 335, 291, 365, 332, 366, 146, 382, 186, 287, 159, 104, 217, 139, 219, 28, 99, 260, 180, 264, 227, 273, 379], 'conv2': [1006, 307, 576, 276, 733, 972, 632, 1411, 320, 301, 350, 1284, 915, 1211, 49, 1502, 503, 1385, 272, 1265, 549, 185, 904, 734, 1413, 326, 984, 468, 764, 186, 551, 122, 969, 347, 1050, 1293, 89, 994, 1185, 697, 261, 1500, 1090, 1532, 930, 1047, 879, 1427, 1290, 519, 973, 167, 1030, 1154, 577, 1086, 786, 1453, 334, 1033, 188, 813, 1400, 294, 241, 796, 627, 1042, 1176, 619, 868, 445, 482, 135, 427, 238, 573, 70, 1239, 953, 726, 138, 1271, 985, 240, 268, 710, 1113, 1326, 1394, 823, 938, 601, 195, 205, 988, 1475, 1184, 1371, 1085, 747, 760, 1465, 617, 377, 1494, 73, 289, 737, 1409, 952, 863, 992, 660, 222, 557, 252, 43, 926, 801, 998, 159, 1493, 1251, 1263, 964, 1029, 844, 539, 1091, 1167, 1066, 177, 108, 1458, 477, 923, 766, 458, 1000, 676, 650, 310, 869, 705, 1129, 13, 269, 260, 75, 1428, 1060, 1195, 571, 562, 1214, 430, 659, 851, 854, 1344, 535, 1213, 777, 369, 187, 1242, 775, 1529, 802, 1225, 97, 779, 126, 640, 833, 174, 595, 158, 194, 1068, 946, 1503, 995, 592, 170, 812, 471, 1307, 486, 1462, 1365, 914, 373, 610, 417, 1327, 128, 1230, 133, 346, 1238, 1253, 910, 44, 1076, 950, 423, 90, 1430, 1324, 348, 1150, 1531, 193, 1412, 171, 1084, 1021, 1032, 1522, 1299, 405, 1471, 958, 1122, 580, 509, 446, 408, 715], 'conv3': [5017, 6047, 1653, 3916, 170, 5041, 2613, 4323, 6098, 2691, 5433, 3067, 4958, 4900, 3563, 1730, 62, 2026, 3435, 2075, 2778, 5255, 4087, 4574, 4983, 634, 5795, 1366, 948, 4473, 4359, 3906, 547, 750, 1229, 5651, 4738, 3197, 5004, 785, 514, 1599, 4502, 2051, 5968, 3718, 1680, 5558, 3938, 353, 5984, 5349, 4768, 815, 5956, 1310, 3516, 4415, 2483, 5292, 2021, 4333, 5515, 4090, 3988, 1149, 2470, 1856, 2392, 549, 4350, 1022, 1921, 999, 4682, 3986, 2462, 921, 3227, 265, 706, 1055, 5541, 4429, 680, 3972, 3828, 1635, 5175, 1587, 1685, 2966, 2696, 1318, 4262, 2037, 4163, 4746, 4699, 2299, 3481, 2398, 3740, 4029, 2413, 5314, 3181, 2804, 837, 3511, 5918, 1575, 3807, 1983, 1778, 63, 3486, 51, 3669, 4924, 1526, 817, 3179, 5498, 6, 1385, 1066, 534, 5733, 3210, 1086, 4357, 2176, 1331, 1670, 4408, 117, 2344, 1181, 3222, 1911, 2427, 2979, 1304, 5752, 2453, 5283, 5188, 745, 5591, 4968, 1344, 3603, 261, 1585, 5, 2786, 5406, 8, 300, 2300, 4905, 2714, 3431, 2661, 5571, 4360, 4556, 3750, 771, 211, 3888, 4486, 813, 2049, 4025, 6114, 1294, 845, 3414, 5312, 5001, 2404, 2769, 4094, 4624, 5065, 4496, 455, 3090, 6070, 3101, 1760, 1037, 1913, 1831, 1036, 1117, 3449, 5919, 418, 5455, 5299, 485, 905, 4733, 5091, 3959, 4739, 183, 2078, 1009, 1043, 4134, 2085, 5717, 3543, 3430, 552, 3830, 1512, 1822, 2914, 5705, 822, 3983, 5860, 688, 4687, 6134, 2201, 5222, 1342, 784, 1112, 1198, 2938, 2041, 4862, 3321, 5579, 5931, 526, 2608, 1872, 1163, 2780, 4218, 4256, 872, 122, 4396, 551, 1437, 5473, 4513, 2584, 1883, 3495, 2136, 5477, 5977, 3159, 1703, 1118, 4538, 1346, 2698, 4397, 3941, 2047, 5531, 2488, 3581, 132, 339, 3502, 2850, 1792, 2418, 5346, 2354, 6054, 3001, 4227, 1790, 4598, 2056, 4100, 3725, 909, 3909, 4451, 4072, 4660, 2635, 1102, 3656, 2581, 176, 1001, 2152, 2444, 3448, 685, 497, 2920, 652, 621, 4390, 3186, 4221, 1097, 2069, 4177, 1620, 4118, 4377, 178, 1615, 1950, 3010, 2092, 2793, 4694, 2103, 5834, 6004, 3905, 632, 844, 4618, 4131, 4063, 1403, 583, 1351, 5469, 1753, 1514, 4602, 4173, 2835, 2750, 3215, 2274, 5457, 3507, 5482, 5981, 3062, 5135, 2847, 2010, 4223, 2662, 3824, 2447, 5847, 165, 5276, 2210, 5493, 129, 198, 5780, 4853, 1544, 2511, 1806, 5910, 4382, 956, 2426, 1443, 1622, 4906, 5146, 4159, 5214, 668, 4876, 1284, 194, 227, 2980, 1187, 6090, 49, 4036, 436, 3470, 6058, 3488, 1594, 5322, 1283, 1864, 4539, 860, 3420, 3138, 818, 1861, 257, 2700, 2139, 2695, 3351, 2379, 4544, 1176, 2342, 2128, 1343, 2514, 4394, 686, 5212, 3895, 3540, 4110, 109, 3312, 893, 5542, 6053, 3473, 4301, 5196, 4937, 1337, 5407, 4219, 2972, 208, 2812, 4266, 4646, 5027, 4938, 3719, 1075, 295, 5436, 78, 5243, 4804, 638, 1564, 1894, 4980, 5929, 5853, 3640, 4964, 5821, 5297, 5201, 398, 5909, 59, 2542, 495, 6119, 4097, 2292, 1560, 2666, 2627, 1873, 3912, 4636, 3436, 26, 1453, 1394, 4947, 5216, 4089, 4166, 1415, 4954, 2083, 1834, 4212, 3809, 4635, 4102, 3866, 2533, 5838, 5412, 221, 6116, 2367, 3944, 1988, 1132, 1380, 1454, 4264, 1499, 5786, 1316, 4306, 3584, 171, 2675, 2667, 5205, 2530, 5108, 5045, 3793, 2703, 1684, 4421, 1191, 294, 2008, 41, 3760, 4280, 2923, 13, 4651, 1309, 1185, 1435, 6122, 2945, 2755, 4114, 1371, 2808, 4319, 2679, 5520, 4897, 6142, 2089, 2563, 5630, 292, 3151, 2668, 4553, 2927, 2156, 5076, 3193, 313, 3670, 3696, 709, 252, 2779, 1108, 1312, 2236, 2098, 2058, 772, 4257, 3330, 5791, 2957, 3569, 65, 5056, 3602, 5739, 1641, 5318, 722, 5445, 3755, 24, 5396, 675, 760, 3873, 673, 926, 3017, 1168, 5354, 4546, 2314, 5607, 259, 3658, 1608, 2007, 4289, 4777, 888, 3434, 4736, 4445, 4107, 1164, 2173, 4431, 576, 3802, 4120, 1879, 3191, 4220, 4076, 4961, 5839, 3150, 5206, 3377, 3096, 255, 5087, 2048, 2393, 1956, 3039, 1162, 4603, 5161, 5268, 491, 2580, 5423, 795, 2597, 463, 5043, 4236, 4291, 1230, 1882, 1978, 1511, 1866, 4728, 5942, 1362, 4879, 2348, 2845, 796, 5582, 4919, 2834, 4402, 6109, 1634, 3735, 6008, 3914, 1960, 5172, 625, 4270, 3556, 4541, 2976, 6089, 2860, 2962, 2082, 3479, 5596, 4688, 5321, 4762, 2711, 3775, 3553, 2251, 4885, 2887, 3657, 1096, 5401, 4640, 5921, 5204, 2033, 2206, 2802, 4412, 2928, 441, 1578, 951, 287, 1357, 3761, 4176, 2101, 14, 4299, 2310, 5936, 2073, 2538, 2077, 4628, 4104, 870, 2318, 4447, 2227, 5932, 2736, 5386, 4162, 2918, 5164, 2859, 3804, 1161, 6102, 4881, 246, 5503, 707, 4840, 2917, 1941, 1836, 2607, 763, 5454, 2759, 1115, 530, 1936, 426, 1755, 980, 620, 846, 1949, 1537, 1846, 865, 223, 3686, 4785, 5697, 1021, 4028, 1887, 2100, 2268, 4449, 3133, 3538, 5626, 826, 5008, 1569, 1457, 780, 1194, 2889, 5103, 229, 2460, 2798, 3918, 5680, 89, 493, 5818, 4658, 3594, 2445, 1151, 1535, 1273, 1169, 1422, 1264, 4239, 1543, 834, 5015, 2203, 3425, 4365, 6125, 3170, 1368, 4752, 1820, 3834, 2680, 2112, 4778, 3965, 5116, 1028, 3027, 2715, 4300, 2443, 1574, 2519, 3947, 1649, 144, 1088, 5854, 2792, 5885, 2382, 3163, 1912, 765, 3023, 4013, 2022, 527, 5980, 4482, 3636, 3283, 2067, 2106, 2071, 3242, 1999, 2449, 5893, 1376, 3025, 966, 1204, 5245, 1470, 4185, 4410, 5693, 4614, 3943, 4034, 3384, 4460, 5199, 3759, 1116, 3902, 2861, 3836, 2585, 179, 1697, 5994, 1702, 2817, 4519, 4330, 4480, 4882, 555, 5081, 1279, 3429, 545, 4154, 590, 4135, 5887, 970, 363, 3032, 2419, 4956, 5642, 1345, 1845, 1438, 4775, 6096, 878, 1600, 425, 1335, 1914, 2016, 5523, 3277, 4051, 6021, 1980, 5862, 4566, 450, 3350, 175, 1665, 1969, 524, 1095, 4363, 3118, 3097, 4669, 5460, 3126, 4242, 5702, 5552, 52, 3733, 4142, 2547, 1839, 271, 674, 511, 4248]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [30/50]	lr: 1.25e-04	time: 00:16:57	Loss_train 0.25921	Acc_train 95.28	/	Loss_test 0.15811	Acc_test 80.69
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLe4977f8a-d515-45b7-aaf1-e9dd54f7b531/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [25, 16, 59, 53, 30, 42, 77, 68, 69, 93, 75, 51, 34, 38, 70], 'conv1': [23, 75, 345, 61, 349, 92, 330, 197, 100, 33, 69, 190, 296, 41, 121, 157, 76, 166, 247, 133, 39, 336, 266, 362, 184, 17, 63, 158, 50, 116, 344, 354, 201, 98, 292, 156, 130, 335, 291, 365, 332, 366, 146, 382, 186, 287, 159, 104, 217, 139, 219, 28, 99, 260, 180, 264, 227, 273, 379], 'conv2': [1006, 307, 576, 276, 733, 972, 632, 1411, 320, 301, 350, 1284, 915, 1211, 49, 1502, 503, 1385, 272, 1265, 549, 185, 904, 734, 1413, 326, 984, 468, 764, 186, 551, 122, 969, 347, 1050, 1293, 89, 994, 1185, 697, 261, 1500, 1090, 1532, 930, 1047, 879, 1427, 1290, 519, 973, 167, 1030, 1154, 577, 1086, 786, 1453, 334, 1033, 188, 813, 1400, 294, 241, 796, 627, 1042, 1176, 619, 868, 445, 482, 135, 427, 238, 573, 70, 1239, 953, 726, 138, 1271, 985, 240, 268, 710, 1113, 1326, 1394, 823, 938, 601, 195, 205, 988, 1475, 1184, 1371, 1085, 747, 760, 1465, 617, 377, 1494, 73, 289, 737, 1409, 952, 863, 992, 660, 222, 557, 252, 43, 926, 801, 998, 159, 1493, 1251, 1263, 964, 1029, 844, 539, 1091, 1167, 1066, 177, 108, 1458, 477, 923, 766, 458, 1000, 676, 650, 310, 869, 705, 1129, 13, 269, 260, 75, 1428, 1060, 1195, 571, 562, 1214, 430, 659, 851, 854, 1344, 535, 1213, 777, 369, 187, 1242, 775, 1529, 802, 1225, 97, 779, 126, 640, 833, 174, 595, 158, 194, 1068, 946, 1503, 995, 592, 170, 812, 471, 1307, 486, 1462, 1365, 914, 373, 610, 417, 1327, 128, 1230, 133, 346, 1238, 1253, 910, 44, 1076, 950, 423, 90, 1430, 1324, 348, 1150, 1531, 193, 1412, 171, 1084, 1021, 1032, 1522, 1299, 405, 1471, 958, 1122, 580, 509, 446, 408, 715], 'conv3': [5017, 6047, 1653, 3916, 170, 5041, 2613, 4323, 6098, 2691, 5433, 3067, 4958, 4900, 3563, 1730, 62, 2026, 3435, 2075, 2778, 5255, 4087, 4574, 4983, 634, 5795, 1366, 948, 4473, 4359, 3906, 547, 750, 1229, 5651, 4738, 3197, 5004, 785, 514, 1599, 4502, 2051, 5968, 3718, 1680, 5558, 3938, 353, 5984, 5349, 4768, 815, 5956, 1310, 3516, 4415, 2483, 5292, 2021, 4333, 5515, 4090, 3988, 1149, 2470, 1856, 2392, 549, 4350, 1022, 1921, 999, 4682, 3986, 2462, 921, 3227, 265, 706, 1055, 5541, 4429, 680, 3972, 3828, 1635, 5175, 1587, 1685, 2966, 2696, 1318, 4262, 2037, 4163, 4746, 4699, 2299, 3481, 2398, 3740, 4029, 2413, 5314, 3181, 2804, 837, 3511, 5918, 1575, 3807, 1983, 1778, 63, 3486, 51, 3669, 4924, 1526, 817, 3179, 5498, 6, 1385, 1066, 534, 5733, 3210, 1086, 4357, 2176, 1331, 1670, 4408, 117, 2344, 1181, 3222, 1911, 2427, 2979, 1304, 5752, 2453, 5283, 5188, 745, 5591, 4968, 1344, 3603, 261, 1585, 5, 2786, 5406, 8, 300, 2300, 4905, 2714, 3431, 2661, 5571, 4360, 4556, 3750, 771, 211, 3888, 4486, 813, 2049, 4025, 6114, 1294, 845, 3414, 5312, 5001, 2404, 2769, 4094, 4624, 5065, 4496, 455, 3090, 6070, 3101, 1760, 1037, 1913, 1831, 1036, 1117, 3449, 5919, 418, 5455, 5299, 485, 905, 4733, 5091, 3959, 4739, 183, 2078, 1009, 1043, 4134, 2085, 5717, 3543, 3430, 552, 3830, 1512, 1822, 2914, 5705, 822, 3983, 5860, 688, 4687, 6134, 2201, 5222, 1342, 784, 1112, 1198, 2938, 2041, 4862, 3321, 5579, 5931, 526, 2608, 1872, 1163, 2780, 4218, 4256, 872, 122, 4396, 551, 1437, 5473, 4513, 2584, 1883, 3495, 2136, 5477, 5977, 3159, 1703, 1118, 4538, 1346, 2698, 4397, 3941, 2047, 5531, 2488, 3581, 132, 339, 3502, 2850, 1792, 2418, 5346, 2354, 6054, 3001, 4227, 1790, 4598, 2056, 4100, 3725, 909, 3909, 4451, 4072, 4660, 2635, 1102, 3656, 2581, 176, 1001, 2152, 2444, 3448, 685, 497, 2920, 652, 621, 4390, 3186, 4221, 1097, 2069, 4177, 1620, 4118, 4377, 178, 1615, 1950, 3010, 2092, 2793, 4694, 2103, 5834, 6004, 3905, 632, 844, 4618, 4131, 4063, 1403, 583, 1351, 5469, 1753, 1514, 4602, 4173, 2835, 2750, 3215, 2274, 5457, 3507, 5482, 5981, 3062, 5135, 2847, 2010, 4223, 2662, 3824, 2447, 5847, 165, 5276, 2210, 5493, 129, 198, 5780, 4853, 1544, 2511, 1806, 5910, 4382, 956, 2426, 1443, 1622, 4906, 5146, 4159, 5214, 668, 4876, 1284, 194, 227, 2980, 1187, 6090, 49, 4036, 436, 3470, 6058, 3488, 1594, 5322, 1283, 1864, 4539, 860, 3420, 3138, 818, 1861, 257, 2700, 2139, 2695, 3351, 2379, 4544, 1176, 2342, 2128, 1343, 2514, 4394, 686, 5212, 3895, 3540, 4110, 109, 3312, 893, 5542, 6053, 3473, 4301, 5196, 4937, 1337, 5407, 4219, 2972, 208, 2812, 4266, 4646, 5027, 4938, 3719, 1075, 295, 5436, 78, 5243, 4804, 638, 1564, 1894, 4980, 5929, 5853, 3640, 4964, 5821, 5297, 5201, 398, 5909, 59, 2542, 495, 6119, 4097, 2292, 1560, 2666, 2627, 1873, 3912, 4636, 3436, 26, 1453, 1394, 4947, 5216, 4089, 4166, 1415, 4954, 2083, 1834, 4212, 3809, 4635, 4102, 3866, 2533, 5838, 5412, 221, 6116, 2367, 3944, 1988, 1132, 1380, 1454, 4264, 1499, 5786, 1316, 4306, 3584, 171, 2675, 2667, 5205, 2530, 5108, 5045, 3793, 2703, 1684, 4421, 1191, 294, 2008, 41, 3760, 4280, 2923, 13, 4651, 1309, 1185, 1435, 6122, 2945, 2755, 4114, 1371, 2808, 4319, 2679, 5520, 4897, 6142, 2089, 2563, 5630, 292, 3151, 2668, 4553, 2927, 2156, 5076, 3193, 313, 3670, 3696, 709, 252, 2779, 1108, 1312, 2236, 2098, 2058, 772, 4257, 3330, 5791, 2957, 3569, 65, 5056, 3602, 5739, 1641, 5318, 722, 5445, 3755, 24, 5396, 675, 760, 3873, 673, 926, 3017, 1168, 5354, 4546, 2314, 5607, 259, 3658, 1608, 2007, 4289, 4777, 888, 3434, 4736, 4445, 4107, 1164, 2173, 4431, 576, 3802, 4120, 1879, 3191, 4220, 4076, 4961, 5839, 3150, 5206, 3377, 3096, 255, 5087, 2048, 2393, 1956, 3039, 1162, 4603, 5161, 5268, 491, 2580, 5423, 795, 2597, 463, 5043, 4236, 4291, 1230, 1882, 1978, 1511, 1866, 4728, 5942, 1362, 4879, 2348, 2845, 796, 5582, 4919, 2834, 4402, 6109, 1634, 3735, 6008, 3914, 1960, 5172, 625, 4270, 3556, 4541, 2976, 6089, 2860, 2962, 2082, 3479, 5596, 4688, 5321, 4762, 2711, 3775, 3553, 2251, 4885, 2887, 3657, 1096, 5401, 4640, 5921, 5204, 2033, 2206, 2802, 4412, 2928, 441, 1578, 951, 287, 1357, 3761, 4176, 2101, 14, 4299, 2310, 5936, 2073, 2538, 2077, 4628, 4104, 870, 2318, 4447, 2227, 5932, 2736, 5386, 4162, 2918, 5164, 2859, 3804, 1161, 6102, 4881, 246, 5503, 707, 4840, 2917, 1941, 1836, 2607, 763, 5454, 2759, 1115, 530, 1936, 426, 1755, 980, 620, 846, 1949, 1537, 1846, 865, 223, 3686, 4785, 5697, 1021, 4028, 1887, 2100, 2268, 4449, 3133, 3538, 5626, 826, 5008, 1569, 1457, 780, 1194, 2889, 5103, 229, 2460, 2798, 3918, 5680, 89, 493, 5818, 4658, 3594, 2445, 1151, 1535, 1273, 1169, 1422, 1264, 4239, 1543, 834, 5015, 2203, 3425, 4365, 6125, 3170, 1368, 4752, 1820, 3834, 2680, 2112, 4778, 3965, 5116, 1028, 3027, 2715, 4300, 2443, 1574, 2519, 3947, 1649, 144, 1088, 5854, 2792, 5885, 2382, 3163, 1912, 765, 3023, 4013, 2022, 527, 5980, 4482, 3636, 3283, 2067, 2106, 2071, 3242, 1999, 2449, 5893, 1376, 3025, 966, 1204, 5245, 1470, 4185, 4410, 5693, 4614, 3943, 4034, 3384, 4460, 5199, 3759, 1116, 3902, 2861, 3836, 2585, 179, 1697, 5994, 1702, 2817, 4519, 4330, 4480, 4882, 555, 5081, 1279, 3429, 545, 4154, 590, 4135, 5887, 970, 363, 3032, 2419, 4956, 5642, 1345, 1845, 1438, 4775, 6096, 878, 1600, 425, 1335, 1914, 2016, 5523, 3277, 4051, 6021, 1980, 5862, 4566, 450, 3350, 175, 1665, 1969, 524, 1095, 4363, 3118, 3097, 4669, 5460, 3126, 4242, 5702, 5552, 52, 3733, 4142, 2547, 1839, 271, 674, 511, 4248]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [40/50]	lr: 3.13e-05	time: 00:21:01	Loss_train 0.14773	Acc_train 96.81	/	Loss_test 0.14175	Acc_test 81.73
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLe4977f8a-d515-45b7-aaf1-e9dd54f7b531/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [25, 16, 59, 53, 30, 42, 77, 68, 69, 93, 75, 51, 34, 38, 70], 'conv1': [23, 75, 345, 61, 349, 92, 330, 197, 100, 33, 69, 190, 296, 41, 121, 157, 76, 166, 247, 133, 39, 336, 266, 362, 184, 17, 63, 158, 50, 116, 344, 354, 201, 98, 292, 156, 130, 335, 291, 365, 332, 366, 146, 382, 186, 287, 159, 104, 217, 139, 219, 28, 99, 260, 180, 264, 227, 273, 379], 'conv2': [1006, 307, 576, 276, 733, 972, 632, 1411, 320, 301, 350, 1284, 915, 1211, 49, 1502, 503, 1385, 272, 1265, 549, 185, 904, 734, 1413, 326, 984, 468, 764, 186, 551, 122, 969, 347, 1050, 1293, 89, 994, 1185, 697, 261, 1500, 1090, 1532, 930, 1047, 879, 1427, 1290, 519, 973, 167, 1030, 1154, 577, 1086, 786, 1453, 334, 1033, 188, 813, 1400, 294, 241, 796, 627, 1042, 1176, 619, 868, 445, 482, 135, 427, 238, 573, 70, 1239, 953, 726, 138, 1271, 985, 240, 268, 710, 1113, 1326, 1394, 823, 938, 601, 195, 205, 988, 1475, 1184, 1371, 1085, 747, 760, 1465, 617, 377, 1494, 73, 289, 737, 1409, 952, 863, 992, 660, 222, 557, 252, 43, 926, 801, 998, 159, 1493, 1251, 1263, 964, 1029, 844, 539, 1091, 1167, 1066, 177, 108, 1458, 477, 923, 766, 458, 1000, 676, 650, 310, 869, 705, 1129, 13, 269, 260, 75, 1428, 1060, 1195, 571, 562, 1214, 430, 659, 851, 854, 1344, 535, 1213, 777, 369, 187, 1242, 775, 1529, 802, 1225, 97, 779, 126, 640, 833, 174, 595, 158, 194, 1068, 946, 1503, 995, 592, 170, 812, 471, 1307, 486, 1462, 1365, 914, 373, 610, 417, 1327, 128, 1230, 133, 346, 1238, 1253, 910, 44, 1076, 950, 423, 90, 1430, 1324, 348, 1150, 1531, 193, 1412, 171, 1084, 1021, 1032, 1522, 1299, 405, 1471, 958, 1122, 580, 509, 446, 408, 715], 'conv3': [5017, 6047, 1653, 3916, 170, 5041, 2613, 4323, 6098, 2691, 5433, 3067, 4958, 4900, 3563, 1730, 62, 2026, 3435, 2075, 2778, 5255, 4087, 4574, 4983, 634, 5795, 1366, 948, 4473, 4359, 3906, 547, 750, 1229, 5651, 4738, 3197, 5004, 785, 514, 1599, 4502, 2051, 5968, 3718, 1680, 5558, 3938, 353, 5984, 5349, 4768, 815, 5956, 1310, 3516, 4415, 2483, 5292, 2021, 4333, 5515, 4090, 3988, 1149, 2470, 1856, 2392, 549, 4350, 1022, 1921, 999, 4682, 3986, 2462, 921, 3227, 265, 706, 1055, 5541, 4429, 680, 3972, 3828, 1635, 5175, 1587, 1685, 2966, 2696, 1318, 4262, 2037, 4163, 4746, 4699, 2299, 3481, 2398, 3740, 4029, 2413, 5314, 3181, 2804, 837, 3511, 5918, 1575, 3807, 1983, 1778, 63, 3486, 51, 3669, 4924, 1526, 817, 3179, 5498, 6, 1385, 1066, 534, 5733, 3210, 1086, 4357, 2176, 1331, 1670, 4408, 117, 2344, 1181, 3222, 1911, 2427, 2979, 1304, 5752, 2453, 5283, 5188, 745, 5591, 4968, 1344, 3603, 261, 1585, 5, 2786, 5406, 8, 300, 2300, 4905, 2714, 3431, 2661, 5571, 4360, 4556, 3750, 771, 211, 3888, 4486, 813, 2049, 4025, 6114, 1294, 845, 3414, 5312, 5001, 2404, 2769, 4094, 4624, 5065, 4496, 455, 3090, 6070, 3101, 1760, 1037, 1913, 1831, 1036, 1117, 3449, 5919, 418, 5455, 5299, 485, 905, 4733, 5091, 3959, 4739, 183, 2078, 1009, 1043, 4134, 2085, 5717, 3543, 3430, 552, 3830, 1512, 1822, 2914, 5705, 822, 3983, 5860, 688, 4687, 6134, 2201, 5222, 1342, 784, 1112, 1198, 2938, 2041, 4862, 3321, 5579, 5931, 526, 2608, 1872, 1163, 2780, 4218, 4256, 872, 122, 4396, 551, 1437, 5473, 4513, 2584, 1883, 3495, 2136, 5477, 5977, 3159, 1703, 1118, 4538, 1346, 2698, 4397, 3941, 2047, 5531, 2488, 3581, 132, 339, 3502, 2850, 1792, 2418, 5346, 2354, 6054, 3001, 4227, 1790, 4598, 2056, 4100, 3725, 909, 3909, 4451, 4072, 4660, 2635, 1102, 3656, 2581, 176, 1001, 2152, 2444, 3448, 685, 497, 2920, 652, 621, 4390, 3186, 4221, 1097, 2069, 4177, 1620, 4118, 4377, 178, 1615, 1950, 3010, 2092, 2793, 4694, 2103, 5834, 6004, 3905, 632, 844, 4618, 4131, 4063, 1403, 583, 1351, 5469, 1753, 1514, 4602, 4173, 2835, 2750, 3215, 2274, 5457, 3507, 5482, 5981, 3062, 5135, 2847, 2010, 4223, 2662, 3824, 2447, 5847, 165, 5276, 2210, 5493, 129, 198, 5780, 4853, 1544, 2511, 1806, 5910, 4382, 956, 2426, 1443, 1622, 4906, 5146, 4159, 5214, 668, 4876, 1284, 194, 227, 2980, 1187, 6090, 49, 4036, 436, 3470, 6058, 3488, 1594, 5322, 1283, 1864, 4539, 860, 3420, 3138, 818, 1861, 257, 2700, 2139, 2695, 3351, 2379, 4544, 1176, 2342, 2128, 1343, 2514, 4394, 686, 5212, 3895, 3540, 4110, 109, 3312, 893, 5542, 6053, 3473, 4301, 5196, 4937, 1337, 5407, 4219, 2972, 208, 2812, 4266, 4646, 5027, 4938, 3719, 1075, 295, 5436, 78, 5243, 4804, 638, 1564, 1894, 4980, 5929, 5853, 3640, 4964, 5821, 5297, 5201, 398, 5909, 59, 2542, 495, 6119, 4097, 2292, 1560, 2666, 2627, 1873, 3912, 4636, 3436, 26, 1453, 1394, 4947, 5216, 4089, 4166, 1415, 4954, 2083, 1834, 4212, 3809, 4635, 4102, 3866, 2533, 5838, 5412, 221, 6116, 2367, 3944, 1988, 1132, 1380, 1454, 4264, 1499, 5786, 1316, 4306, 3584, 171, 2675, 2667, 5205, 2530, 5108, 5045, 3793, 2703, 1684, 4421, 1191, 294, 2008, 41, 3760, 4280, 2923, 13, 4651, 1309, 1185, 1435, 6122, 2945, 2755, 4114, 1371, 2808, 4319, 2679, 5520, 4897, 6142, 2089, 2563, 5630, 292, 3151, 2668, 4553, 2927, 2156, 5076, 3193, 313, 3670, 3696, 709, 252, 2779, 1108, 1312, 2236, 2098, 2058, 772, 4257, 3330, 5791, 2957, 3569, 65, 5056, 3602, 5739, 1641, 5318, 722, 5445, 3755, 24, 5396, 675, 760, 3873, 673, 926, 3017, 1168, 5354, 4546, 2314, 5607, 259, 3658, 1608, 2007, 4289, 4777, 888, 3434, 4736, 4445, 4107, 1164, 2173, 4431, 576, 3802, 4120, 1879, 3191, 4220, 4076, 4961, 5839, 3150, 5206, 3377, 3096, 255, 5087, 2048, 2393, 1956, 3039, 1162, 4603, 5161, 5268, 491, 2580, 5423, 795, 2597, 463, 5043, 4236, 4291, 1230, 1882, 1978, 1511, 1866, 4728, 5942, 1362, 4879, 2348, 2845, 796, 5582, 4919, 2834, 4402, 6109, 1634, 3735, 6008, 3914, 1960, 5172, 625, 4270, 3556, 4541, 2976, 6089, 2860, 2962, 2082, 3479, 5596, 4688, 5321, 4762, 2711, 3775, 3553, 2251, 4885, 2887, 3657, 1096, 5401, 4640, 5921, 5204, 2033, 2206, 2802, 4412, 2928, 441, 1578, 951, 287, 1357, 3761, 4176, 2101, 14, 4299, 2310, 5936, 2073, 2538, 2077, 4628, 4104, 870, 2318, 4447, 2227, 5932, 2736, 5386, 4162, 2918, 5164, 2859, 3804, 1161, 6102, 4881, 246, 5503, 707, 4840, 2917, 1941, 1836, 2607, 763, 5454, 2759, 1115, 530, 1936, 426, 1755, 980, 620, 846, 1949, 1537, 1846, 865, 223, 3686, 4785, 5697, 1021, 4028, 1887, 2100, 2268, 4449, 3133, 3538, 5626, 826, 5008, 1569, 1457, 780, 1194, 2889, 5103, 229, 2460, 2798, 3918, 5680, 89, 493, 5818, 4658, 3594, 2445, 1151, 1535, 1273, 1169, 1422, 1264, 4239, 1543, 834, 5015, 2203, 3425, 4365, 6125, 3170, 1368, 4752, 1820, 3834, 2680, 2112, 4778, 3965, 5116, 1028, 3027, 2715, 4300, 2443, 1574, 2519, 3947, 1649, 144, 1088, 5854, 2792, 5885, 2382, 3163, 1912, 765, 3023, 4013, 2022, 527, 5980, 4482, 3636, 3283, 2067, 2106, 2071, 3242, 1999, 2449, 5893, 1376, 3025, 966, 1204, 5245, 1470, 4185, 4410, 5693, 4614, 3943, 4034, 3384, 4460, 5199, 3759, 1116, 3902, 2861, 3836, 2585, 179, 1697, 5994, 1702, 2817, 4519, 4330, 4480, 4882, 555, 5081, 1279, 3429, 545, 4154, 590, 4135, 5887, 970, 363, 3032, 2419, 4956, 5642, 1345, 1845, 1438, 4775, 6096, 878, 1600, 425, 1335, 1914, 2016, 5523, 3277, 4051, 6021, 1980, 5862, 4566, 450, 3350, 175, 1665, 1969, 524, 1095, 4363, 3118, 3097, 4669, 5460, 3126, 4242, 5702, 5552, 52, 3733, 4142, 2547, 1839, 271, 674, 511, 4248]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [50/50]	lr: 7.81e-06	time: 00:25:06	Loss_train 0.12021	Acc_train 97.24	/	Loss_test 0.14057	Acc_test 81.90
new_head:  {'blocks.4.layer.bias': tensor([ 0.0099, -0.0016, -0.0140, -0.0056,  0.0170,  0.0083,  0.0102,  0.0137,
        -0.0154,  0.0123], device='cuda:0'), 'blocks.4.layer.weight': tensor([[ 0.1073, -0.0009,  0.0072,  ...,  0.2254,  0.0617,  0.0462],
        [-0.0917, -0.0167, -0.0427,  ..., -0.0005,  0.0644,  0.1190],
        [-0.0364, -0.1114,  0.0774,  ..., -0.0271, -0.0134, -0.0558],
        ...,
        [-0.1048,  0.0533,  0.1243,  ..., -0.0253,  0.0444, -0.1016],
        [-0.0325,  0.1420,  0.0787,  ..., -0.0134,  0.0540,  0.0946],
        [-0.0129, -0.1319, -0.2293,  ..., -0.1601, -0.1963, -0.1403]],
       device='cuda:0')}
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLe4977f8a-d515-45b7-aaf1-e9dd54f7b531/models
SAVED HEADS THRESHOLD:  0.8595000076293946
SAVED model.topk_kernels {'conv0': [25, 16, 59, 53, 30, 42, 77, 68, 69, 93, 75, 51, 34, 38, 70], 'conv1': [23, 75, 345, 61, 349, 92, 330, 197, 100, 33, 69, 190, 296, 41, 121, 157, 76, 166, 247, 133, 39, 336, 266, 362, 184, 17, 63, 158, 50, 116, 344, 354, 201, 98, 292, 156, 130, 335, 291, 365, 332, 366, 146, 382, 186, 287, 159, 104, 217, 139, 219, 28, 99, 260, 180, 264, 227, 273, 379], 'conv2': [1006, 307, 576, 276, 733, 972, 632, 1411, 320, 301, 350, 1284, 915, 1211, 49, 1502, 503, 1385, 272, 1265, 549, 185, 904, 734, 1413, 326, 984, 468, 764, 186, 551, 122, 969, 347, 1050, 1293, 89, 994, 1185, 697, 261, 1500, 1090, 1532, 930, 1047, 879, 1427, 1290, 519, 973, 167, 1030, 1154, 577, 1086, 786, 1453, 334, 1033, 188, 813, 1400, 294, 241, 796, 627, 1042, 1176, 619, 868, 445, 482, 135, 427, 238, 573, 70, 1239, 953, 726, 138, 1271, 985, 240, 268, 710, 1113, 1326, 1394, 823, 938, 601, 195, 205, 988, 1475, 1184, 1371, 1085, 747, 760, 1465, 617, 377, 1494, 73, 289, 737, 1409, 952, 863, 992, 660, 222, 557, 252, 43, 926, 801, 998, 159, 1493, 1251, 1263, 964, 1029, 844, 539, 1091, 1167, 1066, 177, 108, 1458, 477, 923, 766, 458, 1000, 676, 650, 310, 869, 705, 1129, 13, 269, 260, 75, 1428, 1060, 1195, 571, 562, 1214, 430, 659, 851, 854, 1344, 535, 1213, 777, 369, 187, 1242, 775, 1529, 802, 1225, 97, 779, 126, 640, 833, 174, 595, 158, 194, 1068, 946, 1503, 995, 592, 170, 812, 471, 1307, 486, 1462, 1365, 914, 373, 610, 417, 1327, 128, 1230, 133, 346, 1238, 1253, 910, 44, 1076, 950, 423, 90, 1430, 1324, 348, 1150, 1531, 193, 1412, 171, 1084, 1021, 1032, 1522, 1299, 405, 1471, 958, 1122, 580, 509, 446, 408, 715], 'conv3': [5017, 6047, 1653, 3916, 170, 5041, 2613, 4323, 6098, 2691, 5433, 3067, 4958, 4900, 3563, 1730, 62, 2026, 3435, 2075, 2778, 5255, 4087, 4574, 4983, 634, 5795, 1366, 948, 4473, 4359, 3906, 547, 750, 1229, 5651, 4738, 3197, 5004, 785, 514, 1599, 4502, 2051, 5968, 3718, 1680, 5558, 3938, 353, 5984, 5349, 4768, 815, 5956, 1310, 3516, 4415, 2483, 5292, 2021, 4333, 5515, 4090, 3988, 1149, 2470, 1856, 2392, 549, 4350, 1022, 1921, 999, 4682, 3986, 2462, 921, 3227, 265, 706, 1055, 5541, 4429, 680, 3972, 3828, 1635, 5175, 1587, 1685, 2966, 2696, 1318, 4262, 2037, 4163, 4746, 4699, 2299, 3481, 2398, 3740, 4029, 2413, 5314, 3181, 2804, 837, 3511, 5918, 1575, 3807, 1983, 1778, 63, 3486, 51, 3669, 4924, 1526, 817, 3179, 5498, 6, 1385, 1066, 534, 5733, 3210, 1086, 4357, 2176, 1331, 1670, 4408, 117, 2344, 1181, 3222, 1911, 2427, 2979, 1304, 5752, 2453, 5283, 5188, 745, 5591, 4968, 1344, 3603, 261, 1585, 5, 2786, 5406, 8, 300, 2300, 4905, 2714, 3431, 2661, 5571, 4360, 4556, 3750, 771, 211, 3888, 4486, 813, 2049, 4025, 6114, 1294, 845, 3414, 5312, 5001, 2404, 2769, 4094, 4624, 5065, 4496, 455, 3090, 6070, 3101, 1760, 1037, 1913, 1831, 1036, 1117, 3449, 5919, 418, 5455, 5299, 485, 905, 4733, 5091, 3959, 4739, 183, 2078, 1009, 1043, 4134, 2085, 5717, 3543, 3430, 552, 3830, 1512, 1822, 2914, 5705, 822, 3983, 5860, 688, 4687, 6134, 2201, 5222, 1342, 784, 1112, 1198, 2938, 2041, 4862, 3321, 5579, 5931, 526, 2608, 1872, 1163, 2780, 4218, 4256, 872, 122, 4396, 551, 1437, 5473, 4513, 2584, 1883, 3495, 2136, 5477, 5977, 3159, 1703, 1118, 4538, 1346, 2698, 4397, 3941, 2047, 5531, 2488, 3581, 132, 339, 3502, 2850, 1792, 2418, 5346, 2354, 6054, 3001, 4227, 1790, 4598, 2056, 4100, 3725, 909, 3909, 4451, 4072, 4660, 2635, 1102, 3656, 2581, 176, 1001, 2152, 2444, 3448, 685, 497, 2920, 652, 621, 4390, 3186, 4221, 1097, 2069, 4177, 1620, 4118, 4377, 178, 1615, 1950, 3010, 2092, 2793, 4694, 2103, 5834, 6004, 3905, 632, 844, 4618, 4131, 4063, 1403, 583, 1351, 5469, 1753, 1514, 4602, 4173, 2835, 2750, 3215, 2274, 5457, 3507, 5482, 5981, 3062, 5135, 2847, 2010, 4223, 2662, 3824, 2447, 5847, 165, 5276, 2210, 5493, 129, 198, 5780, 4853, 1544, 2511, 1806, 5910, 4382, 956, 2426, 1443, 1622, 4906, 5146, 4159, 5214, 668, 4876, 1284, 194, 227, 2980, 1187, 6090, 49, 4036, 436, 3470, 6058, 3488, 1594, 5322, 1283, 1864, 4539, 860, 3420, 3138, 818, 1861, 257, 2700, 2139, 2695, 3351, 2379, 4544, 1176, 2342, 2128, 1343, 2514, 4394, 686, 5212, 3895, 3540, 4110, 109, 3312, 893, 5542, 6053, 3473, 4301, 5196, 4937, 1337, 5407, 4219, 2972, 208, 2812, 4266, 4646, 5027, 4938, 3719, 1075, 295, 5436, 78, 5243, 4804, 638, 1564, 1894, 4980, 5929, 5853, 3640, 4964, 5821, 5297, 5201, 398, 5909, 59, 2542, 495, 6119, 4097, 2292, 1560, 2666, 2627, 1873, 3912, 4636, 3436, 26, 1453, 1394, 4947, 5216, 4089, 4166, 1415, 4954, 2083, 1834, 4212, 3809, 4635, 4102, 3866, 2533, 5838, 5412, 221, 6116, 2367, 3944, 1988, 1132, 1380, 1454, 4264, 1499, 5786, 1316, 4306, 3584, 171, 2675, 2667, 5205, 2530, 5108, 5045, 3793, 2703, 1684, 4421, 1191, 294, 2008, 41, 3760, 4280, 2923, 13, 4651, 1309, 1185, 1435, 6122, 2945, 2755, 4114, 1371, 2808, 4319, 2679, 5520, 4897, 6142, 2089, 2563, 5630, 292, 3151, 2668, 4553, 2927, 2156, 5076, 3193, 313, 3670, 3696, 709, 252, 2779, 1108, 1312, 2236, 2098, 2058, 772, 4257, 3330, 5791, 2957, 3569, 65, 5056, 3602, 5739, 1641, 5318, 722, 5445, 3755, 24, 5396, 675, 760, 3873, 673, 926, 3017, 1168, 5354, 4546, 2314, 5607, 259, 3658, 1608, 2007, 4289, 4777, 888, 3434, 4736, 4445, 4107, 1164, 2173, 4431, 576, 3802, 4120, 1879, 3191, 4220, 4076, 4961, 5839, 3150, 5206, 3377, 3096, 255, 5087, 2048, 2393, 1956, 3039, 1162, 4603, 5161, 5268, 491, 2580, 5423, 795, 2597, 463, 5043, 4236, 4291, 1230, 1882, 1978, 1511, 1866, 4728, 5942, 1362, 4879, 2348, 2845, 796, 5582, 4919, 2834, 4402, 6109, 1634, 3735, 6008, 3914, 1960, 5172, 625, 4270, 3556, 4541, 2976, 6089, 2860, 2962, 2082, 3479, 5596, 4688, 5321, 4762, 2711, 3775, 3553, 2251, 4885, 2887, 3657, 1096, 5401, 4640, 5921, 5204, 2033, 2206, 2802, 4412, 2928, 441, 1578, 951, 287, 1357, 3761, 4176, 2101, 14, 4299, 2310, 5936, 2073, 2538, 2077, 4628, 4104, 870, 2318, 4447, 2227, 5932, 2736, 5386, 4162, 2918, 5164, 2859, 3804, 1161, 6102, 4881, 246, 5503, 707, 4840, 2917, 1941, 1836, 2607, 763, 5454, 2759, 1115, 530, 1936, 426, 1755, 980, 620, 846, 1949, 1537, 1846, 865, 223, 3686, 4785, 5697, 1021, 4028, 1887, 2100, 2268, 4449, 3133, 3538, 5626, 826, 5008, 1569, 1457, 780, 1194, 2889, 5103, 229, 2460, 2798, 3918, 5680, 89, 493, 5818, 4658, 3594, 2445, 1151, 1535, 1273, 1169, 1422, 1264, 4239, 1543, 834, 5015, 2203, 3425, 4365, 6125, 3170, 1368, 4752, 1820, 3834, 2680, 2112, 4778, 3965, 5116, 1028, 3027, 2715, 4300, 2443, 1574, 2519, 3947, 1649, 144, 1088, 5854, 2792, 5885, 2382, 3163, 1912, 765, 3023, 4013, 2022, 527, 5980, 4482, 3636, 3283, 2067, 2106, 2071, 3242, 1999, 2449, 5893, 1376, 3025, 966, 1204, 5245, 1470, 4185, 4410, 5693, 4614, 3943, 4034, 3384, 4460, 5199, 3759, 1116, 3902, 2861, 3836, 2585, 179, 1697, 5994, 1702, 2817, 4519, 4330, 4480, 4882, 555, 5081, 1279, 3429, 545, 4154, 590, 4135, 5887, 970, 363, 3032, 2419, 4956, 5642, 1345, 1845, 1438, 4775, 6096, 878, 1600, 425, 1335, 1914, 2016, 5523, 3277, 4051, 6021, 1980, 5862, 4566, 450, 3350, 175, 1665, 1969, 524, 1095, 4363, 3118, 3097, 4669, 5460, 3126, 4242, 5702, 5552, 52, 3733, 4142, 2547, 1839, 271, 674, 511, 4248]}
SAVED model.heads_thresh 0.8595000076293946
RESULT:  {'train_loss': 0.12020615488290787, 'train_acc': 97.23520278930664, 'test_loss': 0.14057114720344543, 'test_acc': 81.9000015258789, 'convergence': 17.5535945892334, 'R1': 49, 'dataset_sup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 96}, 'dataset_unsup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 96}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 10, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}}
IN R1:  {'count': 1, 'R0': {'train_loss': 0.03978398069739342, 'train_acc': 98.27600121498108, 'test_loss': 3.0680172443389893, 'test_acc': 66.5625, 'convergence': 23.72207260131836, 'R1': 1, 'dataset_sup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 5000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 4, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': True, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 100, 'print_freq': 10, 'validation': False, 'continual_learning': False}, 'dataset_unsup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 100000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 32, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'unlabeled', 'nb_epoch': 1, 'print_freq': 25, 'validation': False, 'continual_learning': False}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 32, 'nb_epoch': 1, 'print_freq': 25, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 100, 'print_freq': 10, 'lr': 0.001}}}, 'model_config': {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.11048543456039805, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 24576, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}, 'R1': {'train_loss': 0.12020615488290787, 'train_acc': 97.23520278930664, 'test_loss': 0.14057114720344543, 'test_acc': 81.9000015258789, 'convergence': 17.5535945892334, 'R1': 49, 'dataset_sup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 96}, 'dataset_unsup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 96}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 10, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}}}
SEED:  0
block 0, size : 96 48 48
96
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.11048543456039805, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 24576, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 2.886751345948129
block 1, size : 384 24 24
384
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.11048543456039805, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 24576, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.8505172717997146
block 2, size : 1536 12 12
1536
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.11048543456039805, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 24576, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.4252586358998573
block 3, size : 6144 6 6
6144
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.11048543456039805, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 24576, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.21262931794992865
NOWWWW
6 6
10
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.11048543456039805, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 221184, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
blocks['b%s' % 4] 221184 221184
range = 0.036828478186799345
train_layer_order: consecutive {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 32, 'nb_epoch': 1, 'print_freq': 25, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 100, 'print_freq': 10, 'lr': 0.001}}
The device used will be: 
True
cuda:0
PARAMSSSS IN LOAD:  {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.036828478186799345, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 221184, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
avg_deltas:  {'blocks.0.layer.weight': tensor([-0.3549,  0.1212,  0.0095,  0.1285, -0.5902, -0.7595,  0.2930, -0.1311,
        -0.6668,  0.2679,  0.4133,  0.4384,  0.0050,  0.0863,  0.9283, -0.2386,
        -0.4347,  0.0211, -0.5482, -0.5881,  0.3635,  0.7974, -0.0708,  0.0277,
         0.3060,  0.6573, -0.0664, -0.1364,  0.2152, -0.0694,  0.9545,  0.5186,
        -0.4286, -0.7605,  0.7025,  0.9891, -0.6683, -0.0308,  0.8484,  0.0912,
         0.4675,  0.1660,  0.9470, -0.3474,  0.5685, -0.6660, -0.3353,  0.5072,
         0.0084,  0.3467, -0.5717,  0.7976,  0.3361,  0.8629, -0.0436,  0.9182,
        -0.4613,  0.7530, -0.3987,  1.0000, -0.1714, -0.0323, -0.3805,  0.1154,
        -0.7352, -0.3378, -0.5991, -0.0573,  0.9557,  0.9710,  0.9160,  0.3482,
        -0.0344, -0.5976, -0.0085, -0.6056,  0.1304,  0.9179, -0.7200,  0.4949,
        -0.3991,  0.7442, -0.6611,  0.6799,  0.0952, -0.2164, -0.6674, -0.3528,
        -0.0951, -0.7665,  0.0833, -0.1890, -0.2624,  0.9089, -0.3644,  0.4514],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.4675e-01,  4.9634e-01,  4.9947e-01,  3.5167e-01,  6.8275e-01,
        -1.5499e-01,  3.8235e-01, -8.2602e-02,  9.2969e-02,  3.4503e-01,
        -6.9911e-02,  6.8859e-01,  4.4754e-01,  4.0567e-01,  5.8537e-01,
         4.8758e-01,  2.1827e-01,  5.5889e-01, -3.6301e-01, -2.1951e-01,
         2.5134e-01,  3.0956e-01,  7.9425e-01,  6.6511e-01,  7.1393e-01,
         8.6881e-01,  1.8570e-01, -8.7449e-02,  6.5183e-02,  5.4472e-01,
         2.1367e-01, -3.4635e-01, -3.3732e-01,  1.6719e-01,  1.6866e-01,
         1.5956e-01,  3.7950e-01,  3.3845e-01,  5.1705e-02,  3.7345e-01,
         4.0139e-01,  3.6994e-01,  2.8768e-01,  3.8171e-01,  7.6911e-01,
         3.2752e-01, -3.0061e-01,  3.1377e-01, -9.3224e-02, -1.2043e-01,
         4.4630e-01,  4.6442e-01, -6.7174e-02, -1.0307e-01, -8.5593e-02,
         4.1887e-01,  4.8836e-01, -2.3651e-01,  3.4604e-01,  9.2028e-02,
         1.6476e-01, -6.3855e-01,  7.0815e-01, -4.7516e-01, -1.3248e-01,
         2.0273e-01, -9.3000e-02,  8.0644e-01,  3.1913e-01, -1.4491e-02,
         1.5819e-01,  5.9817e-02,  1.0919e-01,  5.0951e-01, -3.8428e-02,
         1.5758e-01,  5.2751e-01, -4.7671e-01, -4.2823e-01,  4.0300e-01,
         1.2542e-01,  3.8350e-01,  3.7314e-01,  2.1977e-01,  6.2835e-01,
         1.7641e-01,  4.4517e-01,  6.0281e-02,  1.0899e-01,  8.3355e-01,
         2.6046e-01,  2.2819e-01,  2.6363e-01,  3.1669e-01,  7.0682e-01,
        -1.5943e-01,  5.2125e-01,  1.8858e-01, -3.3286e-01,  2.0695e-01,
         6.1562e-01,  3.3932e-01,  2.5271e-01,  1.3981e-01,  1.2687e-01,
         3.1925e-01,  1.8174e-01, -9.4531e-02,  3.6838e-02,  3.7488e-01,
         1.2717e-01,  4.7130e-01, -1.5721e-01, -7.7476e-02,  8.1014e-01,
        -1.5593e-01, -1.0054e-01,  4.5635e-01,  6.1519e-01,  1.4677e-01,
        -2.2707e-01, -5.5761e-03,  2.6145e-01,  5.9044e-01,  2.2731e-01,
        -2.6443e-02, -2.3644e-03, -1.0726e-01,  7.7391e-01,  3.4043e-01,
         2.8069e-01,  4.8368e-01,  4.9516e-01,  2.3034e-01,  6.8924e-01,
         3.1027e-01,  3.4570e-01,  9.7788e-01,  3.3127e-01,  3.5599e-01,
         5.0713e-01,  7.2506e-01,  6.5209e-01, -4.4852e-02,  4.7526e-01,
         7.1132e-01,  8.4672e-01, -7.5696e-03,  4.1974e-01,  2.1462e-01,
         1.7083e-01, -1.3488e-01,  4.1226e-01,  2.3258e-01,  2.5389e-01,
        -1.5678e-01, -5.3553e-01, -4.7307e-01,  2.7626e-01, -2.5093e-01,
         4.0266e-02,  2.4575e-01,  1.8999e-01, -6.1258e-03, -6.5438e-03,
         4.2919e-01, -1.2408e-01,  1.9037e-01,  5.1006e-01, -6.9298e-02,
         1.7477e-01,  3.5078e-01,  7.3873e-01,  5.5692e-01,  4.3353e-01,
        -2.2003e-01,  3.3680e-01,  6.3033e-01,  2.5604e-01,  3.2412e-01,
         6.3429e-02,  4.2685e-01,  3.0266e-01,  4.5454e-01,  3.0221e-02,
         4.9645e-01,  4.2592e-01,  2.2569e-01,  3.2492e-01, -1.9558e-01,
         6.3491e-01,  1.4616e-01,  2.0872e-01,  1.7695e-01,  3.4186e-01,
         7.6387e-01,  2.4154e-01,  3.4131e-01,  2.5706e-01, -1.7839e-01,
         4.7458e-01, -6.1654e-02, -4.2599e-02,  8.0520e-01,  1.1602e-01,
         4.0244e-01,  5.3733e-01,  1.0871e-01,  2.4645e-02,  7.1771e-01,
        -1.1157e-01,  2.2285e-01,  3.2628e-01,  1.0782e-01,  1.5037e-01,
         4.8782e-01,  4.4199e-01,  8.0079e-01, -1.9772e-01,  4.1425e-01,
         3.3853e-01,  7.4381e-01, -1.5043e-01,  5.4073e-02,  5.8050e-01,
        -2.5929e-02,  8.0517e-01, -3.4948e-01,  2.4886e-01,  5.5591e-01,
         1.8956e-01,  1.2973e-01,  7.4482e-02,  1.6208e-01,  5.5959e-01,
        -8.3063e-02,  7.0622e-01,  4.5518e-01,  4.7128e-01,  7.8142e-01,
        -2.2690e-02,  8.9799e-02,  1.5444e-01, -9.2526e-04,  1.1149e-01,
        -7.5147e-03,  5.6785e-01,  8.0659e-02,  5.8024e-01,  8.5778e-02,
         6.8099e-01,  2.7963e-01,  9.8910e-02, -1.4123e-01, -2.2274e-01,
         3.2634e-01,  4.8895e-01,  2.9214e-01, -1.3606e-01,  2.9738e-01,
         1.8581e-01,  1.2835e-01,  8.0524e-01,  5.1709e-01,  5.6002e-02,
         3.1847e-01, -5.6267e-01,  5.3282e-01,  4.8991e-01,  1.9676e-01,
        -2.5985e-02,  6.1749e-01,  3.1766e-01,  1.6751e-01,  4.4369e-01,
         1.5407e-01,  5.1959e-01,  4.5052e-01,  3.0881e-01,  2.6712e-02,
         1.2838e-01,  7.2508e-01,  4.6165e-01,  2.0328e-01,  2.4748e-01,
         4.8103e-01,  1.8086e-01, -7.3423e-02,  2.9468e-01, -8.2690e-02,
         7.6233e-02, -2.6612e-01,  3.9660e-01,  2.3032e-01,  4.0978e-02,
        -2.4679e-01, -1.2860e-01,  1.0342e-01,  5.5879e-01,  3.9662e-01,
        -1.4361e-01, -1.9822e-01,  5.5379e-01, -3.5044e-02,  6.5483e-01,
         4.9469e-01,  4.6768e-01,  9.9884e-02,  4.0240e-01,  8.2842e-02,
         8.1476e-02,  1.0099e-01,  6.6033e-01,  1.2213e-01,  6.6229e-01,
         6.9984e-02,  4.2777e-01,  7.7067e-01,  1.1222e-02,  1.7325e-01,
         1.4918e-01,  4.4225e-01,  7.1020e-01, -6.4827e-02, -1.0691e-01,
         4.2094e-01,  9.3253e-01,  7.4832e-01,  2.0421e-01,  8.0857e-01,
        -6.9927e-01,  6.9038e-01, -1.4160e-01,  2.9397e-01,  1.8431e-01,
         3.3021e-01,  3.0792e-01,  2.0105e-01,  5.4197e-01,  3.6271e-01,
         3.5776e-01,  2.2193e-01,  6.9557e-02,  8.7006e-01, -2.0600e-01,
        -3.4507e-03,  1.8695e-01,  1.0000e+00,  2.9092e-01, -2.6603e-01,
         2.7547e-01,  7.8514e-02,  1.6481e-01,  4.8246e-01,  5.4929e-01,
        -1.7485e-01,  3.9021e-02,  4.1384e-01,  5.1890e-01, -4.6565e-02,
         3.2173e-01,  1.8334e-01,  1.8262e-01,  1.4842e-01,  6.1002e-01,
         2.9751e-02,  1.2533e-01,  5.3216e-01,  2.0576e-01, -4.0360e-02,
         3.6691e-01,  8.1207e-01,  3.9485e-01,  1.3495e-01,  1.4429e-04,
         3.4063e-01,  3.1047e-02,  3.8775e-01,  2.6841e-01,  5.2225e-01,
         5.2153e-01,  5.3369e-01,  1.9615e-01,  3.0179e-01], device='cuda:0'), 'blocks.2.layer.weight': tensor([-0.0010,  0.0317,  0.4703,  ...,  0.2049, -0.0204,  0.3651],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([0.2656, 0.5401, 0.7364,  ..., 0.3438, 0.5377, 0.2040], device='cuda:0')}
topk_layer:  None
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-0.3549,  0.1212,  0.0095,  0.1285, -0.5902, -0.7595,  0.2930, -0.1311,
        -0.6668,  0.2679,  0.4133,  0.4384,  0.0050,  0.0863,  0.9283, -0.2386,
        -0.4347,  0.0211, -0.5482, -0.5881,  0.3635,  0.7974, -0.0708,  0.0277,
         0.3060,  0.6573, -0.0664, -0.1364,  0.2152, -0.0694,  0.9545,  0.5186,
        -0.4286, -0.7605,  0.7025,  0.9891, -0.6683, -0.0308,  0.8484,  0.0912,
         0.4675,  0.1660,  0.9470, -0.3474,  0.5685, -0.6660, -0.3353,  0.5072,
         0.0084,  0.3467, -0.5717,  0.7976,  0.3361,  0.8629, -0.0436,  0.9182,
        -0.4613,  0.7530, -0.3987,  1.0000, -0.1714, -0.0323, -0.3805,  0.1154,
        -0.7352, -0.3378, -0.5991, -0.0573,  0.9557,  0.9710,  0.9160,  0.3482,
        -0.0344, -0.5976, -0.0085, -0.6056,  0.1304,  0.9179, -0.7200,  0.4949,
        -0.3991,  0.7442, -0.6611,  0.6799,  0.0952, -0.2164, -0.6674, -0.3528,
        -0.0951, -0.7665,  0.0833, -0.1890, -0.2624,  0.9089, -0.3644,  0.4514],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.4675e-01,  4.9634e-01,  4.9947e-01,  3.5167e-01,  6.8275e-01,
        -1.5499e-01,  3.8235e-01, -8.2602e-02,  9.2969e-02,  3.4503e-01,
        -6.9911e-02,  6.8859e-01,  4.4754e-01,  4.0567e-01,  5.8537e-01,
         4.8758e-01,  2.1827e-01,  5.5889e-01, -3.6301e-01, -2.1951e-01,
         2.5134e-01,  3.0956e-01,  7.9425e-01,  6.6511e-01,  7.1393e-01,
         8.6881e-01,  1.8570e-01, -8.7449e-02,  6.5183e-02,  5.4472e-01,
         2.1367e-01, -3.4635e-01, -3.3732e-01,  1.6719e-01,  1.6866e-01,
         1.5956e-01,  3.7950e-01,  3.3845e-01,  5.1705e-02,  3.7345e-01,
         4.0139e-01,  3.6994e-01,  2.8768e-01,  3.8171e-01,  7.6911e-01,
         3.2752e-01, -3.0061e-01,  3.1377e-01, -9.3224e-02, -1.2043e-01,
         4.4630e-01,  4.6442e-01, -6.7174e-02, -1.0307e-01, -8.5593e-02,
         4.1887e-01,  4.8836e-01, -2.3651e-01,  3.4604e-01,  9.2028e-02,
         1.6476e-01, -6.3855e-01,  7.0815e-01, -4.7516e-01, -1.3248e-01,
         2.0273e-01, -9.3000e-02,  8.0644e-01,  3.1913e-01, -1.4491e-02,
         1.5819e-01,  5.9817e-02,  1.0919e-01,  5.0951e-01, -3.8428e-02,
         1.5758e-01,  5.2751e-01, -4.7671e-01, -4.2823e-01,  4.0300e-01,
         1.2542e-01,  3.8350e-01,  3.7314e-01,  2.1977e-01,  6.2835e-01,
         1.7641e-01,  4.4517e-01,  6.0281e-02,  1.0899e-01,  8.3355e-01,
         2.6046e-01,  2.2819e-01,  2.6363e-01,  3.1669e-01,  7.0682e-01,
        -1.5943e-01,  5.2125e-01,  1.8858e-01, -3.3286e-01,  2.0695e-01,
         6.1562e-01,  3.3932e-01,  2.5271e-01,  1.3981e-01,  1.2687e-01,
         3.1925e-01,  1.8174e-01, -9.4531e-02,  3.6838e-02,  3.7488e-01,
         1.2717e-01,  4.7130e-01, -1.5721e-01, -7.7476e-02,  8.1014e-01,
        -1.5593e-01, -1.0054e-01,  4.5635e-01,  6.1519e-01,  1.4677e-01,
        -2.2707e-01, -5.5761e-03,  2.6145e-01,  5.9044e-01,  2.2731e-01,
        -2.6443e-02, -2.3644e-03, -1.0726e-01,  7.7391e-01,  3.4043e-01,
         2.8069e-01,  4.8368e-01,  4.9516e-01,  2.3034e-01,  6.8924e-01,
         3.1027e-01,  3.4570e-01,  9.7788e-01,  3.3127e-01,  3.5599e-01,
         5.0713e-01,  7.2506e-01,  6.5209e-01, -4.4852e-02,  4.7526e-01,
         7.1132e-01,  8.4672e-01, -7.5696e-03,  4.1974e-01,  2.1462e-01,
         1.7083e-01, -1.3488e-01,  4.1226e-01,  2.3258e-01,  2.5389e-01,
        -1.5678e-01, -5.3553e-01, -4.7307e-01,  2.7626e-01, -2.5093e-01,
         4.0266e-02,  2.4575e-01,  1.8999e-01, -6.1258e-03, -6.5438e-03,
         4.2919e-01, -1.2408e-01,  1.9037e-01,  5.1006e-01, -6.9298e-02,
         1.7477e-01,  3.5078e-01,  7.3873e-01,  5.5692e-01,  4.3353e-01,
        -2.2003e-01,  3.3680e-01,  6.3033e-01,  2.5604e-01,  3.2412e-01,
         6.3429e-02,  4.2685e-01,  3.0266e-01,  4.5454e-01,  3.0221e-02,
         4.9645e-01,  4.2592e-01,  2.2569e-01,  3.2492e-01, -1.9558e-01,
         6.3491e-01,  1.4616e-01,  2.0872e-01,  1.7695e-01,  3.4186e-01,
         7.6387e-01,  2.4154e-01,  3.4131e-01,  2.5706e-01, -1.7839e-01,
         4.7458e-01, -6.1654e-02, -4.2599e-02,  8.0520e-01,  1.1602e-01,
         4.0244e-01,  5.3733e-01,  1.0871e-01,  2.4645e-02,  7.1771e-01,
        -1.1157e-01,  2.2285e-01,  3.2628e-01,  1.0782e-01,  1.5037e-01,
         4.8782e-01,  4.4199e-01,  8.0079e-01, -1.9772e-01,  4.1425e-01,
         3.3853e-01,  7.4381e-01, -1.5043e-01,  5.4073e-02,  5.8050e-01,
        -2.5929e-02,  8.0517e-01, -3.4948e-01,  2.4886e-01,  5.5591e-01,
         1.8956e-01,  1.2973e-01,  7.4482e-02,  1.6208e-01,  5.5959e-01,
        -8.3063e-02,  7.0622e-01,  4.5518e-01,  4.7128e-01,  7.8142e-01,
        -2.2690e-02,  8.9799e-02,  1.5444e-01, -9.2526e-04,  1.1149e-01,
        -7.5147e-03,  5.6785e-01,  8.0659e-02,  5.8024e-01,  8.5778e-02,
         6.8099e-01,  2.7963e-01,  9.8910e-02, -1.4123e-01, -2.2274e-01,
         3.2634e-01,  4.8895e-01,  2.9214e-01, -1.3606e-01,  2.9738e-01,
         1.8581e-01,  1.2835e-01,  8.0524e-01,  5.1709e-01,  5.6002e-02,
         3.1847e-01, -5.6267e-01,  5.3282e-01,  4.8991e-01,  1.9676e-01,
        -2.5985e-02,  6.1749e-01,  3.1766e-01,  1.6751e-01,  4.4369e-01,
         1.5407e-01,  5.1959e-01,  4.5052e-01,  3.0881e-01,  2.6712e-02,
         1.2838e-01,  7.2508e-01,  4.6165e-01,  2.0328e-01,  2.4748e-01,
         4.8103e-01,  1.8086e-01, -7.3423e-02,  2.9468e-01, -8.2690e-02,
         7.6233e-02, -2.6612e-01,  3.9660e-01,  2.3032e-01,  4.0978e-02,
        -2.4679e-01, -1.2860e-01,  1.0342e-01,  5.5879e-01,  3.9662e-01,
        -1.4361e-01, -1.9822e-01,  5.5379e-01, -3.5044e-02,  6.5483e-01,
         4.9469e-01,  4.6768e-01,  9.9884e-02,  4.0240e-01,  8.2842e-02,
         8.1476e-02,  1.0099e-01,  6.6033e-01,  1.2213e-01,  6.6229e-01,
         6.9984e-02,  4.2777e-01,  7.7067e-01,  1.1222e-02,  1.7325e-01,
         1.4918e-01,  4.4225e-01,  7.1020e-01, -6.4827e-02, -1.0691e-01,
         4.2094e-01,  9.3253e-01,  7.4832e-01,  2.0421e-01,  8.0857e-01,
        -6.9927e-01,  6.9038e-01, -1.4160e-01,  2.9397e-01,  1.8431e-01,
         3.3021e-01,  3.0792e-01,  2.0105e-01,  5.4197e-01,  3.6271e-01,
         3.5776e-01,  2.2193e-01,  6.9557e-02,  8.7006e-01, -2.0600e-01,
        -3.4507e-03,  1.8695e-01,  1.0000e+00,  2.9092e-01, -2.6603e-01,
         2.7547e-01,  7.8514e-02,  1.6481e-01,  4.8246e-01,  5.4929e-01,
        -1.7485e-01,  3.9021e-02,  4.1384e-01,  5.1890e-01, -4.6565e-02,
         3.2173e-01,  1.8334e-01,  1.8262e-01,  1.4842e-01,  6.1002e-01,
         2.9751e-02,  1.2533e-01,  5.3216e-01,  2.0576e-01, -4.0360e-02,
         3.6691e-01,  8.1207e-01,  3.9485e-01,  1.3495e-01,  1.4429e-04,
         3.4063e-01,  3.1047e-02,  3.8775e-01,  2.6841e-01,  5.2225e-01,
         5.2153e-01,  5.3369e-01,  1.9615e-01,  3.0179e-01], device='cuda:0'), 'blocks.2.layer.weight': tensor([-0.0010,  0.0317,  0.4703,  ...,  0.2049, -0.0204,  0.3651],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([0.2656, 0.5401, 0.7364,  ..., 0.3438, 0.5377, 0.2040], device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.0.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([-0.3549,  0.1212,  0.0095,  0.1285, -0.5902, -0.7595,  0.2930, -0.1311,
        -0.6668,  0.2679,  0.4133,  0.4384,  0.0050,  0.0863,  0.9283, -0.2386,
        -0.4347,  0.0211, -0.5482, -0.5881,  0.3635,  0.7974, -0.0708,  0.0277,
         0.3060,  0.6573, -0.0664, -0.1364,  0.2152, -0.0694,  0.9545,  0.5186,
        -0.4286, -0.7605,  0.7025,  0.9891, -0.6683, -0.0308,  0.8484,  0.0912,
         0.4675,  0.1660,  0.9470, -0.3474,  0.5685, -0.6660, -0.3353,  0.5072,
         0.0084,  0.3467, -0.5717,  0.7976,  0.3361,  0.8629, -0.0436,  0.9182,
        -0.4613,  0.7530, -0.3987,  1.0000, -0.1714, -0.0323, -0.3805,  0.1154,
        -0.7352, -0.3378, -0.5991, -0.0573,  0.9557,  0.9710,  0.9160,  0.3482,
        -0.0344, -0.5976, -0.0085, -0.6056,  0.1304,  0.9179, -0.7200,  0.4949,
        -0.3991,  0.7442, -0.6611,  0.6799,  0.0952, -0.2164, -0.6674, -0.3528,
        -0.0951, -0.7665,  0.0833, -0.1890, -0.2624,  0.9089, -0.3644,  0.4514],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-0.3549,  0.1212,  0.0095,  0.1285, -0.5902, -0.7595,  0.2930, -0.1311,
        -0.6668,  0.2679,  0.4133,  0.4384,  0.0050,  0.0863,  0.9283, -0.2386,
        -0.4347,  0.0211, -0.5482, -0.5881,  0.3635,  0.7974, -0.0708,  0.0277,
         0.3060,  0.6573, -0.0664, -0.1364,  0.2152, -0.0694,  0.9545,  0.5186,
        -0.4286, -0.7605,  0.7025,  0.9891, -0.6683, -0.0308,  0.8484,  0.0912,
         0.4675,  0.1660,  0.9470, -0.3474,  0.5685, -0.6660, -0.3353,  0.5072,
         0.0084,  0.3467, -0.5717,  0.7976,  0.3361,  0.8629, -0.0436,  0.9182,
        -0.4613,  0.7530, -0.3987,  1.0000, -0.1714, -0.0323, -0.3805,  0.1154,
        -0.7352, -0.3378, -0.5991, -0.0573,  0.9557,  0.9710,  0.9160,  0.3482,
        -0.0344, -0.5976, -0.0085, -0.6056,  0.1304,  0.9179, -0.7200,  0.4949,
        -0.3991,  0.7442, -0.6611,  0.6799,  0.0952, -0.2164, -0.6674, -0.3528,
        -0.0951, -0.7665,  0.0833, -0.1890, -0.2624,  0.9089, -0.3644,  0.4514],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.4675e-01,  4.9634e-01,  4.9947e-01,  3.5167e-01,  6.8275e-01,
        -1.5499e-01,  3.8235e-01, -8.2602e-02,  9.2969e-02,  3.4503e-01,
        -6.9911e-02,  6.8859e-01,  4.4754e-01,  4.0567e-01,  5.8537e-01,
         4.8758e-01,  2.1827e-01,  5.5889e-01, -3.6301e-01, -2.1951e-01,
         2.5134e-01,  3.0956e-01,  7.9425e-01,  6.6511e-01,  7.1393e-01,
         8.6881e-01,  1.8570e-01, -8.7449e-02,  6.5183e-02,  5.4472e-01,
         2.1367e-01, -3.4635e-01, -3.3732e-01,  1.6719e-01,  1.6866e-01,
         1.5956e-01,  3.7950e-01,  3.3845e-01,  5.1705e-02,  3.7345e-01,
         4.0139e-01,  3.6994e-01,  2.8768e-01,  3.8171e-01,  7.6911e-01,
         3.2752e-01, -3.0061e-01,  3.1377e-01, -9.3224e-02, -1.2043e-01,
         4.4630e-01,  4.6442e-01, -6.7174e-02, -1.0307e-01, -8.5593e-02,
         4.1887e-01,  4.8836e-01, -2.3651e-01,  3.4604e-01,  9.2028e-02,
         1.6476e-01, -6.3855e-01,  7.0815e-01, -4.7516e-01, -1.3248e-01,
         2.0273e-01, -9.3000e-02,  8.0644e-01,  3.1913e-01, -1.4491e-02,
         1.5819e-01,  5.9817e-02,  1.0919e-01,  5.0951e-01, -3.8428e-02,
         1.5758e-01,  5.2751e-01, -4.7671e-01, -4.2823e-01,  4.0300e-01,
         1.2542e-01,  3.8350e-01,  3.7314e-01,  2.1977e-01,  6.2835e-01,
         1.7641e-01,  4.4517e-01,  6.0281e-02,  1.0899e-01,  8.3355e-01,
         2.6046e-01,  2.2819e-01,  2.6363e-01,  3.1669e-01,  7.0682e-01,
        -1.5943e-01,  5.2125e-01,  1.8858e-01, -3.3286e-01,  2.0695e-01,
         6.1562e-01,  3.3932e-01,  2.5271e-01,  1.3981e-01,  1.2687e-01,
         3.1925e-01,  1.8174e-01, -9.4531e-02,  3.6838e-02,  3.7488e-01,
         1.2717e-01,  4.7130e-01, -1.5721e-01, -7.7476e-02,  8.1014e-01,
        -1.5593e-01, -1.0054e-01,  4.5635e-01,  6.1519e-01,  1.4677e-01,
        -2.2707e-01, -5.5761e-03,  2.6145e-01,  5.9044e-01,  2.2731e-01,
        -2.6443e-02, -2.3644e-03, -1.0726e-01,  7.7391e-01,  3.4043e-01,
         2.8069e-01,  4.8368e-01,  4.9516e-01,  2.3034e-01,  6.8924e-01,
         3.1027e-01,  3.4570e-01,  9.7788e-01,  3.3127e-01,  3.5599e-01,
         5.0713e-01,  7.2506e-01,  6.5209e-01, -4.4852e-02,  4.7526e-01,
         7.1132e-01,  8.4672e-01, -7.5696e-03,  4.1974e-01,  2.1462e-01,
         1.7083e-01, -1.3488e-01,  4.1226e-01,  2.3258e-01,  2.5389e-01,
        -1.5678e-01, -5.3553e-01, -4.7307e-01,  2.7626e-01, -2.5093e-01,
         4.0266e-02,  2.4575e-01,  1.8999e-01, -6.1258e-03, -6.5438e-03,
         4.2919e-01, -1.2408e-01,  1.9037e-01,  5.1006e-01, -6.9298e-02,
         1.7477e-01,  3.5078e-01,  7.3873e-01,  5.5692e-01,  4.3353e-01,
        -2.2003e-01,  3.3680e-01,  6.3033e-01,  2.5604e-01,  3.2412e-01,
         6.3429e-02,  4.2685e-01,  3.0266e-01,  4.5454e-01,  3.0221e-02,
         4.9645e-01,  4.2592e-01,  2.2569e-01,  3.2492e-01, -1.9558e-01,
         6.3491e-01,  1.4616e-01,  2.0872e-01,  1.7695e-01,  3.4186e-01,
         7.6387e-01,  2.4154e-01,  3.4131e-01,  2.5706e-01, -1.7839e-01,
         4.7458e-01, -6.1654e-02, -4.2599e-02,  8.0520e-01,  1.1602e-01,
         4.0244e-01,  5.3733e-01,  1.0871e-01,  2.4645e-02,  7.1771e-01,
        -1.1157e-01,  2.2285e-01,  3.2628e-01,  1.0782e-01,  1.5037e-01,
         4.8782e-01,  4.4199e-01,  8.0079e-01, -1.9772e-01,  4.1425e-01,
         3.3853e-01,  7.4381e-01, -1.5043e-01,  5.4073e-02,  5.8050e-01,
        -2.5929e-02,  8.0517e-01, -3.4948e-01,  2.4886e-01,  5.5591e-01,
         1.8956e-01,  1.2973e-01,  7.4482e-02,  1.6208e-01,  5.5959e-01,
        -8.3063e-02,  7.0622e-01,  4.5518e-01,  4.7128e-01,  7.8142e-01,
        -2.2690e-02,  8.9799e-02,  1.5444e-01, -9.2526e-04,  1.1149e-01,
        -7.5147e-03,  5.6785e-01,  8.0659e-02,  5.8024e-01,  8.5778e-02,
         6.8099e-01,  2.7963e-01,  9.8910e-02, -1.4123e-01, -2.2274e-01,
         3.2634e-01,  4.8895e-01,  2.9214e-01, -1.3606e-01,  2.9738e-01,
         1.8581e-01,  1.2835e-01,  8.0524e-01,  5.1709e-01,  5.6002e-02,
         3.1847e-01, -5.6267e-01,  5.3282e-01,  4.8991e-01,  1.9676e-01,
        -2.5985e-02,  6.1749e-01,  3.1766e-01,  1.6751e-01,  4.4369e-01,
         1.5407e-01,  5.1959e-01,  4.5052e-01,  3.0881e-01,  2.6712e-02,
         1.2838e-01,  7.2508e-01,  4.6165e-01,  2.0328e-01,  2.4748e-01,
         4.8103e-01,  1.8086e-01, -7.3423e-02,  2.9468e-01, -8.2690e-02,
         7.6233e-02, -2.6612e-01,  3.9660e-01,  2.3032e-01,  4.0978e-02,
        -2.4679e-01, -1.2860e-01,  1.0342e-01,  5.5879e-01,  3.9662e-01,
        -1.4361e-01, -1.9822e-01,  5.5379e-01, -3.5044e-02,  6.5483e-01,
         4.9469e-01,  4.6768e-01,  9.9884e-02,  4.0240e-01,  8.2842e-02,
         8.1476e-02,  1.0099e-01,  6.6033e-01,  1.2213e-01,  6.6229e-01,
         6.9984e-02,  4.2777e-01,  7.7067e-01,  1.1222e-02,  1.7325e-01,
         1.4918e-01,  4.4225e-01,  7.1020e-01, -6.4827e-02, -1.0691e-01,
         4.2094e-01,  9.3253e-01,  7.4832e-01,  2.0421e-01,  8.0857e-01,
        -6.9927e-01,  6.9038e-01, -1.4160e-01,  2.9397e-01,  1.8431e-01,
         3.3021e-01,  3.0792e-01,  2.0105e-01,  5.4197e-01,  3.6271e-01,
         3.5776e-01,  2.2193e-01,  6.9557e-02,  8.7006e-01, -2.0600e-01,
        -3.4507e-03,  1.8695e-01,  1.0000e+00,  2.9092e-01, -2.6603e-01,
         2.7547e-01,  7.8514e-02,  1.6481e-01,  4.8246e-01,  5.4929e-01,
        -1.7485e-01,  3.9021e-02,  4.1384e-01,  5.1890e-01, -4.6565e-02,
         3.2173e-01,  1.8334e-01,  1.8262e-01,  1.4842e-01,  6.1002e-01,
         2.9751e-02,  1.2533e-01,  5.3216e-01,  2.0576e-01, -4.0360e-02,
         3.6691e-01,  8.1207e-01,  3.9485e-01,  1.3495e-01,  1.4429e-04,
         3.4063e-01,  3.1047e-02,  3.8775e-01,  2.6841e-01,  5.2225e-01,
         5.2153e-01,  5.3369e-01,  1.9615e-01,  3.0179e-01], device='cuda:0'), 'blocks.2.layer.weight': tensor([-0.0010,  0.0317,  0.4703,  ...,  0.2049, -0.0204,  0.3651],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([0.2656, 0.5401, 0.7364,  ..., 0.3438, 0.5377, 0.2040], device='cuda:0')}
topk_layer inside model after retrieval:  [25, 16, 59, 53, 30, 42, 77, 68, 69, 93, 75, 51, 34, 38, 70]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-0.3549,  0.1212,  0.0095,  0.1285, -0.5902, -0.7595,  0.2930, -0.1311,
        -0.6668,  0.2679,  0.4133,  0.4384,  0.0050,  0.0863,  0.9283, -0.2386,
        -0.4347,  0.0211, -0.5482, -0.5881,  0.3635,  0.7974, -0.0708,  0.0277,
         0.3060,  0.6573, -0.0664, -0.1364,  0.2152, -0.0694,  0.9545,  0.5186,
        -0.4286, -0.7605,  0.7025,  0.9891, -0.6683, -0.0308,  0.8484,  0.0912,
         0.4675,  0.1660,  0.9470, -0.3474,  0.5685, -0.6660, -0.3353,  0.5072,
         0.0084,  0.3467, -0.5717,  0.7976,  0.3361,  0.8629, -0.0436,  0.9182,
        -0.4613,  0.7530, -0.3987,  1.0000, -0.1714, -0.0323, -0.3805,  0.1154,
        -0.7352, -0.3378, -0.5991, -0.0573,  0.9557,  0.9710,  0.9160,  0.3482,
        -0.0344, -0.5976, -0.0085, -0.6056,  0.1304,  0.9179, -0.7200,  0.4949,
        -0.3991,  0.7442, -0.6611,  0.6799,  0.0952, -0.2164, -0.6674, -0.3528,
        -0.0951, -0.7665,  0.0833, -0.1890, -0.2624,  0.9089, -0.3644,  0.4514],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.4675e-01,  4.9634e-01,  4.9947e-01,  3.5167e-01,  6.8275e-01,
        -1.5499e-01,  3.8235e-01, -8.2602e-02,  9.2969e-02,  3.4503e-01,
        -6.9911e-02,  6.8859e-01,  4.4754e-01,  4.0567e-01,  5.8537e-01,
         4.8758e-01,  2.1827e-01,  5.5889e-01, -3.6301e-01, -2.1951e-01,
         2.5134e-01,  3.0956e-01,  7.9425e-01,  6.6511e-01,  7.1393e-01,
         8.6881e-01,  1.8570e-01, -8.7449e-02,  6.5183e-02,  5.4472e-01,
         2.1367e-01, -3.4635e-01, -3.3732e-01,  1.6719e-01,  1.6866e-01,
         1.5956e-01,  3.7950e-01,  3.3845e-01,  5.1705e-02,  3.7345e-01,
         4.0139e-01,  3.6994e-01,  2.8768e-01,  3.8171e-01,  7.6911e-01,
         3.2752e-01, -3.0061e-01,  3.1377e-01, -9.3224e-02, -1.2043e-01,
         4.4630e-01,  4.6442e-01, -6.7174e-02, -1.0307e-01, -8.5593e-02,
         4.1887e-01,  4.8836e-01, -2.3651e-01,  3.4604e-01,  9.2028e-02,
         1.6476e-01, -6.3855e-01,  7.0815e-01, -4.7516e-01, -1.3248e-01,
         2.0273e-01, -9.3000e-02,  8.0644e-01,  3.1913e-01, -1.4491e-02,
         1.5819e-01,  5.9817e-02,  1.0919e-01,  5.0951e-01, -3.8428e-02,
         1.5758e-01,  5.2751e-01, -4.7671e-01, -4.2823e-01,  4.0300e-01,
         1.2542e-01,  3.8350e-01,  3.7314e-01,  2.1977e-01,  6.2835e-01,
         1.7641e-01,  4.4517e-01,  6.0281e-02,  1.0899e-01,  8.3355e-01,
         2.6046e-01,  2.2819e-01,  2.6363e-01,  3.1669e-01,  7.0682e-01,
        -1.5943e-01,  5.2125e-01,  1.8858e-01, -3.3286e-01,  2.0695e-01,
         6.1562e-01,  3.3932e-01,  2.5271e-01,  1.3981e-01,  1.2687e-01,
         3.1925e-01,  1.8174e-01, -9.4531e-02,  3.6838e-02,  3.7488e-01,
         1.2717e-01,  4.7130e-01, -1.5721e-01, -7.7476e-02,  8.1014e-01,
        -1.5593e-01, -1.0054e-01,  4.5635e-01,  6.1519e-01,  1.4677e-01,
        -2.2707e-01, -5.5761e-03,  2.6145e-01,  5.9044e-01,  2.2731e-01,
        -2.6443e-02, -2.3644e-03, -1.0726e-01,  7.7391e-01,  3.4043e-01,
         2.8069e-01,  4.8368e-01,  4.9516e-01,  2.3034e-01,  6.8924e-01,
         3.1027e-01,  3.4570e-01,  9.7788e-01,  3.3127e-01,  3.5599e-01,
         5.0713e-01,  7.2506e-01,  6.5209e-01, -4.4852e-02,  4.7526e-01,
         7.1132e-01,  8.4672e-01, -7.5696e-03,  4.1974e-01,  2.1462e-01,
         1.7083e-01, -1.3488e-01,  4.1226e-01,  2.3258e-01,  2.5389e-01,
        -1.5678e-01, -5.3553e-01, -4.7307e-01,  2.7626e-01, -2.5093e-01,
         4.0266e-02,  2.4575e-01,  1.8999e-01, -6.1258e-03, -6.5438e-03,
         4.2919e-01, -1.2408e-01,  1.9037e-01,  5.1006e-01, -6.9298e-02,
         1.7477e-01,  3.5078e-01,  7.3873e-01,  5.5692e-01,  4.3353e-01,
        -2.2003e-01,  3.3680e-01,  6.3033e-01,  2.5604e-01,  3.2412e-01,
         6.3429e-02,  4.2685e-01,  3.0266e-01,  4.5454e-01,  3.0221e-02,
         4.9645e-01,  4.2592e-01,  2.2569e-01,  3.2492e-01, -1.9558e-01,
         6.3491e-01,  1.4616e-01,  2.0872e-01,  1.7695e-01,  3.4186e-01,
         7.6387e-01,  2.4154e-01,  3.4131e-01,  2.5706e-01, -1.7839e-01,
         4.7458e-01, -6.1654e-02, -4.2599e-02,  8.0520e-01,  1.1602e-01,
         4.0244e-01,  5.3733e-01,  1.0871e-01,  2.4645e-02,  7.1771e-01,
        -1.1157e-01,  2.2285e-01,  3.2628e-01,  1.0782e-01,  1.5037e-01,
         4.8782e-01,  4.4199e-01,  8.0079e-01, -1.9772e-01,  4.1425e-01,
         3.3853e-01,  7.4381e-01, -1.5043e-01,  5.4073e-02,  5.8050e-01,
        -2.5929e-02,  8.0517e-01, -3.4948e-01,  2.4886e-01,  5.5591e-01,
         1.8956e-01,  1.2973e-01,  7.4482e-02,  1.6208e-01,  5.5959e-01,
        -8.3063e-02,  7.0622e-01,  4.5518e-01,  4.7128e-01,  7.8142e-01,
        -2.2690e-02,  8.9799e-02,  1.5444e-01, -9.2526e-04,  1.1149e-01,
        -7.5147e-03,  5.6785e-01,  8.0659e-02,  5.8024e-01,  8.5778e-02,
         6.8099e-01,  2.7963e-01,  9.8910e-02, -1.4123e-01, -2.2274e-01,
         3.2634e-01,  4.8895e-01,  2.9214e-01, -1.3606e-01,  2.9738e-01,
         1.8581e-01,  1.2835e-01,  8.0524e-01,  5.1709e-01,  5.6002e-02,
         3.1847e-01, -5.6267e-01,  5.3282e-01,  4.8991e-01,  1.9676e-01,
        -2.5985e-02,  6.1749e-01,  3.1766e-01,  1.6751e-01,  4.4369e-01,
         1.5407e-01,  5.1959e-01,  4.5052e-01,  3.0881e-01,  2.6712e-02,
         1.2838e-01,  7.2508e-01,  4.6165e-01,  2.0328e-01,  2.4748e-01,
         4.8103e-01,  1.8086e-01, -7.3423e-02,  2.9468e-01, -8.2690e-02,
         7.6233e-02, -2.6612e-01,  3.9660e-01,  2.3032e-01,  4.0978e-02,
        -2.4679e-01, -1.2860e-01,  1.0342e-01,  5.5879e-01,  3.9662e-01,
        -1.4361e-01, -1.9822e-01,  5.5379e-01, -3.5044e-02,  6.5483e-01,
         4.9469e-01,  4.6768e-01,  9.9884e-02,  4.0240e-01,  8.2842e-02,
         8.1476e-02,  1.0099e-01,  6.6033e-01,  1.2213e-01,  6.6229e-01,
         6.9984e-02,  4.2777e-01,  7.7067e-01,  1.1222e-02,  1.7325e-01,
         1.4918e-01,  4.4225e-01,  7.1020e-01, -6.4827e-02, -1.0691e-01,
         4.2094e-01,  9.3253e-01,  7.4832e-01,  2.0421e-01,  8.0857e-01,
        -6.9927e-01,  6.9038e-01, -1.4160e-01,  2.9397e-01,  1.8431e-01,
         3.3021e-01,  3.0792e-01,  2.0105e-01,  5.4197e-01,  3.6271e-01,
         3.5776e-01,  2.2193e-01,  6.9557e-02,  8.7006e-01, -2.0600e-01,
        -3.4507e-03,  1.8695e-01,  1.0000e+00,  2.9092e-01, -2.6603e-01,
         2.7547e-01,  7.8514e-02,  1.6481e-01,  4.8246e-01,  5.4929e-01,
        -1.7485e-01,  3.9021e-02,  4.1384e-01,  5.1890e-01, -4.6565e-02,
         3.2173e-01,  1.8334e-01,  1.8262e-01,  1.4842e-01,  6.1002e-01,
         2.9751e-02,  1.2533e-01,  5.3216e-01,  2.0576e-01, -4.0360e-02,
         3.6691e-01,  8.1207e-01,  3.9485e-01,  1.3495e-01,  1.4429e-04,
         3.4063e-01,  3.1047e-02,  3.8775e-01,  2.6841e-01,  5.2225e-01,
         5.2153e-01,  5.3369e-01,  1.9615e-01,  3.0179e-01], device='cuda:0'), 'blocks.2.layer.weight': tensor([-0.0010,  0.0317,  0.4703,  ...,  0.2049, -0.0204,  0.3651],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([0.2656, 0.5401, 0.7364,  ..., 0.3438, 0.5377, 0.2040], device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.1.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 3.4675e-01,  4.9634e-01,  4.9947e-01,  3.5167e-01,  6.8275e-01,
        -1.5499e-01,  3.8235e-01, -8.2602e-02,  9.2969e-02,  3.4503e-01,
        -6.9911e-02,  6.8859e-01,  4.4754e-01,  4.0567e-01,  5.8537e-01,
         4.8758e-01,  2.1827e-01,  5.5889e-01, -3.6301e-01, -2.1951e-01,
         2.5134e-01,  3.0956e-01,  7.9425e-01,  6.6511e-01,  7.1393e-01,
         8.6881e-01,  1.8570e-01, -8.7449e-02,  6.5183e-02,  5.4472e-01,
         2.1367e-01, -3.4635e-01, -3.3732e-01,  1.6719e-01,  1.6866e-01,
         1.5956e-01,  3.7950e-01,  3.3845e-01,  5.1705e-02,  3.7345e-01,
         4.0139e-01,  3.6994e-01,  2.8768e-01,  3.8171e-01,  7.6911e-01,
         3.2752e-01, -3.0061e-01,  3.1377e-01, -9.3224e-02, -1.2043e-01,
         4.4630e-01,  4.6442e-01, -6.7174e-02, -1.0307e-01, -8.5593e-02,
         4.1887e-01,  4.8836e-01, -2.3651e-01,  3.4604e-01,  9.2028e-02,
         1.6476e-01, -6.3855e-01,  7.0815e-01, -4.7516e-01, -1.3248e-01,
         2.0273e-01, -9.3000e-02,  8.0644e-01,  3.1913e-01, -1.4491e-02,
         1.5819e-01,  5.9817e-02,  1.0919e-01,  5.0951e-01, -3.8428e-02,
         1.5758e-01,  5.2751e-01, -4.7671e-01, -4.2823e-01,  4.0300e-01,
         1.2542e-01,  3.8350e-01,  3.7314e-01,  2.1977e-01,  6.2835e-01,
         1.7641e-01,  4.4517e-01,  6.0281e-02,  1.0899e-01,  8.3355e-01,
         2.6046e-01,  2.2819e-01,  2.6363e-01,  3.1669e-01,  7.0682e-01,
        -1.5943e-01,  5.2125e-01,  1.8858e-01, -3.3286e-01,  2.0695e-01,
         6.1562e-01,  3.3932e-01,  2.5271e-01,  1.3981e-01,  1.2687e-01,
         3.1925e-01,  1.8174e-01, -9.4531e-02,  3.6838e-02,  3.7488e-01,
         1.2717e-01,  4.7130e-01, -1.5721e-01, -7.7476e-02,  8.1014e-01,
        -1.5593e-01, -1.0054e-01,  4.5635e-01,  6.1519e-01,  1.4677e-01,
        -2.2707e-01, -5.5761e-03,  2.6145e-01,  5.9044e-01,  2.2731e-01,
        -2.6443e-02, -2.3644e-03, -1.0726e-01,  7.7391e-01,  3.4043e-01,
         2.8069e-01,  4.8368e-01,  4.9516e-01,  2.3034e-01,  6.8924e-01,
         3.1027e-01,  3.4570e-01,  9.7788e-01,  3.3127e-01,  3.5599e-01,
         5.0713e-01,  7.2506e-01,  6.5209e-01, -4.4852e-02,  4.7526e-01,
         7.1132e-01,  8.4672e-01, -7.5696e-03,  4.1974e-01,  2.1462e-01,
         1.7083e-01, -1.3488e-01,  4.1226e-01,  2.3258e-01,  2.5389e-01,
        -1.5678e-01, -5.3553e-01, -4.7307e-01,  2.7626e-01, -2.5093e-01,
         4.0266e-02,  2.4575e-01,  1.8999e-01, -6.1258e-03, -6.5438e-03,
         4.2919e-01, -1.2408e-01,  1.9037e-01,  5.1006e-01, -6.9298e-02,
         1.7477e-01,  3.5078e-01,  7.3873e-01,  5.5692e-01,  4.3353e-01,
        -2.2003e-01,  3.3680e-01,  6.3033e-01,  2.5604e-01,  3.2412e-01,
         6.3429e-02,  4.2685e-01,  3.0266e-01,  4.5454e-01,  3.0221e-02,
         4.9645e-01,  4.2592e-01,  2.2569e-01,  3.2492e-01, -1.9558e-01,
         6.3491e-01,  1.4616e-01,  2.0872e-01,  1.7695e-01,  3.4186e-01,
         7.6387e-01,  2.4154e-01,  3.4131e-01,  2.5706e-01, -1.7839e-01,
         4.7458e-01, -6.1654e-02, -4.2599e-02,  8.0520e-01,  1.1602e-01,
         4.0244e-01,  5.3733e-01,  1.0871e-01,  2.4645e-02,  7.1771e-01,
        -1.1157e-01,  2.2285e-01,  3.2628e-01,  1.0782e-01,  1.5037e-01,
         4.8782e-01,  4.4199e-01,  8.0079e-01, -1.9772e-01,  4.1425e-01,
         3.3853e-01,  7.4381e-01, -1.5043e-01,  5.4073e-02,  5.8050e-01,
        -2.5929e-02,  8.0517e-01, -3.4948e-01,  2.4886e-01,  5.5591e-01,
         1.8956e-01,  1.2973e-01,  7.4482e-02,  1.6208e-01,  5.5959e-01,
        -8.3063e-02,  7.0622e-01,  4.5518e-01,  4.7128e-01,  7.8142e-01,
        -2.2690e-02,  8.9799e-02,  1.5444e-01, -9.2526e-04,  1.1149e-01,
        -7.5147e-03,  5.6785e-01,  8.0659e-02,  5.8024e-01,  8.5778e-02,
         6.8099e-01,  2.7963e-01,  9.8910e-02, -1.4123e-01, -2.2274e-01,
         3.2634e-01,  4.8895e-01,  2.9214e-01, -1.3606e-01,  2.9738e-01,
         1.8581e-01,  1.2835e-01,  8.0524e-01,  5.1709e-01,  5.6002e-02,
         3.1847e-01, -5.6267e-01,  5.3282e-01,  4.8991e-01,  1.9676e-01,
        -2.5985e-02,  6.1749e-01,  3.1766e-01,  1.6751e-01,  4.4369e-01,
         1.5407e-01,  5.1959e-01,  4.5052e-01,  3.0881e-01,  2.6712e-02,
         1.2838e-01,  7.2508e-01,  4.6165e-01,  2.0328e-01,  2.4748e-01,
         4.8103e-01,  1.8086e-01, -7.3423e-02,  2.9468e-01, -8.2690e-02,
         7.6233e-02, -2.6612e-01,  3.9660e-01,  2.3032e-01,  4.0978e-02,
        -2.4679e-01, -1.2860e-01,  1.0342e-01,  5.5879e-01,  3.9662e-01,
        -1.4361e-01, -1.9822e-01,  5.5379e-01, -3.5044e-02,  6.5483e-01,
         4.9469e-01,  4.6768e-01,  9.9884e-02,  4.0240e-01,  8.2842e-02,
         8.1476e-02,  1.0099e-01,  6.6033e-01,  1.2213e-01,  6.6229e-01,
         6.9984e-02,  4.2777e-01,  7.7067e-01,  1.1222e-02,  1.7325e-01,
         1.4918e-01,  4.4225e-01,  7.1020e-01, -6.4827e-02, -1.0691e-01,
         4.2094e-01,  9.3253e-01,  7.4832e-01,  2.0421e-01,  8.0857e-01,
        -6.9927e-01,  6.9038e-01, -1.4160e-01,  2.9397e-01,  1.8431e-01,
         3.3021e-01,  3.0792e-01,  2.0105e-01,  5.4197e-01,  3.6271e-01,
         3.5776e-01,  2.2193e-01,  6.9557e-02,  8.7006e-01, -2.0600e-01,
        -3.4507e-03,  1.8695e-01,  1.0000e+00,  2.9092e-01, -2.6603e-01,
         2.7547e-01,  7.8514e-02,  1.6481e-01,  4.8246e-01,  5.4929e-01,
        -1.7485e-01,  3.9021e-02,  4.1384e-01,  5.1890e-01, -4.6565e-02,
         3.2173e-01,  1.8334e-01,  1.8262e-01,  1.4842e-01,  6.1002e-01,
         2.9751e-02,  1.2533e-01,  5.3216e-01,  2.0576e-01, -4.0360e-02,
         3.6691e-01,  8.1207e-01,  3.9485e-01,  1.3495e-01,  1.4429e-04,
         3.4063e-01,  3.1047e-02,  3.8775e-01,  2.6841e-01,  5.2225e-01,
         5.2153e-01,  5.3369e-01,  1.9615e-01,  3.0179e-01], device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-0.3549,  0.1212,  0.0095,  0.1285, -0.5902, -0.7595,  0.2930, -0.1311,
        -0.6668,  0.2679,  0.4133,  0.4384,  0.0050,  0.0863,  0.9283, -0.2386,
        -0.4347,  0.0211, -0.5482, -0.5881,  0.3635,  0.7974, -0.0708,  0.0277,
         0.3060,  0.6573, -0.0664, -0.1364,  0.2152, -0.0694,  0.9545,  0.5186,
        -0.4286, -0.7605,  0.7025,  0.9891, -0.6683, -0.0308,  0.8484,  0.0912,
         0.4675,  0.1660,  0.9470, -0.3474,  0.5685, -0.6660, -0.3353,  0.5072,
         0.0084,  0.3467, -0.5717,  0.7976,  0.3361,  0.8629, -0.0436,  0.9182,
        -0.4613,  0.7530, -0.3987,  1.0000, -0.1714, -0.0323, -0.3805,  0.1154,
        -0.7352, -0.3378, -0.5991, -0.0573,  0.9557,  0.9710,  0.9160,  0.3482,
        -0.0344, -0.5976, -0.0085, -0.6056,  0.1304,  0.9179, -0.7200,  0.4949,
        -0.3991,  0.7442, -0.6611,  0.6799,  0.0952, -0.2164, -0.6674, -0.3528,
        -0.0951, -0.7665,  0.0833, -0.1890, -0.2624,  0.9089, -0.3644,  0.4514],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.4675e-01,  4.9634e-01,  4.9947e-01,  3.5167e-01,  6.8275e-01,
        -1.5499e-01,  3.8235e-01, -8.2602e-02,  9.2969e-02,  3.4503e-01,
        -6.9911e-02,  6.8859e-01,  4.4754e-01,  4.0567e-01,  5.8537e-01,
         4.8758e-01,  2.1827e-01,  5.5889e-01, -3.6301e-01, -2.1951e-01,
         2.5134e-01,  3.0956e-01,  7.9425e-01,  6.6511e-01,  7.1393e-01,
         8.6881e-01,  1.8570e-01, -8.7449e-02,  6.5183e-02,  5.4472e-01,
         2.1367e-01, -3.4635e-01, -3.3732e-01,  1.6719e-01,  1.6866e-01,
         1.5956e-01,  3.7950e-01,  3.3845e-01,  5.1705e-02,  3.7345e-01,
         4.0139e-01,  3.6994e-01,  2.8768e-01,  3.8171e-01,  7.6911e-01,
         3.2752e-01, -3.0061e-01,  3.1377e-01, -9.3224e-02, -1.2043e-01,
         4.4630e-01,  4.6442e-01, -6.7174e-02, -1.0307e-01, -8.5593e-02,
         4.1887e-01,  4.8836e-01, -2.3651e-01,  3.4604e-01,  9.2028e-02,
         1.6476e-01, -6.3855e-01,  7.0815e-01, -4.7516e-01, -1.3248e-01,
         2.0273e-01, -9.3000e-02,  8.0644e-01,  3.1913e-01, -1.4491e-02,
         1.5819e-01,  5.9817e-02,  1.0919e-01,  5.0951e-01, -3.8428e-02,
         1.5758e-01,  5.2751e-01, -4.7671e-01, -4.2823e-01,  4.0300e-01,
         1.2542e-01,  3.8350e-01,  3.7314e-01,  2.1977e-01,  6.2835e-01,
         1.7641e-01,  4.4517e-01,  6.0281e-02,  1.0899e-01,  8.3355e-01,
         2.6046e-01,  2.2819e-01,  2.6363e-01,  3.1669e-01,  7.0682e-01,
        -1.5943e-01,  5.2125e-01,  1.8858e-01, -3.3286e-01,  2.0695e-01,
         6.1562e-01,  3.3932e-01,  2.5271e-01,  1.3981e-01,  1.2687e-01,
         3.1925e-01,  1.8174e-01, -9.4531e-02,  3.6838e-02,  3.7488e-01,
         1.2717e-01,  4.7130e-01, -1.5721e-01, -7.7476e-02,  8.1014e-01,
        -1.5593e-01, -1.0054e-01,  4.5635e-01,  6.1519e-01,  1.4677e-01,
        -2.2707e-01, -5.5761e-03,  2.6145e-01,  5.9044e-01,  2.2731e-01,
        -2.6443e-02, -2.3644e-03, -1.0726e-01,  7.7391e-01,  3.4043e-01,
         2.8069e-01,  4.8368e-01,  4.9516e-01,  2.3034e-01,  6.8924e-01,
         3.1027e-01,  3.4570e-01,  9.7788e-01,  3.3127e-01,  3.5599e-01,
         5.0713e-01,  7.2506e-01,  6.5209e-01, -4.4852e-02,  4.7526e-01,
         7.1132e-01,  8.4672e-01, -7.5696e-03,  4.1974e-01,  2.1462e-01,
         1.7083e-01, -1.3488e-01,  4.1226e-01,  2.3258e-01,  2.5389e-01,
        -1.5678e-01, -5.3553e-01, -4.7307e-01,  2.7626e-01, -2.5093e-01,
         4.0266e-02,  2.4575e-01,  1.8999e-01, -6.1258e-03, -6.5438e-03,
         4.2919e-01, -1.2408e-01,  1.9037e-01,  5.1006e-01, -6.9298e-02,
         1.7477e-01,  3.5078e-01,  7.3873e-01,  5.5692e-01,  4.3353e-01,
        -2.2003e-01,  3.3680e-01,  6.3033e-01,  2.5604e-01,  3.2412e-01,
         6.3429e-02,  4.2685e-01,  3.0266e-01,  4.5454e-01,  3.0221e-02,
         4.9645e-01,  4.2592e-01,  2.2569e-01,  3.2492e-01, -1.9558e-01,
         6.3491e-01,  1.4616e-01,  2.0872e-01,  1.7695e-01,  3.4186e-01,
         7.6387e-01,  2.4154e-01,  3.4131e-01,  2.5706e-01, -1.7839e-01,
         4.7458e-01, -6.1654e-02, -4.2599e-02,  8.0520e-01,  1.1602e-01,
         4.0244e-01,  5.3733e-01,  1.0871e-01,  2.4645e-02,  7.1771e-01,
        -1.1157e-01,  2.2285e-01,  3.2628e-01,  1.0782e-01,  1.5037e-01,
         4.8782e-01,  4.4199e-01,  8.0079e-01, -1.9772e-01,  4.1425e-01,
         3.3853e-01,  7.4381e-01, -1.5043e-01,  5.4073e-02,  5.8050e-01,
        -2.5929e-02,  8.0517e-01, -3.4948e-01,  2.4886e-01,  5.5591e-01,
         1.8956e-01,  1.2973e-01,  7.4482e-02,  1.6208e-01,  5.5959e-01,
        -8.3063e-02,  7.0622e-01,  4.5518e-01,  4.7128e-01,  7.8142e-01,
        -2.2690e-02,  8.9799e-02,  1.5444e-01, -9.2526e-04,  1.1149e-01,
        -7.5147e-03,  5.6785e-01,  8.0659e-02,  5.8024e-01,  8.5778e-02,
         6.8099e-01,  2.7963e-01,  9.8910e-02, -1.4123e-01, -2.2274e-01,
         3.2634e-01,  4.8895e-01,  2.9214e-01, -1.3606e-01,  2.9738e-01,
         1.8581e-01,  1.2835e-01,  8.0524e-01,  5.1709e-01,  5.6002e-02,
         3.1847e-01, -5.6267e-01,  5.3282e-01,  4.8991e-01,  1.9676e-01,
        -2.5985e-02,  6.1749e-01,  3.1766e-01,  1.6751e-01,  4.4369e-01,
         1.5407e-01,  5.1959e-01,  4.5052e-01,  3.0881e-01,  2.6712e-02,
         1.2838e-01,  7.2508e-01,  4.6165e-01,  2.0328e-01,  2.4748e-01,
         4.8103e-01,  1.8086e-01, -7.3423e-02,  2.9468e-01, -8.2690e-02,
         7.6233e-02, -2.6612e-01,  3.9660e-01,  2.3032e-01,  4.0978e-02,
        -2.4679e-01, -1.2860e-01,  1.0342e-01,  5.5879e-01,  3.9662e-01,
        -1.4361e-01, -1.9822e-01,  5.5379e-01, -3.5044e-02,  6.5483e-01,
         4.9469e-01,  4.6768e-01,  9.9884e-02,  4.0240e-01,  8.2842e-02,
         8.1476e-02,  1.0099e-01,  6.6033e-01,  1.2213e-01,  6.6229e-01,
         6.9984e-02,  4.2777e-01,  7.7067e-01,  1.1222e-02,  1.7325e-01,
         1.4918e-01,  4.4225e-01,  7.1020e-01, -6.4827e-02, -1.0691e-01,
         4.2094e-01,  9.3253e-01,  7.4832e-01,  2.0421e-01,  8.0857e-01,
        -6.9927e-01,  6.9038e-01, -1.4160e-01,  2.9397e-01,  1.8431e-01,
         3.3021e-01,  3.0792e-01,  2.0105e-01,  5.4197e-01,  3.6271e-01,
         3.5776e-01,  2.2193e-01,  6.9557e-02,  8.7006e-01, -2.0600e-01,
        -3.4507e-03,  1.8695e-01,  1.0000e+00,  2.9092e-01, -2.6603e-01,
         2.7547e-01,  7.8514e-02,  1.6481e-01,  4.8246e-01,  5.4929e-01,
        -1.7485e-01,  3.9021e-02,  4.1384e-01,  5.1890e-01, -4.6565e-02,
         3.2173e-01,  1.8334e-01,  1.8262e-01,  1.4842e-01,  6.1002e-01,
         2.9751e-02,  1.2533e-01,  5.3216e-01,  2.0576e-01, -4.0360e-02,
         3.6691e-01,  8.1207e-01,  3.9485e-01,  1.3495e-01,  1.4429e-04,
         3.4063e-01,  3.1047e-02,  3.8775e-01,  2.6841e-01,  5.2225e-01,
         5.2153e-01,  5.3369e-01,  1.9615e-01,  3.0179e-01], device='cuda:0'), 'blocks.2.layer.weight': tensor([-0.0010,  0.0317,  0.4703,  ...,  0.2049, -0.0204,  0.3651],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([0.2656, 0.5401, 0.7364,  ..., 0.3438, 0.5377, 0.2040], device='cuda:0')}
topk_layer inside model after retrieval:  [23, 75, 345, 61, 349, 92, 330, 197, 100, 33, 69, 190, 296, 41, 121, 157, 76, 166, 247, 133, 39, 336, 266, 362, 184, 17, 63, 158, 50, 116, 344, 354, 201, 98, 292, 156, 130, 335, 291, 365, 332, 366, 146, 382, 186, 287, 159, 104, 217, 139, 219, 28, 99, 260, 180, 264, 227, 273, 379]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-0.3549,  0.1212,  0.0095,  0.1285, -0.5902, -0.7595,  0.2930, -0.1311,
        -0.6668,  0.2679,  0.4133,  0.4384,  0.0050,  0.0863,  0.9283, -0.2386,
        -0.4347,  0.0211, -0.5482, -0.5881,  0.3635,  0.7974, -0.0708,  0.0277,
         0.3060,  0.6573, -0.0664, -0.1364,  0.2152, -0.0694,  0.9545,  0.5186,
        -0.4286, -0.7605,  0.7025,  0.9891, -0.6683, -0.0308,  0.8484,  0.0912,
         0.4675,  0.1660,  0.9470, -0.3474,  0.5685, -0.6660, -0.3353,  0.5072,
         0.0084,  0.3467, -0.5717,  0.7976,  0.3361,  0.8629, -0.0436,  0.9182,
        -0.4613,  0.7530, -0.3987,  1.0000, -0.1714, -0.0323, -0.3805,  0.1154,
        -0.7352, -0.3378, -0.5991, -0.0573,  0.9557,  0.9710,  0.9160,  0.3482,
        -0.0344, -0.5976, -0.0085, -0.6056,  0.1304,  0.9179, -0.7200,  0.4949,
        -0.3991,  0.7442, -0.6611,  0.6799,  0.0952, -0.2164, -0.6674, -0.3528,
        -0.0951, -0.7665,  0.0833, -0.1890, -0.2624,  0.9089, -0.3644,  0.4514],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.4675e-01,  4.9634e-01,  4.9947e-01,  3.5167e-01,  6.8275e-01,
        -1.5499e-01,  3.8235e-01, -8.2602e-02,  9.2969e-02,  3.4503e-01,
        -6.9911e-02,  6.8859e-01,  4.4754e-01,  4.0567e-01,  5.8537e-01,
         4.8758e-01,  2.1827e-01,  5.5889e-01, -3.6301e-01, -2.1951e-01,
         2.5134e-01,  3.0956e-01,  7.9425e-01,  6.6511e-01,  7.1393e-01,
         8.6881e-01,  1.8570e-01, -8.7449e-02,  6.5183e-02,  5.4472e-01,
         2.1367e-01, -3.4635e-01, -3.3732e-01,  1.6719e-01,  1.6866e-01,
         1.5956e-01,  3.7950e-01,  3.3845e-01,  5.1705e-02,  3.7345e-01,
         4.0139e-01,  3.6994e-01,  2.8768e-01,  3.8171e-01,  7.6911e-01,
         3.2752e-01, -3.0061e-01,  3.1377e-01, -9.3224e-02, -1.2043e-01,
         4.4630e-01,  4.6442e-01, -6.7174e-02, -1.0307e-01, -8.5593e-02,
         4.1887e-01,  4.8836e-01, -2.3651e-01,  3.4604e-01,  9.2028e-02,
         1.6476e-01, -6.3855e-01,  7.0815e-01, -4.7516e-01, -1.3248e-01,
         2.0273e-01, -9.3000e-02,  8.0644e-01,  3.1913e-01, -1.4491e-02,
         1.5819e-01,  5.9817e-02,  1.0919e-01,  5.0951e-01, -3.8428e-02,
         1.5758e-01,  5.2751e-01, -4.7671e-01, -4.2823e-01,  4.0300e-01,
         1.2542e-01,  3.8350e-01,  3.7314e-01,  2.1977e-01,  6.2835e-01,
         1.7641e-01,  4.4517e-01,  6.0281e-02,  1.0899e-01,  8.3355e-01,
         2.6046e-01,  2.2819e-01,  2.6363e-01,  3.1669e-01,  7.0682e-01,
        -1.5943e-01,  5.2125e-01,  1.8858e-01, -3.3286e-01,  2.0695e-01,
         6.1562e-01,  3.3932e-01,  2.5271e-01,  1.3981e-01,  1.2687e-01,
         3.1925e-01,  1.8174e-01, -9.4531e-02,  3.6838e-02,  3.7488e-01,
         1.2717e-01,  4.7130e-01, -1.5721e-01, -7.7476e-02,  8.1014e-01,
        -1.5593e-01, -1.0054e-01,  4.5635e-01,  6.1519e-01,  1.4677e-01,
        -2.2707e-01, -5.5761e-03,  2.6145e-01,  5.9044e-01,  2.2731e-01,
        -2.6443e-02, -2.3644e-03, -1.0726e-01,  7.7391e-01,  3.4043e-01,
         2.8069e-01,  4.8368e-01,  4.9516e-01,  2.3034e-01,  6.8924e-01,
         3.1027e-01,  3.4570e-01,  9.7788e-01,  3.3127e-01,  3.5599e-01,
         5.0713e-01,  7.2506e-01,  6.5209e-01, -4.4852e-02,  4.7526e-01,
         7.1132e-01,  8.4672e-01, -7.5696e-03,  4.1974e-01,  2.1462e-01,
         1.7083e-01, -1.3488e-01,  4.1226e-01,  2.3258e-01,  2.5389e-01,
        -1.5678e-01, -5.3553e-01, -4.7307e-01,  2.7626e-01, -2.5093e-01,
         4.0266e-02,  2.4575e-01,  1.8999e-01, -6.1258e-03, -6.5438e-03,
         4.2919e-01, -1.2408e-01,  1.9037e-01,  5.1006e-01, -6.9298e-02,
         1.7477e-01,  3.5078e-01,  7.3873e-01,  5.5692e-01,  4.3353e-01,
        -2.2003e-01,  3.3680e-01,  6.3033e-01,  2.5604e-01,  3.2412e-01,
         6.3429e-02,  4.2685e-01,  3.0266e-01,  4.5454e-01,  3.0221e-02,
         4.9645e-01,  4.2592e-01,  2.2569e-01,  3.2492e-01, -1.9558e-01,
         6.3491e-01,  1.4616e-01,  2.0872e-01,  1.7695e-01,  3.4186e-01,
         7.6387e-01,  2.4154e-01,  3.4131e-01,  2.5706e-01, -1.7839e-01,
         4.7458e-01, -6.1654e-02, -4.2599e-02,  8.0520e-01,  1.1602e-01,
         4.0244e-01,  5.3733e-01,  1.0871e-01,  2.4645e-02,  7.1771e-01,
        -1.1157e-01,  2.2285e-01,  3.2628e-01,  1.0782e-01,  1.5037e-01,
         4.8782e-01,  4.4199e-01,  8.0079e-01, -1.9772e-01,  4.1425e-01,
         3.3853e-01,  7.4381e-01, -1.5043e-01,  5.4073e-02,  5.8050e-01,
        -2.5929e-02,  8.0517e-01, -3.4948e-01,  2.4886e-01,  5.5591e-01,
         1.8956e-01,  1.2973e-01,  7.4482e-02,  1.6208e-01,  5.5959e-01,
        -8.3063e-02,  7.0622e-01,  4.5518e-01,  4.7128e-01,  7.8142e-01,
        -2.2690e-02,  8.9799e-02,  1.5444e-01, -9.2526e-04,  1.1149e-01,
        -7.5147e-03,  5.6785e-01,  8.0659e-02,  5.8024e-01,  8.5778e-02,
         6.8099e-01,  2.7963e-01,  9.8910e-02, -1.4123e-01, -2.2274e-01,
         3.2634e-01,  4.8895e-01,  2.9214e-01, -1.3606e-01,  2.9738e-01,
         1.8581e-01,  1.2835e-01,  8.0524e-01,  5.1709e-01,  5.6002e-02,
         3.1847e-01, -5.6267e-01,  5.3282e-01,  4.8991e-01,  1.9676e-01,
        -2.5985e-02,  6.1749e-01,  3.1766e-01,  1.6751e-01,  4.4369e-01,
         1.5407e-01,  5.1959e-01,  4.5052e-01,  3.0881e-01,  2.6712e-02,
         1.2838e-01,  7.2508e-01,  4.6165e-01,  2.0328e-01,  2.4748e-01,
         4.8103e-01,  1.8086e-01, -7.3423e-02,  2.9468e-01, -8.2690e-02,
         7.6233e-02, -2.6612e-01,  3.9660e-01,  2.3032e-01,  4.0978e-02,
        -2.4679e-01, -1.2860e-01,  1.0342e-01,  5.5879e-01,  3.9662e-01,
        -1.4361e-01, -1.9822e-01,  5.5379e-01, -3.5044e-02,  6.5483e-01,
         4.9469e-01,  4.6768e-01,  9.9884e-02,  4.0240e-01,  8.2842e-02,
         8.1476e-02,  1.0099e-01,  6.6033e-01,  1.2213e-01,  6.6229e-01,
         6.9984e-02,  4.2777e-01,  7.7067e-01,  1.1222e-02,  1.7325e-01,
         1.4918e-01,  4.4225e-01,  7.1020e-01, -6.4827e-02, -1.0691e-01,
         4.2094e-01,  9.3253e-01,  7.4832e-01,  2.0421e-01,  8.0857e-01,
        -6.9927e-01,  6.9038e-01, -1.4160e-01,  2.9397e-01,  1.8431e-01,
         3.3021e-01,  3.0792e-01,  2.0105e-01,  5.4197e-01,  3.6271e-01,
         3.5776e-01,  2.2193e-01,  6.9557e-02,  8.7006e-01, -2.0600e-01,
        -3.4507e-03,  1.8695e-01,  1.0000e+00,  2.9092e-01, -2.6603e-01,
         2.7547e-01,  7.8514e-02,  1.6481e-01,  4.8246e-01,  5.4929e-01,
        -1.7485e-01,  3.9021e-02,  4.1384e-01,  5.1890e-01, -4.6565e-02,
         3.2173e-01,  1.8334e-01,  1.8262e-01,  1.4842e-01,  6.1002e-01,
         2.9751e-02,  1.2533e-01,  5.3216e-01,  2.0576e-01, -4.0360e-02,
         3.6691e-01,  8.1207e-01,  3.9485e-01,  1.3495e-01,  1.4429e-04,
         3.4063e-01,  3.1047e-02,  3.8775e-01,  2.6841e-01,  5.2225e-01,
         5.2153e-01,  5.3369e-01,  1.9615e-01,  3.0179e-01], device='cuda:0'), 'blocks.2.layer.weight': tensor([-0.0010,  0.0317,  0.4703,  ...,  0.2049, -0.0204,  0.3651],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([0.2656, 0.5401, 0.7364,  ..., 0.3438, 0.5377, 0.2040], device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.2.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([-0.0010,  0.0317,  0.4703,  ...,  0.2049, -0.0204,  0.3651],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-0.3549,  0.1212,  0.0095,  0.1285, -0.5902, -0.7595,  0.2930, -0.1311,
        -0.6668,  0.2679,  0.4133,  0.4384,  0.0050,  0.0863,  0.9283, -0.2386,
        -0.4347,  0.0211, -0.5482, -0.5881,  0.3635,  0.7974, -0.0708,  0.0277,
         0.3060,  0.6573, -0.0664, -0.1364,  0.2152, -0.0694,  0.9545,  0.5186,
        -0.4286, -0.7605,  0.7025,  0.9891, -0.6683, -0.0308,  0.8484,  0.0912,
         0.4675,  0.1660,  0.9470, -0.3474,  0.5685, -0.6660, -0.3353,  0.5072,
         0.0084,  0.3467, -0.5717,  0.7976,  0.3361,  0.8629, -0.0436,  0.9182,
        -0.4613,  0.7530, -0.3987,  1.0000, -0.1714, -0.0323, -0.3805,  0.1154,
        -0.7352, -0.3378, -0.5991, -0.0573,  0.9557,  0.9710,  0.9160,  0.3482,
        -0.0344, -0.5976, -0.0085, -0.6056,  0.1304,  0.9179, -0.7200,  0.4949,
        -0.3991,  0.7442, -0.6611,  0.6799,  0.0952, -0.2164, -0.6674, -0.3528,
        -0.0951, -0.7665,  0.0833, -0.1890, -0.2624,  0.9089, -0.3644,  0.4514],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.4675e-01,  4.9634e-01,  4.9947e-01,  3.5167e-01,  6.8275e-01,
        -1.5499e-01,  3.8235e-01, -8.2602e-02,  9.2969e-02,  3.4503e-01,
        -6.9911e-02,  6.8859e-01,  4.4754e-01,  4.0567e-01,  5.8537e-01,
         4.8758e-01,  2.1827e-01,  5.5889e-01, -3.6301e-01, -2.1951e-01,
         2.5134e-01,  3.0956e-01,  7.9425e-01,  6.6511e-01,  7.1393e-01,
         8.6881e-01,  1.8570e-01, -8.7449e-02,  6.5183e-02,  5.4472e-01,
         2.1367e-01, -3.4635e-01, -3.3732e-01,  1.6719e-01,  1.6866e-01,
         1.5956e-01,  3.7950e-01,  3.3845e-01,  5.1705e-02,  3.7345e-01,
         4.0139e-01,  3.6994e-01,  2.8768e-01,  3.8171e-01,  7.6911e-01,
         3.2752e-01, -3.0061e-01,  3.1377e-01, -9.3224e-02, -1.2043e-01,
         4.4630e-01,  4.6442e-01, -6.7174e-02, -1.0307e-01, -8.5593e-02,
         4.1887e-01,  4.8836e-01, -2.3651e-01,  3.4604e-01,  9.2028e-02,
         1.6476e-01, -6.3855e-01,  7.0815e-01, -4.7516e-01, -1.3248e-01,
         2.0273e-01, -9.3000e-02,  8.0644e-01,  3.1913e-01, -1.4491e-02,
         1.5819e-01,  5.9817e-02,  1.0919e-01,  5.0951e-01, -3.8428e-02,
         1.5758e-01,  5.2751e-01, -4.7671e-01, -4.2823e-01,  4.0300e-01,
         1.2542e-01,  3.8350e-01,  3.7314e-01,  2.1977e-01,  6.2835e-01,
         1.7641e-01,  4.4517e-01,  6.0281e-02,  1.0899e-01,  8.3355e-01,
         2.6046e-01,  2.2819e-01,  2.6363e-01,  3.1669e-01,  7.0682e-01,
        -1.5943e-01,  5.2125e-01,  1.8858e-01, -3.3286e-01,  2.0695e-01,
         6.1562e-01,  3.3932e-01,  2.5271e-01,  1.3981e-01,  1.2687e-01,
         3.1925e-01,  1.8174e-01, -9.4531e-02,  3.6838e-02,  3.7488e-01,
         1.2717e-01,  4.7130e-01, -1.5721e-01, -7.7476e-02,  8.1014e-01,
        -1.5593e-01, -1.0054e-01,  4.5635e-01,  6.1519e-01,  1.4677e-01,
        -2.2707e-01, -5.5761e-03,  2.6145e-01,  5.9044e-01,  2.2731e-01,
        -2.6443e-02, -2.3644e-03, -1.0726e-01,  7.7391e-01,  3.4043e-01,
         2.8069e-01,  4.8368e-01,  4.9516e-01,  2.3034e-01,  6.8924e-01,
         3.1027e-01,  3.4570e-01,  9.7788e-01,  3.3127e-01,  3.5599e-01,
         5.0713e-01,  7.2506e-01,  6.5209e-01, -4.4852e-02,  4.7526e-01,
         7.1132e-01,  8.4672e-01, -7.5696e-03,  4.1974e-01,  2.1462e-01,
         1.7083e-01, -1.3488e-01,  4.1226e-01,  2.3258e-01,  2.5389e-01,
        -1.5678e-01, -5.3553e-01, -4.7307e-01,  2.7626e-01, -2.5093e-01,
         4.0266e-02,  2.4575e-01,  1.8999e-01, -6.1258e-03, -6.5438e-03,
         4.2919e-01, -1.2408e-01,  1.9037e-01,  5.1006e-01, -6.9298e-02,
         1.7477e-01,  3.5078e-01,  7.3873e-01,  5.5692e-01,  4.3353e-01,
        -2.2003e-01,  3.3680e-01,  6.3033e-01,  2.5604e-01,  3.2412e-01,
         6.3429e-02,  4.2685e-01,  3.0266e-01,  4.5454e-01,  3.0221e-02,
         4.9645e-01,  4.2592e-01,  2.2569e-01,  3.2492e-01, -1.9558e-01,
         6.3491e-01,  1.4616e-01,  2.0872e-01,  1.7695e-01,  3.4186e-01,
         7.6387e-01,  2.4154e-01,  3.4131e-01,  2.5706e-01, -1.7839e-01,
         4.7458e-01, -6.1654e-02, -4.2599e-02,  8.0520e-01,  1.1602e-01,
         4.0244e-01,  5.3733e-01,  1.0871e-01,  2.4645e-02,  7.1771e-01,
        -1.1157e-01,  2.2285e-01,  3.2628e-01,  1.0782e-01,  1.5037e-01,
         4.8782e-01,  4.4199e-01,  8.0079e-01, -1.9772e-01,  4.1425e-01,
         3.3853e-01,  7.4381e-01, -1.5043e-01,  5.4073e-02,  5.8050e-01,
        -2.5929e-02,  8.0517e-01, -3.4948e-01,  2.4886e-01,  5.5591e-01,
         1.8956e-01,  1.2973e-01,  7.4482e-02,  1.6208e-01,  5.5959e-01,
        -8.3063e-02,  7.0622e-01,  4.5518e-01,  4.7128e-01,  7.8142e-01,
        -2.2690e-02,  8.9799e-02,  1.5444e-01, -9.2526e-04,  1.1149e-01,
        -7.5147e-03,  5.6785e-01,  8.0659e-02,  5.8024e-01,  8.5778e-02,
         6.8099e-01,  2.7963e-01,  9.8910e-02, -1.4123e-01, -2.2274e-01,
         3.2634e-01,  4.8895e-01,  2.9214e-01, -1.3606e-01,  2.9738e-01,
         1.8581e-01,  1.2835e-01,  8.0524e-01,  5.1709e-01,  5.6002e-02,
         3.1847e-01, -5.6267e-01,  5.3282e-01,  4.8991e-01,  1.9676e-01,
        -2.5985e-02,  6.1749e-01,  3.1766e-01,  1.6751e-01,  4.4369e-01,
         1.5407e-01,  5.1959e-01,  4.5052e-01,  3.0881e-01,  2.6712e-02,
         1.2838e-01,  7.2508e-01,  4.6165e-01,  2.0328e-01,  2.4748e-01,
         4.8103e-01,  1.8086e-01, -7.3423e-02,  2.9468e-01, -8.2690e-02,
         7.6233e-02, -2.6612e-01,  3.9660e-01,  2.3032e-01,  4.0978e-02,
        -2.4679e-01, -1.2860e-01,  1.0342e-01,  5.5879e-01,  3.9662e-01,
        -1.4361e-01, -1.9822e-01,  5.5379e-01, -3.5044e-02,  6.5483e-01,
         4.9469e-01,  4.6768e-01,  9.9884e-02,  4.0240e-01,  8.2842e-02,
         8.1476e-02,  1.0099e-01,  6.6033e-01,  1.2213e-01,  6.6229e-01,
         6.9984e-02,  4.2777e-01,  7.7067e-01,  1.1222e-02,  1.7325e-01,
         1.4918e-01,  4.4225e-01,  7.1020e-01, -6.4827e-02, -1.0691e-01,
         4.2094e-01,  9.3253e-01,  7.4832e-01,  2.0421e-01,  8.0857e-01,
        -6.9927e-01,  6.9038e-01, -1.4160e-01,  2.9397e-01,  1.8431e-01,
         3.3021e-01,  3.0792e-01,  2.0105e-01,  5.4197e-01,  3.6271e-01,
         3.5776e-01,  2.2193e-01,  6.9557e-02,  8.7006e-01, -2.0600e-01,
        -3.4507e-03,  1.8695e-01,  1.0000e+00,  2.9092e-01, -2.6603e-01,
         2.7547e-01,  7.8514e-02,  1.6481e-01,  4.8246e-01,  5.4929e-01,
        -1.7485e-01,  3.9021e-02,  4.1384e-01,  5.1890e-01, -4.6565e-02,
         3.2173e-01,  1.8334e-01,  1.8262e-01,  1.4842e-01,  6.1002e-01,
         2.9751e-02,  1.2533e-01,  5.3216e-01,  2.0576e-01, -4.0360e-02,
         3.6691e-01,  8.1207e-01,  3.9485e-01,  1.3495e-01,  1.4429e-04,
         3.4063e-01,  3.1047e-02,  3.8775e-01,  2.6841e-01,  5.2225e-01,
         5.2153e-01,  5.3369e-01,  1.9615e-01,  3.0179e-01], device='cuda:0'), 'blocks.2.layer.weight': tensor([-0.0010,  0.0317,  0.4703,  ...,  0.2049, -0.0204,  0.3651],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([0.2656, 0.5401, 0.7364,  ..., 0.3438, 0.5377, 0.2040], device='cuda:0')}
topk_layer inside model after retrieval:  [1006, 307, 576, 276, 733, 972, 632, 1411, 320, 301, 350, 1284, 915, 1211, 49, 1502, 503, 1385, 272, 1265, 549, 185, 904, 734, 1413, 326, 984, 468, 764, 186, 551, 122, 969, 347, 1050, 1293, 89, 994, 1185, 697, 261, 1500, 1090, 1532, 930, 1047, 879, 1427, 1290, 519, 973, 167, 1030, 1154, 577, 1086, 786, 1453, 334, 1033, 188, 813, 1400, 294, 241, 796, 627, 1042, 1176, 619, 868, 445, 482, 135, 427, 238, 573, 70, 1239, 953, 726, 138, 1271, 985, 240, 268, 710, 1113, 1326, 1394, 823, 938, 601, 195, 205, 988, 1475, 1184, 1371, 1085, 747, 760, 1465, 617, 377, 1494, 73, 289, 737, 1409, 952, 863, 992, 660, 222, 557, 252, 43, 926, 801, 998, 159, 1493, 1251, 1263, 964, 1029, 844, 539, 1091, 1167, 1066, 177, 108, 1458, 477, 923, 766, 458, 1000, 676, 650, 310, 869, 705, 1129, 13, 269, 260, 75, 1428, 1060, 1195, 571, 562, 1214, 430, 659, 851, 854, 1344, 535, 1213, 777, 369, 187, 1242, 775, 1529, 802, 1225, 97, 779, 126, 640, 833, 174, 595, 158, 194, 1068, 946, 1503, 995, 592, 170, 812, 471, 1307, 486, 1462, 1365, 914, 373, 610, 417, 1327, 128, 1230, 133, 346, 1238, 1253, 910, 44, 1076, 950, 423, 90, 1430, 1324, 348, 1150, 1531, 193, 1412, 171, 1084, 1021, 1032, 1522, 1299, 405, 1471, 958, 1122, 580, 509, 446, 408, 715]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-0.3549,  0.1212,  0.0095,  0.1285, -0.5902, -0.7595,  0.2930, -0.1311,
        -0.6668,  0.2679,  0.4133,  0.4384,  0.0050,  0.0863,  0.9283, -0.2386,
        -0.4347,  0.0211, -0.5482, -0.5881,  0.3635,  0.7974, -0.0708,  0.0277,
         0.3060,  0.6573, -0.0664, -0.1364,  0.2152, -0.0694,  0.9545,  0.5186,
        -0.4286, -0.7605,  0.7025,  0.9891, -0.6683, -0.0308,  0.8484,  0.0912,
         0.4675,  0.1660,  0.9470, -0.3474,  0.5685, -0.6660, -0.3353,  0.5072,
         0.0084,  0.3467, -0.5717,  0.7976,  0.3361,  0.8629, -0.0436,  0.9182,
        -0.4613,  0.7530, -0.3987,  1.0000, -0.1714, -0.0323, -0.3805,  0.1154,
        -0.7352, -0.3378, -0.5991, -0.0573,  0.9557,  0.9710,  0.9160,  0.3482,
        -0.0344, -0.5976, -0.0085, -0.6056,  0.1304,  0.9179, -0.7200,  0.4949,
        -0.3991,  0.7442, -0.6611,  0.6799,  0.0952, -0.2164, -0.6674, -0.3528,
        -0.0951, -0.7665,  0.0833, -0.1890, -0.2624,  0.9089, -0.3644,  0.4514],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.4675e-01,  4.9634e-01,  4.9947e-01,  3.5167e-01,  6.8275e-01,
        -1.5499e-01,  3.8235e-01, -8.2602e-02,  9.2969e-02,  3.4503e-01,
        -6.9911e-02,  6.8859e-01,  4.4754e-01,  4.0567e-01,  5.8537e-01,
         4.8758e-01,  2.1827e-01,  5.5889e-01, -3.6301e-01, -2.1951e-01,
         2.5134e-01,  3.0956e-01,  7.9425e-01,  6.6511e-01,  7.1393e-01,
         8.6881e-01,  1.8570e-01, -8.7449e-02,  6.5183e-02,  5.4472e-01,
         2.1367e-01, -3.4635e-01, -3.3732e-01,  1.6719e-01,  1.6866e-01,
         1.5956e-01,  3.7950e-01,  3.3845e-01,  5.1705e-02,  3.7345e-01,
         4.0139e-01,  3.6994e-01,  2.8768e-01,  3.8171e-01,  7.6911e-01,
         3.2752e-01, -3.0061e-01,  3.1377e-01, -9.3224e-02, -1.2043e-01,
         4.4630e-01,  4.6442e-01, -6.7174e-02, -1.0307e-01, -8.5593e-02,
         4.1887e-01,  4.8836e-01, -2.3651e-01,  3.4604e-01,  9.2028e-02,
         1.6476e-01, -6.3855e-01,  7.0815e-01, -4.7516e-01, -1.3248e-01,
         2.0273e-01, -9.3000e-02,  8.0644e-01,  3.1913e-01, -1.4491e-02,
         1.5819e-01,  5.9817e-02,  1.0919e-01,  5.0951e-01, -3.8428e-02,
         1.5758e-01,  5.2751e-01, -4.7671e-01, -4.2823e-01,  4.0300e-01,
         1.2542e-01,  3.8350e-01,  3.7314e-01,  2.1977e-01,  6.2835e-01,
         1.7641e-01,  4.4517e-01,  6.0281e-02,  1.0899e-01,  8.3355e-01,
         2.6046e-01,  2.2819e-01,  2.6363e-01,  3.1669e-01,  7.0682e-01,
        -1.5943e-01,  5.2125e-01,  1.8858e-01, -3.3286e-01,  2.0695e-01,
         6.1562e-01,  3.3932e-01,  2.5271e-01,  1.3981e-01,  1.2687e-01,
         3.1925e-01,  1.8174e-01, -9.4531e-02,  3.6838e-02,  3.7488e-01,
         1.2717e-01,  4.7130e-01, -1.5721e-01, -7.7476e-02,  8.1014e-01,
        -1.5593e-01, -1.0054e-01,  4.5635e-01,  6.1519e-01,  1.4677e-01,
        -2.2707e-01, -5.5761e-03,  2.6145e-01,  5.9044e-01,  2.2731e-01,
        -2.6443e-02, -2.3644e-03, -1.0726e-01,  7.7391e-01,  3.4043e-01,
         2.8069e-01,  4.8368e-01,  4.9516e-01,  2.3034e-01,  6.8924e-01,
         3.1027e-01,  3.4570e-01,  9.7788e-01,  3.3127e-01,  3.5599e-01,
         5.0713e-01,  7.2506e-01,  6.5209e-01, -4.4852e-02,  4.7526e-01,
         7.1132e-01,  8.4672e-01, -7.5696e-03,  4.1974e-01,  2.1462e-01,
         1.7083e-01, -1.3488e-01,  4.1226e-01,  2.3258e-01,  2.5389e-01,
        -1.5678e-01, -5.3553e-01, -4.7307e-01,  2.7626e-01, -2.5093e-01,
         4.0266e-02,  2.4575e-01,  1.8999e-01, -6.1258e-03, -6.5438e-03,
         4.2919e-01, -1.2408e-01,  1.9037e-01,  5.1006e-01, -6.9298e-02,
         1.7477e-01,  3.5078e-01,  7.3873e-01,  5.5692e-01,  4.3353e-01,
        -2.2003e-01,  3.3680e-01,  6.3033e-01,  2.5604e-01,  3.2412e-01,
         6.3429e-02,  4.2685e-01,  3.0266e-01,  4.5454e-01,  3.0221e-02,
         4.9645e-01,  4.2592e-01,  2.2569e-01,  3.2492e-01, -1.9558e-01,
         6.3491e-01,  1.4616e-01,  2.0872e-01,  1.7695e-01,  3.4186e-01,
         7.6387e-01,  2.4154e-01,  3.4131e-01,  2.5706e-01, -1.7839e-01,
         4.7458e-01, -6.1654e-02, -4.2599e-02,  8.0520e-01,  1.1602e-01,
         4.0244e-01,  5.3733e-01,  1.0871e-01,  2.4645e-02,  7.1771e-01,
        -1.1157e-01,  2.2285e-01,  3.2628e-01,  1.0782e-01,  1.5037e-01,
         4.8782e-01,  4.4199e-01,  8.0079e-01, -1.9772e-01,  4.1425e-01,
         3.3853e-01,  7.4381e-01, -1.5043e-01,  5.4073e-02,  5.8050e-01,
        -2.5929e-02,  8.0517e-01, -3.4948e-01,  2.4886e-01,  5.5591e-01,
         1.8956e-01,  1.2973e-01,  7.4482e-02,  1.6208e-01,  5.5959e-01,
        -8.3063e-02,  7.0622e-01,  4.5518e-01,  4.7128e-01,  7.8142e-01,
        -2.2690e-02,  8.9799e-02,  1.5444e-01, -9.2526e-04,  1.1149e-01,
        -7.5147e-03,  5.6785e-01,  8.0659e-02,  5.8024e-01,  8.5778e-02,
         6.8099e-01,  2.7963e-01,  9.8910e-02, -1.4123e-01, -2.2274e-01,
         3.2634e-01,  4.8895e-01,  2.9214e-01, -1.3606e-01,  2.9738e-01,
         1.8581e-01,  1.2835e-01,  8.0524e-01,  5.1709e-01,  5.6002e-02,
         3.1847e-01, -5.6267e-01,  5.3282e-01,  4.8991e-01,  1.9676e-01,
        -2.5985e-02,  6.1749e-01,  3.1766e-01,  1.6751e-01,  4.4369e-01,
         1.5407e-01,  5.1959e-01,  4.5052e-01,  3.0881e-01,  2.6712e-02,
         1.2838e-01,  7.2508e-01,  4.6165e-01,  2.0328e-01,  2.4748e-01,
         4.8103e-01,  1.8086e-01, -7.3423e-02,  2.9468e-01, -8.2690e-02,
         7.6233e-02, -2.6612e-01,  3.9660e-01,  2.3032e-01,  4.0978e-02,
        -2.4679e-01, -1.2860e-01,  1.0342e-01,  5.5879e-01,  3.9662e-01,
        -1.4361e-01, -1.9822e-01,  5.5379e-01, -3.5044e-02,  6.5483e-01,
         4.9469e-01,  4.6768e-01,  9.9884e-02,  4.0240e-01,  8.2842e-02,
         8.1476e-02,  1.0099e-01,  6.6033e-01,  1.2213e-01,  6.6229e-01,
         6.9984e-02,  4.2777e-01,  7.7067e-01,  1.1222e-02,  1.7325e-01,
         1.4918e-01,  4.4225e-01,  7.1020e-01, -6.4827e-02, -1.0691e-01,
         4.2094e-01,  9.3253e-01,  7.4832e-01,  2.0421e-01,  8.0857e-01,
        -6.9927e-01,  6.9038e-01, -1.4160e-01,  2.9397e-01,  1.8431e-01,
         3.3021e-01,  3.0792e-01,  2.0105e-01,  5.4197e-01,  3.6271e-01,
         3.5776e-01,  2.2193e-01,  6.9557e-02,  8.7006e-01, -2.0600e-01,
        -3.4507e-03,  1.8695e-01,  1.0000e+00,  2.9092e-01, -2.6603e-01,
         2.7547e-01,  7.8514e-02,  1.6481e-01,  4.8246e-01,  5.4929e-01,
        -1.7485e-01,  3.9021e-02,  4.1384e-01,  5.1890e-01, -4.6565e-02,
         3.2173e-01,  1.8334e-01,  1.8262e-01,  1.4842e-01,  6.1002e-01,
         2.9751e-02,  1.2533e-01,  5.3216e-01,  2.0576e-01, -4.0360e-02,
         3.6691e-01,  8.1207e-01,  3.9485e-01,  1.3495e-01,  1.4429e-04,
         3.4063e-01,  3.1047e-02,  3.8775e-01,  2.6841e-01,  5.2225e-01,
         5.2153e-01,  5.3369e-01,  1.9615e-01,  3.0179e-01], device='cuda:0'), 'blocks.2.layer.weight': tensor([-0.0010,  0.0317,  0.4703,  ...,  0.2049, -0.0204,  0.3651],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([0.2656, 0.5401, 0.7364,  ..., 0.3438, 0.5377, 0.2040], device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.3.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([0.2656, 0.5401, 0.7364,  ..., 0.3438, 0.5377, 0.2040], device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-0.3549,  0.1212,  0.0095,  0.1285, -0.5902, -0.7595,  0.2930, -0.1311,
        -0.6668,  0.2679,  0.4133,  0.4384,  0.0050,  0.0863,  0.9283, -0.2386,
        -0.4347,  0.0211, -0.5482, -0.5881,  0.3635,  0.7974, -0.0708,  0.0277,
         0.3060,  0.6573, -0.0664, -0.1364,  0.2152, -0.0694,  0.9545,  0.5186,
        -0.4286, -0.7605,  0.7025,  0.9891, -0.6683, -0.0308,  0.8484,  0.0912,
         0.4675,  0.1660,  0.9470, -0.3474,  0.5685, -0.6660, -0.3353,  0.5072,
         0.0084,  0.3467, -0.5717,  0.7976,  0.3361,  0.8629, -0.0436,  0.9182,
        -0.4613,  0.7530, -0.3987,  1.0000, -0.1714, -0.0323, -0.3805,  0.1154,
        -0.7352, -0.3378, -0.5991, -0.0573,  0.9557,  0.9710,  0.9160,  0.3482,
        -0.0344, -0.5976, -0.0085, -0.6056,  0.1304,  0.9179, -0.7200,  0.4949,
        -0.3991,  0.7442, -0.6611,  0.6799,  0.0952, -0.2164, -0.6674, -0.3528,
        -0.0951, -0.7665,  0.0833, -0.1890, -0.2624,  0.9089, -0.3644,  0.4514],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.4675e-01,  4.9634e-01,  4.9947e-01,  3.5167e-01,  6.8275e-01,
        -1.5499e-01,  3.8235e-01, -8.2602e-02,  9.2969e-02,  3.4503e-01,
        -6.9911e-02,  6.8859e-01,  4.4754e-01,  4.0567e-01,  5.8537e-01,
         4.8758e-01,  2.1827e-01,  5.5889e-01, -3.6301e-01, -2.1951e-01,
         2.5134e-01,  3.0956e-01,  7.9425e-01,  6.6511e-01,  7.1393e-01,
         8.6881e-01,  1.8570e-01, -8.7449e-02,  6.5183e-02,  5.4472e-01,
         2.1367e-01, -3.4635e-01, -3.3732e-01,  1.6719e-01,  1.6866e-01,
         1.5956e-01,  3.7950e-01,  3.3845e-01,  5.1705e-02,  3.7345e-01,
         4.0139e-01,  3.6994e-01,  2.8768e-01,  3.8171e-01,  7.6911e-01,
         3.2752e-01, -3.0061e-01,  3.1377e-01, -9.3224e-02, -1.2043e-01,
         4.4630e-01,  4.6442e-01, -6.7174e-02, -1.0307e-01, -8.5593e-02,
         4.1887e-01,  4.8836e-01, -2.3651e-01,  3.4604e-01,  9.2028e-02,
         1.6476e-01, -6.3855e-01,  7.0815e-01, -4.7516e-01, -1.3248e-01,
         2.0273e-01, -9.3000e-02,  8.0644e-01,  3.1913e-01, -1.4491e-02,
         1.5819e-01,  5.9817e-02,  1.0919e-01,  5.0951e-01, -3.8428e-02,
         1.5758e-01,  5.2751e-01, -4.7671e-01, -4.2823e-01,  4.0300e-01,
         1.2542e-01,  3.8350e-01,  3.7314e-01,  2.1977e-01,  6.2835e-01,
         1.7641e-01,  4.4517e-01,  6.0281e-02,  1.0899e-01,  8.3355e-01,
         2.6046e-01,  2.2819e-01,  2.6363e-01,  3.1669e-01,  7.0682e-01,
        -1.5943e-01,  5.2125e-01,  1.8858e-01, -3.3286e-01,  2.0695e-01,
         6.1562e-01,  3.3932e-01,  2.5271e-01,  1.3981e-01,  1.2687e-01,
         3.1925e-01,  1.8174e-01, -9.4531e-02,  3.6838e-02,  3.7488e-01,
         1.2717e-01,  4.7130e-01, -1.5721e-01, -7.7476e-02,  8.1014e-01,
        -1.5593e-01, -1.0054e-01,  4.5635e-01,  6.1519e-01,  1.4677e-01,
        -2.2707e-01, -5.5761e-03,  2.6145e-01,  5.9044e-01,  2.2731e-01,
        -2.6443e-02, -2.3644e-03, -1.0726e-01,  7.7391e-01,  3.4043e-01,
         2.8069e-01,  4.8368e-01,  4.9516e-01,  2.3034e-01,  6.8924e-01,
         3.1027e-01,  3.4570e-01,  9.7788e-01,  3.3127e-01,  3.5599e-01,
         5.0713e-01,  7.2506e-01,  6.5209e-01, -4.4852e-02,  4.7526e-01,
         7.1132e-01,  8.4672e-01, -7.5696e-03,  4.1974e-01,  2.1462e-01,
         1.7083e-01, -1.3488e-01,  4.1226e-01,  2.3258e-01,  2.5389e-01,
        -1.5678e-01, -5.3553e-01, -4.7307e-01,  2.7626e-01, -2.5093e-01,
         4.0266e-02,  2.4575e-01,  1.8999e-01, -6.1258e-03, -6.5438e-03,
         4.2919e-01, -1.2408e-01,  1.9037e-01,  5.1006e-01, -6.9298e-02,
         1.7477e-01,  3.5078e-01,  7.3873e-01,  5.5692e-01,  4.3353e-01,
        -2.2003e-01,  3.3680e-01,  6.3033e-01,  2.5604e-01,  3.2412e-01,
         6.3429e-02,  4.2685e-01,  3.0266e-01,  4.5454e-01,  3.0221e-02,
         4.9645e-01,  4.2592e-01,  2.2569e-01,  3.2492e-01, -1.9558e-01,
         6.3491e-01,  1.4616e-01,  2.0872e-01,  1.7695e-01,  3.4186e-01,
         7.6387e-01,  2.4154e-01,  3.4131e-01,  2.5706e-01, -1.7839e-01,
         4.7458e-01, -6.1654e-02, -4.2599e-02,  8.0520e-01,  1.1602e-01,
         4.0244e-01,  5.3733e-01,  1.0871e-01,  2.4645e-02,  7.1771e-01,
        -1.1157e-01,  2.2285e-01,  3.2628e-01,  1.0782e-01,  1.5037e-01,
         4.8782e-01,  4.4199e-01,  8.0079e-01, -1.9772e-01,  4.1425e-01,
         3.3853e-01,  7.4381e-01, -1.5043e-01,  5.4073e-02,  5.8050e-01,
        -2.5929e-02,  8.0517e-01, -3.4948e-01,  2.4886e-01,  5.5591e-01,
         1.8956e-01,  1.2973e-01,  7.4482e-02,  1.6208e-01,  5.5959e-01,
        -8.3063e-02,  7.0622e-01,  4.5518e-01,  4.7128e-01,  7.8142e-01,
        -2.2690e-02,  8.9799e-02,  1.5444e-01, -9.2526e-04,  1.1149e-01,
        -7.5147e-03,  5.6785e-01,  8.0659e-02,  5.8024e-01,  8.5778e-02,
         6.8099e-01,  2.7963e-01,  9.8910e-02, -1.4123e-01, -2.2274e-01,
         3.2634e-01,  4.8895e-01,  2.9214e-01, -1.3606e-01,  2.9738e-01,
         1.8581e-01,  1.2835e-01,  8.0524e-01,  5.1709e-01,  5.6002e-02,
         3.1847e-01, -5.6267e-01,  5.3282e-01,  4.8991e-01,  1.9676e-01,
        -2.5985e-02,  6.1749e-01,  3.1766e-01,  1.6751e-01,  4.4369e-01,
         1.5407e-01,  5.1959e-01,  4.5052e-01,  3.0881e-01,  2.6712e-02,
         1.2838e-01,  7.2508e-01,  4.6165e-01,  2.0328e-01,  2.4748e-01,
         4.8103e-01,  1.8086e-01, -7.3423e-02,  2.9468e-01, -8.2690e-02,
         7.6233e-02, -2.6612e-01,  3.9660e-01,  2.3032e-01,  4.0978e-02,
        -2.4679e-01, -1.2860e-01,  1.0342e-01,  5.5879e-01,  3.9662e-01,
        -1.4361e-01, -1.9822e-01,  5.5379e-01, -3.5044e-02,  6.5483e-01,
         4.9469e-01,  4.6768e-01,  9.9884e-02,  4.0240e-01,  8.2842e-02,
         8.1476e-02,  1.0099e-01,  6.6033e-01,  1.2213e-01,  6.6229e-01,
         6.9984e-02,  4.2777e-01,  7.7067e-01,  1.1222e-02,  1.7325e-01,
         1.4918e-01,  4.4225e-01,  7.1020e-01, -6.4827e-02, -1.0691e-01,
         4.2094e-01,  9.3253e-01,  7.4832e-01,  2.0421e-01,  8.0857e-01,
        -6.9927e-01,  6.9038e-01, -1.4160e-01,  2.9397e-01,  1.8431e-01,
         3.3021e-01,  3.0792e-01,  2.0105e-01,  5.4197e-01,  3.6271e-01,
         3.5776e-01,  2.2193e-01,  6.9557e-02,  8.7006e-01, -2.0600e-01,
        -3.4507e-03,  1.8695e-01,  1.0000e+00,  2.9092e-01, -2.6603e-01,
         2.7547e-01,  7.8514e-02,  1.6481e-01,  4.8246e-01,  5.4929e-01,
        -1.7485e-01,  3.9021e-02,  4.1384e-01,  5.1890e-01, -4.6565e-02,
         3.2173e-01,  1.8334e-01,  1.8262e-01,  1.4842e-01,  6.1002e-01,
         2.9751e-02,  1.2533e-01,  5.3216e-01,  2.0576e-01, -4.0360e-02,
         3.6691e-01,  8.1207e-01,  3.9485e-01,  1.3495e-01,  1.4429e-04,
         3.4063e-01,  3.1047e-02,  3.8775e-01,  2.6841e-01,  5.2225e-01,
         5.2153e-01,  5.3369e-01,  1.9615e-01,  3.0179e-01], device='cuda:0'), 'blocks.2.layer.weight': tensor([-0.0010,  0.0317,  0.4703,  ...,  0.2049, -0.0204,  0.3651],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([0.2656, 0.5401, 0.7364,  ..., 0.3438, 0.5377, 0.2040], device='cuda:0')}
topk_layer inside model after retrieval:  [5017, 6047, 1653, 3916, 170, 5041, 2613, 4323, 6098, 2691, 5433, 3067, 4958, 4900, 3563, 1730, 62, 2026, 3435, 2075, 2778, 5255, 4087, 4574, 4983, 634, 5795, 1366, 948, 4473, 4359, 3906, 547, 750, 1229, 5651, 4738, 3197, 5004, 785, 514, 1599, 4502, 2051, 5968, 3718, 1680, 5558, 3938, 353, 5984, 5349, 4768, 815, 5956, 1310, 3516, 4415, 2483, 5292, 2021, 4333, 5515, 4090, 3988, 1149, 2470, 1856, 2392, 549, 4350, 1022, 1921, 999, 4682, 3986, 2462, 921, 3227, 265, 706, 1055, 5541, 4429, 680, 3972, 3828, 1635, 5175, 1587, 1685, 2966, 2696, 1318, 4262, 2037, 4163, 4746, 4699, 2299, 3481, 2398, 3740, 4029, 2413, 5314, 3181, 2804, 837, 3511, 5918, 1575, 3807, 1983, 1778, 63, 3486, 51, 3669, 4924, 1526, 817, 3179, 5498, 6, 1385, 1066, 534, 5733, 3210, 1086, 4357, 2176, 1331, 1670, 4408, 117, 2344, 1181, 3222, 1911, 2427, 2979, 1304, 5752, 2453, 5283, 5188, 745, 5591, 4968, 1344, 3603, 261, 1585, 5, 2786, 5406, 8, 300, 2300, 4905, 2714, 3431, 2661, 5571, 4360, 4556, 3750, 771, 211, 3888, 4486, 813, 2049, 4025, 6114, 1294, 845, 3414, 5312, 5001, 2404, 2769, 4094, 4624, 5065, 4496, 455, 3090, 6070, 3101, 1760, 1037, 1913, 1831, 1036, 1117, 3449, 5919, 418, 5455, 5299, 485, 905, 4733, 5091, 3959, 4739, 183, 2078, 1009, 1043, 4134, 2085, 5717, 3543, 3430, 552, 3830, 1512, 1822, 2914, 5705, 822, 3983, 5860, 688, 4687, 6134, 2201, 5222, 1342, 784, 1112, 1198, 2938, 2041, 4862, 3321, 5579, 5931, 526, 2608, 1872, 1163, 2780, 4218, 4256, 872, 122, 4396, 551, 1437, 5473, 4513, 2584, 1883, 3495, 2136, 5477, 5977, 3159, 1703, 1118, 4538, 1346, 2698, 4397, 3941, 2047, 5531, 2488, 3581, 132, 339, 3502, 2850, 1792, 2418, 5346, 2354, 6054, 3001, 4227, 1790, 4598, 2056, 4100, 3725, 909, 3909, 4451, 4072, 4660, 2635, 1102, 3656, 2581, 176, 1001, 2152, 2444, 3448, 685, 497, 2920, 652, 621, 4390, 3186, 4221, 1097, 2069, 4177, 1620, 4118, 4377, 178, 1615, 1950, 3010, 2092, 2793, 4694, 2103, 5834, 6004, 3905, 632, 844, 4618, 4131, 4063, 1403, 583, 1351, 5469, 1753, 1514, 4602, 4173, 2835, 2750, 3215, 2274, 5457, 3507, 5482, 5981, 3062, 5135, 2847, 2010, 4223, 2662, 3824, 2447, 5847, 165, 5276, 2210, 5493, 129, 198, 5780, 4853, 1544, 2511, 1806, 5910, 4382, 956, 2426, 1443, 1622, 4906, 5146, 4159, 5214, 668, 4876, 1284, 194, 227, 2980, 1187, 6090, 49, 4036, 436, 3470, 6058, 3488, 1594, 5322, 1283, 1864, 4539, 860, 3420, 3138, 818, 1861, 257, 2700, 2139, 2695, 3351, 2379, 4544, 1176, 2342, 2128, 1343, 2514, 4394, 686, 5212, 3895, 3540, 4110, 109, 3312, 893, 5542, 6053, 3473, 4301, 5196, 4937, 1337, 5407, 4219, 2972, 208, 2812, 4266, 4646, 5027, 4938, 3719, 1075, 295, 5436, 78, 5243, 4804, 638, 1564, 1894, 4980, 5929, 5853, 3640, 4964, 5821, 5297, 5201, 398, 5909, 59, 2542, 495, 6119, 4097, 2292, 1560, 2666, 2627, 1873, 3912, 4636, 3436, 26, 1453, 1394, 4947, 5216, 4089, 4166, 1415, 4954, 2083, 1834, 4212, 3809, 4635, 4102, 3866, 2533, 5838, 5412, 221, 6116, 2367, 3944, 1988, 1132, 1380, 1454, 4264, 1499, 5786, 1316, 4306, 3584, 171, 2675, 2667, 5205, 2530, 5108, 5045, 3793, 2703, 1684, 4421, 1191, 294, 2008, 41, 3760, 4280, 2923, 13, 4651, 1309, 1185, 1435, 6122, 2945, 2755, 4114, 1371, 2808, 4319, 2679, 5520, 4897, 6142, 2089, 2563, 5630, 292, 3151, 2668, 4553, 2927, 2156, 5076, 3193, 313, 3670, 3696, 709, 252, 2779, 1108, 1312, 2236, 2098, 2058, 772, 4257, 3330, 5791, 2957, 3569, 65, 5056, 3602, 5739, 1641, 5318, 722, 5445, 3755, 24, 5396, 675, 760, 3873, 673, 926, 3017, 1168, 5354, 4546, 2314, 5607, 259, 3658, 1608, 2007, 4289, 4777, 888, 3434, 4736, 4445, 4107, 1164, 2173, 4431, 576, 3802, 4120, 1879, 3191, 4220, 4076, 4961, 5839, 3150, 5206, 3377, 3096, 255, 5087, 2048, 2393, 1956, 3039, 1162, 4603, 5161, 5268, 491, 2580, 5423, 795, 2597, 463, 5043, 4236, 4291, 1230, 1882, 1978, 1511, 1866, 4728, 5942, 1362, 4879, 2348, 2845, 796, 5582, 4919, 2834, 4402, 6109, 1634, 3735, 6008, 3914, 1960, 5172, 625, 4270, 3556, 4541, 2976, 6089, 2860, 2962, 2082, 3479, 5596, 4688, 5321, 4762, 2711, 3775, 3553, 2251, 4885, 2887, 3657, 1096, 5401, 4640, 5921, 5204, 2033, 2206, 2802, 4412, 2928, 441, 1578, 951, 287, 1357, 3761, 4176, 2101, 14, 4299, 2310, 5936, 2073, 2538, 2077, 4628, 4104, 870, 2318, 4447, 2227, 5932, 2736, 5386, 4162, 2918, 5164, 2859, 3804, 1161, 6102, 4881, 246, 5503, 707, 4840, 2917, 1941, 1836, 2607, 763, 5454, 2759, 1115, 530, 1936, 426, 1755, 980, 620, 846, 1949, 1537, 1846, 865, 223, 3686, 4785, 5697, 1021, 4028, 1887, 2100, 2268, 4449, 3133, 3538, 5626, 826, 5008, 1569, 1457, 780, 1194, 2889, 5103, 229, 2460, 2798, 3918, 5680, 89, 493, 5818, 4658, 3594, 2445, 1151, 1535, 1273, 1169, 1422, 1264, 4239, 1543, 834, 5015, 2203, 3425, 4365, 6125, 3170, 1368, 4752, 1820, 3834, 2680, 2112, 4778, 3965, 5116, 1028, 3027, 2715, 4300, 2443, 1574, 2519, 3947, 1649, 144, 1088, 5854, 2792, 5885, 2382, 3163, 1912, 765, 3023, 4013, 2022, 527, 5980, 4482, 3636, 3283, 2067, 2106, 2071, 3242, 1999, 2449, 5893, 1376, 3025, 966, 1204, 5245, 1470, 4185, 4410, 5693, 4614, 3943, 4034, 3384, 4460, 5199, 3759, 1116, 3902, 2861, 3836, 2585, 179, 1697, 5994, 1702, 2817, 4519, 4330, 4480, 4882, 555, 5081, 1279, 3429, 545, 4154, 590, 4135, 5887, 970, 363, 3032, 2419, 4956, 5642, 1345, 1845, 1438, 4775, 6096, 878, 1600, 425, 1335, 1914, 2016, 5523, 3277, 4051, 6021, 1980, 5862, 4566, 450, 3350, 175, 1665, 1969, 524, 1095, 4363, 3118, 3097, 4669, 5460, 3126, 4242, 5702, 5552, 52, 3733, 4142, 2547, 1839, 271, 674, 511, 4248]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}

 Model STL10_CIFAR10_CLe4977f8a-d515-45b7-aaf1-e9dd54f7b531 loaded successfuly with best perf



 ----- Architecture Block BatchNorm2dSK3962(5, 5)1.0reflect, number 0 -----
- BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- AvgPool2d(kernel_size=4, stride=2, padding=1)

 ----- Architecture Block BatchNorm2dSK963842(3, 3)0.6499999761581421reflect, number 1 -----
- BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK38415362(3, 3)0.44999998807907104reflect, number 2 -----
- BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.4)
- AvgPool2d(kernel_size=4, stride=2, padding=1)

 ----- Architecture Block BatchNorm2dSK153661442(3, 3)0.25reflect, number 3 -----
- BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 4 -----
- Flatten(start_dim=1, end_dim=-1)
- Dropout(p=0.5, inplace=False)
- Linear(in_features=221184, out_features=10, bias=True)
model.heads:  [{'blocks.4.layer.bias': tensor([ 0.0016, -0.0015,  0.0004, -0.0005,  0.0020,  0.0031,  0.0054, -0.0016,
         0.0026,  0.0032], device='cuda:0'), 'blocks.4.layer.weight': tensor([[ 0.0400,  0.0110,  0.0159,  ..., -0.0202, -0.0415, -0.0304],
        [ 0.0107, -0.0007,  0.0315,  ..., -0.0158,  0.0113,  0.0158],
        [ 0.0364, -0.0160,  0.0407,  ...,  0.0141,  0.0031,  0.0035],
        ...,
        [ 0.0219,  0.0111,  0.0037,  ...,  0.0494,  0.0133, -0.0125],
        [-0.0186,  0.0274, -0.0178,  ..., -0.0034, -0.0105,  0.0038],
        [-0.0212, -0.0315, -0.0387,  ..., -0.0497,  0.0010, -0.0065]],
       device='cuda:0')}, {'blocks.4.layer.bias': tensor([ 0.0099, -0.0016, -0.0140, -0.0056,  0.0170,  0.0083,  0.0102,  0.0137,
        -0.0154,  0.0123], device='cuda:0'), 'blocks.4.layer.weight': tensor([[ 0.1073, -0.0009,  0.0072,  ...,  0.2254,  0.0617,  0.0462],
        [-0.0917, -0.0167, -0.0427,  ..., -0.0005,  0.0644,  0.1190],
        [-0.0364, -0.1114,  0.0774,  ..., -0.0271, -0.0134, -0.0558],
        ...,
        [-0.1048,  0.0533,  0.1243,  ..., -0.0253,  0.0444, -0.1016],
        [-0.0325,  0.1420,  0.0787,  ..., -0.0134,  0.0540,  0.0946],
        [-0.0129, -0.1319, -0.2293,  ..., -0.1601, -0.1963, -0.1403]],
       device='cuda:0')}]
[Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=1.4)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=221184, out_features=10, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)]
LAYER NAME:  Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=1.4)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=221184, out_features=10, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)
LAYER CHILDREN:  [BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
  (activation): Triangle(power=1.4)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1)
), BasicBlock(
  (operations): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
  )
  (layer): Linear(in_features=221184, out_features=10, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)]
subsubl NAME:  Sequential(
  (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=4, stride=2, padding=1)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=4, stride=2, padding=1)
subsubl NAME:  Triangle(power=1.4)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1)
subsubl NAME:  Sequential(
  (0): Flatten(start_dim=1, end_dim=-1)
)
subsubl NAME:  Linear(in_features=221184, out_features=10, bias=True)
subsubl NAME:  Dropout(p=0.5, inplace=False)
CONFIG MODE:  unsupervised
CONFIG MODE:  supervised
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
INDICES:  5000
IMAGE SIZE: torch.Size([64, 3, 96, 96])
Accuracy of the network on the 1st dataset: 21.325 %
Test loss on the 1st dataset: 48.583
results:  {'count': 3, 'R0': {'train_loss': 0.03978398069739342, 'train_acc': 98.27600121498108, 'test_loss': 3.0680172443389893, 'test_acc': 66.5625, 'convergence': 23.72207260131836, 'R1': 1, 'dataset_sup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 5000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 4, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': True, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 100, 'print_freq': 10, 'validation': False, 'continual_learning': False}, 'dataset_unsup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 100000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 32, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'unlabeled', 'nb_epoch': 1, 'print_freq': 25, 'validation': False, 'continual_learning': False}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 32, 'nb_epoch': 1, 'print_freq': 25, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 100, 'print_freq': 10, 'lr': 0.001}}}, 'model_config': {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.036828478186799345, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 221184, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}, 'R1': {'train_loss': 0.12020615488290787, 'train_acc': 97.23520278930664, 'test_loss': 0.14057114720344543, 'test_acc': 81.9000015258789, 'convergence': 17.5535945892334, 'R1': 49, 'dataset_sup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 96}, 'dataset_unsup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 96}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 10, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}}, 'cl_hyper': {'training_mode': 'consecutive', 'cf_sol': False, 'head_sol': False, 'top_k': 0.15, 'topk_lock': False, 'high_lr': 0.15, 'low_lr': 1.0, 't_criteria': 'activations', 'delta_w_interval': 50.0, 'heads_basis_t': 0.8595000076293946, 'n_tasks': 2}, 'eval_0': {'test_loss': 48.583187103271484, 'test_acc': 21.325000762939453, 'convergence': 24.0014705657959, 'R1': 0, 'dataset_sup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 5000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 4, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': True, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 100, 'print_freq': 10, 'validation': False, 'continual_learning': False}, 'dataset_unsup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 100000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 32, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'unlabeled', 'nb_epoch': 1, 'print_freq': 25, 'validation': False, 'continual_learning': False}}, 'model_name': 'STL10_CIFAR10_CLe4977f8a-d515-45b7-aaf1-e9dd54f7b531'}
MKDIR
MULTD_CL_STL10__1_CIFAR10/STL10_CIFAR10_CLe4977f8a-d515-45b7-aaf1-e9dd54f7b531.json

The device used will be: 
True
cuda:0
BLOCKS:  {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.25, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10}, 'pool': None, 'activation': None}}
[[0,3],[5,7]]


 ########### WARNING ############


 Invalid combination of parameters, provide either: [--classes, --dataset-sup, --dataset-unsup] or [--dataset-sup-1, --dataset-unsup-1, --dataset-sup-2, --dataset-unsup-2]
The continual learning is implemented per tasks where each task is made up of different classes 
 of the same dataset, so only one dataset will be considered.


 ################################


{'training_mode': 'consecutive', 'cf_sol': False, 'head_sol': True, 'top_k': 0.15, 'topk_lock': False, 'high_lr': 0.15, 'low_lr': 1.0, 't_criteria': 'activations', 'delta_w_interval': 50.0, 'heads_basis_t': 0.9, 'n_tasks': 2}
CL:  True
{'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 5000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': True, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 100, 'print_freq': 10, 'validation': False, 'continual_learning': True}
CL:  False
{'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 5000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': True, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 100, 'print_freq': 10, 'validation': False, 'continual_learning': False}
CL:  False
{'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 100000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 32, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'unlabeled', 'nb_epoch': 1, 'print_freq': 25, 'validation': False, 'continual_learning': False}
SEED:  0
block 0, size : 96 48 48
96
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.25, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10}, 'pool': None, 'activation': None}}
range = 2.886751345948129
block 1, size : 384 24 24
384
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.25, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10}, 'pool': None, 'activation': None}}
range = 0.8505172717997146
block 2, size : 1536 12 12
1536
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.25, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10}, 'pool': None, 'activation': None}}
range = 0.4252586358998573
block 3, size : 6144 6 6
6144
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.25, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10}, 'pool': None, 'activation': None}}
range = 0.21262931794992865
NOWWWW
6 6
10
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.25, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 221184, 'old_channels': 6144}, 'pool': None, 'activation': None}}
blocks['b%s' % 4] 221184 221184
range = 0.036828478186799345
train_layer_order: consecutive {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 32, 'nb_epoch': 1, 'print_freq': 25, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 100, 'print_freq': 10, 'lr': 0.001}}
The device used will be: 
True
cuda:0
avg_deltas:  {}
topk_layer:  None
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}

 ----- Architecture Block BatchNorm2dSK3962(5, 5)1.0reflect, number 0 -----
- BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- AvgPool2d(kernel_size=4, stride=2, padding=1)

 ----- Architecture Block BatchNorm2dSK963842(3, 3)0.6499999761581421reflect, number 1 -----
- BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK38415362(3, 3)0.44999998807907104reflect, number 2 -----
- BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.4)
- AvgPool2d(kernel_size=4, stride=2, padding=1)

 ----- Architecture Block BatchNorm2dSK153661442(3, 3)0.25reflect, number 3 -----
- BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 4 -----
- Flatten(start_dim=1, end_dim=-1)
- Dropout(p=0.5, inplace=False)
- Linear(in_features=221184, out_features=10, bias=True)
model.heads:  []
[Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=1.4)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=221184, out_features=10, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)]
LAYER NAME:  Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=1.4)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=221184, out_features=10, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)
LAYER CHILDREN:  [BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
  (activation): Triangle(power=1.4)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1)
), BasicBlock(
  (operations): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
  )
  (layer): Linear(in_features=221184, out_features=10, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)]
subsubl NAME:  Sequential(
  (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=4, stride=2, padding=1)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=4, stride=2, padding=1)
subsubl NAME:  Triangle(power=1.4)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1)
subsubl NAME:  Sequential(
  (0): Flatten(start_dim=1, end_dim=-1)
)
subsubl NAME:  Linear(in_features=221184, out_features=10, bias=True)
subsubl NAME:  Dropout(p=0.5, inplace=False)
CONFIG MODE:  unsupervised

 ********** Hebbian Unsupervised learning of blocks [0, 1, 2, 3] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
INDICES:  5000
IMAGE SIZE: torch.Size([32, 3, 96, 96])
SAVING FOLDER FOR UNSUP:  STL10_CIFAR10_CLbe1cb3cb-b5f8-4a9d-bda0-16cc8cbeda29
prev_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
activations_sum.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
DEPTH:  4
WTA IN delta_weight:  tensor([[[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         ...,
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00]],

        [[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         ...,
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00]],

        [[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         ...,
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00]],

        ...,

        [[-9.4754e-16, -9.4754e-16, -9.4754e-16,  ..., -9.4754e-16,
          -9.4754e-16, -9.4754e-16],
         [-9.4754e-16, -9.4754e-16, -9.4754e-16,  ..., -9.4754e-16,
          -9.4754e-16, -9.4754e-16],
         [-9.4754e-16, -9.4754e-16, -9.4754e-16,  ..., -9.4754e-16,
          -9.4754e-16, -9.4754e-16],
         ...,
         [-9.4754e-16, -9.4754e-16, -9.4754e-16,  ..., -9.4754e-16,
          -9.4754e-16, -9.4754e-16],
         [-9.4754e-16, -9.4754e-16, -9.4754e-16,  ..., -9.4754e-16,
          -9.4754e-16, -9.4754e-16],
         [-9.4754e-16, -9.4754e-16, -9.4754e-16,  ..., -9.4754e-16,
          -9.4754e-16, -9.4754e-16]],

        [[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         ...,
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00]],

        [[-5.5351e-43, -5.5351e-43, -5.5351e-43,  ..., -5.5351e-43,
          -5.5351e-43, -5.5351e-43],
         [-5.5351e-43, -5.5351e-43, -5.5351e-43,  ..., -5.5351e-43,
          -5.5351e-43, -5.5351e-43],
         [-5.5351e-43, -5.5351e-43, -5.5351e-43,  ..., -5.5351e-43,
          -5.5351e-43, -5.5351e-43],
         ...,
         [-5.5351e-43, -5.5351e-43, -5.5351e-43,  ..., -5.5351e-43,
          -5.5351e-43, -5.5351e-43],
         [-5.5351e-43, -5.5351e-43, -5.5351e-43,  ..., -5.5351e-43,
          -5.5351e-43, -5.5351e-43],
         [-5.5351e-43, -5.5351e-43, -5.5351e-43,  ..., -5.5351e-43,
          -5.5351e-43, -5.5351e-43]]], device='cuda:0')
self.avg_deltas_layer:  <class 'NoneType'>
self.topk_layer:  <class 'NoneType'>
self.avg_deltas_layer:  <class 'NoneType'>
self.topk_layer:  <class 'NoneType'>
self.avg_deltas_layer:  <class 'NoneType'>
self.topk_layer:  <class 'NoneType'>
self.avg_deltas_layer:  <class 'NoneType'>
self.topk_layer:  <class 'NoneType'>
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
activations_sum[k] 96
activations_sum[k]:  [68, 25, 59, 30, 77, 93, 69, 55, 35, 14]
topk_kernels len:  1
topk_kernels keys:  ['conv0']
topk_kernels:  {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38]}
activations_sum[k] len:  96
activations_sum[k] 384
activations_sum[k]:  [345, 365, 75, 354, 190, 100, 56, 180, 191, 105]
topk_kernels len:  2
topk_kernels keys:  ['conv0', 'conv1']
topk_kernels:  {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286]}
activations_sum[k] len:  384
activations_sum[k] 1536
activations_sum[k]:  [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35]
topk_kernels len:  3
topk_kernels keys:  ['conv0', 'conv1', 'conv2']
topk_kernels:  {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276]}
activations_sum[k] len:  1536
activations_sum[k] 6144
activations_sum[k]:  [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822]
topk_kernels len:  4
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3']
topk_kernels:  {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
activations_sum[k] len:  6144
['conv0', 'conv1', 'conv2', 'conv3']
delta_weights INFO: 
NUM OF TRACKED COV LAYERS:  4
NUM OF TRACKED WEIGHTS CHANGES PER LAYER:  4
avg_deltas INFO:  <class 'dict'>
avg_deltas keys:  ['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight']
################################################
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 4.52e-02	time: 00:00:39	Acc_train 0.00	Acc_test 0.00	convergence: 2.37e+01	R1: 1	Info MB:0.000e+00/SB:0.000e+00/MW:4.460e-06/SW:2.112e-01/MR:2.472e+01/SR:2.331e+00/MeD:1.064e+00/MaD:2.372e+01/MW:0.543/MAW:0.457
|       0 |       1 |        2 |        3 |        4 |        5 |        6 |        7 |        8 |        9 |       10 |       11 |       12 |       13 |       14 |       15 |       16 |       17 |       18 |       19 |       20 |       21 |       22 |       23 |       24 |       25 |       26 |      27 |       28 |       29 |
|---------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------|
|   0.049 |   0.049 |   0.0496 |   0.0495 |   0.0494 |   0.0489 |   0.0501 |   0.0491 |   0.0384 |   0.0488 |   0.0495 |   0.0491 |   0.0489 |   0.0495 |   0.0484 |   0.0497 |   0.0494 |   0.0495 |   0.0492 |   0.0493 |   0.0492 |   0.0463 |   0.0491 |   0.0487 |   0.0484 |   0.0491 |   0.0499 |   0.049 |   0.0495 |   0.0496 |
|  24.97  |  24.96  |  25.6    |  25.52   |  25.43   |  24.9    |  26.1    |  25.07   |  15.72   |  24.85   |  25.52   |  25.09   |  24.91   |  25.5    |  24.4    |  25.67   |  25.36   |  25.47   |  25.17   |  25.29   |  25.17   |  22.48   |  25.11   |  24.76   |  24.47   |  25.11   |  25.91   |  25.05  |  25.49   |  25.62   |
|   0.01  |   0.01  |   0      |   0      |   0.01   |   0.02   |   0.01   |   0      |   0.12   |   0      |   0.01   |   0.01   |   0      |   0      |   0.02   |   0      |   0      |   0      |   0      |   0      |   0      |   0.04   |   0      |   0.01   |   0.01   |   0.01   |   0.02   |   0     |   0.01   |   0      |
| nan     | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      |
| nan     | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      |
| nan     | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:1.522e-04/SW:3.711e-01/MR:2.156e+01/SR:3.332e+00/MeD:2.742e+00/MaD:1.700e+01/MW:0.596/MAW:0.404
|        0 |        1 |        2 |        3 |        4 |        5 |        6 |        7 |        8 |       9 |       10 |       11 |       12 |       13 |       14 |       15 |       16 |       17 |       18 |       19 |      20 |       21 |       22 |       23 |       24 |       25 |       26 |       27 |       28 |       29 |
|----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------|
|   0.0491 |   0.0483 |   0.0451 |   0.0462 |   0.0428 |   0.0428 |   0.0492 |   0.0405 |   0.0453 |   0.049 |   0.0398 |   0.0475 |   0.0434 |   0.0357 |   0.0474 |   0.0448 |   0.0387 |   0.0497 |   0.0469 |   0.0492 |   0.041 |   0.0467 |   0.0404 |   0.0478 |   0.0432 |   0.0429 |   0.0488 |   0.0457 |   0.0397 |   0.0486 |
|  25.08   |  24.36   |  21.36   |  22.34   |  19.33   |  19.35   |  25.22   |  17.37   |  21.51   |  25.05  |  16.81   |  23.55   |  19.85   |  13.75   |  23.48   |  21.08   |  16.01   |  25.68   |  22.97   |  25.2    |  17.82  |  22.81   |  17.32   |  23.87   |  19.69   |  19.39   |  24.77   |  21.84   |  16.78   |  24.63   |
|   0      |   0.01   |   0.06   |   0.06   |   0.06   |   0.1    |   0      |   0.07   |   0.06   |   0     |   0.1    |   0.03   |   0.07   |   0.21   |   0.03   |   0.04   |   0.22   |   0      |   0.02   |   0      |   0.14  |   0.03   |   0.2    |   0.02   |   0.05   |   0.05   |   0      |   0.04   |   0.11   |   0      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:-3.013e-03/SW:7.925e-01/MR:2.315e+01/SR:2.575e+00/MeD:1.984e+00/MaD:1.146e+01/MW:0.695/MAW:0.305
|        0 |        1 |        2 |        3 |        4 |        5 |        6 |        7 |        8 |        9 |       10 |       11 |       12 |       13 |       14 |       15 |       16 |       17 |       18 |       19 |       20 |       21 |       22 |       23 |       24 |       25 |       26 |      27 |      28 |       29 |
|----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+---------+----------|
|   0.0245 |   0.0246 |   0.0242 |   0.0241 |   0.0199 |   0.0235 |   0.0244 |   0.0249 |   0.0244 |   0.0234 |   0.0248 |   0.0239 |   0.0242 |   0.0242 |   0.0242 |   0.0241 |   0.0241 |   0.0219 |   0.0243 |   0.0237 |   0.0231 |   0.0248 |   0.0244 |   0.0247 |   0.0233 |   0.0226 |   0.0213 |   0.024 |   0.024 |   0.0251 |
|  25.05   |  25.12   |  24.39   |  24.27   |  16.82   |  23.03   |  24.87   |  25.72   |  24.84   |  22.85   |  25.62   |  23.77   |  24.33   |  24.47   |  24.36   |  24.18   |  24.22   |  20.12   |  24.69   |  23.54   |  22.28   |  25.7    |  24.78   |  25.37   |  22.75   |  21.47   |  19.17   |  24.11  |  24.04  |  26.12   |
|   0.04   |   0      |   0.02   |   0.01   |   1.6    |   0.19   |   0.02   |   0.05   |   0.01   |   0.2    |   0      |   0.07   |   0.16   |   0.12   |   0.02   |   0.06   |   0      |   0.66   |   0.01   |   0.16   |   0.16   |   0      |   0.12   |   0.11   |   0.28   |   0.49   |   1.15   |   0.03  |   0.07  |   0.02   |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan     | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan     | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan     | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:-1.311e-02/SW:1.508e+00/MR:1.280e+01/SR:2.610e+00/MeD:2.095e+00/MaD:7.090e+00/MW:0.547/MAW:0.453
|       0 |      1 |       2 |      3 |      4 |       5 |       6 |       7 |       8 |       9 |      10 |      11 |      12 |      13 |      14 |      15 |      16 |      17 |      18 |     19 |      20 |     21 |      22 |      23 |      24 |      25 |      26 |      27 |      28 |      29 |
|---------+--------+---------+--------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------|
|   0.256 |   0.33 |   0.295 |   0.31 |   0.24 |   0.219 |   0.277 |   0.329 |   0.245 |   0.348 |   0.254 |   0.272 |   0.289 |   0.305 |   0.269 |   0.309 |   0.203 |   0.276 |   0.275 |   0.26 |   0.303 |   0.27 |   0.287 |   0.324 |   0.305 |   0.236 |   0.303 |   0.292 |   0.276 |   0.328 |
|  11.22  |  17.99 |  14.58  |  16.02 |  10.03 |   8.5   |  12.95  |  17.88  |  10.42  |  19.89  |  11.08  |  12.58  |  14.04  |  15.56  |  12.34  |  15.91  |   7.44  |  12.89  |  12.84  |  11.56 |  15.33  |  12.4  |  13.85  |  17.37  |  15.56  |   9.67  |  15.39  |  14.35  |  12.92  |  17.79  |
|   0.64  |   0.18 |   0.17  |   0.26 |   1.98 |   3.7   |   0.76  |   0.33  |   1.3   |   0.13  |   1.05  |   0.49  |   0.54  |   0.25  |   1.82  |   0.22  |   3.77  |   0.5   |   0.66  |   1.12 |   0.45  |   0.84 |   0.42  |   0.27  |   0.39  |   2.91  |   0.37  |   0.58  |   0.34  |   0.18  |
| nan     | nan    | nan     | nan    | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     |
| nan     | nan    | nan     | nan    | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     |
| nan     | nan    | nan     | nan    | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLbe1cb3cb-b5f8-4a9d-bda0-16cc8cbeda29/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
CONFIG MODE:  supervised

 ********** Supervised learning of blocks [4] **********
SAVING FOLDER FOR SUP:  STL10_CIFAR10_CLbe1cb3cb-b5f8-4a9d-bda0-16cc8cbeda29
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
INDICES:  5000
IMAGE SIZE: torch.Size([64, 3, 96, 96])
num_blocks:  5
Epoch: [1/100]	lr: 1.00e-03	time: 00:01:06	Loss_train 7.66232	Acc_train 36.62	/	Loss_test 3.46408	Acc_test 46.84
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLbe1cb3cb-b5f8-4a9d-bda0-16cc8cbeda29/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [10/100]	lr: 1.00e-03	time: 00:01:46	Loss_train 2.96399	Acc_train 61.89	/	Loss_test 4.18236	Acc_test 58.64
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLbe1cb3cb-b5f8-4a9d-bda0-16cc8cbeda29/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [20/100]	lr: 1.00e-03	time: 00:02:29	Loss_train 1.76923	Acc_train 76.53	/	Loss_test 5.84089	Acc_test 55.47
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLbe1cb3cb-b5f8-4a9d-bda0-16cc8cbeda29/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [30/100]	lr: 5.00e-04	time: 00:03:12	Loss_train 0.56039	Acc_train 88.94	/	Loss_test 3.81810	Acc_test 62.55
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLbe1cb3cb-b5f8-4a9d-bda0-16cc8cbeda29/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [40/100]	lr: 2.50e-04	time: 00:03:56	Loss_train 0.33715	Acc_train 92.10	/	Loss_test 3.39348	Acc_test 64.84
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLbe1cb3cb-b5f8-4a9d-bda0-16cc8cbeda29/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [50/100]	lr: 2.50e-04	time: 00:04:39	Loss_train 0.18524	Acc_train 94.79	/	Loss_test 3.65853	Acc_test 64.54
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLbe1cb3cb-b5f8-4a9d-bda0-16cc8cbeda29/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [60/100]	lr: 1.25e-04	time: 00:05:22	Loss_train 0.10238	Acc_train 96.51	/	Loss_test 3.27746	Acc_test 65.28
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLbe1cb3cb-b5f8-4a9d-bda0-16cc8cbeda29/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [70/100]	lr: 6.25e-05	time: 00:06:05	Loss_train 0.06835	Acc_train 97.37	/	Loss_test 3.15753	Acc_test 66.12
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLbe1cb3cb-b5f8-4a9d-bda0-16cc8cbeda29/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [80/100]	lr: 3.13e-05	time: 00:06:48	Loss_train 0.05019	Acc_train 97.93	/	Loss_test 3.11265	Acc_test 66.12
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLbe1cb3cb-b5f8-4a9d-bda0-16cc8cbeda29/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [90/100]	lr: 1.56e-05	time: 00:07:32	Loss_train 0.04391	Acc_train 98.04	/	Loss_test 3.07585	Acc_test 66.49
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLbe1cb3cb-b5f8-4a9d-bda0-16cc8cbeda29/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [100/100]	lr: 7.81e-06	time: 00:08:15	Loss_train 0.03978	Acc_train 98.28	/	Loss_test 3.06802	Acc_test 66.56
new_head:  {'blocks.4.layer.bias': tensor([ 0.0016, -0.0015,  0.0004, -0.0005,  0.0020,  0.0031,  0.0054, -0.0016,
         0.0026,  0.0032], device='cuda:0'), 'blocks.4.layer.weight': tensor([[ 0.0400,  0.0110,  0.0159,  ..., -0.0202, -0.0415, -0.0304],
        [ 0.0107, -0.0007,  0.0315,  ..., -0.0158,  0.0113,  0.0158],
        [ 0.0364, -0.0160,  0.0407,  ...,  0.0141,  0.0031,  0.0035],
        ...,
        [ 0.0219,  0.0111,  0.0037,  ...,  0.0494,  0.0133, -0.0125],
        [-0.0186,  0.0274, -0.0178,  ..., -0.0034, -0.0105,  0.0038],
        [-0.0212, -0.0315, -0.0387,  ..., -0.0497,  0.0010, -0.0065]],
       device='cuda:0')}
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLbe1cb3cb-b5f8-4a9d-bda0-16cc8cbeda29/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
RESULT:  {'train_loss': 0.03978398069739342, 'train_acc': 98.27600121498108, 'test_loss': 3.0680172443389893, 'test_acc': 66.5625, 'convergence': 23.72207260131836, 'R1': 1, 'dataset_sup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 5000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 4, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': True, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 100, 'print_freq': 10, 'validation': False, 'continual_learning': False}, 'dataset_unsup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 100000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 32, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'unlabeled', 'nb_epoch': 1, 'print_freq': 25, 'validation': False, 'continual_learning': False}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 32, 'nb_epoch': 1, 'print_freq': 25, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 100, 'print_freq': 10, 'lr': 0.001}}}
IN R0:  {'count': 0, 'R0': {'train_loss': 0.03978398069739342, 'train_acc': 98.27600121498108, 'test_loss': 3.0680172443389893, 'test_acc': 66.5625, 'convergence': 23.72207260131836, 'R1': 1, 'dataset_sup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 5000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 4, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': True, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 100, 'print_freq': 10, 'validation': False, 'continual_learning': False}, 'dataset_unsup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 100000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 32, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'unlabeled', 'nb_epoch': 1, 'print_freq': 25, 'validation': False, 'continual_learning': False}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 32, 'nb_epoch': 1, 'print_freq': 25, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 100, 'print_freq': 10, 'lr': 0.001}}}}
CL:  True
{'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True}
CL:  True
{'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': True}
SEED:  0
block 0, size : 96 16 16
96
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.036828478186799345, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 221184, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 2.886751345948129
block 1, size : 384 8 8
384
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.036828478186799345, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 221184, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.8505172717997146
block 2, size : 1536 4 4
1536
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.036828478186799345, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 221184, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.4252586358998573
block 3, size : 6144 2 2
6144
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.036828478186799345, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 221184, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.21262931794992865
NOWWWW
2 2
10
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.036828478186799345, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 24576, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
blocks['b%s' % 4] 24576 24576
range = 0.11048543456039805
train_layer_order: consecutive {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 10, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}
The device used will be: 
True
cuda:0
PARAMSSSS IN LOAD:  {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.036828478186799345, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 221184, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
avg_deltas:  {'blocks.0.layer.weight': tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')}
topk_layer:  None
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.0.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')}
topk_layer inside model after retrieval:  [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.1.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')}
topk_layer inside model after retrieval:  [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.2.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')}
topk_layer inside model after retrieval:  [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.3.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')}
topk_layer inside model after retrieval:  [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
The device used will be: 
True
cuda:0
The device used will be: 
True
cuda:0
chosen_head:  {'blocks.4.layer.bias': tensor([ 0.0016, -0.0015,  0.0004, -0.0005,  0.0020,  0.0031,  0.0054, -0.0016,
         0.0026,  0.0032], device='cuda:0'), 'blocks.4.layer.weight': tensor([[ 0.0400,  0.0110,  0.0159,  ..., -0.0202, -0.0415, -0.0304],
        [ 0.0107, -0.0007,  0.0315,  ..., -0.0158,  0.0113,  0.0158],
        [ 0.0364, -0.0160,  0.0407,  ...,  0.0141,  0.0031,  0.0035],
        ...,
        [ 0.0219,  0.0111,  0.0037,  ...,  0.0494,  0.0133, -0.0125],
        [-0.0186,  0.0274, -0.0178,  ..., -0.0034, -0.0105,  0.0038],
        [-0.0212, -0.0315, -0.0387,  ..., -0.0497,  0.0010, -0.0065]],
       device='cuda:0')}

 Model STL10_CIFAR10_CLbe1cb3cb-b5f8-4a9d-bda0-16cc8cbeda29 loaded successfuly with best perf



 ----- Architecture Block BatchNorm2dSK3962(5, 5)1.0reflect, number 0 -----
- BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- AvgPool2d(kernel_size=4, stride=2, padding=1)

 ----- Architecture Block BatchNorm2dSK963842(3, 3)0.6499999761581421reflect, number 1 -----
- BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK38415362(3, 3)0.44999998807907104reflect, number 2 -----
- BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.4)
- AvgPool2d(kernel_size=4, stride=2, padding=1)

 ----- Architecture Block BatchNorm2dSK153661442(3, 3)0.25reflect, number 3 -----
- BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 4 -----
- Flatten(start_dim=1, end_dim=-1)
- Dropout(p=0.5, inplace=False)
- Linear(in_features=221184, out_features=10, bias=True)
model.heads:  [{'blocks.4.layer.bias': tensor([ 0.0016, -0.0015,  0.0004, -0.0005,  0.0020,  0.0031,  0.0054, -0.0016,
         0.0026,  0.0032], device='cuda:0'), 'blocks.4.layer.weight': tensor([[ 0.0400,  0.0110,  0.0159,  ..., -0.0202, -0.0415, -0.0304],
        [ 0.0107, -0.0007,  0.0315,  ..., -0.0158,  0.0113,  0.0158],
        [ 0.0364, -0.0160,  0.0407,  ...,  0.0141,  0.0031,  0.0035],
        ...,
        [ 0.0219,  0.0111,  0.0037,  ...,  0.0494,  0.0133, -0.0125],
        [-0.0186,  0.0274, -0.0178,  ..., -0.0034, -0.0105,  0.0038],
        [-0.0212, -0.0315, -0.0387,  ..., -0.0497,  0.0010, -0.0065]],
       device='cuda:0')}]
[Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=1.4)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=221184, out_features=10, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)]
LAYER NAME:  Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=1.4)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=221184, out_features=10, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)
LAYER CHILDREN:  [BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
  (activation): Triangle(power=1.4)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1)
), BasicBlock(
  (operations): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
  )
  (layer): Linear(in_features=221184, out_features=10, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)]
subsubl NAME:  Sequential(
  (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=4, stride=2, padding=1)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=4, stride=2, padding=1)
subsubl NAME:  Triangle(power=1.4)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1)
subsubl NAME:  Sequential(
  (0): Flatten(start_dim=1, end_dim=-1)
)
subsubl NAME:  Linear(in_features=221184, out_features=10, bias=True)
subsubl NAME:  Dropout(p=0.5, inplace=False)
CONFIG MODE:  unsupervised

 ********** Hebbian Unsupervised learning of blocks [0, 1, 2, 3] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
INDICES:  50000
IMAGE SIZE: torch.Size([10, 3, 96, 96])
SAVING FOLDER FOR UNSUP:  STL10_CIFAR10_CLbe1cb3cb-b5f8-4a9d-bda0-16cc8cbeda29
prev_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
activations_sum.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
DEPTH:  4
ACTIVATIONS SHAPE:  tensor([[ 7.0837e+00, -1.7342e+02, -5.2790e+01,  ..., -3.0243e+02,
         -3.1626e+02, -8.0347e+01],
        [-8.1204e+01, -2.1264e+02, -1.1070e+02,  ..., -3.5674e+02,
         -3.7442e+02, -1.3079e+02],
        [-1.0510e+01, -2.5627e+02, -1.4349e+02,  ..., -2.6999e+02,
         -3.1838e+02, -2.5646e+01],
        ...,
        [ 2.0937e+02,  2.8768e-01,  5.9503e+01,  ..., -6.2056e+01,
         -1.4965e+02,  1.2563e+02],
        [ 4.0981e+02,  2.2179e+02,  2.8451e+02,  ...,  1.7146e+02,
          3.1691e+01,  3.1062e+02],
        [ 1.0649e+02, -1.2042e+02, -1.1781e+02,  ..., -1.8817e+02,
         -2.5834e+02,  1.3789e+00]], device='cuda:0')
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.topk_layer:  <class 'list'>
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.topk_layer:  <class 'list'>
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.topk_layer:  <class 'list'>
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.topk_layer:  <class 'list'>
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
activations_sum[k] 96
activations_sum[k]:  [25, 16, 59, 53, 30, 42, 77, 68, 69, 93]
topk_kernels len:  4
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3']
topk_kernels:  {'conv0': [25, 16, 59, 53, 30, 42, 77, 68, 69, 93, 75, 51, 34, 38, 70], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
activations_sum[k] len:  96
activations_sum[k] 384
activations_sum[k]:  [23, 75, 345, 61, 349, 92, 330, 197, 100, 33]
topk_kernels len:  4
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3']
topk_kernels:  {'conv0': [25, 16, 59, 53, 30, 42, 77, 68, 69, 93, 75, 51, 34, 38, 70], 'conv1': [23, 75, 345, 61, 349, 92, 330, 197, 100, 33, 69, 190, 296, 41, 121, 157, 76, 166, 247, 133, 39, 336, 266, 362, 184, 17, 63, 158, 50, 116, 344, 354, 201, 98, 292, 156, 130, 335, 291, 365, 332, 366, 146, 382, 186, 287, 159, 104, 217, 139, 219, 28, 99, 260, 180, 264, 227, 273, 379], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
activations_sum[k] len:  384
activations_sum[k] 1536
activations_sum[k]:  [1006, 307, 576, 276, 733, 972, 632, 1411, 320, 301]
topk_kernels len:  4
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3']
topk_kernels:  {'conv0': [25, 16, 59, 53, 30, 42, 77, 68, 69, 93, 75, 51, 34, 38, 70], 'conv1': [23, 75, 345, 61, 349, 92, 330, 197, 100, 33, 69, 190, 296, 41, 121, 157, 76, 166, 247, 133, 39, 336, 266, 362, 184, 17, 63, 158, 50, 116, 344, 354, 201, 98, 292, 156, 130, 335, 291, 365, 332, 366, 146, 382, 186, 287, 159, 104, 217, 139, 219, 28, 99, 260, 180, 264, 227, 273, 379], 'conv2': [1006, 307, 576, 276, 733, 972, 632, 1411, 320, 301, 350, 1284, 915, 1211, 49, 1502, 503, 1385, 272, 1265, 549, 185, 904, 734, 1413, 326, 984, 468, 764, 186, 551, 122, 969, 347, 1050, 1293, 89, 994, 1185, 697, 261, 1500, 1090, 1532, 930, 1047, 879, 1427, 1290, 519, 973, 167, 1030, 1154, 577, 1086, 786, 1453, 334, 1033, 188, 813, 1400, 294, 241, 796, 627, 1042, 1176, 619, 868, 445, 482, 135, 427, 238, 573, 70, 1239, 953, 726, 138, 1271, 985, 240, 268, 710, 1113, 1326, 1394, 823, 938, 601, 195, 205, 988, 1475, 1184, 1371, 1085, 747, 760, 1465, 617, 377, 1494, 73, 289, 737, 1409, 952, 863, 992, 660, 222, 557, 252, 43, 926, 801, 998, 159, 1493, 1251, 1263, 964, 1029, 844, 539, 1091, 1167, 1066, 177, 108, 1458, 477, 923, 766, 458, 1000, 676, 650, 310, 869, 705, 1129, 13, 269, 260, 75, 1428, 1060, 1195, 571, 562, 1214, 430, 659, 851, 854, 1344, 535, 1213, 777, 369, 187, 1242, 775, 1529, 802, 1225, 97, 779, 126, 640, 833, 174, 595, 158, 194, 1068, 946, 1503, 995, 592, 170, 812, 471, 1307, 486, 1462, 1365, 914, 373, 610, 417, 1327, 128, 1230, 133, 346, 1238, 1253, 910, 44, 1076, 950, 423, 90, 1430, 1324, 348, 1150, 1531, 193, 1412, 171, 1084, 1021, 1032, 1522, 1299, 405, 1471, 958, 1122, 580, 509, 446, 408, 715], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
activations_sum[k] len:  1536
activations_sum[k] 6144
activations_sum[k]:  [5017, 6047, 1653, 3916, 170, 5041, 2613, 4323, 6098, 2691]
topk_kernels len:  4
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3']
topk_kernels:  {'conv0': [25, 16, 59, 53, 30, 42, 77, 68, 69, 93, 75, 51, 34, 38, 70], 'conv1': [23, 75, 345, 61, 349, 92, 330, 197, 100, 33, 69, 190, 296, 41, 121, 157, 76, 166, 247, 133, 39, 336, 266, 362, 184, 17, 63, 158, 50, 116, 344, 354, 201, 98, 292, 156, 130, 335, 291, 365, 332, 366, 146, 382, 186, 287, 159, 104, 217, 139, 219, 28, 99, 260, 180, 264, 227, 273, 379], 'conv2': [1006, 307, 576, 276, 733, 972, 632, 1411, 320, 301, 350, 1284, 915, 1211, 49, 1502, 503, 1385, 272, 1265, 549, 185, 904, 734, 1413, 326, 984, 468, 764, 186, 551, 122, 969, 347, 1050, 1293, 89, 994, 1185, 697, 261, 1500, 1090, 1532, 930, 1047, 879, 1427, 1290, 519, 973, 167, 1030, 1154, 577, 1086, 786, 1453, 334, 1033, 188, 813, 1400, 294, 241, 796, 627, 1042, 1176, 619, 868, 445, 482, 135, 427, 238, 573, 70, 1239, 953, 726, 138, 1271, 985, 240, 268, 710, 1113, 1326, 1394, 823, 938, 601, 195, 205, 988, 1475, 1184, 1371, 1085, 747, 760, 1465, 617, 377, 1494, 73, 289, 737, 1409, 952, 863, 992, 660, 222, 557, 252, 43, 926, 801, 998, 159, 1493, 1251, 1263, 964, 1029, 844, 539, 1091, 1167, 1066, 177, 108, 1458, 477, 923, 766, 458, 1000, 676, 650, 310, 869, 705, 1129, 13, 269, 260, 75, 1428, 1060, 1195, 571, 562, 1214, 430, 659, 851, 854, 1344, 535, 1213, 777, 369, 187, 1242, 775, 1529, 802, 1225, 97, 779, 126, 640, 833, 174, 595, 158, 194, 1068, 946, 1503, 995, 592, 170, 812, 471, 1307, 486, 1462, 1365, 914, 373, 610, 417, 1327, 128, 1230, 133, 346, 1238, 1253, 910, 44, 1076, 950, 423, 90, 1430, 1324, 348, 1150, 1531, 193, 1412, 171, 1084, 1021, 1032, 1522, 1299, 405, 1471, 958, 1122, 580, 509, 446, 408, 715], 'conv3': [5017, 6047, 1653, 3916, 170, 5041, 2613, 4323, 6098, 2691, 5433, 3067, 4958, 4900, 3563, 1730, 62, 2026, 3435, 2075, 2778, 5255, 4087, 4574, 4983, 634, 5795, 1366, 948, 4473, 4359, 3906, 547, 750, 1229, 5651, 4738, 3197, 5004, 785, 514, 1599, 4502, 2051, 5968, 3718, 1680, 5558, 3938, 353, 5984, 5349, 4768, 815, 5956, 1310, 3516, 4415, 2483, 5292, 2021, 4333, 5515, 4090, 3988, 1149, 2470, 1856, 2392, 549, 4350, 1022, 1921, 999, 4682, 3986, 2462, 921, 3227, 265, 706, 1055, 5541, 4429, 680, 3972, 3828, 1635, 5175, 1587, 1685, 2966, 2696, 1318, 4262, 2037, 4163, 4746, 4699, 2299, 3481, 2398, 3740, 4029, 2413, 5314, 3181, 2804, 837, 3511, 5918, 1575, 3807, 1983, 1778, 63, 3486, 51, 3669, 4924, 1526, 817, 3179, 5498, 6, 1385, 1066, 534, 5733, 3210, 1086, 4357, 2176, 1331, 1670, 4408, 117, 2344, 1181, 3222, 1911, 2427, 2979, 1304, 5752, 2453, 5283, 5188, 745, 5591, 4968, 1344, 3603, 261, 1585, 5, 2786, 5406, 8, 300, 2300, 4905, 2714, 3431, 2661, 5571, 4360, 4556, 3750, 771, 211, 3888, 4486, 813, 2049, 4025, 6114, 1294, 845, 3414, 5312, 5001, 2404, 2769, 4094, 4624, 5065, 4496, 455, 3090, 6070, 3101, 1760, 1037, 1913, 1831, 1036, 1117, 3449, 5919, 418, 5455, 5299, 485, 905, 4733, 5091, 3959, 4739, 183, 2078, 1009, 1043, 4134, 2085, 5717, 3543, 3430, 552, 3830, 1512, 1822, 2914, 5705, 822, 3983, 5860, 688, 4687, 6134, 2201, 5222, 1342, 784, 1112, 1198, 2938, 2041, 4862, 3321, 5579, 5931, 526, 2608, 1872, 1163, 2780, 4218, 4256, 872, 122, 4396, 551, 1437, 5473, 4513, 2584, 1883, 3495, 2136, 5477, 5977, 3159, 1703, 1118, 4538, 1346, 2698, 4397, 3941, 2047, 5531, 2488, 3581, 132, 339, 3502, 2850, 1792, 2418, 5346, 2354, 6054, 3001, 4227, 1790, 4598, 2056, 4100, 3725, 909, 3909, 4451, 4072, 4660, 2635, 1102, 3656, 2581, 176, 1001, 2152, 2444, 3448, 685, 497, 2920, 652, 621, 4390, 3186, 4221, 1097, 2069, 4177, 1620, 4118, 4377, 178, 1615, 1950, 3010, 2092, 2793, 4694, 2103, 5834, 6004, 3905, 632, 844, 4618, 4131, 4063, 1403, 583, 1351, 5469, 1753, 1514, 4602, 4173, 2835, 2750, 3215, 2274, 5457, 3507, 5482, 5981, 3062, 5135, 2847, 2010, 4223, 2662, 3824, 2447, 5847, 165, 5276, 2210, 5493, 129, 198, 5780, 4853, 1544, 2511, 1806, 5910, 4382, 956, 2426, 1443, 1622, 4906, 5146, 4159, 5214, 668, 4876, 1284, 194, 227, 2980, 1187, 6090, 49, 4036, 436, 3470, 6058, 3488, 1594, 5322, 1283, 1864, 4539, 860, 3420, 3138, 818, 1861, 257, 2700, 2139, 2695, 3351, 2379, 4544, 1176, 2342, 2128, 1343, 2514, 4394, 686, 5212, 3895, 3540, 4110, 109, 3312, 893, 5542, 6053, 3473, 4301, 5196, 4937, 1337, 5407, 4219, 2972, 208, 2812, 4266, 4646, 5027, 4938, 3719, 1075, 295, 5436, 78, 5243, 4804, 638, 1564, 1894, 4980, 5929, 5853, 3640, 4964, 5821, 5297, 5201, 398, 5909, 59, 2542, 495, 6119, 4097, 2292, 1560, 2666, 2627, 1873, 3912, 4636, 3436, 26, 1453, 1394, 4947, 5216, 4089, 4166, 1415, 4954, 2083, 1834, 4212, 3809, 4635, 4102, 3866, 2533, 5838, 5412, 221, 6116, 2367, 3944, 1988, 1132, 1380, 1454, 4264, 1499, 5786, 1316, 4306, 3584, 171, 2675, 2667, 5205, 2530, 5108, 5045, 3793, 2703, 1684, 4421, 1191, 294, 2008, 41, 3760, 4280, 2923, 13, 4651, 1309, 1185, 1435, 6122, 2945, 2755, 4114, 1371, 2808, 4319, 2679, 5520, 4897, 6142, 2089, 2563, 5630, 292, 3151, 2668, 4553, 2927, 2156, 5076, 3193, 313, 3670, 3696, 709, 252, 2779, 1108, 1312, 2236, 2098, 2058, 772, 4257, 3330, 5791, 2957, 3569, 65, 5056, 3602, 5739, 1641, 5318, 722, 5445, 3755, 24, 5396, 675, 760, 3873, 673, 926, 3017, 1168, 5354, 4546, 2314, 5607, 259, 3658, 1608, 2007, 4289, 4777, 888, 3434, 4736, 4445, 4107, 1164, 2173, 4431, 576, 3802, 4120, 1879, 3191, 4220, 4076, 4961, 5839, 3150, 5206, 3377, 3096, 255, 5087, 2048, 2393, 1956, 3039, 1162, 4603, 5161, 5268, 491, 2580, 5423, 795, 2597, 463, 5043, 4236, 4291, 1230, 1882, 1978, 1511, 1866, 4728, 5942, 1362, 4879, 2348, 2845, 796, 5582, 4919, 2834, 4402, 6109, 1634, 3735, 6008, 3914, 1960, 5172, 625, 4270, 3556, 4541, 2976, 6089, 2860, 2962, 2082, 3479, 5596, 4688, 5321, 4762, 2711, 3775, 3553, 2251, 4885, 2887, 3657, 1096, 5401, 4640, 5921, 5204, 2033, 2206, 2802, 4412, 2928, 441, 1578, 951, 287, 1357, 3761, 4176, 2101, 14, 4299, 2310, 5936, 2073, 2538, 2077, 4628, 4104, 870, 2318, 4447, 2227, 5932, 2736, 5386, 4162, 2918, 5164, 2859, 3804, 1161, 6102, 4881, 246, 5503, 707, 4840, 2917, 1941, 1836, 2607, 763, 5454, 2759, 1115, 530, 1936, 426, 1755, 980, 620, 846, 1949, 1537, 1846, 865, 223, 3686, 4785, 5697, 1021, 4028, 1887, 2100, 2268, 4449, 3133, 3538, 5626, 826, 5008, 1569, 1457, 780, 1194, 2889, 5103, 229, 2460, 2798, 3918, 5680, 89, 493, 5818, 4658, 3594, 2445, 1151, 1535, 1273, 1169, 1422, 1264, 4239, 1543, 834, 5015, 2203, 3425, 4365, 6125, 3170, 1368, 4752, 1820, 3834, 2680, 2112, 4778, 3965, 5116, 1028, 3027, 2715, 4300, 2443, 1574, 2519, 3947, 1649, 144, 1088, 5854, 2792, 5885, 2382, 3163, 1912, 765, 3023, 4013, 2022, 527, 5980, 4482, 3636, 3283, 2067, 2106, 2071, 3242, 1999, 2449, 5893, 1376, 3025, 966, 1204, 5245, 1470, 4185, 4410, 5693, 4614, 3943, 4034, 3384, 4460, 5199, 3759, 1116, 3902, 2861, 3836, 2585, 179, 1697, 5994, 1702, 2817, 4519, 4330, 4480, 4882, 555, 5081, 1279, 3429, 545, 4154, 590, 4135, 5887, 970, 363, 3032, 2419, 4956, 5642, 1345, 1845, 1438, 4775, 6096, 878, 1600, 425, 1335, 1914, 2016, 5523, 3277, 4051, 6021, 1980, 5862, 4566, 450, 3350, 175, 1665, 1969, 524, 1095, 4363, 3118, 3097, 4669, 5460, 3126, 4242, 5702, 5552, 52, 3733, 4142, 2547, 1839, 271, 674, 511, 4248]}
activations_sum[k] len:  6144
['conv0', 'conv1', 'conv2', 'conv3']
delta_weights INFO: 
NUM OF TRACKED COV LAYERS:  4
NUM OF TRACKED WEIGHTS CHANGES PER LAYER:  100
avg_deltas INFO:  <class 'dict'>
avg_deltas keys:  ['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight']
################################################
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 2.66e-02	time: 00:04:23	Acc_train 0.00	Acc_test 0.00	convergence: 1.76e+01	R1: 49	Info MB:0.000e+00/SB:0.000e+00/MW:1.723e-03/SW:1.630e-01/MR:1.855e+01/SR:4.787e+00/MeD:3.463e+00/MaD:1.755e+01/MW:0.489/MAW:0.511
|       0 |        1 |        2 |        3 |        4 |        5 |        6 |        7 |        8 |        9 |       10 |       11 |       12 |       13 |       14 |       15 |      16 |       17 |       18 |       19 |       20 |       21 |       22 |       23 |       24 |       25 |       26 |      27 |       28 |       29 |
|---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------|
|   0.044 |   0.0458 |   0.0412 |   0.0401 |   0.0392 |   0.0396 |   0.0377 |   0.0449 |   0.0224 |   0.0471 |   0.0472 |   0.0472 |   0.0442 |   0.0435 |   0.0384 |   0.0505 |   0.027 |   0.0395 |   0.0466 |   0.0457 |   0.0368 |   0.0477 |   0.0395 |   0.0433 |   0.0446 |   0.0428 |   0.0464 |   0.043 |   0.0473 |   0.0483 |
|  20.35  |  22.02   |  17.97   |  17.04   |  16.36   |  16.65   |  15.22   |  21.14   |   6      |  23.16   |  23.25   |  23.28   |  20.55   |  19.93   |  15.72   |  26.55   |   8.28  |  16.57   |  22.68   |  21.92   |  14.51   |  23.73   |  16.59   |  19.77   |  20.9    |  19.35   |  22.54   |  19.45  |  23.42   |  24.28   |
|   0.01  |   0.02   |   0.03   |   0.02   |   0.01   |   0.01   |   0.02   |   0.01   |   0.01   |   0.01   |   0.01   |   0.01   |   0      |   0.02   |   0.01   |   0.01   |   0.01  |   0.02   |   0.01   |   0.01   |   0.01   |   0.01   |   0.02   |   0.01   |   0.01   |   0.01   |   0.02   |   0     |   0.02   |   0.01   |
| nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      |
| nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      |
| nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:2.754e-03/SW:1.704e-01/MR:9.165e+00/SR:4.044e+00/MeD:3.238e+00/MaD:8.165e+00/MW:0.585/MAW:0.415
|         0 |        1 |        2 |        3 |        4 |        5 |        6 |        7 |       8 |        9 |     10 |       11 |       12 |       13 |       14 |       15 |       16 |       17 |       18 |      19 |       20 |       21 |       22 |       23 |       24 |       25 |        26 |       27 |       28 |       29 |
|-----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+--------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------+----------+----------+-----------+----------+----------+----------|
|   0.00934 |   0.0316 |   0.0244 |   0.0312 |   0.0327 |   0.0314 |   0.0331 |   0.0298 |   0.014 |   0.0286 |   0.02 |   0.0256 |   0.0365 |   0.0311 |   0.0232 |   0.0366 |   0.0341 |   0.0356 |   0.0111 |   0.036 |   0.0312 |   0.0336 |   0.0318 |   0.0248 |   0.0359 |   0.0331 |   0.00699 |   0.0264 |   0.0274 |   0.0299 |
|   1.87    |  10.99   |   6.93   |  10.74   |  11.71   |  10.83   |  11.95   |   9.91   |   2.96  |   9.21   |   5.01 |   7.57   |  14.3    |  10.68   |   6.39   |  14.37   |  12.63   |  13.69   |   2.23   |  13.99  |  10.75   |  12.28   |  11.13   |   7.16   |  13.92   |  11.94   |   1.49    |   7.95   |   8.49   |   9.95   |
|   0.04    |   0.01   |   0.03   |   0.05   |   0.03   |   0.03   |   0.04   |   0.04   |   0.03  |   0.02   |   0.03 |   0.03   |   0.04   |   0.03   |   0.06   |   0.03   |   0.06   |   0.04   |   0.04   |   0.03  |   0.05   |   0.04   |   0.05   |   0.03   |   0.05   |   0.02   |   0.02    |   0.03   |   0.02   |   0.03   |
| nan       | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan    | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan       | nan      | nan      | nan      |
| nan       | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan    | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan       | nan      | nan      | nan      |
| nan       | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan    | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan       | nan      | nan      | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:1.597e-02/SW:1.467e-01/MR:3.808e+00/SR:2.081e+00/MeD:1.769e+00/MaD:6.026e+00/MW:0.584/MAW:0.416
|        0 |        1 |        2 |         3 |        4 |         5 |         6 |        7 |         8 |         9 |        10 |        11 |        12 |       13 |       14 |        15 |        16 |        17 |       18 |        19 |        20 |         21 |        22 |        23 |        24 |       25 |        26 |        27 |        28 |        29 |
|----------+----------+----------+-----------+----------+-----------+-----------+----------+-----------+-----------+-----------+-----------+-----------+----------+----------+-----------+-----------+-----------+----------+-----------+-----------+------------+-----------+-----------+-----------+----------+-----------+-----------+-----------+-----------|
|   0.0112 |   0.0107 |   0.0103 |   0.00923 |   0.0017 |   0.00783 |   0.00536 |   0.0066 |   0.00818 |   0.00725 |   0.00868 |   0.00942 |   0.00104 |   0.0051 |   0.0115 |   0.00982 |   0.00971 |   0.00598 |   0.0026 |   0.00195 |   0.00714 |   0.000184 |   0.00546 |   0.00658 |   0.00156 |   0.0111 |   0.00937 |   0.00858 |   0.00481 |   0.00481 |
|   6.04   |   5.6    |   5.25   |   4.41    |   1.12   |   3.45    |   2.15    |   2.74   |   3.68    |   3.1     |   4.01    |   4.55    |   1.04    |   2.04   |   6.25   |   4.86    |   4.77    |   2.43    |   1.27   |   1.15    |   3.04    |   1        |   2.19    |   2.73    |   1.1     |   5.94   |   4.51    |   3.94    |   1.92    |   1.92    |
|   0.19   |   0.21   |   0.18   |   0.15    |   0.49   |   0.11    |   0.15    |   0.13   |   0.16    |   0.17    |   0.09    |   0.21    |   0.36    |   0.26   |   0.2    |   0.16    |   0.23    |   0.29    |   0.15   |   0.19    |   0.16    |   1.83     |   0.39    |   0.31    |   0.31    |   0.32   |   0.2     |   0.11    |   0.2     |   0.24    |
| nan      | nan      | nan      | nan       | nan      | nan       | nan       | nan      | nan       | nan       | nan       | nan       | nan       | nan      | nan      | nan       | nan       | nan       | nan      | nan       | nan       | nan        | nan       | nan       | nan       | nan      | nan       | nan       | nan       | nan       |
| nan      | nan      | nan      | nan       | nan      | nan       | nan       | nan      | nan       | nan       | nan       | nan       | nan       | nan      | nan      | nan       | nan       | nan       | nan      | nan       | nan       | nan        | nan       | nan       | nan       | nan      | nan       | nan       | nan       | nan       |
| nan      | nan      | nan      | nan       | nan      | nan       | nan       | nan      | nan       | nan       | nan       | nan       | nan       | nan      | nan      | nan       | nan       | nan       | nan      | nan       | nan       | nan        | nan       | nan       | nan       | nan      | nan       | nan       | nan       | nan       |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:1.179e-02/SW:2.138e+00/MR:1.694e+01/SR:7.510e+00/MeD:6.027e+00/MaD:1.730e+01/MW:0.598/MAW:0.402
|      0 |       1 |       2 |       3 |       4 |       5 |       6 |       7 |       8 |      9 |        10 |      11 |      12 |      13 |      14 |      15 |        16 |      17 |      18 |      19 |      20 |      21 |      22 |      23 |      24 |        25 |      26 |     27 |      28 |      29 |
|--------+---------+---------+---------+---------+---------+---------+---------+---------+--------+-----------+---------+---------+---------+---------+---------+-----------+---------+---------+---------+---------+---------+---------+---------+---------+-----------+---------+--------+---------+---------|
|   0.31 |   0.404 |   0.381 |   0.356 |   0.319 |   0.323 |   0.326 |   0.461 |   0.314 |   0.39 |   0.00415 |   0.331 |   0.371 |   0.353 |   0.314 |   0.409 |   0.00115 |   0.311 |   0.363 |   0.373 |   0.375 |   0.332 |   0.331 |   0.423 |   0.406 |   0.00117 |   0.367 |   0.2  |   0.253 |   0.381 |
|  16.04 |  26.44  |  23.64  |  20.78  |  16.89  |  17.34  |  17.66  |  34.24  |  16.39  |  24.77 |   1       |  18.11  |  22.52  |  20.48  |  16.44  |  27.1   |   1       |  16.08  |  21.58  |  22.78  |  23.03  |  18.19  |  18.13  |  28.99  |  26.74  |   1       |  22.04  |   7.28 |  11     |  23.66  |
|   0.48 |   0.6   |   0.4   |   0.53  |   0.77  |   0.92  |   0.56  |   0.74  |   0.64  |   0.28 |   1.32    |   0.6   |   0.58  |   0.45  |   0.73  |   0.44  |  14.01    |   0.47  |   0.41  |   0.92  |   0.61  |   0.68  |   0.45  |   0.51  |   0.43  |  17.09    |   0.55  |   0.94 |   0.45  |   0.43  |
| nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan       | nan     | nan     | nan     | nan     | nan     | nan       | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan       | nan     | nan    | nan     | nan     |
| nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan       | nan     | nan     | nan     | nan     | nan     | nan       | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan       | nan     | nan    | nan     | nan     |
| nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan       | nan     | nan     | nan     | nan     | nan     | nan       | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan       | nan     | nan    | nan     | nan     |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLbe1cb3cb-b5f8-4a9d-bda0-16cc8cbeda29/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [25, 16, 59, 53, 30, 42, 77, 68, 69, 93, 75, 51, 34, 38, 70], 'conv1': [23, 75, 345, 61, 349, 92, 330, 197, 100, 33, 69, 190, 296, 41, 121, 157, 76, 166, 247, 133, 39, 336, 266, 362, 184, 17, 63, 158, 50, 116, 344, 354, 201, 98, 292, 156, 130, 335, 291, 365, 332, 366, 146, 382, 186, 287, 159, 104, 217, 139, 219, 28, 99, 260, 180, 264, 227, 273, 379], 'conv2': [1006, 307, 576, 276, 733, 972, 632, 1411, 320, 301, 350, 1284, 915, 1211, 49, 1502, 503, 1385, 272, 1265, 549, 185, 904, 734, 1413, 326, 984, 468, 764, 186, 551, 122, 969, 347, 1050, 1293, 89, 994, 1185, 697, 261, 1500, 1090, 1532, 930, 1047, 879, 1427, 1290, 519, 973, 167, 1030, 1154, 577, 1086, 786, 1453, 334, 1033, 188, 813, 1400, 294, 241, 796, 627, 1042, 1176, 619, 868, 445, 482, 135, 427, 238, 573, 70, 1239, 953, 726, 138, 1271, 985, 240, 268, 710, 1113, 1326, 1394, 823, 938, 601, 195, 205, 988, 1475, 1184, 1371, 1085, 747, 760, 1465, 617, 377, 1494, 73, 289, 737, 1409, 952, 863, 992, 660, 222, 557, 252, 43, 926, 801, 998, 159, 1493, 1251, 1263, 964, 1029, 844, 539, 1091, 1167, 1066, 177, 108, 1458, 477, 923, 766, 458, 1000, 676, 650, 310, 869, 705, 1129, 13, 269, 260, 75, 1428, 1060, 1195, 571, 562, 1214, 430, 659, 851, 854, 1344, 535, 1213, 777, 369, 187, 1242, 775, 1529, 802, 1225, 97, 779, 126, 640, 833, 174, 595, 158, 194, 1068, 946, 1503, 995, 592, 170, 812, 471, 1307, 486, 1462, 1365, 914, 373, 610, 417, 1327, 128, 1230, 133, 346, 1238, 1253, 910, 44, 1076, 950, 423, 90, 1430, 1324, 348, 1150, 1531, 193, 1412, 171, 1084, 1021, 1032, 1522, 1299, 405, 1471, 958, 1122, 580, 509, 446, 408, 715], 'conv3': [5017, 6047, 1653, 3916, 170, 5041, 2613, 4323, 6098, 2691, 5433, 3067, 4958, 4900, 3563, 1730, 62, 2026, 3435, 2075, 2778, 5255, 4087, 4574, 4983, 634, 5795, 1366, 948, 4473, 4359, 3906, 547, 750, 1229, 5651, 4738, 3197, 5004, 785, 514, 1599, 4502, 2051, 5968, 3718, 1680, 5558, 3938, 353, 5984, 5349, 4768, 815, 5956, 1310, 3516, 4415, 2483, 5292, 2021, 4333, 5515, 4090, 3988, 1149, 2470, 1856, 2392, 549, 4350, 1022, 1921, 999, 4682, 3986, 2462, 921, 3227, 265, 706, 1055, 5541, 4429, 680, 3972, 3828, 1635, 5175, 1587, 1685, 2966, 2696, 1318, 4262, 2037, 4163, 4746, 4699, 2299, 3481, 2398, 3740, 4029, 2413, 5314, 3181, 2804, 837, 3511, 5918, 1575, 3807, 1983, 1778, 63, 3486, 51, 3669, 4924, 1526, 817, 3179, 5498, 6, 1385, 1066, 534, 5733, 3210, 1086, 4357, 2176, 1331, 1670, 4408, 117, 2344, 1181, 3222, 1911, 2427, 2979, 1304, 5752, 2453, 5283, 5188, 745, 5591, 4968, 1344, 3603, 261, 1585, 5, 2786, 5406, 8, 300, 2300, 4905, 2714, 3431, 2661, 5571, 4360, 4556, 3750, 771, 211, 3888, 4486, 813, 2049, 4025, 6114, 1294, 845, 3414, 5312, 5001, 2404, 2769, 4094, 4624, 5065, 4496, 455, 3090, 6070, 3101, 1760, 1037, 1913, 1831, 1036, 1117, 3449, 5919, 418, 5455, 5299, 485, 905, 4733, 5091, 3959, 4739, 183, 2078, 1009, 1043, 4134, 2085, 5717, 3543, 3430, 552, 3830, 1512, 1822, 2914, 5705, 822, 3983, 5860, 688, 4687, 6134, 2201, 5222, 1342, 784, 1112, 1198, 2938, 2041, 4862, 3321, 5579, 5931, 526, 2608, 1872, 1163, 2780, 4218, 4256, 872, 122, 4396, 551, 1437, 5473, 4513, 2584, 1883, 3495, 2136, 5477, 5977, 3159, 1703, 1118, 4538, 1346, 2698, 4397, 3941, 2047, 5531, 2488, 3581, 132, 339, 3502, 2850, 1792, 2418, 5346, 2354, 6054, 3001, 4227, 1790, 4598, 2056, 4100, 3725, 909, 3909, 4451, 4072, 4660, 2635, 1102, 3656, 2581, 176, 1001, 2152, 2444, 3448, 685, 497, 2920, 652, 621, 4390, 3186, 4221, 1097, 2069, 4177, 1620, 4118, 4377, 178, 1615, 1950, 3010, 2092, 2793, 4694, 2103, 5834, 6004, 3905, 632, 844, 4618, 4131, 4063, 1403, 583, 1351, 5469, 1753, 1514, 4602, 4173, 2835, 2750, 3215, 2274, 5457, 3507, 5482, 5981, 3062, 5135, 2847, 2010, 4223, 2662, 3824, 2447, 5847, 165, 5276, 2210, 5493, 129, 198, 5780, 4853, 1544, 2511, 1806, 5910, 4382, 956, 2426, 1443, 1622, 4906, 5146, 4159, 5214, 668, 4876, 1284, 194, 227, 2980, 1187, 6090, 49, 4036, 436, 3470, 6058, 3488, 1594, 5322, 1283, 1864, 4539, 860, 3420, 3138, 818, 1861, 257, 2700, 2139, 2695, 3351, 2379, 4544, 1176, 2342, 2128, 1343, 2514, 4394, 686, 5212, 3895, 3540, 4110, 109, 3312, 893, 5542, 6053, 3473, 4301, 5196, 4937, 1337, 5407, 4219, 2972, 208, 2812, 4266, 4646, 5027, 4938, 3719, 1075, 295, 5436, 78, 5243, 4804, 638, 1564, 1894, 4980, 5929, 5853, 3640, 4964, 5821, 5297, 5201, 398, 5909, 59, 2542, 495, 6119, 4097, 2292, 1560, 2666, 2627, 1873, 3912, 4636, 3436, 26, 1453, 1394, 4947, 5216, 4089, 4166, 1415, 4954, 2083, 1834, 4212, 3809, 4635, 4102, 3866, 2533, 5838, 5412, 221, 6116, 2367, 3944, 1988, 1132, 1380, 1454, 4264, 1499, 5786, 1316, 4306, 3584, 171, 2675, 2667, 5205, 2530, 5108, 5045, 3793, 2703, 1684, 4421, 1191, 294, 2008, 41, 3760, 4280, 2923, 13, 4651, 1309, 1185, 1435, 6122, 2945, 2755, 4114, 1371, 2808, 4319, 2679, 5520, 4897, 6142, 2089, 2563, 5630, 292, 3151, 2668, 4553, 2927, 2156, 5076, 3193, 313, 3670, 3696, 709, 252, 2779, 1108, 1312, 2236, 2098, 2058, 772, 4257, 3330, 5791, 2957, 3569, 65, 5056, 3602, 5739, 1641, 5318, 722, 5445, 3755, 24, 5396, 675, 760, 3873, 673, 926, 3017, 1168, 5354, 4546, 2314, 5607, 259, 3658, 1608, 2007, 4289, 4777, 888, 3434, 4736, 4445, 4107, 1164, 2173, 4431, 576, 3802, 4120, 1879, 3191, 4220, 4076, 4961, 5839, 3150, 5206, 3377, 3096, 255, 5087, 2048, 2393, 1956, 3039, 1162, 4603, 5161, 5268, 491, 2580, 5423, 795, 2597, 463, 5043, 4236, 4291, 1230, 1882, 1978, 1511, 1866, 4728, 5942, 1362, 4879, 2348, 2845, 796, 5582, 4919, 2834, 4402, 6109, 1634, 3735, 6008, 3914, 1960, 5172, 625, 4270, 3556, 4541, 2976, 6089, 2860, 2962, 2082, 3479, 5596, 4688, 5321, 4762, 2711, 3775, 3553, 2251, 4885, 2887, 3657, 1096, 5401, 4640, 5921, 5204, 2033, 2206, 2802, 4412, 2928, 441, 1578, 951, 287, 1357, 3761, 4176, 2101, 14, 4299, 2310, 5936, 2073, 2538, 2077, 4628, 4104, 870, 2318, 4447, 2227, 5932, 2736, 5386, 4162, 2918, 5164, 2859, 3804, 1161, 6102, 4881, 246, 5503, 707, 4840, 2917, 1941, 1836, 2607, 763, 5454, 2759, 1115, 530, 1936, 426, 1755, 980, 620, 846, 1949, 1537, 1846, 865, 223, 3686, 4785, 5697, 1021, 4028, 1887, 2100, 2268, 4449, 3133, 3538, 5626, 826, 5008, 1569, 1457, 780, 1194, 2889, 5103, 229, 2460, 2798, 3918, 5680, 89, 493, 5818, 4658, 3594, 2445, 1151, 1535, 1273, 1169, 1422, 1264, 4239, 1543, 834, 5015, 2203, 3425, 4365, 6125, 3170, 1368, 4752, 1820, 3834, 2680, 2112, 4778, 3965, 5116, 1028, 3027, 2715, 4300, 2443, 1574, 2519, 3947, 1649, 144, 1088, 5854, 2792, 5885, 2382, 3163, 1912, 765, 3023, 4013, 2022, 527, 5980, 4482, 3636, 3283, 2067, 2106, 2071, 3242, 1999, 2449, 5893, 1376, 3025, 966, 1204, 5245, 1470, 4185, 4410, 5693, 4614, 3943, 4034, 3384, 4460, 5199, 3759, 1116, 3902, 2861, 3836, 2585, 179, 1697, 5994, 1702, 2817, 4519, 4330, 4480, 4882, 555, 5081, 1279, 3429, 545, 4154, 590, 4135, 5887, 970, 363, 3032, 2419, 4956, 5642, 1345, 1845, 1438, 4775, 6096, 878, 1600, 425, 1335, 1914, 2016, 5523, 3277, 4051, 6021, 1980, 5862, 4566, 450, 3350, 175, 1665, 1969, 524, 1095, 4363, 3118, 3097, 4669, 5460, 3126, 4242, 5702, 5552, 52, 3733, 4142, 2547, 1839, 271, 674, 511, 4248]}
SAVED model.heads_thresh 0.9
CONFIG MODE:  supervised

 ********** Supervised learning of blocks [4] **********
SAVING FOLDER FOR SUP:  STL10_CIFAR10_CLbe1cb3cb-b5f8-4a9d-bda0-16cc8cbeda29
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
INDICES:  50000
IMAGE SIZE: torch.Size([64, 3, 96, 96])
num_blocks:  5
Epoch: [1/50]	lr: 1.00e-03	time: 00:05:02	Loss_train 3.99276	Acc_train 58.52	/	Loss_test 0.20008	Acc_test 66.18
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLbe1cb3cb-b5f8-4a9d-bda0-16cc8cbeda29/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [25, 16, 59, 53, 30, 42, 77, 68, 69, 93, 75, 51, 34, 38, 70], 'conv1': [23, 75, 345, 61, 349, 92, 330, 197, 100, 33, 69, 190, 296, 41, 121, 157, 76, 166, 247, 133, 39, 336, 266, 362, 184, 17, 63, 158, 50, 116, 344, 354, 201, 98, 292, 156, 130, 335, 291, 365, 332, 366, 146, 382, 186, 287, 159, 104, 217, 139, 219, 28, 99, 260, 180, 264, 227, 273, 379], 'conv2': [1006, 307, 576, 276, 733, 972, 632, 1411, 320, 301, 350, 1284, 915, 1211, 49, 1502, 503, 1385, 272, 1265, 549, 185, 904, 734, 1413, 326, 984, 468, 764, 186, 551, 122, 969, 347, 1050, 1293, 89, 994, 1185, 697, 261, 1500, 1090, 1532, 930, 1047, 879, 1427, 1290, 519, 973, 167, 1030, 1154, 577, 1086, 786, 1453, 334, 1033, 188, 813, 1400, 294, 241, 796, 627, 1042, 1176, 619, 868, 445, 482, 135, 427, 238, 573, 70, 1239, 953, 726, 138, 1271, 985, 240, 268, 710, 1113, 1326, 1394, 823, 938, 601, 195, 205, 988, 1475, 1184, 1371, 1085, 747, 760, 1465, 617, 377, 1494, 73, 289, 737, 1409, 952, 863, 992, 660, 222, 557, 252, 43, 926, 801, 998, 159, 1493, 1251, 1263, 964, 1029, 844, 539, 1091, 1167, 1066, 177, 108, 1458, 477, 923, 766, 458, 1000, 676, 650, 310, 869, 705, 1129, 13, 269, 260, 75, 1428, 1060, 1195, 571, 562, 1214, 430, 659, 851, 854, 1344, 535, 1213, 777, 369, 187, 1242, 775, 1529, 802, 1225, 97, 779, 126, 640, 833, 174, 595, 158, 194, 1068, 946, 1503, 995, 592, 170, 812, 471, 1307, 486, 1462, 1365, 914, 373, 610, 417, 1327, 128, 1230, 133, 346, 1238, 1253, 910, 44, 1076, 950, 423, 90, 1430, 1324, 348, 1150, 1531, 193, 1412, 171, 1084, 1021, 1032, 1522, 1299, 405, 1471, 958, 1122, 580, 509, 446, 408, 715], 'conv3': [5017, 6047, 1653, 3916, 170, 5041, 2613, 4323, 6098, 2691, 5433, 3067, 4958, 4900, 3563, 1730, 62, 2026, 3435, 2075, 2778, 5255, 4087, 4574, 4983, 634, 5795, 1366, 948, 4473, 4359, 3906, 547, 750, 1229, 5651, 4738, 3197, 5004, 785, 514, 1599, 4502, 2051, 5968, 3718, 1680, 5558, 3938, 353, 5984, 5349, 4768, 815, 5956, 1310, 3516, 4415, 2483, 5292, 2021, 4333, 5515, 4090, 3988, 1149, 2470, 1856, 2392, 549, 4350, 1022, 1921, 999, 4682, 3986, 2462, 921, 3227, 265, 706, 1055, 5541, 4429, 680, 3972, 3828, 1635, 5175, 1587, 1685, 2966, 2696, 1318, 4262, 2037, 4163, 4746, 4699, 2299, 3481, 2398, 3740, 4029, 2413, 5314, 3181, 2804, 837, 3511, 5918, 1575, 3807, 1983, 1778, 63, 3486, 51, 3669, 4924, 1526, 817, 3179, 5498, 6, 1385, 1066, 534, 5733, 3210, 1086, 4357, 2176, 1331, 1670, 4408, 117, 2344, 1181, 3222, 1911, 2427, 2979, 1304, 5752, 2453, 5283, 5188, 745, 5591, 4968, 1344, 3603, 261, 1585, 5, 2786, 5406, 8, 300, 2300, 4905, 2714, 3431, 2661, 5571, 4360, 4556, 3750, 771, 211, 3888, 4486, 813, 2049, 4025, 6114, 1294, 845, 3414, 5312, 5001, 2404, 2769, 4094, 4624, 5065, 4496, 455, 3090, 6070, 3101, 1760, 1037, 1913, 1831, 1036, 1117, 3449, 5919, 418, 5455, 5299, 485, 905, 4733, 5091, 3959, 4739, 183, 2078, 1009, 1043, 4134, 2085, 5717, 3543, 3430, 552, 3830, 1512, 1822, 2914, 5705, 822, 3983, 5860, 688, 4687, 6134, 2201, 5222, 1342, 784, 1112, 1198, 2938, 2041, 4862, 3321, 5579, 5931, 526, 2608, 1872, 1163, 2780, 4218, 4256, 872, 122, 4396, 551, 1437, 5473, 4513, 2584, 1883, 3495, 2136, 5477, 5977, 3159, 1703, 1118, 4538, 1346, 2698, 4397, 3941, 2047, 5531, 2488, 3581, 132, 339, 3502, 2850, 1792, 2418, 5346, 2354, 6054, 3001, 4227, 1790, 4598, 2056, 4100, 3725, 909, 3909, 4451, 4072, 4660, 2635, 1102, 3656, 2581, 176, 1001, 2152, 2444, 3448, 685, 497, 2920, 652, 621, 4390, 3186, 4221, 1097, 2069, 4177, 1620, 4118, 4377, 178, 1615, 1950, 3010, 2092, 2793, 4694, 2103, 5834, 6004, 3905, 632, 844, 4618, 4131, 4063, 1403, 583, 1351, 5469, 1753, 1514, 4602, 4173, 2835, 2750, 3215, 2274, 5457, 3507, 5482, 5981, 3062, 5135, 2847, 2010, 4223, 2662, 3824, 2447, 5847, 165, 5276, 2210, 5493, 129, 198, 5780, 4853, 1544, 2511, 1806, 5910, 4382, 956, 2426, 1443, 1622, 4906, 5146, 4159, 5214, 668, 4876, 1284, 194, 227, 2980, 1187, 6090, 49, 4036, 436, 3470, 6058, 3488, 1594, 5322, 1283, 1864, 4539, 860, 3420, 3138, 818, 1861, 257, 2700, 2139, 2695, 3351, 2379, 4544, 1176, 2342, 2128, 1343, 2514, 4394, 686, 5212, 3895, 3540, 4110, 109, 3312, 893, 5542, 6053, 3473, 4301, 5196, 4937, 1337, 5407, 4219, 2972, 208, 2812, 4266, 4646, 5027, 4938, 3719, 1075, 295, 5436, 78, 5243, 4804, 638, 1564, 1894, 4980, 5929, 5853, 3640, 4964, 5821, 5297, 5201, 398, 5909, 59, 2542, 495, 6119, 4097, 2292, 1560, 2666, 2627, 1873, 3912, 4636, 3436, 26, 1453, 1394, 4947, 5216, 4089, 4166, 1415, 4954, 2083, 1834, 4212, 3809, 4635, 4102, 3866, 2533, 5838, 5412, 221, 6116, 2367, 3944, 1988, 1132, 1380, 1454, 4264, 1499, 5786, 1316, 4306, 3584, 171, 2675, 2667, 5205, 2530, 5108, 5045, 3793, 2703, 1684, 4421, 1191, 294, 2008, 41, 3760, 4280, 2923, 13, 4651, 1309, 1185, 1435, 6122, 2945, 2755, 4114, 1371, 2808, 4319, 2679, 5520, 4897, 6142, 2089, 2563, 5630, 292, 3151, 2668, 4553, 2927, 2156, 5076, 3193, 313, 3670, 3696, 709, 252, 2779, 1108, 1312, 2236, 2098, 2058, 772, 4257, 3330, 5791, 2957, 3569, 65, 5056, 3602, 5739, 1641, 5318, 722, 5445, 3755, 24, 5396, 675, 760, 3873, 673, 926, 3017, 1168, 5354, 4546, 2314, 5607, 259, 3658, 1608, 2007, 4289, 4777, 888, 3434, 4736, 4445, 4107, 1164, 2173, 4431, 576, 3802, 4120, 1879, 3191, 4220, 4076, 4961, 5839, 3150, 5206, 3377, 3096, 255, 5087, 2048, 2393, 1956, 3039, 1162, 4603, 5161, 5268, 491, 2580, 5423, 795, 2597, 463, 5043, 4236, 4291, 1230, 1882, 1978, 1511, 1866, 4728, 5942, 1362, 4879, 2348, 2845, 796, 5582, 4919, 2834, 4402, 6109, 1634, 3735, 6008, 3914, 1960, 5172, 625, 4270, 3556, 4541, 2976, 6089, 2860, 2962, 2082, 3479, 5596, 4688, 5321, 4762, 2711, 3775, 3553, 2251, 4885, 2887, 3657, 1096, 5401, 4640, 5921, 5204, 2033, 2206, 2802, 4412, 2928, 441, 1578, 951, 287, 1357, 3761, 4176, 2101, 14, 4299, 2310, 5936, 2073, 2538, 2077, 4628, 4104, 870, 2318, 4447, 2227, 5932, 2736, 5386, 4162, 2918, 5164, 2859, 3804, 1161, 6102, 4881, 246, 5503, 707, 4840, 2917, 1941, 1836, 2607, 763, 5454, 2759, 1115, 530, 1936, 426, 1755, 980, 620, 846, 1949, 1537, 1846, 865, 223, 3686, 4785, 5697, 1021, 4028, 1887, 2100, 2268, 4449, 3133, 3538, 5626, 826, 5008, 1569, 1457, 780, 1194, 2889, 5103, 229, 2460, 2798, 3918, 5680, 89, 493, 5818, 4658, 3594, 2445, 1151, 1535, 1273, 1169, 1422, 1264, 4239, 1543, 834, 5015, 2203, 3425, 4365, 6125, 3170, 1368, 4752, 1820, 3834, 2680, 2112, 4778, 3965, 5116, 1028, 3027, 2715, 4300, 2443, 1574, 2519, 3947, 1649, 144, 1088, 5854, 2792, 5885, 2382, 3163, 1912, 765, 3023, 4013, 2022, 527, 5980, 4482, 3636, 3283, 2067, 2106, 2071, 3242, 1999, 2449, 5893, 1376, 3025, 966, 1204, 5245, 1470, 4185, 4410, 5693, 4614, 3943, 4034, 3384, 4460, 5199, 3759, 1116, 3902, 2861, 3836, 2585, 179, 1697, 5994, 1702, 2817, 4519, 4330, 4480, 4882, 555, 5081, 1279, 3429, 545, 4154, 590, 4135, 5887, 970, 363, 3032, 2419, 4956, 5642, 1345, 1845, 1438, 4775, 6096, 878, 1600, 425, 1335, 1914, 2016, 5523, 3277, 4051, 6021, 1980, 5862, 4566, 450, 3350, 175, 1665, 1969, 524, 1095, 4363, 3118, 3097, 4669, 5460, 3126, 4242, 5702, 5552, 52, 3733, 4142, 2547, 1839, 271, 674, 511, 4248]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [10/50]	lr: 1.00e-03	time: 00:08:41	Loss_train 1.86903	Acc_train 79.95	/	Loss_test 0.21385	Acc_test 76.52
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLbe1cb3cb-b5f8-4a9d-bda0-16cc8cbeda29/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [25, 16, 59, 53, 30, 42, 77, 68, 69, 93, 75, 51, 34, 38, 70], 'conv1': [23, 75, 345, 61, 349, 92, 330, 197, 100, 33, 69, 190, 296, 41, 121, 157, 76, 166, 247, 133, 39, 336, 266, 362, 184, 17, 63, 158, 50, 116, 344, 354, 201, 98, 292, 156, 130, 335, 291, 365, 332, 366, 146, 382, 186, 287, 159, 104, 217, 139, 219, 28, 99, 260, 180, 264, 227, 273, 379], 'conv2': [1006, 307, 576, 276, 733, 972, 632, 1411, 320, 301, 350, 1284, 915, 1211, 49, 1502, 503, 1385, 272, 1265, 549, 185, 904, 734, 1413, 326, 984, 468, 764, 186, 551, 122, 969, 347, 1050, 1293, 89, 994, 1185, 697, 261, 1500, 1090, 1532, 930, 1047, 879, 1427, 1290, 519, 973, 167, 1030, 1154, 577, 1086, 786, 1453, 334, 1033, 188, 813, 1400, 294, 241, 796, 627, 1042, 1176, 619, 868, 445, 482, 135, 427, 238, 573, 70, 1239, 953, 726, 138, 1271, 985, 240, 268, 710, 1113, 1326, 1394, 823, 938, 601, 195, 205, 988, 1475, 1184, 1371, 1085, 747, 760, 1465, 617, 377, 1494, 73, 289, 737, 1409, 952, 863, 992, 660, 222, 557, 252, 43, 926, 801, 998, 159, 1493, 1251, 1263, 964, 1029, 844, 539, 1091, 1167, 1066, 177, 108, 1458, 477, 923, 766, 458, 1000, 676, 650, 310, 869, 705, 1129, 13, 269, 260, 75, 1428, 1060, 1195, 571, 562, 1214, 430, 659, 851, 854, 1344, 535, 1213, 777, 369, 187, 1242, 775, 1529, 802, 1225, 97, 779, 126, 640, 833, 174, 595, 158, 194, 1068, 946, 1503, 995, 592, 170, 812, 471, 1307, 486, 1462, 1365, 914, 373, 610, 417, 1327, 128, 1230, 133, 346, 1238, 1253, 910, 44, 1076, 950, 423, 90, 1430, 1324, 348, 1150, 1531, 193, 1412, 171, 1084, 1021, 1032, 1522, 1299, 405, 1471, 958, 1122, 580, 509, 446, 408, 715], 'conv3': [5017, 6047, 1653, 3916, 170, 5041, 2613, 4323, 6098, 2691, 5433, 3067, 4958, 4900, 3563, 1730, 62, 2026, 3435, 2075, 2778, 5255, 4087, 4574, 4983, 634, 5795, 1366, 948, 4473, 4359, 3906, 547, 750, 1229, 5651, 4738, 3197, 5004, 785, 514, 1599, 4502, 2051, 5968, 3718, 1680, 5558, 3938, 353, 5984, 5349, 4768, 815, 5956, 1310, 3516, 4415, 2483, 5292, 2021, 4333, 5515, 4090, 3988, 1149, 2470, 1856, 2392, 549, 4350, 1022, 1921, 999, 4682, 3986, 2462, 921, 3227, 265, 706, 1055, 5541, 4429, 680, 3972, 3828, 1635, 5175, 1587, 1685, 2966, 2696, 1318, 4262, 2037, 4163, 4746, 4699, 2299, 3481, 2398, 3740, 4029, 2413, 5314, 3181, 2804, 837, 3511, 5918, 1575, 3807, 1983, 1778, 63, 3486, 51, 3669, 4924, 1526, 817, 3179, 5498, 6, 1385, 1066, 534, 5733, 3210, 1086, 4357, 2176, 1331, 1670, 4408, 117, 2344, 1181, 3222, 1911, 2427, 2979, 1304, 5752, 2453, 5283, 5188, 745, 5591, 4968, 1344, 3603, 261, 1585, 5, 2786, 5406, 8, 300, 2300, 4905, 2714, 3431, 2661, 5571, 4360, 4556, 3750, 771, 211, 3888, 4486, 813, 2049, 4025, 6114, 1294, 845, 3414, 5312, 5001, 2404, 2769, 4094, 4624, 5065, 4496, 455, 3090, 6070, 3101, 1760, 1037, 1913, 1831, 1036, 1117, 3449, 5919, 418, 5455, 5299, 485, 905, 4733, 5091, 3959, 4739, 183, 2078, 1009, 1043, 4134, 2085, 5717, 3543, 3430, 552, 3830, 1512, 1822, 2914, 5705, 822, 3983, 5860, 688, 4687, 6134, 2201, 5222, 1342, 784, 1112, 1198, 2938, 2041, 4862, 3321, 5579, 5931, 526, 2608, 1872, 1163, 2780, 4218, 4256, 872, 122, 4396, 551, 1437, 5473, 4513, 2584, 1883, 3495, 2136, 5477, 5977, 3159, 1703, 1118, 4538, 1346, 2698, 4397, 3941, 2047, 5531, 2488, 3581, 132, 339, 3502, 2850, 1792, 2418, 5346, 2354, 6054, 3001, 4227, 1790, 4598, 2056, 4100, 3725, 909, 3909, 4451, 4072, 4660, 2635, 1102, 3656, 2581, 176, 1001, 2152, 2444, 3448, 685, 497, 2920, 652, 621, 4390, 3186, 4221, 1097, 2069, 4177, 1620, 4118, 4377, 178, 1615, 1950, 3010, 2092, 2793, 4694, 2103, 5834, 6004, 3905, 632, 844, 4618, 4131, 4063, 1403, 583, 1351, 5469, 1753, 1514, 4602, 4173, 2835, 2750, 3215, 2274, 5457, 3507, 5482, 5981, 3062, 5135, 2847, 2010, 4223, 2662, 3824, 2447, 5847, 165, 5276, 2210, 5493, 129, 198, 5780, 4853, 1544, 2511, 1806, 5910, 4382, 956, 2426, 1443, 1622, 4906, 5146, 4159, 5214, 668, 4876, 1284, 194, 227, 2980, 1187, 6090, 49, 4036, 436, 3470, 6058, 3488, 1594, 5322, 1283, 1864, 4539, 860, 3420, 3138, 818, 1861, 257, 2700, 2139, 2695, 3351, 2379, 4544, 1176, 2342, 2128, 1343, 2514, 4394, 686, 5212, 3895, 3540, 4110, 109, 3312, 893, 5542, 6053, 3473, 4301, 5196, 4937, 1337, 5407, 4219, 2972, 208, 2812, 4266, 4646, 5027, 4938, 3719, 1075, 295, 5436, 78, 5243, 4804, 638, 1564, 1894, 4980, 5929, 5853, 3640, 4964, 5821, 5297, 5201, 398, 5909, 59, 2542, 495, 6119, 4097, 2292, 1560, 2666, 2627, 1873, 3912, 4636, 3436, 26, 1453, 1394, 4947, 5216, 4089, 4166, 1415, 4954, 2083, 1834, 4212, 3809, 4635, 4102, 3866, 2533, 5838, 5412, 221, 6116, 2367, 3944, 1988, 1132, 1380, 1454, 4264, 1499, 5786, 1316, 4306, 3584, 171, 2675, 2667, 5205, 2530, 5108, 5045, 3793, 2703, 1684, 4421, 1191, 294, 2008, 41, 3760, 4280, 2923, 13, 4651, 1309, 1185, 1435, 6122, 2945, 2755, 4114, 1371, 2808, 4319, 2679, 5520, 4897, 6142, 2089, 2563, 5630, 292, 3151, 2668, 4553, 2927, 2156, 5076, 3193, 313, 3670, 3696, 709, 252, 2779, 1108, 1312, 2236, 2098, 2058, 772, 4257, 3330, 5791, 2957, 3569, 65, 5056, 3602, 5739, 1641, 5318, 722, 5445, 3755, 24, 5396, 675, 760, 3873, 673, 926, 3017, 1168, 5354, 4546, 2314, 5607, 259, 3658, 1608, 2007, 4289, 4777, 888, 3434, 4736, 4445, 4107, 1164, 2173, 4431, 576, 3802, 4120, 1879, 3191, 4220, 4076, 4961, 5839, 3150, 5206, 3377, 3096, 255, 5087, 2048, 2393, 1956, 3039, 1162, 4603, 5161, 5268, 491, 2580, 5423, 795, 2597, 463, 5043, 4236, 4291, 1230, 1882, 1978, 1511, 1866, 4728, 5942, 1362, 4879, 2348, 2845, 796, 5582, 4919, 2834, 4402, 6109, 1634, 3735, 6008, 3914, 1960, 5172, 625, 4270, 3556, 4541, 2976, 6089, 2860, 2962, 2082, 3479, 5596, 4688, 5321, 4762, 2711, 3775, 3553, 2251, 4885, 2887, 3657, 1096, 5401, 4640, 5921, 5204, 2033, 2206, 2802, 4412, 2928, 441, 1578, 951, 287, 1357, 3761, 4176, 2101, 14, 4299, 2310, 5936, 2073, 2538, 2077, 4628, 4104, 870, 2318, 4447, 2227, 5932, 2736, 5386, 4162, 2918, 5164, 2859, 3804, 1161, 6102, 4881, 246, 5503, 707, 4840, 2917, 1941, 1836, 2607, 763, 5454, 2759, 1115, 530, 1936, 426, 1755, 980, 620, 846, 1949, 1537, 1846, 865, 223, 3686, 4785, 5697, 1021, 4028, 1887, 2100, 2268, 4449, 3133, 3538, 5626, 826, 5008, 1569, 1457, 780, 1194, 2889, 5103, 229, 2460, 2798, 3918, 5680, 89, 493, 5818, 4658, 3594, 2445, 1151, 1535, 1273, 1169, 1422, 1264, 4239, 1543, 834, 5015, 2203, 3425, 4365, 6125, 3170, 1368, 4752, 1820, 3834, 2680, 2112, 4778, 3965, 5116, 1028, 3027, 2715, 4300, 2443, 1574, 2519, 3947, 1649, 144, 1088, 5854, 2792, 5885, 2382, 3163, 1912, 765, 3023, 4013, 2022, 527, 5980, 4482, 3636, 3283, 2067, 2106, 2071, 3242, 1999, 2449, 5893, 1376, 3025, 966, 1204, 5245, 1470, 4185, 4410, 5693, 4614, 3943, 4034, 3384, 4460, 5199, 3759, 1116, 3902, 2861, 3836, 2585, 179, 1697, 5994, 1702, 2817, 4519, 4330, 4480, 4882, 555, 5081, 1279, 3429, 545, 4154, 590, 4135, 5887, 970, 363, 3032, 2419, 4956, 5642, 1345, 1845, 1438, 4775, 6096, 878, 1600, 425, 1335, 1914, 2016, 5523, 3277, 4051, 6021, 1980, 5862, 4566, 450, 3350, 175, 1665, 1969, 524, 1095, 4363, 3118, 3097, 4669, 5460, 3126, 4242, 5702, 5552, 52, 3733, 4142, 2547, 1839, 271, 674, 511, 4248]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [20/50]	lr: 2.50e-04	time: 00:12:45	Loss_train 0.57331	Acc_train 91.90	/	Loss_test 0.15852	Acc_test 80.43
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLbe1cb3cb-b5f8-4a9d-bda0-16cc8cbeda29/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [25, 16, 59, 53, 30, 42, 77, 68, 69, 93, 75, 51, 34, 38, 70], 'conv1': [23, 75, 345, 61, 349, 92, 330, 197, 100, 33, 69, 190, 296, 41, 121, 157, 76, 166, 247, 133, 39, 336, 266, 362, 184, 17, 63, 158, 50, 116, 344, 354, 201, 98, 292, 156, 130, 335, 291, 365, 332, 366, 146, 382, 186, 287, 159, 104, 217, 139, 219, 28, 99, 260, 180, 264, 227, 273, 379], 'conv2': [1006, 307, 576, 276, 733, 972, 632, 1411, 320, 301, 350, 1284, 915, 1211, 49, 1502, 503, 1385, 272, 1265, 549, 185, 904, 734, 1413, 326, 984, 468, 764, 186, 551, 122, 969, 347, 1050, 1293, 89, 994, 1185, 697, 261, 1500, 1090, 1532, 930, 1047, 879, 1427, 1290, 519, 973, 167, 1030, 1154, 577, 1086, 786, 1453, 334, 1033, 188, 813, 1400, 294, 241, 796, 627, 1042, 1176, 619, 868, 445, 482, 135, 427, 238, 573, 70, 1239, 953, 726, 138, 1271, 985, 240, 268, 710, 1113, 1326, 1394, 823, 938, 601, 195, 205, 988, 1475, 1184, 1371, 1085, 747, 760, 1465, 617, 377, 1494, 73, 289, 737, 1409, 952, 863, 992, 660, 222, 557, 252, 43, 926, 801, 998, 159, 1493, 1251, 1263, 964, 1029, 844, 539, 1091, 1167, 1066, 177, 108, 1458, 477, 923, 766, 458, 1000, 676, 650, 310, 869, 705, 1129, 13, 269, 260, 75, 1428, 1060, 1195, 571, 562, 1214, 430, 659, 851, 854, 1344, 535, 1213, 777, 369, 187, 1242, 775, 1529, 802, 1225, 97, 779, 126, 640, 833, 174, 595, 158, 194, 1068, 946, 1503, 995, 592, 170, 812, 471, 1307, 486, 1462, 1365, 914, 373, 610, 417, 1327, 128, 1230, 133, 346, 1238, 1253, 910, 44, 1076, 950, 423, 90, 1430, 1324, 348, 1150, 1531, 193, 1412, 171, 1084, 1021, 1032, 1522, 1299, 405, 1471, 958, 1122, 580, 509, 446, 408, 715], 'conv3': [5017, 6047, 1653, 3916, 170, 5041, 2613, 4323, 6098, 2691, 5433, 3067, 4958, 4900, 3563, 1730, 62, 2026, 3435, 2075, 2778, 5255, 4087, 4574, 4983, 634, 5795, 1366, 948, 4473, 4359, 3906, 547, 750, 1229, 5651, 4738, 3197, 5004, 785, 514, 1599, 4502, 2051, 5968, 3718, 1680, 5558, 3938, 353, 5984, 5349, 4768, 815, 5956, 1310, 3516, 4415, 2483, 5292, 2021, 4333, 5515, 4090, 3988, 1149, 2470, 1856, 2392, 549, 4350, 1022, 1921, 999, 4682, 3986, 2462, 921, 3227, 265, 706, 1055, 5541, 4429, 680, 3972, 3828, 1635, 5175, 1587, 1685, 2966, 2696, 1318, 4262, 2037, 4163, 4746, 4699, 2299, 3481, 2398, 3740, 4029, 2413, 5314, 3181, 2804, 837, 3511, 5918, 1575, 3807, 1983, 1778, 63, 3486, 51, 3669, 4924, 1526, 817, 3179, 5498, 6, 1385, 1066, 534, 5733, 3210, 1086, 4357, 2176, 1331, 1670, 4408, 117, 2344, 1181, 3222, 1911, 2427, 2979, 1304, 5752, 2453, 5283, 5188, 745, 5591, 4968, 1344, 3603, 261, 1585, 5, 2786, 5406, 8, 300, 2300, 4905, 2714, 3431, 2661, 5571, 4360, 4556, 3750, 771, 211, 3888, 4486, 813, 2049, 4025, 6114, 1294, 845, 3414, 5312, 5001, 2404, 2769, 4094, 4624, 5065, 4496, 455, 3090, 6070, 3101, 1760, 1037, 1913, 1831, 1036, 1117, 3449, 5919, 418, 5455, 5299, 485, 905, 4733, 5091, 3959, 4739, 183, 2078, 1009, 1043, 4134, 2085, 5717, 3543, 3430, 552, 3830, 1512, 1822, 2914, 5705, 822, 3983, 5860, 688, 4687, 6134, 2201, 5222, 1342, 784, 1112, 1198, 2938, 2041, 4862, 3321, 5579, 5931, 526, 2608, 1872, 1163, 2780, 4218, 4256, 872, 122, 4396, 551, 1437, 5473, 4513, 2584, 1883, 3495, 2136, 5477, 5977, 3159, 1703, 1118, 4538, 1346, 2698, 4397, 3941, 2047, 5531, 2488, 3581, 132, 339, 3502, 2850, 1792, 2418, 5346, 2354, 6054, 3001, 4227, 1790, 4598, 2056, 4100, 3725, 909, 3909, 4451, 4072, 4660, 2635, 1102, 3656, 2581, 176, 1001, 2152, 2444, 3448, 685, 497, 2920, 652, 621, 4390, 3186, 4221, 1097, 2069, 4177, 1620, 4118, 4377, 178, 1615, 1950, 3010, 2092, 2793, 4694, 2103, 5834, 6004, 3905, 632, 844, 4618, 4131, 4063, 1403, 583, 1351, 5469, 1753, 1514, 4602, 4173, 2835, 2750, 3215, 2274, 5457, 3507, 5482, 5981, 3062, 5135, 2847, 2010, 4223, 2662, 3824, 2447, 5847, 165, 5276, 2210, 5493, 129, 198, 5780, 4853, 1544, 2511, 1806, 5910, 4382, 956, 2426, 1443, 1622, 4906, 5146, 4159, 5214, 668, 4876, 1284, 194, 227, 2980, 1187, 6090, 49, 4036, 436, 3470, 6058, 3488, 1594, 5322, 1283, 1864, 4539, 860, 3420, 3138, 818, 1861, 257, 2700, 2139, 2695, 3351, 2379, 4544, 1176, 2342, 2128, 1343, 2514, 4394, 686, 5212, 3895, 3540, 4110, 109, 3312, 893, 5542, 6053, 3473, 4301, 5196, 4937, 1337, 5407, 4219, 2972, 208, 2812, 4266, 4646, 5027, 4938, 3719, 1075, 295, 5436, 78, 5243, 4804, 638, 1564, 1894, 4980, 5929, 5853, 3640, 4964, 5821, 5297, 5201, 398, 5909, 59, 2542, 495, 6119, 4097, 2292, 1560, 2666, 2627, 1873, 3912, 4636, 3436, 26, 1453, 1394, 4947, 5216, 4089, 4166, 1415, 4954, 2083, 1834, 4212, 3809, 4635, 4102, 3866, 2533, 5838, 5412, 221, 6116, 2367, 3944, 1988, 1132, 1380, 1454, 4264, 1499, 5786, 1316, 4306, 3584, 171, 2675, 2667, 5205, 2530, 5108, 5045, 3793, 2703, 1684, 4421, 1191, 294, 2008, 41, 3760, 4280, 2923, 13, 4651, 1309, 1185, 1435, 6122, 2945, 2755, 4114, 1371, 2808, 4319, 2679, 5520, 4897, 6142, 2089, 2563, 5630, 292, 3151, 2668, 4553, 2927, 2156, 5076, 3193, 313, 3670, 3696, 709, 252, 2779, 1108, 1312, 2236, 2098, 2058, 772, 4257, 3330, 5791, 2957, 3569, 65, 5056, 3602, 5739, 1641, 5318, 722, 5445, 3755, 24, 5396, 675, 760, 3873, 673, 926, 3017, 1168, 5354, 4546, 2314, 5607, 259, 3658, 1608, 2007, 4289, 4777, 888, 3434, 4736, 4445, 4107, 1164, 2173, 4431, 576, 3802, 4120, 1879, 3191, 4220, 4076, 4961, 5839, 3150, 5206, 3377, 3096, 255, 5087, 2048, 2393, 1956, 3039, 1162, 4603, 5161, 5268, 491, 2580, 5423, 795, 2597, 463, 5043, 4236, 4291, 1230, 1882, 1978, 1511, 1866, 4728, 5942, 1362, 4879, 2348, 2845, 796, 5582, 4919, 2834, 4402, 6109, 1634, 3735, 6008, 3914, 1960, 5172, 625, 4270, 3556, 4541, 2976, 6089, 2860, 2962, 2082, 3479, 5596, 4688, 5321, 4762, 2711, 3775, 3553, 2251, 4885, 2887, 3657, 1096, 5401, 4640, 5921, 5204, 2033, 2206, 2802, 4412, 2928, 441, 1578, 951, 287, 1357, 3761, 4176, 2101, 14, 4299, 2310, 5936, 2073, 2538, 2077, 4628, 4104, 870, 2318, 4447, 2227, 5932, 2736, 5386, 4162, 2918, 5164, 2859, 3804, 1161, 6102, 4881, 246, 5503, 707, 4840, 2917, 1941, 1836, 2607, 763, 5454, 2759, 1115, 530, 1936, 426, 1755, 980, 620, 846, 1949, 1537, 1846, 865, 223, 3686, 4785, 5697, 1021, 4028, 1887, 2100, 2268, 4449, 3133, 3538, 5626, 826, 5008, 1569, 1457, 780, 1194, 2889, 5103, 229, 2460, 2798, 3918, 5680, 89, 493, 5818, 4658, 3594, 2445, 1151, 1535, 1273, 1169, 1422, 1264, 4239, 1543, 834, 5015, 2203, 3425, 4365, 6125, 3170, 1368, 4752, 1820, 3834, 2680, 2112, 4778, 3965, 5116, 1028, 3027, 2715, 4300, 2443, 1574, 2519, 3947, 1649, 144, 1088, 5854, 2792, 5885, 2382, 3163, 1912, 765, 3023, 4013, 2022, 527, 5980, 4482, 3636, 3283, 2067, 2106, 2071, 3242, 1999, 2449, 5893, 1376, 3025, 966, 1204, 5245, 1470, 4185, 4410, 5693, 4614, 3943, 4034, 3384, 4460, 5199, 3759, 1116, 3902, 2861, 3836, 2585, 179, 1697, 5994, 1702, 2817, 4519, 4330, 4480, 4882, 555, 5081, 1279, 3429, 545, 4154, 590, 4135, 5887, 970, 363, 3032, 2419, 4956, 5642, 1345, 1845, 1438, 4775, 6096, 878, 1600, 425, 1335, 1914, 2016, 5523, 3277, 4051, 6021, 1980, 5862, 4566, 450, 3350, 175, 1665, 1969, 524, 1095, 4363, 3118, 3097, 4669, 5460, 3126, 4242, 5702, 5552, 52, 3733, 4142, 2547, 1839, 271, 674, 511, 4248]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [30/50]	lr: 1.25e-04	time: 00:16:48	Loss_train 0.25921	Acc_train 95.28	/	Loss_test 0.15811	Acc_test 80.69
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLbe1cb3cb-b5f8-4a9d-bda0-16cc8cbeda29/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [25, 16, 59, 53, 30, 42, 77, 68, 69, 93, 75, 51, 34, 38, 70], 'conv1': [23, 75, 345, 61, 349, 92, 330, 197, 100, 33, 69, 190, 296, 41, 121, 157, 76, 166, 247, 133, 39, 336, 266, 362, 184, 17, 63, 158, 50, 116, 344, 354, 201, 98, 292, 156, 130, 335, 291, 365, 332, 366, 146, 382, 186, 287, 159, 104, 217, 139, 219, 28, 99, 260, 180, 264, 227, 273, 379], 'conv2': [1006, 307, 576, 276, 733, 972, 632, 1411, 320, 301, 350, 1284, 915, 1211, 49, 1502, 503, 1385, 272, 1265, 549, 185, 904, 734, 1413, 326, 984, 468, 764, 186, 551, 122, 969, 347, 1050, 1293, 89, 994, 1185, 697, 261, 1500, 1090, 1532, 930, 1047, 879, 1427, 1290, 519, 973, 167, 1030, 1154, 577, 1086, 786, 1453, 334, 1033, 188, 813, 1400, 294, 241, 796, 627, 1042, 1176, 619, 868, 445, 482, 135, 427, 238, 573, 70, 1239, 953, 726, 138, 1271, 985, 240, 268, 710, 1113, 1326, 1394, 823, 938, 601, 195, 205, 988, 1475, 1184, 1371, 1085, 747, 760, 1465, 617, 377, 1494, 73, 289, 737, 1409, 952, 863, 992, 660, 222, 557, 252, 43, 926, 801, 998, 159, 1493, 1251, 1263, 964, 1029, 844, 539, 1091, 1167, 1066, 177, 108, 1458, 477, 923, 766, 458, 1000, 676, 650, 310, 869, 705, 1129, 13, 269, 260, 75, 1428, 1060, 1195, 571, 562, 1214, 430, 659, 851, 854, 1344, 535, 1213, 777, 369, 187, 1242, 775, 1529, 802, 1225, 97, 779, 126, 640, 833, 174, 595, 158, 194, 1068, 946, 1503, 995, 592, 170, 812, 471, 1307, 486, 1462, 1365, 914, 373, 610, 417, 1327, 128, 1230, 133, 346, 1238, 1253, 910, 44, 1076, 950, 423, 90, 1430, 1324, 348, 1150, 1531, 193, 1412, 171, 1084, 1021, 1032, 1522, 1299, 405, 1471, 958, 1122, 580, 509, 446, 408, 715], 'conv3': [5017, 6047, 1653, 3916, 170, 5041, 2613, 4323, 6098, 2691, 5433, 3067, 4958, 4900, 3563, 1730, 62, 2026, 3435, 2075, 2778, 5255, 4087, 4574, 4983, 634, 5795, 1366, 948, 4473, 4359, 3906, 547, 750, 1229, 5651, 4738, 3197, 5004, 785, 514, 1599, 4502, 2051, 5968, 3718, 1680, 5558, 3938, 353, 5984, 5349, 4768, 815, 5956, 1310, 3516, 4415, 2483, 5292, 2021, 4333, 5515, 4090, 3988, 1149, 2470, 1856, 2392, 549, 4350, 1022, 1921, 999, 4682, 3986, 2462, 921, 3227, 265, 706, 1055, 5541, 4429, 680, 3972, 3828, 1635, 5175, 1587, 1685, 2966, 2696, 1318, 4262, 2037, 4163, 4746, 4699, 2299, 3481, 2398, 3740, 4029, 2413, 5314, 3181, 2804, 837, 3511, 5918, 1575, 3807, 1983, 1778, 63, 3486, 51, 3669, 4924, 1526, 817, 3179, 5498, 6, 1385, 1066, 534, 5733, 3210, 1086, 4357, 2176, 1331, 1670, 4408, 117, 2344, 1181, 3222, 1911, 2427, 2979, 1304, 5752, 2453, 5283, 5188, 745, 5591, 4968, 1344, 3603, 261, 1585, 5, 2786, 5406, 8, 300, 2300, 4905, 2714, 3431, 2661, 5571, 4360, 4556, 3750, 771, 211, 3888, 4486, 813, 2049, 4025, 6114, 1294, 845, 3414, 5312, 5001, 2404, 2769, 4094, 4624, 5065, 4496, 455, 3090, 6070, 3101, 1760, 1037, 1913, 1831, 1036, 1117, 3449, 5919, 418, 5455, 5299, 485, 905, 4733, 5091, 3959, 4739, 183, 2078, 1009, 1043, 4134, 2085, 5717, 3543, 3430, 552, 3830, 1512, 1822, 2914, 5705, 822, 3983, 5860, 688, 4687, 6134, 2201, 5222, 1342, 784, 1112, 1198, 2938, 2041, 4862, 3321, 5579, 5931, 526, 2608, 1872, 1163, 2780, 4218, 4256, 872, 122, 4396, 551, 1437, 5473, 4513, 2584, 1883, 3495, 2136, 5477, 5977, 3159, 1703, 1118, 4538, 1346, 2698, 4397, 3941, 2047, 5531, 2488, 3581, 132, 339, 3502, 2850, 1792, 2418, 5346, 2354, 6054, 3001, 4227, 1790, 4598, 2056, 4100, 3725, 909, 3909, 4451, 4072, 4660, 2635, 1102, 3656, 2581, 176, 1001, 2152, 2444, 3448, 685, 497, 2920, 652, 621, 4390, 3186, 4221, 1097, 2069, 4177, 1620, 4118, 4377, 178, 1615, 1950, 3010, 2092, 2793, 4694, 2103, 5834, 6004, 3905, 632, 844, 4618, 4131, 4063, 1403, 583, 1351, 5469, 1753, 1514, 4602, 4173, 2835, 2750, 3215, 2274, 5457, 3507, 5482, 5981, 3062, 5135, 2847, 2010, 4223, 2662, 3824, 2447, 5847, 165, 5276, 2210, 5493, 129, 198, 5780, 4853, 1544, 2511, 1806, 5910, 4382, 956, 2426, 1443, 1622, 4906, 5146, 4159, 5214, 668, 4876, 1284, 194, 227, 2980, 1187, 6090, 49, 4036, 436, 3470, 6058, 3488, 1594, 5322, 1283, 1864, 4539, 860, 3420, 3138, 818, 1861, 257, 2700, 2139, 2695, 3351, 2379, 4544, 1176, 2342, 2128, 1343, 2514, 4394, 686, 5212, 3895, 3540, 4110, 109, 3312, 893, 5542, 6053, 3473, 4301, 5196, 4937, 1337, 5407, 4219, 2972, 208, 2812, 4266, 4646, 5027, 4938, 3719, 1075, 295, 5436, 78, 5243, 4804, 638, 1564, 1894, 4980, 5929, 5853, 3640, 4964, 5821, 5297, 5201, 398, 5909, 59, 2542, 495, 6119, 4097, 2292, 1560, 2666, 2627, 1873, 3912, 4636, 3436, 26, 1453, 1394, 4947, 5216, 4089, 4166, 1415, 4954, 2083, 1834, 4212, 3809, 4635, 4102, 3866, 2533, 5838, 5412, 221, 6116, 2367, 3944, 1988, 1132, 1380, 1454, 4264, 1499, 5786, 1316, 4306, 3584, 171, 2675, 2667, 5205, 2530, 5108, 5045, 3793, 2703, 1684, 4421, 1191, 294, 2008, 41, 3760, 4280, 2923, 13, 4651, 1309, 1185, 1435, 6122, 2945, 2755, 4114, 1371, 2808, 4319, 2679, 5520, 4897, 6142, 2089, 2563, 5630, 292, 3151, 2668, 4553, 2927, 2156, 5076, 3193, 313, 3670, 3696, 709, 252, 2779, 1108, 1312, 2236, 2098, 2058, 772, 4257, 3330, 5791, 2957, 3569, 65, 5056, 3602, 5739, 1641, 5318, 722, 5445, 3755, 24, 5396, 675, 760, 3873, 673, 926, 3017, 1168, 5354, 4546, 2314, 5607, 259, 3658, 1608, 2007, 4289, 4777, 888, 3434, 4736, 4445, 4107, 1164, 2173, 4431, 576, 3802, 4120, 1879, 3191, 4220, 4076, 4961, 5839, 3150, 5206, 3377, 3096, 255, 5087, 2048, 2393, 1956, 3039, 1162, 4603, 5161, 5268, 491, 2580, 5423, 795, 2597, 463, 5043, 4236, 4291, 1230, 1882, 1978, 1511, 1866, 4728, 5942, 1362, 4879, 2348, 2845, 796, 5582, 4919, 2834, 4402, 6109, 1634, 3735, 6008, 3914, 1960, 5172, 625, 4270, 3556, 4541, 2976, 6089, 2860, 2962, 2082, 3479, 5596, 4688, 5321, 4762, 2711, 3775, 3553, 2251, 4885, 2887, 3657, 1096, 5401, 4640, 5921, 5204, 2033, 2206, 2802, 4412, 2928, 441, 1578, 951, 287, 1357, 3761, 4176, 2101, 14, 4299, 2310, 5936, 2073, 2538, 2077, 4628, 4104, 870, 2318, 4447, 2227, 5932, 2736, 5386, 4162, 2918, 5164, 2859, 3804, 1161, 6102, 4881, 246, 5503, 707, 4840, 2917, 1941, 1836, 2607, 763, 5454, 2759, 1115, 530, 1936, 426, 1755, 980, 620, 846, 1949, 1537, 1846, 865, 223, 3686, 4785, 5697, 1021, 4028, 1887, 2100, 2268, 4449, 3133, 3538, 5626, 826, 5008, 1569, 1457, 780, 1194, 2889, 5103, 229, 2460, 2798, 3918, 5680, 89, 493, 5818, 4658, 3594, 2445, 1151, 1535, 1273, 1169, 1422, 1264, 4239, 1543, 834, 5015, 2203, 3425, 4365, 6125, 3170, 1368, 4752, 1820, 3834, 2680, 2112, 4778, 3965, 5116, 1028, 3027, 2715, 4300, 2443, 1574, 2519, 3947, 1649, 144, 1088, 5854, 2792, 5885, 2382, 3163, 1912, 765, 3023, 4013, 2022, 527, 5980, 4482, 3636, 3283, 2067, 2106, 2071, 3242, 1999, 2449, 5893, 1376, 3025, 966, 1204, 5245, 1470, 4185, 4410, 5693, 4614, 3943, 4034, 3384, 4460, 5199, 3759, 1116, 3902, 2861, 3836, 2585, 179, 1697, 5994, 1702, 2817, 4519, 4330, 4480, 4882, 555, 5081, 1279, 3429, 545, 4154, 590, 4135, 5887, 970, 363, 3032, 2419, 4956, 5642, 1345, 1845, 1438, 4775, 6096, 878, 1600, 425, 1335, 1914, 2016, 5523, 3277, 4051, 6021, 1980, 5862, 4566, 450, 3350, 175, 1665, 1969, 524, 1095, 4363, 3118, 3097, 4669, 5460, 3126, 4242, 5702, 5552, 52, 3733, 4142, 2547, 1839, 271, 674, 511, 4248]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [40/50]	lr: 3.13e-05	time: 00:20:51	Loss_train 0.14773	Acc_train 96.81	/	Loss_test 0.14175	Acc_test 81.73
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLbe1cb3cb-b5f8-4a9d-bda0-16cc8cbeda29/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [25, 16, 59, 53, 30, 42, 77, 68, 69, 93, 75, 51, 34, 38, 70], 'conv1': [23, 75, 345, 61, 349, 92, 330, 197, 100, 33, 69, 190, 296, 41, 121, 157, 76, 166, 247, 133, 39, 336, 266, 362, 184, 17, 63, 158, 50, 116, 344, 354, 201, 98, 292, 156, 130, 335, 291, 365, 332, 366, 146, 382, 186, 287, 159, 104, 217, 139, 219, 28, 99, 260, 180, 264, 227, 273, 379], 'conv2': [1006, 307, 576, 276, 733, 972, 632, 1411, 320, 301, 350, 1284, 915, 1211, 49, 1502, 503, 1385, 272, 1265, 549, 185, 904, 734, 1413, 326, 984, 468, 764, 186, 551, 122, 969, 347, 1050, 1293, 89, 994, 1185, 697, 261, 1500, 1090, 1532, 930, 1047, 879, 1427, 1290, 519, 973, 167, 1030, 1154, 577, 1086, 786, 1453, 334, 1033, 188, 813, 1400, 294, 241, 796, 627, 1042, 1176, 619, 868, 445, 482, 135, 427, 238, 573, 70, 1239, 953, 726, 138, 1271, 985, 240, 268, 710, 1113, 1326, 1394, 823, 938, 601, 195, 205, 988, 1475, 1184, 1371, 1085, 747, 760, 1465, 617, 377, 1494, 73, 289, 737, 1409, 952, 863, 992, 660, 222, 557, 252, 43, 926, 801, 998, 159, 1493, 1251, 1263, 964, 1029, 844, 539, 1091, 1167, 1066, 177, 108, 1458, 477, 923, 766, 458, 1000, 676, 650, 310, 869, 705, 1129, 13, 269, 260, 75, 1428, 1060, 1195, 571, 562, 1214, 430, 659, 851, 854, 1344, 535, 1213, 777, 369, 187, 1242, 775, 1529, 802, 1225, 97, 779, 126, 640, 833, 174, 595, 158, 194, 1068, 946, 1503, 995, 592, 170, 812, 471, 1307, 486, 1462, 1365, 914, 373, 610, 417, 1327, 128, 1230, 133, 346, 1238, 1253, 910, 44, 1076, 950, 423, 90, 1430, 1324, 348, 1150, 1531, 193, 1412, 171, 1084, 1021, 1032, 1522, 1299, 405, 1471, 958, 1122, 580, 509, 446, 408, 715], 'conv3': [5017, 6047, 1653, 3916, 170, 5041, 2613, 4323, 6098, 2691, 5433, 3067, 4958, 4900, 3563, 1730, 62, 2026, 3435, 2075, 2778, 5255, 4087, 4574, 4983, 634, 5795, 1366, 948, 4473, 4359, 3906, 547, 750, 1229, 5651, 4738, 3197, 5004, 785, 514, 1599, 4502, 2051, 5968, 3718, 1680, 5558, 3938, 353, 5984, 5349, 4768, 815, 5956, 1310, 3516, 4415, 2483, 5292, 2021, 4333, 5515, 4090, 3988, 1149, 2470, 1856, 2392, 549, 4350, 1022, 1921, 999, 4682, 3986, 2462, 921, 3227, 265, 706, 1055, 5541, 4429, 680, 3972, 3828, 1635, 5175, 1587, 1685, 2966, 2696, 1318, 4262, 2037, 4163, 4746, 4699, 2299, 3481, 2398, 3740, 4029, 2413, 5314, 3181, 2804, 837, 3511, 5918, 1575, 3807, 1983, 1778, 63, 3486, 51, 3669, 4924, 1526, 817, 3179, 5498, 6, 1385, 1066, 534, 5733, 3210, 1086, 4357, 2176, 1331, 1670, 4408, 117, 2344, 1181, 3222, 1911, 2427, 2979, 1304, 5752, 2453, 5283, 5188, 745, 5591, 4968, 1344, 3603, 261, 1585, 5, 2786, 5406, 8, 300, 2300, 4905, 2714, 3431, 2661, 5571, 4360, 4556, 3750, 771, 211, 3888, 4486, 813, 2049, 4025, 6114, 1294, 845, 3414, 5312, 5001, 2404, 2769, 4094, 4624, 5065, 4496, 455, 3090, 6070, 3101, 1760, 1037, 1913, 1831, 1036, 1117, 3449, 5919, 418, 5455, 5299, 485, 905, 4733, 5091, 3959, 4739, 183, 2078, 1009, 1043, 4134, 2085, 5717, 3543, 3430, 552, 3830, 1512, 1822, 2914, 5705, 822, 3983, 5860, 688, 4687, 6134, 2201, 5222, 1342, 784, 1112, 1198, 2938, 2041, 4862, 3321, 5579, 5931, 526, 2608, 1872, 1163, 2780, 4218, 4256, 872, 122, 4396, 551, 1437, 5473, 4513, 2584, 1883, 3495, 2136, 5477, 5977, 3159, 1703, 1118, 4538, 1346, 2698, 4397, 3941, 2047, 5531, 2488, 3581, 132, 339, 3502, 2850, 1792, 2418, 5346, 2354, 6054, 3001, 4227, 1790, 4598, 2056, 4100, 3725, 909, 3909, 4451, 4072, 4660, 2635, 1102, 3656, 2581, 176, 1001, 2152, 2444, 3448, 685, 497, 2920, 652, 621, 4390, 3186, 4221, 1097, 2069, 4177, 1620, 4118, 4377, 178, 1615, 1950, 3010, 2092, 2793, 4694, 2103, 5834, 6004, 3905, 632, 844, 4618, 4131, 4063, 1403, 583, 1351, 5469, 1753, 1514, 4602, 4173, 2835, 2750, 3215, 2274, 5457, 3507, 5482, 5981, 3062, 5135, 2847, 2010, 4223, 2662, 3824, 2447, 5847, 165, 5276, 2210, 5493, 129, 198, 5780, 4853, 1544, 2511, 1806, 5910, 4382, 956, 2426, 1443, 1622, 4906, 5146, 4159, 5214, 668, 4876, 1284, 194, 227, 2980, 1187, 6090, 49, 4036, 436, 3470, 6058, 3488, 1594, 5322, 1283, 1864, 4539, 860, 3420, 3138, 818, 1861, 257, 2700, 2139, 2695, 3351, 2379, 4544, 1176, 2342, 2128, 1343, 2514, 4394, 686, 5212, 3895, 3540, 4110, 109, 3312, 893, 5542, 6053, 3473, 4301, 5196, 4937, 1337, 5407, 4219, 2972, 208, 2812, 4266, 4646, 5027, 4938, 3719, 1075, 295, 5436, 78, 5243, 4804, 638, 1564, 1894, 4980, 5929, 5853, 3640, 4964, 5821, 5297, 5201, 398, 5909, 59, 2542, 495, 6119, 4097, 2292, 1560, 2666, 2627, 1873, 3912, 4636, 3436, 26, 1453, 1394, 4947, 5216, 4089, 4166, 1415, 4954, 2083, 1834, 4212, 3809, 4635, 4102, 3866, 2533, 5838, 5412, 221, 6116, 2367, 3944, 1988, 1132, 1380, 1454, 4264, 1499, 5786, 1316, 4306, 3584, 171, 2675, 2667, 5205, 2530, 5108, 5045, 3793, 2703, 1684, 4421, 1191, 294, 2008, 41, 3760, 4280, 2923, 13, 4651, 1309, 1185, 1435, 6122, 2945, 2755, 4114, 1371, 2808, 4319, 2679, 5520, 4897, 6142, 2089, 2563, 5630, 292, 3151, 2668, 4553, 2927, 2156, 5076, 3193, 313, 3670, 3696, 709, 252, 2779, 1108, 1312, 2236, 2098, 2058, 772, 4257, 3330, 5791, 2957, 3569, 65, 5056, 3602, 5739, 1641, 5318, 722, 5445, 3755, 24, 5396, 675, 760, 3873, 673, 926, 3017, 1168, 5354, 4546, 2314, 5607, 259, 3658, 1608, 2007, 4289, 4777, 888, 3434, 4736, 4445, 4107, 1164, 2173, 4431, 576, 3802, 4120, 1879, 3191, 4220, 4076, 4961, 5839, 3150, 5206, 3377, 3096, 255, 5087, 2048, 2393, 1956, 3039, 1162, 4603, 5161, 5268, 491, 2580, 5423, 795, 2597, 463, 5043, 4236, 4291, 1230, 1882, 1978, 1511, 1866, 4728, 5942, 1362, 4879, 2348, 2845, 796, 5582, 4919, 2834, 4402, 6109, 1634, 3735, 6008, 3914, 1960, 5172, 625, 4270, 3556, 4541, 2976, 6089, 2860, 2962, 2082, 3479, 5596, 4688, 5321, 4762, 2711, 3775, 3553, 2251, 4885, 2887, 3657, 1096, 5401, 4640, 5921, 5204, 2033, 2206, 2802, 4412, 2928, 441, 1578, 951, 287, 1357, 3761, 4176, 2101, 14, 4299, 2310, 5936, 2073, 2538, 2077, 4628, 4104, 870, 2318, 4447, 2227, 5932, 2736, 5386, 4162, 2918, 5164, 2859, 3804, 1161, 6102, 4881, 246, 5503, 707, 4840, 2917, 1941, 1836, 2607, 763, 5454, 2759, 1115, 530, 1936, 426, 1755, 980, 620, 846, 1949, 1537, 1846, 865, 223, 3686, 4785, 5697, 1021, 4028, 1887, 2100, 2268, 4449, 3133, 3538, 5626, 826, 5008, 1569, 1457, 780, 1194, 2889, 5103, 229, 2460, 2798, 3918, 5680, 89, 493, 5818, 4658, 3594, 2445, 1151, 1535, 1273, 1169, 1422, 1264, 4239, 1543, 834, 5015, 2203, 3425, 4365, 6125, 3170, 1368, 4752, 1820, 3834, 2680, 2112, 4778, 3965, 5116, 1028, 3027, 2715, 4300, 2443, 1574, 2519, 3947, 1649, 144, 1088, 5854, 2792, 5885, 2382, 3163, 1912, 765, 3023, 4013, 2022, 527, 5980, 4482, 3636, 3283, 2067, 2106, 2071, 3242, 1999, 2449, 5893, 1376, 3025, 966, 1204, 5245, 1470, 4185, 4410, 5693, 4614, 3943, 4034, 3384, 4460, 5199, 3759, 1116, 3902, 2861, 3836, 2585, 179, 1697, 5994, 1702, 2817, 4519, 4330, 4480, 4882, 555, 5081, 1279, 3429, 545, 4154, 590, 4135, 5887, 970, 363, 3032, 2419, 4956, 5642, 1345, 1845, 1438, 4775, 6096, 878, 1600, 425, 1335, 1914, 2016, 5523, 3277, 4051, 6021, 1980, 5862, 4566, 450, 3350, 175, 1665, 1969, 524, 1095, 4363, 3118, 3097, 4669, 5460, 3126, 4242, 5702, 5552, 52, 3733, 4142, 2547, 1839, 271, 674, 511, 4248]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [50/50]	lr: 7.81e-06	time: 00:24:55	Loss_train 0.12021	Acc_train 97.24	/	Loss_test 0.14057	Acc_test 81.90
new_head:  {'blocks.4.layer.bias': tensor([ 0.0099, -0.0016, -0.0140, -0.0056,  0.0170,  0.0083,  0.0102,  0.0137,
        -0.0154,  0.0123], device='cuda:0'), 'blocks.4.layer.weight': tensor([[ 0.1073, -0.0009,  0.0072,  ...,  0.2254,  0.0617,  0.0462],
        [-0.0917, -0.0167, -0.0427,  ..., -0.0005,  0.0644,  0.1190],
        [-0.0364, -0.1114,  0.0774,  ..., -0.0271, -0.0134, -0.0558],
        ...,
        [-0.1048,  0.0533,  0.1243,  ..., -0.0253,  0.0444, -0.1016],
        [-0.0325,  0.1420,  0.0787,  ..., -0.0134,  0.0540,  0.0946],
        [-0.0129, -0.1319, -0.2293,  ..., -0.1601, -0.1963, -0.1403]],
       device='cuda:0')}
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CLbe1cb3cb-b5f8-4a9d-bda0-16cc8cbeda29/models
SAVED HEADS THRESHOLD:  0.8595000076293946
SAVED model.topk_kernels {'conv0': [25, 16, 59, 53, 30, 42, 77, 68, 69, 93, 75, 51, 34, 38, 70], 'conv1': [23, 75, 345, 61, 349, 92, 330, 197, 100, 33, 69, 190, 296, 41, 121, 157, 76, 166, 247, 133, 39, 336, 266, 362, 184, 17, 63, 158, 50, 116, 344, 354, 201, 98, 292, 156, 130, 335, 291, 365, 332, 366, 146, 382, 186, 287, 159, 104, 217, 139, 219, 28, 99, 260, 180, 264, 227, 273, 379], 'conv2': [1006, 307, 576, 276, 733, 972, 632, 1411, 320, 301, 350, 1284, 915, 1211, 49, 1502, 503, 1385, 272, 1265, 549, 185, 904, 734, 1413, 326, 984, 468, 764, 186, 551, 122, 969, 347, 1050, 1293, 89, 994, 1185, 697, 261, 1500, 1090, 1532, 930, 1047, 879, 1427, 1290, 519, 973, 167, 1030, 1154, 577, 1086, 786, 1453, 334, 1033, 188, 813, 1400, 294, 241, 796, 627, 1042, 1176, 619, 868, 445, 482, 135, 427, 238, 573, 70, 1239, 953, 726, 138, 1271, 985, 240, 268, 710, 1113, 1326, 1394, 823, 938, 601, 195, 205, 988, 1475, 1184, 1371, 1085, 747, 760, 1465, 617, 377, 1494, 73, 289, 737, 1409, 952, 863, 992, 660, 222, 557, 252, 43, 926, 801, 998, 159, 1493, 1251, 1263, 964, 1029, 844, 539, 1091, 1167, 1066, 177, 108, 1458, 477, 923, 766, 458, 1000, 676, 650, 310, 869, 705, 1129, 13, 269, 260, 75, 1428, 1060, 1195, 571, 562, 1214, 430, 659, 851, 854, 1344, 535, 1213, 777, 369, 187, 1242, 775, 1529, 802, 1225, 97, 779, 126, 640, 833, 174, 595, 158, 194, 1068, 946, 1503, 995, 592, 170, 812, 471, 1307, 486, 1462, 1365, 914, 373, 610, 417, 1327, 128, 1230, 133, 346, 1238, 1253, 910, 44, 1076, 950, 423, 90, 1430, 1324, 348, 1150, 1531, 193, 1412, 171, 1084, 1021, 1032, 1522, 1299, 405, 1471, 958, 1122, 580, 509, 446, 408, 715], 'conv3': [5017, 6047, 1653, 3916, 170, 5041, 2613, 4323, 6098, 2691, 5433, 3067, 4958, 4900, 3563, 1730, 62, 2026, 3435, 2075, 2778, 5255, 4087, 4574, 4983, 634, 5795, 1366, 948, 4473, 4359, 3906, 547, 750, 1229, 5651, 4738, 3197, 5004, 785, 514, 1599, 4502, 2051, 5968, 3718, 1680, 5558, 3938, 353, 5984, 5349, 4768, 815, 5956, 1310, 3516, 4415, 2483, 5292, 2021, 4333, 5515, 4090, 3988, 1149, 2470, 1856, 2392, 549, 4350, 1022, 1921, 999, 4682, 3986, 2462, 921, 3227, 265, 706, 1055, 5541, 4429, 680, 3972, 3828, 1635, 5175, 1587, 1685, 2966, 2696, 1318, 4262, 2037, 4163, 4746, 4699, 2299, 3481, 2398, 3740, 4029, 2413, 5314, 3181, 2804, 837, 3511, 5918, 1575, 3807, 1983, 1778, 63, 3486, 51, 3669, 4924, 1526, 817, 3179, 5498, 6, 1385, 1066, 534, 5733, 3210, 1086, 4357, 2176, 1331, 1670, 4408, 117, 2344, 1181, 3222, 1911, 2427, 2979, 1304, 5752, 2453, 5283, 5188, 745, 5591, 4968, 1344, 3603, 261, 1585, 5, 2786, 5406, 8, 300, 2300, 4905, 2714, 3431, 2661, 5571, 4360, 4556, 3750, 771, 211, 3888, 4486, 813, 2049, 4025, 6114, 1294, 845, 3414, 5312, 5001, 2404, 2769, 4094, 4624, 5065, 4496, 455, 3090, 6070, 3101, 1760, 1037, 1913, 1831, 1036, 1117, 3449, 5919, 418, 5455, 5299, 485, 905, 4733, 5091, 3959, 4739, 183, 2078, 1009, 1043, 4134, 2085, 5717, 3543, 3430, 552, 3830, 1512, 1822, 2914, 5705, 822, 3983, 5860, 688, 4687, 6134, 2201, 5222, 1342, 784, 1112, 1198, 2938, 2041, 4862, 3321, 5579, 5931, 526, 2608, 1872, 1163, 2780, 4218, 4256, 872, 122, 4396, 551, 1437, 5473, 4513, 2584, 1883, 3495, 2136, 5477, 5977, 3159, 1703, 1118, 4538, 1346, 2698, 4397, 3941, 2047, 5531, 2488, 3581, 132, 339, 3502, 2850, 1792, 2418, 5346, 2354, 6054, 3001, 4227, 1790, 4598, 2056, 4100, 3725, 909, 3909, 4451, 4072, 4660, 2635, 1102, 3656, 2581, 176, 1001, 2152, 2444, 3448, 685, 497, 2920, 652, 621, 4390, 3186, 4221, 1097, 2069, 4177, 1620, 4118, 4377, 178, 1615, 1950, 3010, 2092, 2793, 4694, 2103, 5834, 6004, 3905, 632, 844, 4618, 4131, 4063, 1403, 583, 1351, 5469, 1753, 1514, 4602, 4173, 2835, 2750, 3215, 2274, 5457, 3507, 5482, 5981, 3062, 5135, 2847, 2010, 4223, 2662, 3824, 2447, 5847, 165, 5276, 2210, 5493, 129, 198, 5780, 4853, 1544, 2511, 1806, 5910, 4382, 956, 2426, 1443, 1622, 4906, 5146, 4159, 5214, 668, 4876, 1284, 194, 227, 2980, 1187, 6090, 49, 4036, 436, 3470, 6058, 3488, 1594, 5322, 1283, 1864, 4539, 860, 3420, 3138, 818, 1861, 257, 2700, 2139, 2695, 3351, 2379, 4544, 1176, 2342, 2128, 1343, 2514, 4394, 686, 5212, 3895, 3540, 4110, 109, 3312, 893, 5542, 6053, 3473, 4301, 5196, 4937, 1337, 5407, 4219, 2972, 208, 2812, 4266, 4646, 5027, 4938, 3719, 1075, 295, 5436, 78, 5243, 4804, 638, 1564, 1894, 4980, 5929, 5853, 3640, 4964, 5821, 5297, 5201, 398, 5909, 59, 2542, 495, 6119, 4097, 2292, 1560, 2666, 2627, 1873, 3912, 4636, 3436, 26, 1453, 1394, 4947, 5216, 4089, 4166, 1415, 4954, 2083, 1834, 4212, 3809, 4635, 4102, 3866, 2533, 5838, 5412, 221, 6116, 2367, 3944, 1988, 1132, 1380, 1454, 4264, 1499, 5786, 1316, 4306, 3584, 171, 2675, 2667, 5205, 2530, 5108, 5045, 3793, 2703, 1684, 4421, 1191, 294, 2008, 41, 3760, 4280, 2923, 13, 4651, 1309, 1185, 1435, 6122, 2945, 2755, 4114, 1371, 2808, 4319, 2679, 5520, 4897, 6142, 2089, 2563, 5630, 292, 3151, 2668, 4553, 2927, 2156, 5076, 3193, 313, 3670, 3696, 709, 252, 2779, 1108, 1312, 2236, 2098, 2058, 772, 4257, 3330, 5791, 2957, 3569, 65, 5056, 3602, 5739, 1641, 5318, 722, 5445, 3755, 24, 5396, 675, 760, 3873, 673, 926, 3017, 1168, 5354, 4546, 2314, 5607, 259, 3658, 1608, 2007, 4289, 4777, 888, 3434, 4736, 4445, 4107, 1164, 2173, 4431, 576, 3802, 4120, 1879, 3191, 4220, 4076, 4961, 5839, 3150, 5206, 3377, 3096, 255, 5087, 2048, 2393, 1956, 3039, 1162, 4603, 5161, 5268, 491, 2580, 5423, 795, 2597, 463, 5043, 4236, 4291, 1230, 1882, 1978, 1511, 1866, 4728, 5942, 1362, 4879, 2348, 2845, 796, 5582, 4919, 2834, 4402, 6109, 1634, 3735, 6008, 3914, 1960, 5172, 625, 4270, 3556, 4541, 2976, 6089, 2860, 2962, 2082, 3479, 5596, 4688, 5321, 4762, 2711, 3775, 3553, 2251, 4885, 2887, 3657, 1096, 5401, 4640, 5921, 5204, 2033, 2206, 2802, 4412, 2928, 441, 1578, 951, 287, 1357, 3761, 4176, 2101, 14, 4299, 2310, 5936, 2073, 2538, 2077, 4628, 4104, 870, 2318, 4447, 2227, 5932, 2736, 5386, 4162, 2918, 5164, 2859, 3804, 1161, 6102, 4881, 246, 5503, 707, 4840, 2917, 1941, 1836, 2607, 763, 5454, 2759, 1115, 530, 1936, 426, 1755, 980, 620, 846, 1949, 1537, 1846, 865, 223, 3686, 4785, 5697, 1021, 4028, 1887, 2100, 2268, 4449, 3133, 3538, 5626, 826, 5008, 1569, 1457, 780, 1194, 2889, 5103, 229, 2460, 2798, 3918, 5680, 89, 493, 5818, 4658, 3594, 2445, 1151, 1535, 1273, 1169, 1422, 1264, 4239, 1543, 834, 5015, 2203, 3425, 4365, 6125, 3170, 1368, 4752, 1820, 3834, 2680, 2112, 4778, 3965, 5116, 1028, 3027, 2715, 4300, 2443, 1574, 2519, 3947, 1649, 144, 1088, 5854, 2792, 5885, 2382, 3163, 1912, 765, 3023, 4013, 2022, 527, 5980, 4482, 3636, 3283, 2067, 2106, 2071, 3242, 1999, 2449, 5893, 1376, 3025, 966, 1204, 5245, 1470, 4185, 4410, 5693, 4614, 3943, 4034, 3384, 4460, 5199, 3759, 1116, 3902, 2861, 3836, 2585, 179, 1697, 5994, 1702, 2817, 4519, 4330, 4480, 4882, 555, 5081, 1279, 3429, 545, 4154, 590, 4135, 5887, 970, 363, 3032, 2419, 4956, 5642, 1345, 1845, 1438, 4775, 6096, 878, 1600, 425, 1335, 1914, 2016, 5523, 3277, 4051, 6021, 1980, 5862, 4566, 450, 3350, 175, 1665, 1969, 524, 1095, 4363, 3118, 3097, 4669, 5460, 3126, 4242, 5702, 5552, 52, 3733, 4142, 2547, 1839, 271, 674, 511, 4248]}
SAVED model.heads_thresh 0.8595000076293946
RESULT:  {'train_loss': 0.12020615488290787, 'train_acc': 97.23520278930664, 'test_loss': 0.14057114720344543, 'test_acc': 81.9000015258789, 'convergence': 17.5535945892334, 'R1': 49, 'dataset_sup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 96}, 'dataset_unsup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 96}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 10, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}}
IN R1:  {'count': 1, 'R0': {'train_loss': 0.03978398069739342, 'train_acc': 98.27600121498108, 'test_loss': 3.0680172443389893, 'test_acc': 66.5625, 'convergence': 23.72207260131836, 'R1': 1, 'dataset_sup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 5000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 4, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': True, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 100, 'print_freq': 10, 'validation': False, 'continual_learning': False}, 'dataset_unsup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 100000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 32, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'unlabeled', 'nb_epoch': 1, 'print_freq': 25, 'validation': False, 'continual_learning': False}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 32, 'nb_epoch': 1, 'print_freq': 25, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 100, 'print_freq': 10, 'lr': 0.001}}}, 'model_config': {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.11048543456039805, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 24576, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}, 'R1': {'train_loss': 0.12020615488290787, 'train_acc': 97.23520278930664, 'test_loss': 0.14057114720344543, 'test_acc': 81.9000015258789, 'convergence': 17.5535945892334, 'R1': 49, 'dataset_sup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 96}, 'dataset_unsup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 96}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 10, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}}}
SEED:  0
block 0, size : 96 48 48
96
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.11048543456039805, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 24576, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 2.886751345948129
block 1, size : 384 24 24
384
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.11048543456039805, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 24576, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.8505172717997146
block 2, size : 1536 12 12
1536
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.11048543456039805, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 24576, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.4252586358998573
block 3, size : 6144 6 6
6144
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.11048543456039805, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 24576, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.21262931794992865
NOWWWW
6 6
10
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.11048543456039805, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 221184, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
blocks['b%s' % 4] 221184 221184
range = 0.036828478186799345
train_layer_order: consecutive {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 32, 'nb_epoch': 1, 'print_freq': 25, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 100, 'print_freq': 10, 'lr': 0.001}}
The device used will be: 
True
cuda:0
PARAMSSSS IN LOAD:  {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.036828478186799345, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 221184, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
avg_deltas:  {'blocks.0.layer.weight': tensor([-0.3549,  0.1212,  0.0095,  0.1285, -0.5902, -0.7595,  0.2930, -0.1311,
        -0.6668,  0.2679,  0.4133,  0.4384,  0.0050,  0.0863,  0.9283, -0.2386,
        -0.4347,  0.0211, -0.5482, -0.5881,  0.3635,  0.7974, -0.0708,  0.0277,
         0.3060,  0.6573, -0.0664, -0.1364,  0.2152, -0.0694,  0.9545,  0.5186,
        -0.4286, -0.7605,  0.7025,  0.9891, -0.6683, -0.0308,  0.8484,  0.0912,
         0.4675,  0.1660,  0.9470, -0.3474,  0.5685, -0.6660, -0.3353,  0.5072,
         0.0084,  0.3467, -0.5717,  0.7976,  0.3361,  0.8629, -0.0436,  0.9182,
        -0.4613,  0.7530, -0.3987,  1.0000, -0.1714, -0.0323, -0.3805,  0.1154,
        -0.7352, -0.3378, -0.5991, -0.0573,  0.9557,  0.9710,  0.9160,  0.3482,
        -0.0344, -0.5976, -0.0085, -0.6056,  0.1304,  0.9179, -0.7200,  0.4949,
        -0.3991,  0.7442, -0.6611,  0.6799,  0.0952, -0.2164, -0.6674, -0.3528,
        -0.0951, -0.7665,  0.0833, -0.1890, -0.2624,  0.9089, -0.3644,  0.4514],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.4675e-01,  4.9634e-01,  4.9947e-01,  3.5167e-01,  6.8275e-01,
        -1.5499e-01,  3.8235e-01, -8.2602e-02,  9.2969e-02,  3.4503e-01,
        -6.9911e-02,  6.8859e-01,  4.4754e-01,  4.0567e-01,  5.8537e-01,
         4.8758e-01,  2.1827e-01,  5.5889e-01, -3.6301e-01, -2.1951e-01,
         2.5134e-01,  3.0956e-01,  7.9425e-01,  6.6511e-01,  7.1393e-01,
         8.6881e-01,  1.8570e-01, -8.7449e-02,  6.5183e-02,  5.4472e-01,
         2.1367e-01, -3.4635e-01, -3.3732e-01,  1.6719e-01,  1.6866e-01,
         1.5956e-01,  3.7950e-01,  3.3845e-01,  5.1705e-02,  3.7345e-01,
         4.0139e-01,  3.6994e-01,  2.8768e-01,  3.8171e-01,  7.6911e-01,
         3.2752e-01, -3.0061e-01,  3.1377e-01, -9.3224e-02, -1.2043e-01,
         4.4630e-01,  4.6442e-01, -6.7174e-02, -1.0307e-01, -8.5593e-02,
         4.1887e-01,  4.8836e-01, -2.3651e-01,  3.4604e-01,  9.2028e-02,
         1.6476e-01, -6.3855e-01,  7.0815e-01, -4.7516e-01, -1.3248e-01,
         2.0273e-01, -9.3000e-02,  8.0644e-01,  3.1913e-01, -1.4491e-02,
         1.5819e-01,  5.9817e-02,  1.0919e-01,  5.0951e-01, -3.8428e-02,
         1.5758e-01,  5.2751e-01, -4.7671e-01, -4.2823e-01,  4.0300e-01,
         1.2542e-01,  3.8350e-01,  3.7314e-01,  2.1977e-01,  6.2835e-01,
         1.7641e-01,  4.4517e-01,  6.0281e-02,  1.0899e-01,  8.3355e-01,
         2.6046e-01,  2.2819e-01,  2.6363e-01,  3.1669e-01,  7.0682e-01,
        -1.5943e-01,  5.2125e-01,  1.8858e-01, -3.3286e-01,  2.0695e-01,
         6.1562e-01,  3.3932e-01,  2.5271e-01,  1.3981e-01,  1.2687e-01,
         3.1925e-01,  1.8174e-01, -9.4531e-02,  3.6838e-02,  3.7488e-01,
         1.2717e-01,  4.7130e-01, -1.5721e-01, -7.7476e-02,  8.1014e-01,
        -1.5593e-01, -1.0054e-01,  4.5635e-01,  6.1519e-01,  1.4677e-01,
        -2.2707e-01, -5.5761e-03,  2.6145e-01,  5.9044e-01,  2.2731e-01,
        -2.6443e-02, -2.3644e-03, -1.0726e-01,  7.7391e-01,  3.4043e-01,
         2.8069e-01,  4.8368e-01,  4.9516e-01,  2.3034e-01,  6.8924e-01,
         3.1027e-01,  3.4570e-01,  9.7788e-01,  3.3127e-01,  3.5599e-01,
         5.0713e-01,  7.2506e-01,  6.5209e-01, -4.4852e-02,  4.7526e-01,
         7.1132e-01,  8.4672e-01, -7.5696e-03,  4.1974e-01,  2.1462e-01,
         1.7083e-01, -1.3488e-01,  4.1226e-01,  2.3258e-01,  2.5389e-01,
        -1.5678e-01, -5.3553e-01, -4.7307e-01,  2.7626e-01, -2.5093e-01,
         4.0266e-02,  2.4575e-01,  1.8999e-01, -6.1258e-03, -6.5438e-03,
         4.2919e-01, -1.2408e-01,  1.9037e-01,  5.1006e-01, -6.9298e-02,
         1.7477e-01,  3.5078e-01,  7.3873e-01,  5.5692e-01,  4.3353e-01,
        -2.2003e-01,  3.3680e-01,  6.3033e-01,  2.5604e-01,  3.2412e-01,
         6.3429e-02,  4.2685e-01,  3.0266e-01,  4.5454e-01,  3.0221e-02,
         4.9645e-01,  4.2592e-01,  2.2569e-01,  3.2492e-01, -1.9558e-01,
         6.3491e-01,  1.4616e-01,  2.0872e-01,  1.7695e-01,  3.4186e-01,
         7.6387e-01,  2.4154e-01,  3.4131e-01,  2.5706e-01, -1.7839e-01,
         4.7458e-01, -6.1654e-02, -4.2599e-02,  8.0520e-01,  1.1602e-01,
         4.0244e-01,  5.3733e-01,  1.0871e-01,  2.4645e-02,  7.1771e-01,
        -1.1157e-01,  2.2285e-01,  3.2628e-01,  1.0782e-01,  1.5037e-01,
         4.8782e-01,  4.4199e-01,  8.0079e-01, -1.9772e-01,  4.1425e-01,
         3.3853e-01,  7.4381e-01, -1.5043e-01,  5.4073e-02,  5.8050e-01,
        -2.5929e-02,  8.0517e-01, -3.4948e-01,  2.4886e-01,  5.5591e-01,
         1.8956e-01,  1.2973e-01,  7.4482e-02,  1.6208e-01,  5.5959e-01,
        -8.3063e-02,  7.0622e-01,  4.5518e-01,  4.7128e-01,  7.8142e-01,
        -2.2690e-02,  8.9799e-02,  1.5444e-01, -9.2526e-04,  1.1149e-01,
        -7.5147e-03,  5.6785e-01,  8.0659e-02,  5.8024e-01,  8.5778e-02,
         6.8099e-01,  2.7963e-01,  9.8910e-02, -1.4123e-01, -2.2274e-01,
         3.2634e-01,  4.8895e-01,  2.9214e-01, -1.3606e-01,  2.9738e-01,
         1.8581e-01,  1.2835e-01,  8.0524e-01,  5.1709e-01,  5.6002e-02,
         3.1847e-01, -5.6267e-01,  5.3282e-01,  4.8991e-01,  1.9676e-01,
        -2.5985e-02,  6.1749e-01,  3.1766e-01,  1.6751e-01,  4.4369e-01,
         1.5407e-01,  5.1959e-01,  4.5052e-01,  3.0881e-01,  2.6712e-02,
         1.2838e-01,  7.2508e-01,  4.6165e-01,  2.0328e-01,  2.4748e-01,
         4.8103e-01,  1.8086e-01, -7.3423e-02,  2.9468e-01, -8.2690e-02,
         7.6233e-02, -2.6612e-01,  3.9660e-01,  2.3032e-01,  4.0978e-02,
        -2.4679e-01, -1.2860e-01,  1.0342e-01,  5.5879e-01,  3.9662e-01,
        -1.4361e-01, -1.9822e-01,  5.5379e-01, -3.5044e-02,  6.5483e-01,
         4.9469e-01,  4.6768e-01,  9.9884e-02,  4.0240e-01,  8.2842e-02,
         8.1476e-02,  1.0099e-01,  6.6033e-01,  1.2213e-01,  6.6229e-01,
         6.9984e-02,  4.2777e-01,  7.7067e-01,  1.1222e-02,  1.7325e-01,
         1.4918e-01,  4.4225e-01,  7.1020e-01, -6.4827e-02, -1.0691e-01,
         4.2094e-01,  9.3253e-01,  7.4832e-01,  2.0421e-01,  8.0857e-01,
        -6.9927e-01,  6.9038e-01, -1.4160e-01,  2.9397e-01,  1.8431e-01,
         3.3021e-01,  3.0792e-01,  2.0105e-01,  5.4197e-01,  3.6271e-01,
         3.5776e-01,  2.2193e-01,  6.9557e-02,  8.7006e-01, -2.0600e-01,
        -3.4507e-03,  1.8695e-01,  1.0000e+00,  2.9092e-01, -2.6603e-01,
         2.7547e-01,  7.8514e-02,  1.6481e-01,  4.8246e-01,  5.4929e-01,
        -1.7485e-01,  3.9021e-02,  4.1384e-01,  5.1890e-01, -4.6565e-02,
         3.2173e-01,  1.8334e-01,  1.8262e-01,  1.4842e-01,  6.1002e-01,
         2.9751e-02,  1.2533e-01,  5.3216e-01,  2.0576e-01, -4.0360e-02,
         3.6691e-01,  8.1207e-01,  3.9485e-01,  1.3495e-01,  1.4429e-04,
         3.4063e-01,  3.1047e-02,  3.8775e-01,  2.6841e-01,  5.2225e-01,
         5.2153e-01,  5.3369e-01,  1.9615e-01,  3.0179e-01], device='cuda:0'), 'blocks.2.layer.weight': tensor([-0.0010,  0.0317,  0.4703,  ...,  0.2049, -0.0204,  0.3651],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([0.2656, 0.5401, 0.7364,  ..., 0.3438, 0.5377, 0.2040], device='cuda:0')}
topk_layer:  None
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-0.3549,  0.1212,  0.0095,  0.1285, -0.5902, -0.7595,  0.2930, -0.1311,
        -0.6668,  0.2679,  0.4133,  0.4384,  0.0050,  0.0863,  0.9283, -0.2386,
        -0.4347,  0.0211, -0.5482, -0.5881,  0.3635,  0.7974, -0.0708,  0.0277,
         0.3060,  0.6573, -0.0664, -0.1364,  0.2152, -0.0694,  0.9545,  0.5186,
        -0.4286, -0.7605,  0.7025,  0.9891, -0.6683, -0.0308,  0.8484,  0.0912,
         0.4675,  0.1660,  0.9470, -0.3474,  0.5685, -0.6660, -0.3353,  0.5072,
         0.0084,  0.3467, -0.5717,  0.7976,  0.3361,  0.8629, -0.0436,  0.9182,
        -0.4613,  0.7530, -0.3987,  1.0000, -0.1714, -0.0323, -0.3805,  0.1154,
        -0.7352, -0.3378, -0.5991, -0.0573,  0.9557,  0.9710,  0.9160,  0.3482,
        -0.0344, -0.5976, -0.0085, -0.6056,  0.1304,  0.9179, -0.7200,  0.4949,
        -0.3991,  0.7442, -0.6611,  0.6799,  0.0952, -0.2164, -0.6674, -0.3528,
        -0.0951, -0.7665,  0.0833, -0.1890, -0.2624,  0.9089, -0.3644,  0.4514],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.4675e-01,  4.9634e-01,  4.9947e-01,  3.5167e-01,  6.8275e-01,
        -1.5499e-01,  3.8235e-01, -8.2602e-02,  9.2969e-02,  3.4503e-01,
        -6.9911e-02,  6.8859e-01,  4.4754e-01,  4.0567e-01,  5.8537e-01,
         4.8758e-01,  2.1827e-01,  5.5889e-01, -3.6301e-01, -2.1951e-01,
         2.5134e-01,  3.0956e-01,  7.9425e-01,  6.6511e-01,  7.1393e-01,
         8.6881e-01,  1.8570e-01, -8.7449e-02,  6.5183e-02,  5.4472e-01,
         2.1367e-01, -3.4635e-01, -3.3732e-01,  1.6719e-01,  1.6866e-01,
         1.5956e-01,  3.7950e-01,  3.3845e-01,  5.1705e-02,  3.7345e-01,
         4.0139e-01,  3.6994e-01,  2.8768e-01,  3.8171e-01,  7.6911e-01,
         3.2752e-01, -3.0061e-01,  3.1377e-01, -9.3224e-02, -1.2043e-01,
         4.4630e-01,  4.6442e-01, -6.7174e-02, -1.0307e-01, -8.5593e-02,
         4.1887e-01,  4.8836e-01, -2.3651e-01,  3.4604e-01,  9.2028e-02,
         1.6476e-01, -6.3855e-01,  7.0815e-01, -4.7516e-01, -1.3248e-01,
         2.0273e-01, -9.3000e-02,  8.0644e-01,  3.1913e-01, -1.4491e-02,
         1.5819e-01,  5.9817e-02,  1.0919e-01,  5.0951e-01, -3.8428e-02,
         1.5758e-01,  5.2751e-01, -4.7671e-01, -4.2823e-01,  4.0300e-01,
         1.2542e-01,  3.8350e-01,  3.7314e-01,  2.1977e-01,  6.2835e-01,
         1.7641e-01,  4.4517e-01,  6.0281e-02,  1.0899e-01,  8.3355e-01,
         2.6046e-01,  2.2819e-01,  2.6363e-01,  3.1669e-01,  7.0682e-01,
        -1.5943e-01,  5.2125e-01,  1.8858e-01, -3.3286e-01,  2.0695e-01,
         6.1562e-01,  3.3932e-01,  2.5271e-01,  1.3981e-01,  1.2687e-01,
         3.1925e-01,  1.8174e-01, -9.4531e-02,  3.6838e-02,  3.7488e-01,
         1.2717e-01,  4.7130e-01, -1.5721e-01, -7.7476e-02,  8.1014e-01,
        -1.5593e-01, -1.0054e-01,  4.5635e-01,  6.1519e-01,  1.4677e-01,
        -2.2707e-01, -5.5761e-03,  2.6145e-01,  5.9044e-01,  2.2731e-01,
        -2.6443e-02, -2.3644e-03, -1.0726e-01,  7.7391e-01,  3.4043e-01,
         2.8069e-01,  4.8368e-01,  4.9516e-01,  2.3034e-01,  6.8924e-01,
         3.1027e-01,  3.4570e-01,  9.7788e-01,  3.3127e-01,  3.5599e-01,
         5.0713e-01,  7.2506e-01,  6.5209e-01, -4.4852e-02,  4.7526e-01,
         7.1132e-01,  8.4672e-01, -7.5696e-03,  4.1974e-01,  2.1462e-01,
         1.7083e-01, -1.3488e-01,  4.1226e-01,  2.3258e-01,  2.5389e-01,
        -1.5678e-01, -5.3553e-01, -4.7307e-01,  2.7626e-01, -2.5093e-01,
         4.0266e-02,  2.4575e-01,  1.8999e-01, -6.1258e-03, -6.5438e-03,
         4.2919e-01, -1.2408e-01,  1.9037e-01,  5.1006e-01, -6.9298e-02,
         1.7477e-01,  3.5078e-01,  7.3873e-01,  5.5692e-01,  4.3353e-01,
        -2.2003e-01,  3.3680e-01,  6.3033e-01,  2.5604e-01,  3.2412e-01,
         6.3429e-02,  4.2685e-01,  3.0266e-01,  4.5454e-01,  3.0221e-02,
         4.9645e-01,  4.2592e-01,  2.2569e-01,  3.2492e-01, -1.9558e-01,
         6.3491e-01,  1.4616e-01,  2.0872e-01,  1.7695e-01,  3.4186e-01,
         7.6387e-01,  2.4154e-01,  3.4131e-01,  2.5706e-01, -1.7839e-01,
         4.7458e-01, -6.1654e-02, -4.2599e-02,  8.0520e-01,  1.1602e-01,
         4.0244e-01,  5.3733e-01,  1.0871e-01,  2.4645e-02,  7.1771e-01,
        -1.1157e-01,  2.2285e-01,  3.2628e-01,  1.0782e-01,  1.5037e-01,
         4.8782e-01,  4.4199e-01,  8.0079e-01, -1.9772e-01,  4.1425e-01,
         3.3853e-01,  7.4381e-01, -1.5043e-01,  5.4073e-02,  5.8050e-01,
        -2.5929e-02,  8.0517e-01, -3.4948e-01,  2.4886e-01,  5.5591e-01,
         1.8956e-01,  1.2973e-01,  7.4482e-02,  1.6208e-01,  5.5959e-01,
        -8.3063e-02,  7.0622e-01,  4.5518e-01,  4.7128e-01,  7.8142e-01,
        -2.2690e-02,  8.9799e-02,  1.5444e-01, -9.2526e-04,  1.1149e-01,
        -7.5147e-03,  5.6785e-01,  8.0659e-02,  5.8024e-01,  8.5778e-02,
         6.8099e-01,  2.7963e-01,  9.8910e-02, -1.4123e-01, -2.2274e-01,
         3.2634e-01,  4.8895e-01,  2.9214e-01, -1.3606e-01,  2.9738e-01,
         1.8581e-01,  1.2835e-01,  8.0524e-01,  5.1709e-01,  5.6002e-02,
         3.1847e-01, -5.6267e-01,  5.3282e-01,  4.8991e-01,  1.9676e-01,
        -2.5985e-02,  6.1749e-01,  3.1766e-01,  1.6751e-01,  4.4369e-01,
         1.5407e-01,  5.1959e-01,  4.5052e-01,  3.0881e-01,  2.6712e-02,
         1.2838e-01,  7.2508e-01,  4.6165e-01,  2.0328e-01,  2.4748e-01,
         4.8103e-01,  1.8086e-01, -7.3423e-02,  2.9468e-01, -8.2690e-02,
         7.6233e-02, -2.6612e-01,  3.9660e-01,  2.3032e-01,  4.0978e-02,
        -2.4679e-01, -1.2860e-01,  1.0342e-01,  5.5879e-01,  3.9662e-01,
        -1.4361e-01, -1.9822e-01,  5.5379e-01, -3.5044e-02,  6.5483e-01,
         4.9469e-01,  4.6768e-01,  9.9884e-02,  4.0240e-01,  8.2842e-02,
         8.1476e-02,  1.0099e-01,  6.6033e-01,  1.2213e-01,  6.6229e-01,
         6.9984e-02,  4.2777e-01,  7.7067e-01,  1.1222e-02,  1.7325e-01,
         1.4918e-01,  4.4225e-01,  7.1020e-01, -6.4827e-02, -1.0691e-01,
         4.2094e-01,  9.3253e-01,  7.4832e-01,  2.0421e-01,  8.0857e-01,
        -6.9927e-01,  6.9038e-01, -1.4160e-01,  2.9397e-01,  1.8431e-01,
         3.3021e-01,  3.0792e-01,  2.0105e-01,  5.4197e-01,  3.6271e-01,
         3.5776e-01,  2.2193e-01,  6.9557e-02,  8.7006e-01, -2.0600e-01,
        -3.4507e-03,  1.8695e-01,  1.0000e+00,  2.9092e-01, -2.6603e-01,
         2.7547e-01,  7.8514e-02,  1.6481e-01,  4.8246e-01,  5.4929e-01,
        -1.7485e-01,  3.9021e-02,  4.1384e-01,  5.1890e-01, -4.6565e-02,
         3.2173e-01,  1.8334e-01,  1.8262e-01,  1.4842e-01,  6.1002e-01,
         2.9751e-02,  1.2533e-01,  5.3216e-01,  2.0576e-01, -4.0360e-02,
         3.6691e-01,  8.1207e-01,  3.9485e-01,  1.3495e-01,  1.4429e-04,
         3.4063e-01,  3.1047e-02,  3.8775e-01,  2.6841e-01,  5.2225e-01,
         5.2153e-01,  5.3369e-01,  1.9615e-01,  3.0179e-01], device='cuda:0'), 'blocks.2.layer.weight': tensor([-0.0010,  0.0317,  0.4703,  ...,  0.2049, -0.0204,  0.3651],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([0.2656, 0.5401, 0.7364,  ..., 0.3438, 0.5377, 0.2040], device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.0.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([-0.3549,  0.1212,  0.0095,  0.1285, -0.5902, -0.7595,  0.2930, -0.1311,
        -0.6668,  0.2679,  0.4133,  0.4384,  0.0050,  0.0863,  0.9283, -0.2386,
        -0.4347,  0.0211, -0.5482, -0.5881,  0.3635,  0.7974, -0.0708,  0.0277,
         0.3060,  0.6573, -0.0664, -0.1364,  0.2152, -0.0694,  0.9545,  0.5186,
        -0.4286, -0.7605,  0.7025,  0.9891, -0.6683, -0.0308,  0.8484,  0.0912,
         0.4675,  0.1660,  0.9470, -0.3474,  0.5685, -0.6660, -0.3353,  0.5072,
         0.0084,  0.3467, -0.5717,  0.7976,  0.3361,  0.8629, -0.0436,  0.9182,
        -0.4613,  0.7530, -0.3987,  1.0000, -0.1714, -0.0323, -0.3805,  0.1154,
        -0.7352, -0.3378, -0.5991, -0.0573,  0.9557,  0.9710,  0.9160,  0.3482,
        -0.0344, -0.5976, -0.0085, -0.6056,  0.1304,  0.9179, -0.7200,  0.4949,
        -0.3991,  0.7442, -0.6611,  0.6799,  0.0952, -0.2164, -0.6674, -0.3528,
        -0.0951, -0.7665,  0.0833, -0.1890, -0.2624,  0.9089, -0.3644,  0.4514],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-0.3549,  0.1212,  0.0095,  0.1285, -0.5902, -0.7595,  0.2930, -0.1311,
        -0.6668,  0.2679,  0.4133,  0.4384,  0.0050,  0.0863,  0.9283, -0.2386,
        -0.4347,  0.0211, -0.5482, -0.5881,  0.3635,  0.7974, -0.0708,  0.0277,
         0.3060,  0.6573, -0.0664, -0.1364,  0.2152, -0.0694,  0.9545,  0.5186,
        -0.4286, -0.7605,  0.7025,  0.9891, -0.6683, -0.0308,  0.8484,  0.0912,
         0.4675,  0.1660,  0.9470, -0.3474,  0.5685, -0.6660, -0.3353,  0.5072,
         0.0084,  0.3467, -0.5717,  0.7976,  0.3361,  0.8629, -0.0436,  0.9182,
        -0.4613,  0.7530, -0.3987,  1.0000, -0.1714, -0.0323, -0.3805,  0.1154,
        -0.7352, -0.3378, -0.5991, -0.0573,  0.9557,  0.9710,  0.9160,  0.3482,
        -0.0344, -0.5976, -0.0085, -0.6056,  0.1304,  0.9179, -0.7200,  0.4949,
        -0.3991,  0.7442, -0.6611,  0.6799,  0.0952, -0.2164, -0.6674, -0.3528,
        -0.0951, -0.7665,  0.0833, -0.1890, -0.2624,  0.9089, -0.3644,  0.4514],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.4675e-01,  4.9634e-01,  4.9947e-01,  3.5167e-01,  6.8275e-01,
        -1.5499e-01,  3.8235e-01, -8.2602e-02,  9.2969e-02,  3.4503e-01,
        -6.9911e-02,  6.8859e-01,  4.4754e-01,  4.0567e-01,  5.8537e-01,
         4.8758e-01,  2.1827e-01,  5.5889e-01, -3.6301e-01, -2.1951e-01,
         2.5134e-01,  3.0956e-01,  7.9425e-01,  6.6511e-01,  7.1393e-01,
         8.6881e-01,  1.8570e-01, -8.7449e-02,  6.5183e-02,  5.4472e-01,
         2.1367e-01, -3.4635e-01, -3.3732e-01,  1.6719e-01,  1.6866e-01,
         1.5956e-01,  3.7950e-01,  3.3845e-01,  5.1705e-02,  3.7345e-01,
         4.0139e-01,  3.6994e-01,  2.8768e-01,  3.8171e-01,  7.6911e-01,
         3.2752e-01, -3.0061e-01,  3.1377e-01, -9.3224e-02, -1.2043e-01,
         4.4630e-01,  4.6442e-01, -6.7174e-02, -1.0307e-01, -8.5593e-02,
         4.1887e-01,  4.8836e-01, -2.3651e-01,  3.4604e-01,  9.2028e-02,
         1.6476e-01, -6.3855e-01,  7.0815e-01, -4.7516e-01, -1.3248e-01,
         2.0273e-01, -9.3000e-02,  8.0644e-01,  3.1913e-01, -1.4491e-02,
         1.5819e-01,  5.9817e-02,  1.0919e-01,  5.0951e-01, -3.8428e-02,
         1.5758e-01,  5.2751e-01, -4.7671e-01, -4.2823e-01,  4.0300e-01,
         1.2542e-01,  3.8350e-01,  3.7314e-01,  2.1977e-01,  6.2835e-01,
         1.7641e-01,  4.4517e-01,  6.0281e-02,  1.0899e-01,  8.3355e-01,
         2.6046e-01,  2.2819e-01,  2.6363e-01,  3.1669e-01,  7.0682e-01,
        -1.5943e-01,  5.2125e-01,  1.8858e-01, -3.3286e-01,  2.0695e-01,
         6.1562e-01,  3.3932e-01,  2.5271e-01,  1.3981e-01,  1.2687e-01,
         3.1925e-01,  1.8174e-01, -9.4531e-02,  3.6838e-02,  3.7488e-01,
         1.2717e-01,  4.7130e-01, -1.5721e-01, -7.7476e-02,  8.1014e-01,
        -1.5593e-01, -1.0054e-01,  4.5635e-01,  6.1519e-01,  1.4677e-01,
        -2.2707e-01, -5.5761e-03,  2.6145e-01,  5.9044e-01,  2.2731e-01,
        -2.6443e-02, -2.3644e-03, -1.0726e-01,  7.7391e-01,  3.4043e-01,
         2.8069e-01,  4.8368e-01,  4.9516e-01,  2.3034e-01,  6.8924e-01,
         3.1027e-01,  3.4570e-01,  9.7788e-01,  3.3127e-01,  3.5599e-01,
         5.0713e-01,  7.2506e-01,  6.5209e-01, -4.4852e-02,  4.7526e-01,
         7.1132e-01,  8.4672e-01, -7.5696e-03,  4.1974e-01,  2.1462e-01,
         1.7083e-01, -1.3488e-01,  4.1226e-01,  2.3258e-01,  2.5389e-01,
        -1.5678e-01, -5.3553e-01, -4.7307e-01,  2.7626e-01, -2.5093e-01,
         4.0266e-02,  2.4575e-01,  1.8999e-01, -6.1258e-03, -6.5438e-03,
         4.2919e-01, -1.2408e-01,  1.9037e-01,  5.1006e-01, -6.9298e-02,
         1.7477e-01,  3.5078e-01,  7.3873e-01,  5.5692e-01,  4.3353e-01,
        -2.2003e-01,  3.3680e-01,  6.3033e-01,  2.5604e-01,  3.2412e-01,
         6.3429e-02,  4.2685e-01,  3.0266e-01,  4.5454e-01,  3.0221e-02,
         4.9645e-01,  4.2592e-01,  2.2569e-01,  3.2492e-01, -1.9558e-01,
         6.3491e-01,  1.4616e-01,  2.0872e-01,  1.7695e-01,  3.4186e-01,
         7.6387e-01,  2.4154e-01,  3.4131e-01,  2.5706e-01, -1.7839e-01,
         4.7458e-01, -6.1654e-02, -4.2599e-02,  8.0520e-01,  1.1602e-01,
         4.0244e-01,  5.3733e-01,  1.0871e-01,  2.4645e-02,  7.1771e-01,
        -1.1157e-01,  2.2285e-01,  3.2628e-01,  1.0782e-01,  1.5037e-01,
         4.8782e-01,  4.4199e-01,  8.0079e-01, -1.9772e-01,  4.1425e-01,
         3.3853e-01,  7.4381e-01, -1.5043e-01,  5.4073e-02,  5.8050e-01,
        -2.5929e-02,  8.0517e-01, -3.4948e-01,  2.4886e-01,  5.5591e-01,
         1.8956e-01,  1.2973e-01,  7.4482e-02,  1.6208e-01,  5.5959e-01,
        -8.3063e-02,  7.0622e-01,  4.5518e-01,  4.7128e-01,  7.8142e-01,
        -2.2690e-02,  8.9799e-02,  1.5444e-01, -9.2526e-04,  1.1149e-01,
        -7.5147e-03,  5.6785e-01,  8.0659e-02,  5.8024e-01,  8.5778e-02,
         6.8099e-01,  2.7963e-01,  9.8910e-02, -1.4123e-01, -2.2274e-01,
         3.2634e-01,  4.8895e-01,  2.9214e-01, -1.3606e-01,  2.9738e-01,
         1.8581e-01,  1.2835e-01,  8.0524e-01,  5.1709e-01,  5.6002e-02,
         3.1847e-01, -5.6267e-01,  5.3282e-01,  4.8991e-01,  1.9676e-01,
        -2.5985e-02,  6.1749e-01,  3.1766e-01,  1.6751e-01,  4.4369e-01,
         1.5407e-01,  5.1959e-01,  4.5052e-01,  3.0881e-01,  2.6712e-02,
         1.2838e-01,  7.2508e-01,  4.6165e-01,  2.0328e-01,  2.4748e-01,
         4.8103e-01,  1.8086e-01, -7.3423e-02,  2.9468e-01, -8.2690e-02,
         7.6233e-02, -2.6612e-01,  3.9660e-01,  2.3032e-01,  4.0978e-02,
        -2.4679e-01, -1.2860e-01,  1.0342e-01,  5.5879e-01,  3.9662e-01,
        -1.4361e-01, -1.9822e-01,  5.5379e-01, -3.5044e-02,  6.5483e-01,
         4.9469e-01,  4.6768e-01,  9.9884e-02,  4.0240e-01,  8.2842e-02,
         8.1476e-02,  1.0099e-01,  6.6033e-01,  1.2213e-01,  6.6229e-01,
         6.9984e-02,  4.2777e-01,  7.7067e-01,  1.1222e-02,  1.7325e-01,
         1.4918e-01,  4.4225e-01,  7.1020e-01, -6.4827e-02, -1.0691e-01,
         4.2094e-01,  9.3253e-01,  7.4832e-01,  2.0421e-01,  8.0857e-01,
        -6.9927e-01,  6.9038e-01, -1.4160e-01,  2.9397e-01,  1.8431e-01,
         3.3021e-01,  3.0792e-01,  2.0105e-01,  5.4197e-01,  3.6271e-01,
         3.5776e-01,  2.2193e-01,  6.9557e-02,  8.7006e-01, -2.0600e-01,
        -3.4507e-03,  1.8695e-01,  1.0000e+00,  2.9092e-01, -2.6603e-01,
         2.7547e-01,  7.8514e-02,  1.6481e-01,  4.8246e-01,  5.4929e-01,
        -1.7485e-01,  3.9021e-02,  4.1384e-01,  5.1890e-01, -4.6565e-02,
         3.2173e-01,  1.8334e-01,  1.8262e-01,  1.4842e-01,  6.1002e-01,
         2.9751e-02,  1.2533e-01,  5.3216e-01,  2.0576e-01, -4.0360e-02,
         3.6691e-01,  8.1207e-01,  3.9485e-01,  1.3495e-01,  1.4429e-04,
         3.4063e-01,  3.1047e-02,  3.8775e-01,  2.6841e-01,  5.2225e-01,
         5.2153e-01,  5.3369e-01,  1.9615e-01,  3.0179e-01], device='cuda:0'), 'blocks.2.layer.weight': tensor([-0.0010,  0.0317,  0.4703,  ...,  0.2049, -0.0204,  0.3651],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([0.2656, 0.5401, 0.7364,  ..., 0.3438, 0.5377, 0.2040], device='cuda:0')}
topk_layer inside model after retrieval:  [25, 16, 59, 53, 30, 42, 77, 68, 69, 93, 75, 51, 34, 38, 70]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-0.3549,  0.1212,  0.0095,  0.1285, -0.5902, -0.7595,  0.2930, -0.1311,
        -0.6668,  0.2679,  0.4133,  0.4384,  0.0050,  0.0863,  0.9283, -0.2386,
        -0.4347,  0.0211, -0.5482, -0.5881,  0.3635,  0.7974, -0.0708,  0.0277,
         0.3060,  0.6573, -0.0664, -0.1364,  0.2152, -0.0694,  0.9545,  0.5186,
        -0.4286, -0.7605,  0.7025,  0.9891, -0.6683, -0.0308,  0.8484,  0.0912,
         0.4675,  0.1660,  0.9470, -0.3474,  0.5685, -0.6660, -0.3353,  0.5072,
         0.0084,  0.3467, -0.5717,  0.7976,  0.3361,  0.8629, -0.0436,  0.9182,
        -0.4613,  0.7530, -0.3987,  1.0000, -0.1714, -0.0323, -0.3805,  0.1154,
        -0.7352, -0.3378, -0.5991, -0.0573,  0.9557,  0.9710,  0.9160,  0.3482,
        -0.0344, -0.5976, -0.0085, -0.6056,  0.1304,  0.9179, -0.7200,  0.4949,
        -0.3991,  0.7442, -0.6611,  0.6799,  0.0952, -0.2164, -0.6674, -0.3528,
        -0.0951, -0.7665,  0.0833, -0.1890, -0.2624,  0.9089, -0.3644,  0.4514],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.4675e-01,  4.9634e-01,  4.9947e-01,  3.5167e-01,  6.8275e-01,
        -1.5499e-01,  3.8235e-01, -8.2602e-02,  9.2969e-02,  3.4503e-01,
        -6.9911e-02,  6.8859e-01,  4.4754e-01,  4.0567e-01,  5.8537e-01,
         4.8758e-01,  2.1827e-01,  5.5889e-01, -3.6301e-01, -2.1951e-01,
         2.5134e-01,  3.0956e-01,  7.9425e-01,  6.6511e-01,  7.1393e-01,
         8.6881e-01,  1.8570e-01, -8.7449e-02,  6.5183e-02,  5.4472e-01,
         2.1367e-01, -3.4635e-01, -3.3732e-01,  1.6719e-01,  1.6866e-01,
         1.5956e-01,  3.7950e-01,  3.3845e-01,  5.1705e-02,  3.7345e-01,
         4.0139e-01,  3.6994e-01,  2.8768e-01,  3.8171e-01,  7.6911e-01,
         3.2752e-01, -3.0061e-01,  3.1377e-01, -9.3224e-02, -1.2043e-01,
         4.4630e-01,  4.6442e-01, -6.7174e-02, -1.0307e-01, -8.5593e-02,
         4.1887e-01,  4.8836e-01, -2.3651e-01,  3.4604e-01,  9.2028e-02,
         1.6476e-01, -6.3855e-01,  7.0815e-01, -4.7516e-01, -1.3248e-01,
         2.0273e-01, -9.3000e-02,  8.0644e-01,  3.1913e-01, -1.4491e-02,
         1.5819e-01,  5.9817e-02,  1.0919e-01,  5.0951e-01, -3.8428e-02,
         1.5758e-01,  5.2751e-01, -4.7671e-01, -4.2823e-01,  4.0300e-01,
         1.2542e-01,  3.8350e-01,  3.7314e-01,  2.1977e-01,  6.2835e-01,
         1.7641e-01,  4.4517e-01,  6.0281e-02,  1.0899e-01,  8.3355e-01,
         2.6046e-01,  2.2819e-01,  2.6363e-01,  3.1669e-01,  7.0682e-01,
        -1.5943e-01,  5.2125e-01,  1.8858e-01, -3.3286e-01,  2.0695e-01,
         6.1562e-01,  3.3932e-01,  2.5271e-01,  1.3981e-01,  1.2687e-01,
         3.1925e-01,  1.8174e-01, -9.4531e-02,  3.6838e-02,  3.7488e-01,
         1.2717e-01,  4.7130e-01, -1.5721e-01, -7.7476e-02,  8.1014e-01,
        -1.5593e-01, -1.0054e-01,  4.5635e-01,  6.1519e-01,  1.4677e-01,
        -2.2707e-01, -5.5761e-03,  2.6145e-01,  5.9044e-01,  2.2731e-01,
        -2.6443e-02, -2.3644e-03, -1.0726e-01,  7.7391e-01,  3.4043e-01,
         2.8069e-01,  4.8368e-01,  4.9516e-01,  2.3034e-01,  6.8924e-01,
         3.1027e-01,  3.4570e-01,  9.7788e-01,  3.3127e-01,  3.5599e-01,
         5.0713e-01,  7.2506e-01,  6.5209e-01, -4.4852e-02,  4.7526e-01,
         7.1132e-01,  8.4672e-01, -7.5696e-03,  4.1974e-01,  2.1462e-01,
         1.7083e-01, -1.3488e-01,  4.1226e-01,  2.3258e-01,  2.5389e-01,
        -1.5678e-01, -5.3553e-01, -4.7307e-01,  2.7626e-01, -2.5093e-01,
         4.0266e-02,  2.4575e-01,  1.8999e-01, -6.1258e-03, -6.5438e-03,
         4.2919e-01, -1.2408e-01,  1.9037e-01,  5.1006e-01, -6.9298e-02,
         1.7477e-01,  3.5078e-01,  7.3873e-01,  5.5692e-01,  4.3353e-01,
        -2.2003e-01,  3.3680e-01,  6.3033e-01,  2.5604e-01,  3.2412e-01,
         6.3429e-02,  4.2685e-01,  3.0266e-01,  4.5454e-01,  3.0221e-02,
         4.9645e-01,  4.2592e-01,  2.2569e-01,  3.2492e-01, -1.9558e-01,
         6.3491e-01,  1.4616e-01,  2.0872e-01,  1.7695e-01,  3.4186e-01,
         7.6387e-01,  2.4154e-01,  3.4131e-01,  2.5706e-01, -1.7839e-01,
         4.7458e-01, -6.1654e-02, -4.2599e-02,  8.0520e-01,  1.1602e-01,
         4.0244e-01,  5.3733e-01,  1.0871e-01,  2.4645e-02,  7.1771e-01,
        -1.1157e-01,  2.2285e-01,  3.2628e-01,  1.0782e-01,  1.5037e-01,
         4.8782e-01,  4.4199e-01,  8.0079e-01, -1.9772e-01,  4.1425e-01,
         3.3853e-01,  7.4381e-01, -1.5043e-01,  5.4073e-02,  5.8050e-01,
        -2.5929e-02,  8.0517e-01, -3.4948e-01,  2.4886e-01,  5.5591e-01,
         1.8956e-01,  1.2973e-01,  7.4482e-02,  1.6208e-01,  5.5959e-01,
        -8.3063e-02,  7.0622e-01,  4.5518e-01,  4.7128e-01,  7.8142e-01,
        -2.2690e-02,  8.9799e-02,  1.5444e-01, -9.2526e-04,  1.1149e-01,
        -7.5147e-03,  5.6785e-01,  8.0659e-02,  5.8024e-01,  8.5778e-02,
         6.8099e-01,  2.7963e-01,  9.8910e-02, -1.4123e-01, -2.2274e-01,
         3.2634e-01,  4.8895e-01,  2.9214e-01, -1.3606e-01,  2.9738e-01,
         1.8581e-01,  1.2835e-01,  8.0524e-01,  5.1709e-01,  5.6002e-02,
         3.1847e-01, -5.6267e-01,  5.3282e-01,  4.8991e-01,  1.9676e-01,
        -2.5985e-02,  6.1749e-01,  3.1766e-01,  1.6751e-01,  4.4369e-01,
         1.5407e-01,  5.1959e-01,  4.5052e-01,  3.0881e-01,  2.6712e-02,
         1.2838e-01,  7.2508e-01,  4.6165e-01,  2.0328e-01,  2.4748e-01,
         4.8103e-01,  1.8086e-01, -7.3423e-02,  2.9468e-01, -8.2690e-02,
         7.6233e-02, -2.6612e-01,  3.9660e-01,  2.3032e-01,  4.0978e-02,
        -2.4679e-01, -1.2860e-01,  1.0342e-01,  5.5879e-01,  3.9662e-01,
        -1.4361e-01, -1.9822e-01,  5.5379e-01, -3.5044e-02,  6.5483e-01,
         4.9469e-01,  4.6768e-01,  9.9884e-02,  4.0240e-01,  8.2842e-02,
         8.1476e-02,  1.0099e-01,  6.6033e-01,  1.2213e-01,  6.6229e-01,
         6.9984e-02,  4.2777e-01,  7.7067e-01,  1.1222e-02,  1.7325e-01,
         1.4918e-01,  4.4225e-01,  7.1020e-01, -6.4827e-02, -1.0691e-01,
         4.2094e-01,  9.3253e-01,  7.4832e-01,  2.0421e-01,  8.0857e-01,
        -6.9927e-01,  6.9038e-01, -1.4160e-01,  2.9397e-01,  1.8431e-01,
         3.3021e-01,  3.0792e-01,  2.0105e-01,  5.4197e-01,  3.6271e-01,
         3.5776e-01,  2.2193e-01,  6.9557e-02,  8.7006e-01, -2.0600e-01,
        -3.4507e-03,  1.8695e-01,  1.0000e+00,  2.9092e-01, -2.6603e-01,
         2.7547e-01,  7.8514e-02,  1.6481e-01,  4.8246e-01,  5.4929e-01,
        -1.7485e-01,  3.9021e-02,  4.1384e-01,  5.1890e-01, -4.6565e-02,
         3.2173e-01,  1.8334e-01,  1.8262e-01,  1.4842e-01,  6.1002e-01,
         2.9751e-02,  1.2533e-01,  5.3216e-01,  2.0576e-01, -4.0360e-02,
         3.6691e-01,  8.1207e-01,  3.9485e-01,  1.3495e-01,  1.4429e-04,
         3.4063e-01,  3.1047e-02,  3.8775e-01,  2.6841e-01,  5.2225e-01,
         5.2153e-01,  5.3369e-01,  1.9615e-01,  3.0179e-01], device='cuda:0'), 'blocks.2.layer.weight': tensor([-0.0010,  0.0317,  0.4703,  ...,  0.2049, -0.0204,  0.3651],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([0.2656, 0.5401, 0.7364,  ..., 0.3438, 0.5377, 0.2040], device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.1.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 3.4675e-01,  4.9634e-01,  4.9947e-01,  3.5167e-01,  6.8275e-01,
        -1.5499e-01,  3.8235e-01, -8.2602e-02,  9.2969e-02,  3.4503e-01,
        -6.9911e-02,  6.8859e-01,  4.4754e-01,  4.0567e-01,  5.8537e-01,
         4.8758e-01,  2.1827e-01,  5.5889e-01, -3.6301e-01, -2.1951e-01,
         2.5134e-01,  3.0956e-01,  7.9425e-01,  6.6511e-01,  7.1393e-01,
         8.6881e-01,  1.8570e-01, -8.7449e-02,  6.5183e-02,  5.4472e-01,
         2.1367e-01, -3.4635e-01, -3.3732e-01,  1.6719e-01,  1.6866e-01,
         1.5956e-01,  3.7950e-01,  3.3845e-01,  5.1705e-02,  3.7345e-01,
         4.0139e-01,  3.6994e-01,  2.8768e-01,  3.8171e-01,  7.6911e-01,
         3.2752e-01, -3.0061e-01,  3.1377e-01, -9.3224e-02, -1.2043e-01,
         4.4630e-01,  4.6442e-01, -6.7174e-02, -1.0307e-01, -8.5593e-02,
         4.1887e-01,  4.8836e-01, -2.3651e-01,  3.4604e-01,  9.2028e-02,
         1.6476e-01, -6.3855e-01,  7.0815e-01, -4.7516e-01, -1.3248e-01,
         2.0273e-01, -9.3000e-02,  8.0644e-01,  3.1913e-01, -1.4491e-02,
         1.5819e-01,  5.9817e-02,  1.0919e-01,  5.0951e-01, -3.8428e-02,
         1.5758e-01,  5.2751e-01, -4.7671e-01, -4.2823e-01,  4.0300e-01,
         1.2542e-01,  3.8350e-01,  3.7314e-01,  2.1977e-01,  6.2835e-01,
         1.7641e-01,  4.4517e-01,  6.0281e-02,  1.0899e-01,  8.3355e-01,
         2.6046e-01,  2.2819e-01,  2.6363e-01,  3.1669e-01,  7.0682e-01,
        -1.5943e-01,  5.2125e-01,  1.8858e-01, -3.3286e-01,  2.0695e-01,
         6.1562e-01,  3.3932e-01,  2.5271e-01,  1.3981e-01,  1.2687e-01,
         3.1925e-01,  1.8174e-01, -9.4531e-02,  3.6838e-02,  3.7488e-01,
         1.2717e-01,  4.7130e-01, -1.5721e-01, -7.7476e-02,  8.1014e-01,
        -1.5593e-01, -1.0054e-01,  4.5635e-01,  6.1519e-01,  1.4677e-01,
        -2.2707e-01, -5.5761e-03,  2.6145e-01,  5.9044e-01,  2.2731e-01,
        -2.6443e-02, -2.3644e-03, -1.0726e-01,  7.7391e-01,  3.4043e-01,
         2.8069e-01,  4.8368e-01,  4.9516e-01,  2.3034e-01,  6.8924e-01,
         3.1027e-01,  3.4570e-01,  9.7788e-01,  3.3127e-01,  3.5599e-01,
         5.0713e-01,  7.2506e-01,  6.5209e-01, -4.4852e-02,  4.7526e-01,
         7.1132e-01,  8.4672e-01, -7.5696e-03,  4.1974e-01,  2.1462e-01,
         1.7083e-01, -1.3488e-01,  4.1226e-01,  2.3258e-01,  2.5389e-01,
        -1.5678e-01, -5.3553e-01, -4.7307e-01,  2.7626e-01, -2.5093e-01,
         4.0266e-02,  2.4575e-01,  1.8999e-01, -6.1258e-03, -6.5438e-03,
         4.2919e-01, -1.2408e-01,  1.9037e-01,  5.1006e-01, -6.9298e-02,
         1.7477e-01,  3.5078e-01,  7.3873e-01,  5.5692e-01,  4.3353e-01,
        -2.2003e-01,  3.3680e-01,  6.3033e-01,  2.5604e-01,  3.2412e-01,
         6.3429e-02,  4.2685e-01,  3.0266e-01,  4.5454e-01,  3.0221e-02,
         4.9645e-01,  4.2592e-01,  2.2569e-01,  3.2492e-01, -1.9558e-01,
         6.3491e-01,  1.4616e-01,  2.0872e-01,  1.7695e-01,  3.4186e-01,
         7.6387e-01,  2.4154e-01,  3.4131e-01,  2.5706e-01, -1.7839e-01,
         4.7458e-01, -6.1654e-02, -4.2599e-02,  8.0520e-01,  1.1602e-01,
         4.0244e-01,  5.3733e-01,  1.0871e-01,  2.4645e-02,  7.1771e-01,
        -1.1157e-01,  2.2285e-01,  3.2628e-01,  1.0782e-01,  1.5037e-01,
         4.8782e-01,  4.4199e-01,  8.0079e-01, -1.9772e-01,  4.1425e-01,
         3.3853e-01,  7.4381e-01, -1.5043e-01,  5.4073e-02,  5.8050e-01,
        -2.5929e-02,  8.0517e-01, -3.4948e-01,  2.4886e-01,  5.5591e-01,
         1.8956e-01,  1.2973e-01,  7.4482e-02,  1.6208e-01,  5.5959e-01,
        -8.3063e-02,  7.0622e-01,  4.5518e-01,  4.7128e-01,  7.8142e-01,
        -2.2690e-02,  8.9799e-02,  1.5444e-01, -9.2526e-04,  1.1149e-01,
        -7.5147e-03,  5.6785e-01,  8.0659e-02,  5.8024e-01,  8.5778e-02,
         6.8099e-01,  2.7963e-01,  9.8910e-02, -1.4123e-01, -2.2274e-01,
         3.2634e-01,  4.8895e-01,  2.9214e-01, -1.3606e-01,  2.9738e-01,
         1.8581e-01,  1.2835e-01,  8.0524e-01,  5.1709e-01,  5.6002e-02,
         3.1847e-01, -5.6267e-01,  5.3282e-01,  4.8991e-01,  1.9676e-01,
        -2.5985e-02,  6.1749e-01,  3.1766e-01,  1.6751e-01,  4.4369e-01,
         1.5407e-01,  5.1959e-01,  4.5052e-01,  3.0881e-01,  2.6712e-02,
         1.2838e-01,  7.2508e-01,  4.6165e-01,  2.0328e-01,  2.4748e-01,
         4.8103e-01,  1.8086e-01, -7.3423e-02,  2.9468e-01, -8.2690e-02,
         7.6233e-02, -2.6612e-01,  3.9660e-01,  2.3032e-01,  4.0978e-02,
        -2.4679e-01, -1.2860e-01,  1.0342e-01,  5.5879e-01,  3.9662e-01,
        -1.4361e-01, -1.9822e-01,  5.5379e-01, -3.5044e-02,  6.5483e-01,
         4.9469e-01,  4.6768e-01,  9.9884e-02,  4.0240e-01,  8.2842e-02,
         8.1476e-02,  1.0099e-01,  6.6033e-01,  1.2213e-01,  6.6229e-01,
         6.9984e-02,  4.2777e-01,  7.7067e-01,  1.1222e-02,  1.7325e-01,
         1.4918e-01,  4.4225e-01,  7.1020e-01, -6.4827e-02, -1.0691e-01,
         4.2094e-01,  9.3253e-01,  7.4832e-01,  2.0421e-01,  8.0857e-01,
        -6.9927e-01,  6.9038e-01, -1.4160e-01,  2.9397e-01,  1.8431e-01,
         3.3021e-01,  3.0792e-01,  2.0105e-01,  5.4197e-01,  3.6271e-01,
         3.5776e-01,  2.2193e-01,  6.9557e-02,  8.7006e-01, -2.0600e-01,
        -3.4507e-03,  1.8695e-01,  1.0000e+00,  2.9092e-01, -2.6603e-01,
         2.7547e-01,  7.8514e-02,  1.6481e-01,  4.8246e-01,  5.4929e-01,
        -1.7485e-01,  3.9021e-02,  4.1384e-01,  5.1890e-01, -4.6565e-02,
         3.2173e-01,  1.8334e-01,  1.8262e-01,  1.4842e-01,  6.1002e-01,
         2.9751e-02,  1.2533e-01,  5.3216e-01,  2.0576e-01, -4.0360e-02,
         3.6691e-01,  8.1207e-01,  3.9485e-01,  1.3495e-01,  1.4429e-04,
         3.4063e-01,  3.1047e-02,  3.8775e-01,  2.6841e-01,  5.2225e-01,
         5.2153e-01,  5.3369e-01,  1.9615e-01,  3.0179e-01], device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-0.3549,  0.1212,  0.0095,  0.1285, -0.5902, -0.7595,  0.2930, -0.1311,
        -0.6668,  0.2679,  0.4133,  0.4384,  0.0050,  0.0863,  0.9283, -0.2386,
        -0.4347,  0.0211, -0.5482, -0.5881,  0.3635,  0.7974, -0.0708,  0.0277,
         0.3060,  0.6573, -0.0664, -0.1364,  0.2152, -0.0694,  0.9545,  0.5186,
        -0.4286, -0.7605,  0.7025,  0.9891, -0.6683, -0.0308,  0.8484,  0.0912,
         0.4675,  0.1660,  0.9470, -0.3474,  0.5685, -0.6660, -0.3353,  0.5072,
         0.0084,  0.3467, -0.5717,  0.7976,  0.3361,  0.8629, -0.0436,  0.9182,
        -0.4613,  0.7530, -0.3987,  1.0000, -0.1714, -0.0323, -0.3805,  0.1154,
        -0.7352, -0.3378, -0.5991, -0.0573,  0.9557,  0.9710,  0.9160,  0.3482,
        -0.0344, -0.5976, -0.0085, -0.6056,  0.1304,  0.9179, -0.7200,  0.4949,
        -0.3991,  0.7442, -0.6611,  0.6799,  0.0952, -0.2164, -0.6674, -0.3528,
        -0.0951, -0.7665,  0.0833, -0.1890, -0.2624,  0.9089, -0.3644,  0.4514],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.4675e-01,  4.9634e-01,  4.9947e-01,  3.5167e-01,  6.8275e-01,
        -1.5499e-01,  3.8235e-01, -8.2602e-02,  9.2969e-02,  3.4503e-01,
        -6.9911e-02,  6.8859e-01,  4.4754e-01,  4.0567e-01,  5.8537e-01,
         4.8758e-01,  2.1827e-01,  5.5889e-01, -3.6301e-01, -2.1951e-01,
         2.5134e-01,  3.0956e-01,  7.9425e-01,  6.6511e-01,  7.1393e-01,
         8.6881e-01,  1.8570e-01, -8.7449e-02,  6.5183e-02,  5.4472e-01,
         2.1367e-01, -3.4635e-01, -3.3732e-01,  1.6719e-01,  1.6866e-01,
         1.5956e-01,  3.7950e-01,  3.3845e-01,  5.1705e-02,  3.7345e-01,
         4.0139e-01,  3.6994e-01,  2.8768e-01,  3.8171e-01,  7.6911e-01,
         3.2752e-01, -3.0061e-01,  3.1377e-01, -9.3224e-02, -1.2043e-01,
         4.4630e-01,  4.6442e-01, -6.7174e-02, -1.0307e-01, -8.5593e-02,
         4.1887e-01,  4.8836e-01, -2.3651e-01,  3.4604e-01,  9.2028e-02,
         1.6476e-01, -6.3855e-01,  7.0815e-01, -4.7516e-01, -1.3248e-01,
         2.0273e-01, -9.3000e-02,  8.0644e-01,  3.1913e-01, -1.4491e-02,
         1.5819e-01,  5.9817e-02,  1.0919e-01,  5.0951e-01, -3.8428e-02,
         1.5758e-01,  5.2751e-01, -4.7671e-01, -4.2823e-01,  4.0300e-01,
         1.2542e-01,  3.8350e-01,  3.7314e-01,  2.1977e-01,  6.2835e-01,
         1.7641e-01,  4.4517e-01,  6.0281e-02,  1.0899e-01,  8.3355e-01,
         2.6046e-01,  2.2819e-01,  2.6363e-01,  3.1669e-01,  7.0682e-01,
        -1.5943e-01,  5.2125e-01,  1.8858e-01, -3.3286e-01,  2.0695e-01,
         6.1562e-01,  3.3932e-01,  2.5271e-01,  1.3981e-01,  1.2687e-01,
         3.1925e-01,  1.8174e-01, -9.4531e-02,  3.6838e-02,  3.7488e-01,
         1.2717e-01,  4.7130e-01, -1.5721e-01, -7.7476e-02,  8.1014e-01,
        -1.5593e-01, -1.0054e-01,  4.5635e-01,  6.1519e-01,  1.4677e-01,
        -2.2707e-01, -5.5761e-03,  2.6145e-01,  5.9044e-01,  2.2731e-01,
        -2.6443e-02, -2.3644e-03, -1.0726e-01,  7.7391e-01,  3.4043e-01,
         2.8069e-01,  4.8368e-01,  4.9516e-01,  2.3034e-01,  6.8924e-01,
         3.1027e-01,  3.4570e-01,  9.7788e-01,  3.3127e-01,  3.5599e-01,
         5.0713e-01,  7.2506e-01,  6.5209e-01, -4.4852e-02,  4.7526e-01,
         7.1132e-01,  8.4672e-01, -7.5696e-03,  4.1974e-01,  2.1462e-01,
         1.7083e-01, -1.3488e-01,  4.1226e-01,  2.3258e-01,  2.5389e-01,
        -1.5678e-01, -5.3553e-01, -4.7307e-01,  2.7626e-01, -2.5093e-01,
         4.0266e-02,  2.4575e-01,  1.8999e-01, -6.1258e-03, -6.5438e-03,
         4.2919e-01, -1.2408e-01,  1.9037e-01,  5.1006e-01, -6.9298e-02,
         1.7477e-01,  3.5078e-01,  7.3873e-01,  5.5692e-01,  4.3353e-01,
        -2.2003e-01,  3.3680e-01,  6.3033e-01,  2.5604e-01,  3.2412e-01,
         6.3429e-02,  4.2685e-01,  3.0266e-01,  4.5454e-01,  3.0221e-02,
         4.9645e-01,  4.2592e-01,  2.2569e-01,  3.2492e-01, -1.9558e-01,
         6.3491e-01,  1.4616e-01,  2.0872e-01,  1.7695e-01,  3.4186e-01,
         7.6387e-01,  2.4154e-01,  3.4131e-01,  2.5706e-01, -1.7839e-01,
         4.7458e-01, -6.1654e-02, -4.2599e-02,  8.0520e-01,  1.1602e-01,
         4.0244e-01,  5.3733e-01,  1.0871e-01,  2.4645e-02,  7.1771e-01,
        -1.1157e-01,  2.2285e-01,  3.2628e-01,  1.0782e-01,  1.5037e-01,
         4.8782e-01,  4.4199e-01,  8.0079e-01, -1.9772e-01,  4.1425e-01,
         3.3853e-01,  7.4381e-01, -1.5043e-01,  5.4073e-02,  5.8050e-01,
        -2.5929e-02,  8.0517e-01, -3.4948e-01,  2.4886e-01,  5.5591e-01,
         1.8956e-01,  1.2973e-01,  7.4482e-02,  1.6208e-01,  5.5959e-01,
        -8.3063e-02,  7.0622e-01,  4.5518e-01,  4.7128e-01,  7.8142e-01,
        -2.2690e-02,  8.9799e-02,  1.5444e-01, -9.2526e-04,  1.1149e-01,
        -7.5147e-03,  5.6785e-01,  8.0659e-02,  5.8024e-01,  8.5778e-02,
         6.8099e-01,  2.7963e-01,  9.8910e-02, -1.4123e-01, -2.2274e-01,
         3.2634e-01,  4.8895e-01,  2.9214e-01, -1.3606e-01,  2.9738e-01,
         1.8581e-01,  1.2835e-01,  8.0524e-01,  5.1709e-01,  5.6002e-02,
         3.1847e-01, -5.6267e-01,  5.3282e-01,  4.8991e-01,  1.9676e-01,
        -2.5985e-02,  6.1749e-01,  3.1766e-01,  1.6751e-01,  4.4369e-01,
         1.5407e-01,  5.1959e-01,  4.5052e-01,  3.0881e-01,  2.6712e-02,
         1.2838e-01,  7.2508e-01,  4.6165e-01,  2.0328e-01,  2.4748e-01,
         4.8103e-01,  1.8086e-01, -7.3423e-02,  2.9468e-01, -8.2690e-02,
         7.6233e-02, -2.6612e-01,  3.9660e-01,  2.3032e-01,  4.0978e-02,
        -2.4679e-01, -1.2860e-01,  1.0342e-01,  5.5879e-01,  3.9662e-01,
        -1.4361e-01, -1.9822e-01,  5.5379e-01, -3.5044e-02,  6.5483e-01,
         4.9469e-01,  4.6768e-01,  9.9884e-02,  4.0240e-01,  8.2842e-02,
         8.1476e-02,  1.0099e-01,  6.6033e-01,  1.2213e-01,  6.6229e-01,
         6.9984e-02,  4.2777e-01,  7.7067e-01,  1.1222e-02,  1.7325e-01,
         1.4918e-01,  4.4225e-01,  7.1020e-01, -6.4827e-02, -1.0691e-01,
         4.2094e-01,  9.3253e-01,  7.4832e-01,  2.0421e-01,  8.0857e-01,
        -6.9927e-01,  6.9038e-01, -1.4160e-01,  2.9397e-01,  1.8431e-01,
         3.3021e-01,  3.0792e-01,  2.0105e-01,  5.4197e-01,  3.6271e-01,
         3.5776e-01,  2.2193e-01,  6.9557e-02,  8.7006e-01, -2.0600e-01,
        -3.4507e-03,  1.8695e-01,  1.0000e+00,  2.9092e-01, -2.6603e-01,
         2.7547e-01,  7.8514e-02,  1.6481e-01,  4.8246e-01,  5.4929e-01,
        -1.7485e-01,  3.9021e-02,  4.1384e-01,  5.1890e-01, -4.6565e-02,
         3.2173e-01,  1.8334e-01,  1.8262e-01,  1.4842e-01,  6.1002e-01,
         2.9751e-02,  1.2533e-01,  5.3216e-01,  2.0576e-01, -4.0360e-02,
         3.6691e-01,  8.1207e-01,  3.9485e-01,  1.3495e-01,  1.4429e-04,
         3.4063e-01,  3.1047e-02,  3.8775e-01,  2.6841e-01,  5.2225e-01,
         5.2153e-01,  5.3369e-01,  1.9615e-01,  3.0179e-01], device='cuda:0'), 'blocks.2.layer.weight': tensor([-0.0010,  0.0317,  0.4703,  ...,  0.2049, -0.0204,  0.3651],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([0.2656, 0.5401, 0.7364,  ..., 0.3438, 0.5377, 0.2040], device='cuda:0')}
topk_layer inside model after retrieval:  [23, 75, 345, 61, 349, 92, 330, 197, 100, 33, 69, 190, 296, 41, 121, 157, 76, 166, 247, 133, 39, 336, 266, 362, 184, 17, 63, 158, 50, 116, 344, 354, 201, 98, 292, 156, 130, 335, 291, 365, 332, 366, 146, 382, 186, 287, 159, 104, 217, 139, 219, 28, 99, 260, 180, 264, 227, 273, 379]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-0.3549,  0.1212,  0.0095,  0.1285, -0.5902, -0.7595,  0.2930, -0.1311,
        -0.6668,  0.2679,  0.4133,  0.4384,  0.0050,  0.0863,  0.9283, -0.2386,
        -0.4347,  0.0211, -0.5482, -0.5881,  0.3635,  0.7974, -0.0708,  0.0277,
         0.3060,  0.6573, -0.0664, -0.1364,  0.2152, -0.0694,  0.9545,  0.5186,
        -0.4286, -0.7605,  0.7025,  0.9891, -0.6683, -0.0308,  0.8484,  0.0912,
         0.4675,  0.1660,  0.9470, -0.3474,  0.5685, -0.6660, -0.3353,  0.5072,
         0.0084,  0.3467, -0.5717,  0.7976,  0.3361,  0.8629, -0.0436,  0.9182,
        -0.4613,  0.7530, -0.3987,  1.0000, -0.1714, -0.0323, -0.3805,  0.1154,
        -0.7352, -0.3378, -0.5991, -0.0573,  0.9557,  0.9710,  0.9160,  0.3482,
        -0.0344, -0.5976, -0.0085, -0.6056,  0.1304,  0.9179, -0.7200,  0.4949,
        -0.3991,  0.7442, -0.6611,  0.6799,  0.0952, -0.2164, -0.6674, -0.3528,
        -0.0951, -0.7665,  0.0833, -0.1890, -0.2624,  0.9089, -0.3644,  0.4514],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.4675e-01,  4.9634e-01,  4.9947e-01,  3.5167e-01,  6.8275e-01,
        -1.5499e-01,  3.8235e-01, -8.2602e-02,  9.2969e-02,  3.4503e-01,
        -6.9911e-02,  6.8859e-01,  4.4754e-01,  4.0567e-01,  5.8537e-01,
         4.8758e-01,  2.1827e-01,  5.5889e-01, -3.6301e-01, -2.1951e-01,
         2.5134e-01,  3.0956e-01,  7.9425e-01,  6.6511e-01,  7.1393e-01,
         8.6881e-01,  1.8570e-01, -8.7449e-02,  6.5183e-02,  5.4472e-01,
         2.1367e-01, -3.4635e-01, -3.3732e-01,  1.6719e-01,  1.6866e-01,
         1.5956e-01,  3.7950e-01,  3.3845e-01,  5.1705e-02,  3.7345e-01,
         4.0139e-01,  3.6994e-01,  2.8768e-01,  3.8171e-01,  7.6911e-01,
         3.2752e-01, -3.0061e-01,  3.1377e-01, -9.3224e-02, -1.2043e-01,
         4.4630e-01,  4.6442e-01, -6.7174e-02, -1.0307e-01, -8.5593e-02,
         4.1887e-01,  4.8836e-01, -2.3651e-01,  3.4604e-01,  9.2028e-02,
         1.6476e-01, -6.3855e-01,  7.0815e-01, -4.7516e-01, -1.3248e-01,
         2.0273e-01, -9.3000e-02,  8.0644e-01,  3.1913e-01, -1.4491e-02,
         1.5819e-01,  5.9817e-02,  1.0919e-01,  5.0951e-01, -3.8428e-02,
         1.5758e-01,  5.2751e-01, -4.7671e-01, -4.2823e-01,  4.0300e-01,
         1.2542e-01,  3.8350e-01,  3.7314e-01,  2.1977e-01,  6.2835e-01,
         1.7641e-01,  4.4517e-01,  6.0281e-02,  1.0899e-01,  8.3355e-01,
         2.6046e-01,  2.2819e-01,  2.6363e-01,  3.1669e-01,  7.0682e-01,
        -1.5943e-01,  5.2125e-01,  1.8858e-01, -3.3286e-01,  2.0695e-01,
         6.1562e-01,  3.3932e-01,  2.5271e-01,  1.3981e-01,  1.2687e-01,
         3.1925e-01,  1.8174e-01, -9.4531e-02,  3.6838e-02,  3.7488e-01,
         1.2717e-01,  4.7130e-01, -1.5721e-01, -7.7476e-02,  8.1014e-01,
        -1.5593e-01, -1.0054e-01,  4.5635e-01,  6.1519e-01,  1.4677e-01,
        -2.2707e-01, -5.5761e-03,  2.6145e-01,  5.9044e-01,  2.2731e-01,
        -2.6443e-02, -2.3644e-03, -1.0726e-01,  7.7391e-01,  3.4043e-01,
         2.8069e-01,  4.8368e-01,  4.9516e-01,  2.3034e-01,  6.8924e-01,
         3.1027e-01,  3.4570e-01,  9.7788e-01,  3.3127e-01,  3.5599e-01,
         5.0713e-01,  7.2506e-01,  6.5209e-01, -4.4852e-02,  4.7526e-01,
         7.1132e-01,  8.4672e-01, -7.5696e-03,  4.1974e-01,  2.1462e-01,
         1.7083e-01, -1.3488e-01,  4.1226e-01,  2.3258e-01,  2.5389e-01,
        -1.5678e-01, -5.3553e-01, -4.7307e-01,  2.7626e-01, -2.5093e-01,
         4.0266e-02,  2.4575e-01,  1.8999e-01, -6.1258e-03, -6.5438e-03,
         4.2919e-01, -1.2408e-01,  1.9037e-01,  5.1006e-01, -6.9298e-02,
         1.7477e-01,  3.5078e-01,  7.3873e-01,  5.5692e-01,  4.3353e-01,
        -2.2003e-01,  3.3680e-01,  6.3033e-01,  2.5604e-01,  3.2412e-01,
         6.3429e-02,  4.2685e-01,  3.0266e-01,  4.5454e-01,  3.0221e-02,
         4.9645e-01,  4.2592e-01,  2.2569e-01,  3.2492e-01, -1.9558e-01,
         6.3491e-01,  1.4616e-01,  2.0872e-01,  1.7695e-01,  3.4186e-01,
         7.6387e-01,  2.4154e-01,  3.4131e-01,  2.5706e-01, -1.7839e-01,
         4.7458e-01, -6.1654e-02, -4.2599e-02,  8.0520e-01,  1.1602e-01,
         4.0244e-01,  5.3733e-01,  1.0871e-01,  2.4645e-02,  7.1771e-01,
        -1.1157e-01,  2.2285e-01,  3.2628e-01,  1.0782e-01,  1.5037e-01,
         4.8782e-01,  4.4199e-01,  8.0079e-01, -1.9772e-01,  4.1425e-01,
         3.3853e-01,  7.4381e-01, -1.5043e-01,  5.4073e-02,  5.8050e-01,
        -2.5929e-02,  8.0517e-01, -3.4948e-01,  2.4886e-01,  5.5591e-01,
         1.8956e-01,  1.2973e-01,  7.4482e-02,  1.6208e-01,  5.5959e-01,
        -8.3063e-02,  7.0622e-01,  4.5518e-01,  4.7128e-01,  7.8142e-01,
        -2.2690e-02,  8.9799e-02,  1.5444e-01, -9.2526e-04,  1.1149e-01,
        -7.5147e-03,  5.6785e-01,  8.0659e-02,  5.8024e-01,  8.5778e-02,
         6.8099e-01,  2.7963e-01,  9.8910e-02, -1.4123e-01, -2.2274e-01,
         3.2634e-01,  4.8895e-01,  2.9214e-01, -1.3606e-01,  2.9738e-01,
         1.8581e-01,  1.2835e-01,  8.0524e-01,  5.1709e-01,  5.6002e-02,
         3.1847e-01, -5.6267e-01,  5.3282e-01,  4.8991e-01,  1.9676e-01,
        -2.5985e-02,  6.1749e-01,  3.1766e-01,  1.6751e-01,  4.4369e-01,
         1.5407e-01,  5.1959e-01,  4.5052e-01,  3.0881e-01,  2.6712e-02,
         1.2838e-01,  7.2508e-01,  4.6165e-01,  2.0328e-01,  2.4748e-01,
         4.8103e-01,  1.8086e-01, -7.3423e-02,  2.9468e-01, -8.2690e-02,
         7.6233e-02, -2.6612e-01,  3.9660e-01,  2.3032e-01,  4.0978e-02,
        -2.4679e-01, -1.2860e-01,  1.0342e-01,  5.5879e-01,  3.9662e-01,
        -1.4361e-01, -1.9822e-01,  5.5379e-01, -3.5044e-02,  6.5483e-01,
         4.9469e-01,  4.6768e-01,  9.9884e-02,  4.0240e-01,  8.2842e-02,
         8.1476e-02,  1.0099e-01,  6.6033e-01,  1.2213e-01,  6.6229e-01,
         6.9984e-02,  4.2777e-01,  7.7067e-01,  1.1222e-02,  1.7325e-01,
         1.4918e-01,  4.4225e-01,  7.1020e-01, -6.4827e-02, -1.0691e-01,
         4.2094e-01,  9.3253e-01,  7.4832e-01,  2.0421e-01,  8.0857e-01,
        -6.9927e-01,  6.9038e-01, -1.4160e-01,  2.9397e-01,  1.8431e-01,
         3.3021e-01,  3.0792e-01,  2.0105e-01,  5.4197e-01,  3.6271e-01,
         3.5776e-01,  2.2193e-01,  6.9557e-02,  8.7006e-01, -2.0600e-01,
        -3.4507e-03,  1.8695e-01,  1.0000e+00,  2.9092e-01, -2.6603e-01,
         2.7547e-01,  7.8514e-02,  1.6481e-01,  4.8246e-01,  5.4929e-01,
        -1.7485e-01,  3.9021e-02,  4.1384e-01,  5.1890e-01, -4.6565e-02,
         3.2173e-01,  1.8334e-01,  1.8262e-01,  1.4842e-01,  6.1002e-01,
         2.9751e-02,  1.2533e-01,  5.3216e-01,  2.0576e-01, -4.0360e-02,
         3.6691e-01,  8.1207e-01,  3.9485e-01,  1.3495e-01,  1.4429e-04,
         3.4063e-01,  3.1047e-02,  3.8775e-01,  2.6841e-01,  5.2225e-01,
         5.2153e-01,  5.3369e-01,  1.9615e-01,  3.0179e-01], device='cuda:0'), 'blocks.2.layer.weight': tensor([-0.0010,  0.0317,  0.4703,  ...,  0.2049, -0.0204,  0.3651],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([0.2656, 0.5401, 0.7364,  ..., 0.3438, 0.5377, 0.2040], device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.2.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([-0.0010,  0.0317,  0.4703,  ...,  0.2049, -0.0204,  0.3651],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-0.3549,  0.1212,  0.0095,  0.1285, -0.5902, -0.7595,  0.2930, -0.1311,
        -0.6668,  0.2679,  0.4133,  0.4384,  0.0050,  0.0863,  0.9283, -0.2386,
        -0.4347,  0.0211, -0.5482, -0.5881,  0.3635,  0.7974, -0.0708,  0.0277,
         0.3060,  0.6573, -0.0664, -0.1364,  0.2152, -0.0694,  0.9545,  0.5186,
        -0.4286, -0.7605,  0.7025,  0.9891, -0.6683, -0.0308,  0.8484,  0.0912,
         0.4675,  0.1660,  0.9470, -0.3474,  0.5685, -0.6660, -0.3353,  0.5072,
         0.0084,  0.3467, -0.5717,  0.7976,  0.3361,  0.8629, -0.0436,  0.9182,
        -0.4613,  0.7530, -0.3987,  1.0000, -0.1714, -0.0323, -0.3805,  0.1154,
        -0.7352, -0.3378, -0.5991, -0.0573,  0.9557,  0.9710,  0.9160,  0.3482,
        -0.0344, -0.5976, -0.0085, -0.6056,  0.1304,  0.9179, -0.7200,  0.4949,
        -0.3991,  0.7442, -0.6611,  0.6799,  0.0952, -0.2164, -0.6674, -0.3528,
        -0.0951, -0.7665,  0.0833, -0.1890, -0.2624,  0.9089, -0.3644,  0.4514],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.4675e-01,  4.9634e-01,  4.9947e-01,  3.5167e-01,  6.8275e-01,
        -1.5499e-01,  3.8235e-01, -8.2602e-02,  9.2969e-02,  3.4503e-01,
        -6.9911e-02,  6.8859e-01,  4.4754e-01,  4.0567e-01,  5.8537e-01,
         4.8758e-01,  2.1827e-01,  5.5889e-01, -3.6301e-01, -2.1951e-01,
         2.5134e-01,  3.0956e-01,  7.9425e-01,  6.6511e-01,  7.1393e-01,
         8.6881e-01,  1.8570e-01, -8.7449e-02,  6.5183e-02,  5.4472e-01,
         2.1367e-01, -3.4635e-01, -3.3732e-01,  1.6719e-01,  1.6866e-01,
         1.5956e-01,  3.7950e-01,  3.3845e-01,  5.1705e-02,  3.7345e-01,
         4.0139e-01,  3.6994e-01,  2.8768e-01,  3.8171e-01,  7.6911e-01,
         3.2752e-01, -3.0061e-01,  3.1377e-01, -9.3224e-02, -1.2043e-01,
         4.4630e-01,  4.6442e-01, -6.7174e-02, -1.0307e-01, -8.5593e-02,
         4.1887e-01,  4.8836e-01, -2.3651e-01,  3.4604e-01,  9.2028e-02,
         1.6476e-01, -6.3855e-01,  7.0815e-01, -4.7516e-01, -1.3248e-01,
         2.0273e-01, -9.3000e-02,  8.0644e-01,  3.1913e-01, -1.4491e-02,
         1.5819e-01,  5.9817e-02,  1.0919e-01,  5.0951e-01, -3.8428e-02,
         1.5758e-01,  5.2751e-01, -4.7671e-01, -4.2823e-01,  4.0300e-01,
         1.2542e-01,  3.8350e-01,  3.7314e-01,  2.1977e-01,  6.2835e-01,
         1.7641e-01,  4.4517e-01,  6.0281e-02,  1.0899e-01,  8.3355e-01,
         2.6046e-01,  2.2819e-01,  2.6363e-01,  3.1669e-01,  7.0682e-01,
        -1.5943e-01,  5.2125e-01,  1.8858e-01, -3.3286e-01,  2.0695e-01,
         6.1562e-01,  3.3932e-01,  2.5271e-01,  1.3981e-01,  1.2687e-01,
         3.1925e-01,  1.8174e-01, -9.4531e-02,  3.6838e-02,  3.7488e-01,
         1.2717e-01,  4.7130e-01, -1.5721e-01, -7.7476e-02,  8.1014e-01,
        -1.5593e-01, -1.0054e-01,  4.5635e-01,  6.1519e-01,  1.4677e-01,
        -2.2707e-01, -5.5761e-03,  2.6145e-01,  5.9044e-01,  2.2731e-01,
        -2.6443e-02, -2.3644e-03, -1.0726e-01,  7.7391e-01,  3.4043e-01,
         2.8069e-01,  4.8368e-01,  4.9516e-01,  2.3034e-01,  6.8924e-01,
         3.1027e-01,  3.4570e-01,  9.7788e-01,  3.3127e-01,  3.5599e-01,
         5.0713e-01,  7.2506e-01,  6.5209e-01, -4.4852e-02,  4.7526e-01,
         7.1132e-01,  8.4672e-01, -7.5696e-03,  4.1974e-01,  2.1462e-01,
         1.7083e-01, -1.3488e-01,  4.1226e-01,  2.3258e-01,  2.5389e-01,
        -1.5678e-01, -5.3553e-01, -4.7307e-01,  2.7626e-01, -2.5093e-01,
         4.0266e-02,  2.4575e-01,  1.8999e-01, -6.1258e-03, -6.5438e-03,
         4.2919e-01, -1.2408e-01,  1.9037e-01,  5.1006e-01, -6.9298e-02,
         1.7477e-01,  3.5078e-01,  7.3873e-01,  5.5692e-01,  4.3353e-01,
        -2.2003e-01,  3.3680e-01,  6.3033e-01,  2.5604e-01,  3.2412e-01,
         6.3429e-02,  4.2685e-01,  3.0266e-01,  4.5454e-01,  3.0221e-02,
         4.9645e-01,  4.2592e-01,  2.2569e-01,  3.2492e-01, -1.9558e-01,
         6.3491e-01,  1.4616e-01,  2.0872e-01,  1.7695e-01,  3.4186e-01,
         7.6387e-01,  2.4154e-01,  3.4131e-01,  2.5706e-01, -1.7839e-01,
         4.7458e-01, -6.1654e-02, -4.2599e-02,  8.0520e-01,  1.1602e-01,
         4.0244e-01,  5.3733e-01,  1.0871e-01,  2.4645e-02,  7.1771e-01,
        -1.1157e-01,  2.2285e-01,  3.2628e-01,  1.0782e-01,  1.5037e-01,
         4.8782e-01,  4.4199e-01,  8.0079e-01, -1.9772e-01,  4.1425e-01,
         3.3853e-01,  7.4381e-01, -1.5043e-01,  5.4073e-02,  5.8050e-01,
        -2.5929e-02,  8.0517e-01, -3.4948e-01,  2.4886e-01,  5.5591e-01,
         1.8956e-01,  1.2973e-01,  7.4482e-02,  1.6208e-01,  5.5959e-01,
        -8.3063e-02,  7.0622e-01,  4.5518e-01,  4.7128e-01,  7.8142e-01,
        -2.2690e-02,  8.9799e-02,  1.5444e-01, -9.2526e-04,  1.1149e-01,
        -7.5147e-03,  5.6785e-01,  8.0659e-02,  5.8024e-01,  8.5778e-02,
         6.8099e-01,  2.7963e-01,  9.8910e-02, -1.4123e-01, -2.2274e-01,
         3.2634e-01,  4.8895e-01,  2.9214e-01, -1.3606e-01,  2.9738e-01,
         1.8581e-01,  1.2835e-01,  8.0524e-01,  5.1709e-01,  5.6002e-02,
         3.1847e-01, -5.6267e-01,  5.3282e-01,  4.8991e-01,  1.9676e-01,
        -2.5985e-02,  6.1749e-01,  3.1766e-01,  1.6751e-01,  4.4369e-01,
         1.5407e-01,  5.1959e-01,  4.5052e-01,  3.0881e-01,  2.6712e-02,
         1.2838e-01,  7.2508e-01,  4.6165e-01,  2.0328e-01,  2.4748e-01,
         4.8103e-01,  1.8086e-01, -7.3423e-02,  2.9468e-01, -8.2690e-02,
         7.6233e-02, -2.6612e-01,  3.9660e-01,  2.3032e-01,  4.0978e-02,
        -2.4679e-01, -1.2860e-01,  1.0342e-01,  5.5879e-01,  3.9662e-01,
        -1.4361e-01, -1.9822e-01,  5.5379e-01, -3.5044e-02,  6.5483e-01,
         4.9469e-01,  4.6768e-01,  9.9884e-02,  4.0240e-01,  8.2842e-02,
         8.1476e-02,  1.0099e-01,  6.6033e-01,  1.2213e-01,  6.6229e-01,
         6.9984e-02,  4.2777e-01,  7.7067e-01,  1.1222e-02,  1.7325e-01,
         1.4918e-01,  4.4225e-01,  7.1020e-01, -6.4827e-02, -1.0691e-01,
         4.2094e-01,  9.3253e-01,  7.4832e-01,  2.0421e-01,  8.0857e-01,
        -6.9927e-01,  6.9038e-01, -1.4160e-01,  2.9397e-01,  1.8431e-01,
         3.3021e-01,  3.0792e-01,  2.0105e-01,  5.4197e-01,  3.6271e-01,
         3.5776e-01,  2.2193e-01,  6.9557e-02,  8.7006e-01, -2.0600e-01,
        -3.4507e-03,  1.8695e-01,  1.0000e+00,  2.9092e-01, -2.6603e-01,
         2.7547e-01,  7.8514e-02,  1.6481e-01,  4.8246e-01,  5.4929e-01,
        -1.7485e-01,  3.9021e-02,  4.1384e-01,  5.1890e-01, -4.6565e-02,
         3.2173e-01,  1.8334e-01,  1.8262e-01,  1.4842e-01,  6.1002e-01,
         2.9751e-02,  1.2533e-01,  5.3216e-01,  2.0576e-01, -4.0360e-02,
         3.6691e-01,  8.1207e-01,  3.9485e-01,  1.3495e-01,  1.4429e-04,
         3.4063e-01,  3.1047e-02,  3.8775e-01,  2.6841e-01,  5.2225e-01,
         5.2153e-01,  5.3369e-01,  1.9615e-01,  3.0179e-01], device='cuda:0'), 'blocks.2.layer.weight': tensor([-0.0010,  0.0317,  0.4703,  ...,  0.2049, -0.0204,  0.3651],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([0.2656, 0.5401, 0.7364,  ..., 0.3438, 0.5377, 0.2040], device='cuda:0')}
topk_layer inside model after retrieval:  [1006, 307, 576, 276, 733, 972, 632, 1411, 320, 301, 350, 1284, 915, 1211, 49, 1502, 503, 1385, 272, 1265, 549, 185, 904, 734, 1413, 326, 984, 468, 764, 186, 551, 122, 969, 347, 1050, 1293, 89, 994, 1185, 697, 261, 1500, 1090, 1532, 930, 1047, 879, 1427, 1290, 519, 973, 167, 1030, 1154, 577, 1086, 786, 1453, 334, 1033, 188, 813, 1400, 294, 241, 796, 627, 1042, 1176, 619, 868, 445, 482, 135, 427, 238, 573, 70, 1239, 953, 726, 138, 1271, 985, 240, 268, 710, 1113, 1326, 1394, 823, 938, 601, 195, 205, 988, 1475, 1184, 1371, 1085, 747, 760, 1465, 617, 377, 1494, 73, 289, 737, 1409, 952, 863, 992, 660, 222, 557, 252, 43, 926, 801, 998, 159, 1493, 1251, 1263, 964, 1029, 844, 539, 1091, 1167, 1066, 177, 108, 1458, 477, 923, 766, 458, 1000, 676, 650, 310, 869, 705, 1129, 13, 269, 260, 75, 1428, 1060, 1195, 571, 562, 1214, 430, 659, 851, 854, 1344, 535, 1213, 777, 369, 187, 1242, 775, 1529, 802, 1225, 97, 779, 126, 640, 833, 174, 595, 158, 194, 1068, 946, 1503, 995, 592, 170, 812, 471, 1307, 486, 1462, 1365, 914, 373, 610, 417, 1327, 128, 1230, 133, 346, 1238, 1253, 910, 44, 1076, 950, 423, 90, 1430, 1324, 348, 1150, 1531, 193, 1412, 171, 1084, 1021, 1032, 1522, 1299, 405, 1471, 958, 1122, 580, 509, 446, 408, 715]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-0.3549,  0.1212,  0.0095,  0.1285, -0.5902, -0.7595,  0.2930, -0.1311,
        -0.6668,  0.2679,  0.4133,  0.4384,  0.0050,  0.0863,  0.9283, -0.2386,
        -0.4347,  0.0211, -0.5482, -0.5881,  0.3635,  0.7974, -0.0708,  0.0277,
         0.3060,  0.6573, -0.0664, -0.1364,  0.2152, -0.0694,  0.9545,  0.5186,
        -0.4286, -0.7605,  0.7025,  0.9891, -0.6683, -0.0308,  0.8484,  0.0912,
         0.4675,  0.1660,  0.9470, -0.3474,  0.5685, -0.6660, -0.3353,  0.5072,
         0.0084,  0.3467, -0.5717,  0.7976,  0.3361,  0.8629, -0.0436,  0.9182,
        -0.4613,  0.7530, -0.3987,  1.0000, -0.1714, -0.0323, -0.3805,  0.1154,
        -0.7352, -0.3378, -0.5991, -0.0573,  0.9557,  0.9710,  0.9160,  0.3482,
        -0.0344, -0.5976, -0.0085, -0.6056,  0.1304,  0.9179, -0.7200,  0.4949,
        -0.3991,  0.7442, -0.6611,  0.6799,  0.0952, -0.2164, -0.6674, -0.3528,
        -0.0951, -0.7665,  0.0833, -0.1890, -0.2624,  0.9089, -0.3644,  0.4514],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.4675e-01,  4.9634e-01,  4.9947e-01,  3.5167e-01,  6.8275e-01,
        -1.5499e-01,  3.8235e-01, -8.2602e-02,  9.2969e-02,  3.4503e-01,
        -6.9911e-02,  6.8859e-01,  4.4754e-01,  4.0567e-01,  5.8537e-01,
         4.8758e-01,  2.1827e-01,  5.5889e-01, -3.6301e-01, -2.1951e-01,
         2.5134e-01,  3.0956e-01,  7.9425e-01,  6.6511e-01,  7.1393e-01,
         8.6881e-01,  1.8570e-01, -8.7449e-02,  6.5183e-02,  5.4472e-01,
         2.1367e-01, -3.4635e-01, -3.3732e-01,  1.6719e-01,  1.6866e-01,
         1.5956e-01,  3.7950e-01,  3.3845e-01,  5.1705e-02,  3.7345e-01,
         4.0139e-01,  3.6994e-01,  2.8768e-01,  3.8171e-01,  7.6911e-01,
         3.2752e-01, -3.0061e-01,  3.1377e-01, -9.3224e-02, -1.2043e-01,
         4.4630e-01,  4.6442e-01, -6.7174e-02, -1.0307e-01, -8.5593e-02,
         4.1887e-01,  4.8836e-01, -2.3651e-01,  3.4604e-01,  9.2028e-02,
         1.6476e-01, -6.3855e-01,  7.0815e-01, -4.7516e-01, -1.3248e-01,
         2.0273e-01, -9.3000e-02,  8.0644e-01,  3.1913e-01, -1.4491e-02,
         1.5819e-01,  5.9817e-02,  1.0919e-01,  5.0951e-01, -3.8428e-02,
         1.5758e-01,  5.2751e-01, -4.7671e-01, -4.2823e-01,  4.0300e-01,
         1.2542e-01,  3.8350e-01,  3.7314e-01,  2.1977e-01,  6.2835e-01,
         1.7641e-01,  4.4517e-01,  6.0281e-02,  1.0899e-01,  8.3355e-01,
         2.6046e-01,  2.2819e-01,  2.6363e-01,  3.1669e-01,  7.0682e-01,
        -1.5943e-01,  5.2125e-01,  1.8858e-01, -3.3286e-01,  2.0695e-01,
         6.1562e-01,  3.3932e-01,  2.5271e-01,  1.3981e-01,  1.2687e-01,
         3.1925e-01,  1.8174e-01, -9.4531e-02,  3.6838e-02,  3.7488e-01,
         1.2717e-01,  4.7130e-01, -1.5721e-01, -7.7476e-02,  8.1014e-01,
        -1.5593e-01, -1.0054e-01,  4.5635e-01,  6.1519e-01,  1.4677e-01,
        -2.2707e-01, -5.5761e-03,  2.6145e-01,  5.9044e-01,  2.2731e-01,
        -2.6443e-02, -2.3644e-03, -1.0726e-01,  7.7391e-01,  3.4043e-01,
         2.8069e-01,  4.8368e-01,  4.9516e-01,  2.3034e-01,  6.8924e-01,
         3.1027e-01,  3.4570e-01,  9.7788e-01,  3.3127e-01,  3.5599e-01,
         5.0713e-01,  7.2506e-01,  6.5209e-01, -4.4852e-02,  4.7526e-01,
         7.1132e-01,  8.4672e-01, -7.5696e-03,  4.1974e-01,  2.1462e-01,
         1.7083e-01, -1.3488e-01,  4.1226e-01,  2.3258e-01,  2.5389e-01,
        -1.5678e-01, -5.3553e-01, -4.7307e-01,  2.7626e-01, -2.5093e-01,
         4.0266e-02,  2.4575e-01,  1.8999e-01, -6.1258e-03, -6.5438e-03,
         4.2919e-01, -1.2408e-01,  1.9037e-01,  5.1006e-01, -6.9298e-02,
         1.7477e-01,  3.5078e-01,  7.3873e-01,  5.5692e-01,  4.3353e-01,
        -2.2003e-01,  3.3680e-01,  6.3033e-01,  2.5604e-01,  3.2412e-01,
         6.3429e-02,  4.2685e-01,  3.0266e-01,  4.5454e-01,  3.0221e-02,
         4.9645e-01,  4.2592e-01,  2.2569e-01,  3.2492e-01, -1.9558e-01,
         6.3491e-01,  1.4616e-01,  2.0872e-01,  1.7695e-01,  3.4186e-01,
         7.6387e-01,  2.4154e-01,  3.4131e-01,  2.5706e-01, -1.7839e-01,
         4.7458e-01, -6.1654e-02, -4.2599e-02,  8.0520e-01,  1.1602e-01,
         4.0244e-01,  5.3733e-01,  1.0871e-01,  2.4645e-02,  7.1771e-01,
        -1.1157e-01,  2.2285e-01,  3.2628e-01,  1.0782e-01,  1.5037e-01,
         4.8782e-01,  4.4199e-01,  8.0079e-01, -1.9772e-01,  4.1425e-01,
         3.3853e-01,  7.4381e-01, -1.5043e-01,  5.4073e-02,  5.8050e-01,
        -2.5929e-02,  8.0517e-01, -3.4948e-01,  2.4886e-01,  5.5591e-01,
         1.8956e-01,  1.2973e-01,  7.4482e-02,  1.6208e-01,  5.5959e-01,
        -8.3063e-02,  7.0622e-01,  4.5518e-01,  4.7128e-01,  7.8142e-01,
        -2.2690e-02,  8.9799e-02,  1.5444e-01, -9.2526e-04,  1.1149e-01,
        -7.5147e-03,  5.6785e-01,  8.0659e-02,  5.8024e-01,  8.5778e-02,
         6.8099e-01,  2.7963e-01,  9.8910e-02, -1.4123e-01, -2.2274e-01,
         3.2634e-01,  4.8895e-01,  2.9214e-01, -1.3606e-01,  2.9738e-01,
         1.8581e-01,  1.2835e-01,  8.0524e-01,  5.1709e-01,  5.6002e-02,
         3.1847e-01, -5.6267e-01,  5.3282e-01,  4.8991e-01,  1.9676e-01,
        -2.5985e-02,  6.1749e-01,  3.1766e-01,  1.6751e-01,  4.4369e-01,
         1.5407e-01,  5.1959e-01,  4.5052e-01,  3.0881e-01,  2.6712e-02,
         1.2838e-01,  7.2508e-01,  4.6165e-01,  2.0328e-01,  2.4748e-01,
         4.8103e-01,  1.8086e-01, -7.3423e-02,  2.9468e-01, -8.2690e-02,
         7.6233e-02, -2.6612e-01,  3.9660e-01,  2.3032e-01,  4.0978e-02,
        -2.4679e-01, -1.2860e-01,  1.0342e-01,  5.5879e-01,  3.9662e-01,
        -1.4361e-01, -1.9822e-01,  5.5379e-01, -3.5044e-02,  6.5483e-01,
         4.9469e-01,  4.6768e-01,  9.9884e-02,  4.0240e-01,  8.2842e-02,
         8.1476e-02,  1.0099e-01,  6.6033e-01,  1.2213e-01,  6.6229e-01,
         6.9984e-02,  4.2777e-01,  7.7067e-01,  1.1222e-02,  1.7325e-01,
         1.4918e-01,  4.4225e-01,  7.1020e-01, -6.4827e-02, -1.0691e-01,
         4.2094e-01,  9.3253e-01,  7.4832e-01,  2.0421e-01,  8.0857e-01,
        -6.9927e-01,  6.9038e-01, -1.4160e-01,  2.9397e-01,  1.8431e-01,
         3.3021e-01,  3.0792e-01,  2.0105e-01,  5.4197e-01,  3.6271e-01,
         3.5776e-01,  2.2193e-01,  6.9557e-02,  8.7006e-01, -2.0600e-01,
        -3.4507e-03,  1.8695e-01,  1.0000e+00,  2.9092e-01, -2.6603e-01,
         2.7547e-01,  7.8514e-02,  1.6481e-01,  4.8246e-01,  5.4929e-01,
        -1.7485e-01,  3.9021e-02,  4.1384e-01,  5.1890e-01, -4.6565e-02,
         3.2173e-01,  1.8334e-01,  1.8262e-01,  1.4842e-01,  6.1002e-01,
         2.9751e-02,  1.2533e-01,  5.3216e-01,  2.0576e-01, -4.0360e-02,
         3.6691e-01,  8.1207e-01,  3.9485e-01,  1.3495e-01,  1.4429e-04,
         3.4063e-01,  3.1047e-02,  3.8775e-01,  2.6841e-01,  5.2225e-01,
         5.2153e-01,  5.3369e-01,  1.9615e-01,  3.0179e-01], device='cuda:0'), 'blocks.2.layer.weight': tensor([-0.0010,  0.0317,  0.4703,  ...,  0.2049, -0.0204,  0.3651],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([0.2656, 0.5401, 0.7364,  ..., 0.3438, 0.5377, 0.2040], device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.3.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([0.2656, 0.5401, 0.7364,  ..., 0.3438, 0.5377, 0.2040], device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-0.3549,  0.1212,  0.0095,  0.1285, -0.5902, -0.7595,  0.2930, -0.1311,
        -0.6668,  0.2679,  0.4133,  0.4384,  0.0050,  0.0863,  0.9283, -0.2386,
        -0.4347,  0.0211, -0.5482, -0.5881,  0.3635,  0.7974, -0.0708,  0.0277,
         0.3060,  0.6573, -0.0664, -0.1364,  0.2152, -0.0694,  0.9545,  0.5186,
        -0.4286, -0.7605,  0.7025,  0.9891, -0.6683, -0.0308,  0.8484,  0.0912,
         0.4675,  0.1660,  0.9470, -0.3474,  0.5685, -0.6660, -0.3353,  0.5072,
         0.0084,  0.3467, -0.5717,  0.7976,  0.3361,  0.8629, -0.0436,  0.9182,
        -0.4613,  0.7530, -0.3987,  1.0000, -0.1714, -0.0323, -0.3805,  0.1154,
        -0.7352, -0.3378, -0.5991, -0.0573,  0.9557,  0.9710,  0.9160,  0.3482,
        -0.0344, -0.5976, -0.0085, -0.6056,  0.1304,  0.9179, -0.7200,  0.4949,
        -0.3991,  0.7442, -0.6611,  0.6799,  0.0952, -0.2164, -0.6674, -0.3528,
        -0.0951, -0.7665,  0.0833, -0.1890, -0.2624,  0.9089, -0.3644,  0.4514],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.4675e-01,  4.9634e-01,  4.9947e-01,  3.5167e-01,  6.8275e-01,
        -1.5499e-01,  3.8235e-01, -8.2602e-02,  9.2969e-02,  3.4503e-01,
        -6.9911e-02,  6.8859e-01,  4.4754e-01,  4.0567e-01,  5.8537e-01,
         4.8758e-01,  2.1827e-01,  5.5889e-01, -3.6301e-01, -2.1951e-01,
         2.5134e-01,  3.0956e-01,  7.9425e-01,  6.6511e-01,  7.1393e-01,
         8.6881e-01,  1.8570e-01, -8.7449e-02,  6.5183e-02,  5.4472e-01,
         2.1367e-01, -3.4635e-01, -3.3732e-01,  1.6719e-01,  1.6866e-01,
         1.5956e-01,  3.7950e-01,  3.3845e-01,  5.1705e-02,  3.7345e-01,
         4.0139e-01,  3.6994e-01,  2.8768e-01,  3.8171e-01,  7.6911e-01,
         3.2752e-01, -3.0061e-01,  3.1377e-01, -9.3224e-02, -1.2043e-01,
         4.4630e-01,  4.6442e-01, -6.7174e-02, -1.0307e-01, -8.5593e-02,
         4.1887e-01,  4.8836e-01, -2.3651e-01,  3.4604e-01,  9.2028e-02,
         1.6476e-01, -6.3855e-01,  7.0815e-01, -4.7516e-01, -1.3248e-01,
         2.0273e-01, -9.3000e-02,  8.0644e-01,  3.1913e-01, -1.4491e-02,
         1.5819e-01,  5.9817e-02,  1.0919e-01,  5.0951e-01, -3.8428e-02,
         1.5758e-01,  5.2751e-01, -4.7671e-01, -4.2823e-01,  4.0300e-01,
         1.2542e-01,  3.8350e-01,  3.7314e-01,  2.1977e-01,  6.2835e-01,
         1.7641e-01,  4.4517e-01,  6.0281e-02,  1.0899e-01,  8.3355e-01,
         2.6046e-01,  2.2819e-01,  2.6363e-01,  3.1669e-01,  7.0682e-01,
        -1.5943e-01,  5.2125e-01,  1.8858e-01, -3.3286e-01,  2.0695e-01,
         6.1562e-01,  3.3932e-01,  2.5271e-01,  1.3981e-01,  1.2687e-01,
         3.1925e-01,  1.8174e-01, -9.4531e-02,  3.6838e-02,  3.7488e-01,
         1.2717e-01,  4.7130e-01, -1.5721e-01, -7.7476e-02,  8.1014e-01,
        -1.5593e-01, -1.0054e-01,  4.5635e-01,  6.1519e-01,  1.4677e-01,
        -2.2707e-01, -5.5761e-03,  2.6145e-01,  5.9044e-01,  2.2731e-01,
        -2.6443e-02, -2.3644e-03, -1.0726e-01,  7.7391e-01,  3.4043e-01,
         2.8069e-01,  4.8368e-01,  4.9516e-01,  2.3034e-01,  6.8924e-01,
         3.1027e-01,  3.4570e-01,  9.7788e-01,  3.3127e-01,  3.5599e-01,
         5.0713e-01,  7.2506e-01,  6.5209e-01, -4.4852e-02,  4.7526e-01,
         7.1132e-01,  8.4672e-01, -7.5696e-03,  4.1974e-01,  2.1462e-01,
         1.7083e-01, -1.3488e-01,  4.1226e-01,  2.3258e-01,  2.5389e-01,
        -1.5678e-01, -5.3553e-01, -4.7307e-01,  2.7626e-01, -2.5093e-01,
         4.0266e-02,  2.4575e-01,  1.8999e-01, -6.1258e-03, -6.5438e-03,
         4.2919e-01, -1.2408e-01,  1.9037e-01,  5.1006e-01, -6.9298e-02,
         1.7477e-01,  3.5078e-01,  7.3873e-01,  5.5692e-01,  4.3353e-01,
        -2.2003e-01,  3.3680e-01,  6.3033e-01,  2.5604e-01,  3.2412e-01,
         6.3429e-02,  4.2685e-01,  3.0266e-01,  4.5454e-01,  3.0221e-02,
         4.9645e-01,  4.2592e-01,  2.2569e-01,  3.2492e-01, -1.9558e-01,
         6.3491e-01,  1.4616e-01,  2.0872e-01,  1.7695e-01,  3.4186e-01,
         7.6387e-01,  2.4154e-01,  3.4131e-01,  2.5706e-01, -1.7839e-01,
         4.7458e-01, -6.1654e-02, -4.2599e-02,  8.0520e-01,  1.1602e-01,
         4.0244e-01,  5.3733e-01,  1.0871e-01,  2.4645e-02,  7.1771e-01,
        -1.1157e-01,  2.2285e-01,  3.2628e-01,  1.0782e-01,  1.5037e-01,
         4.8782e-01,  4.4199e-01,  8.0079e-01, -1.9772e-01,  4.1425e-01,
         3.3853e-01,  7.4381e-01, -1.5043e-01,  5.4073e-02,  5.8050e-01,
        -2.5929e-02,  8.0517e-01, -3.4948e-01,  2.4886e-01,  5.5591e-01,
         1.8956e-01,  1.2973e-01,  7.4482e-02,  1.6208e-01,  5.5959e-01,
        -8.3063e-02,  7.0622e-01,  4.5518e-01,  4.7128e-01,  7.8142e-01,
        -2.2690e-02,  8.9799e-02,  1.5444e-01, -9.2526e-04,  1.1149e-01,
        -7.5147e-03,  5.6785e-01,  8.0659e-02,  5.8024e-01,  8.5778e-02,
         6.8099e-01,  2.7963e-01,  9.8910e-02, -1.4123e-01, -2.2274e-01,
         3.2634e-01,  4.8895e-01,  2.9214e-01, -1.3606e-01,  2.9738e-01,
         1.8581e-01,  1.2835e-01,  8.0524e-01,  5.1709e-01,  5.6002e-02,
         3.1847e-01, -5.6267e-01,  5.3282e-01,  4.8991e-01,  1.9676e-01,
        -2.5985e-02,  6.1749e-01,  3.1766e-01,  1.6751e-01,  4.4369e-01,
         1.5407e-01,  5.1959e-01,  4.5052e-01,  3.0881e-01,  2.6712e-02,
         1.2838e-01,  7.2508e-01,  4.6165e-01,  2.0328e-01,  2.4748e-01,
         4.8103e-01,  1.8086e-01, -7.3423e-02,  2.9468e-01, -8.2690e-02,
         7.6233e-02, -2.6612e-01,  3.9660e-01,  2.3032e-01,  4.0978e-02,
        -2.4679e-01, -1.2860e-01,  1.0342e-01,  5.5879e-01,  3.9662e-01,
        -1.4361e-01, -1.9822e-01,  5.5379e-01, -3.5044e-02,  6.5483e-01,
         4.9469e-01,  4.6768e-01,  9.9884e-02,  4.0240e-01,  8.2842e-02,
         8.1476e-02,  1.0099e-01,  6.6033e-01,  1.2213e-01,  6.6229e-01,
         6.9984e-02,  4.2777e-01,  7.7067e-01,  1.1222e-02,  1.7325e-01,
         1.4918e-01,  4.4225e-01,  7.1020e-01, -6.4827e-02, -1.0691e-01,
         4.2094e-01,  9.3253e-01,  7.4832e-01,  2.0421e-01,  8.0857e-01,
        -6.9927e-01,  6.9038e-01, -1.4160e-01,  2.9397e-01,  1.8431e-01,
         3.3021e-01,  3.0792e-01,  2.0105e-01,  5.4197e-01,  3.6271e-01,
         3.5776e-01,  2.2193e-01,  6.9557e-02,  8.7006e-01, -2.0600e-01,
        -3.4507e-03,  1.8695e-01,  1.0000e+00,  2.9092e-01, -2.6603e-01,
         2.7547e-01,  7.8514e-02,  1.6481e-01,  4.8246e-01,  5.4929e-01,
        -1.7485e-01,  3.9021e-02,  4.1384e-01,  5.1890e-01, -4.6565e-02,
         3.2173e-01,  1.8334e-01,  1.8262e-01,  1.4842e-01,  6.1002e-01,
         2.9751e-02,  1.2533e-01,  5.3216e-01,  2.0576e-01, -4.0360e-02,
         3.6691e-01,  8.1207e-01,  3.9485e-01,  1.3495e-01,  1.4429e-04,
         3.4063e-01,  3.1047e-02,  3.8775e-01,  2.6841e-01,  5.2225e-01,
         5.2153e-01,  5.3369e-01,  1.9615e-01,  3.0179e-01], device='cuda:0'), 'blocks.2.layer.weight': tensor([-0.0010,  0.0317,  0.4703,  ...,  0.2049, -0.0204,  0.3651],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([0.2656, 0.5401, 0.7364,  ..., 0.3438, 0.5377, 0.2040], device='cuda:0')}
topk_layer inside model after retrieval:  [5017, 6047, 1653, 3916, 170, 5041, 2613, 4323, 6098, 2691, 5433, 3067, 4958, 4900, 3563, 1730, 62, 2026, 3435, 2075, 2778, 5255, 4087, 4574, 4983, 634, 5795, 1366, 948, 4473, 4359, 3906, 547, 750, 1229, 5651, 4738, 3197, 5004, 785, 514, 1599, 4502, 2051, 5968, 3718, 1680, 5558, 3938, 353, 5984, 5349, 4768, 815, 5956, 1310, 3516, 4415, 2483, 5292, 2021, 4333, 5515, 4090, 3988, 1149, 2470, 1856, 2392, 549, 4350, 1022, 1921, 999, 4682, 3986, 2462, 921, 3227, 265, 706, 1055, 5541, 4429, 680, 3972, 3828, 1635, 5175, 1587, 1685, 2966, 2696, 1318, 4262, 2037, 4163, 4746, 4699, 2299, 3481, 2398, 3740, 4029, 2413, 5314, 3181, 2804, 837, 3511, 5918, 1575, 3807, 1983, 1778, 63, 3486, 51, 3669, 4924, 1526, 817, 3179, 5498, 6, 1385, 1066, 534, 5733, 3210, 1086, 4357, 2176, 1331, 1670, 4408, 117, 2344, 1181, 3222, 1911, 2427, 2979, 1304, 5752, 2453, 5283, 5188, 745, 5591, 4968, 1344, 3603, 261, 1585, 5, 2786, 5406, 8, 300, 2300, 4905, 2714, 3431, 2661, 5571, 4360, 4556, 3750, 771, 211, 3888, 4486, 813, 2049, 4025, 6114, 1294, 845, 3414, 5312, 5001, 2404, 2769, 4094, 4624, 5065, 4496, 455, 3090, 6070, 3101, 1760, 1037, 1913, 1831, 1036, 1117, 3449, 5919, 418, 5455, 5299, 485, 905, 4733, 5091, 3959, 4739, 183, 2078, 1009, 1043, 4134, 2085, 5717, 3543, 3430, 552, 3830, 1512, 1822, 2914, 5705, 822, 3983, 5860, 688, 4687, 6134, 2201, 5222, 1342, 784, 1112, 1198, 2938, 2041, 4862, 3321, 5579, 5931, 526, 2608, 1872, 1163, 2780, 4218, 4256, 872, 122, 4396, 551, 1437, 5473, 4513, 2584, 1883, 3495, 2136, 5477, 5977, 3159, 1703, 1118, 4538, 1346, 2698, 4397, 3941, 2047, 5531, 2488, 3581, 132, 339, 3502, 2850, 1792, 2418, 5346, 2354, 6054, 3001, 4227, 1790, 4598, 2056, 4100, 3725, 909, 3909, 4451, 4072, 4660, 2635, 1102, 3656, 2581, 176, 1001, 2152, 2444, 3448, 685, 497, 2920, 652, 621, 4390, 3186, 4221, 1097, 2069, 4177, 1620, 4118, 4377, 178, 1615, 1950, 3010, 2092, 2793, 4694, 2103, 5834, 6004, 3905, 632, 844, 4618, 4131, 4063, 1403, 583, 1351, 5469, 1753, 1514, 4602, 4173, 2835, 2750, 3215, 2274, 5457, 3507, 5482, 5981, 3062, 5135, 2847, 2010, 4223, 2662, 3824, 2447, 5847, 165, 5276, 2210, 5493, 129, 198, 5780, 4853, 1544, 2511, 1806, 5910, 4382, 956, 2426, 1443, 1622, 4906, 5146, 4159, 5214, 668, 4876, 1284, 194, 227, 2980, 1187, 6090, 49, 4036, 436, 3470, 6058, 3488, 1594, 5322, 1283, 1864, 4539, 860, 3420, 3138, 818, 1861, 257, 2700, 2139, 2695, 3351, 2379, 4544, 1176, 2342, 2128, 1343, 2514, 4394, 686, 5212, 3895, 3540, 4110, 109, 3312, 893, 5542, 6053, 3473, 4301, 5196, 4937, 1337, 5407, 4219, 2972, 208, 2812, 4266, 4646, 5027, 4938, 3719, 1075, 295, 5436, 78, 5243, 4804, 638, 1564, 1894, 4980, 5929, 5853, 3640, 4964, 5821, 5297, 5201, 398, 5909, 59, 2542, 495, 6119, 4097, 2292, 1560, 2666, 2627, 1873, 3912, 4636, 3436, 26, 1453, 1394, 4947, 5216, 4089, 4166, 1415, 4954, 2083, 1834, 4212, 3809, 4635, 4102, 3866, 2533, 5838, 5412, 221, 6116, 2367, 3944, 1988, 1132, 1380, 1454, 4264, 1499, 5786, 1316, 4306, 3584, 171, 2675, 2667, 5205, 2530, 5108, 5045, 3793, 2703, 1684, 4421, 1191, 294, 2008, 41, 3760, 4280, 2923, 13, 4651, 1309, 1185, 1435, 6122, 2945, 2755, 4114, 1371, 2808, 4319, 2679, 5520, 4897, 6142, 2089, 2563, 5630, 292, 3151, 2668, 4553, 2927, 2156, 5076, 3193, 313, 3670, 3696, 709, 252, 2779, 1108, 1312, 2236, 2098, 2058, 772, 4257, 3330, 5791, 2957, 3569, 65, 5056, 3602, 5739, 1641, 5318, 722, 5445, 3755, 24, 5396, 675, 760, 3873, 673, 926, 3017, 1168, 5354, 4546, 2314, 5607, 259, 3658, 1608, 2007, 4289, 4777, 888, 3434, 4736, 4445, 4107, 1164, 2173, 4431, 576, 3802, 4120, 1879, 3191, 4220, 4076, 4961, 5839, 3150, 5206, 3377, 3096, 255, 5087, 2048, 2393, 1956, 3039, 1162, 4603, 5161, 5268, 491, 2580, 5423, 795, 2597, 463, 5043, 4236, 4291, 1230, 1882, 1978, 1511, 1866, 4728, 5942, 1362, 4879, 2348, 2845, 796, 5582, 4919, 2834, 4402, 6109, 1634, 3735, 6008, 3914, 1960, 5172, 625, 4270, 3556, 4541, 2976, 6089, 2860, 2962, 2082, 3479, 5596, 4688, 5321, 4762, 2711, 3775, 3553, 2251, 4885, 2887, 3657, 1096, 5401, 4640, 5921, 5204, 2033, 2206, 2802, 4412, 2928, 441, 1578, 951, 287, 1357, 3761, 4176, 2101, 14, 4299, 2310, 5936, 2073, 2538, 2077, 4628, 4104, 870, 2318, 4447, 2227, 5932, 2736, 5386, 4162, 2918, 5164, 2859, 3804, 1161, 6102, 4881, 246, 5503, 707, 4840, 2917, 1941, 1836, 2607, 763, 5454, 2759, 1115, 530, 1936, 426, 1755, 980, 620, 846, 1949, 1537, 1846, 865, 223, 3686, 4785, 5697, 1021, 4028, 1887, 2100, 2268, 4449, 3133, 3538, 5626, 826, 5008, 1569, 1457, 780, 1194, 2889, 5103, 229, 2460, 2798, 3918, 5680, 89, 493, 5818, 4658, 3594, 2445, 1151, 1535, 1273, 1169, 1422, 1264, 4239, 1543, 834, 5015, 2203, 3425, 4365, 6125, 3170, 1368, 4752, 1820, 3834, 2680, 2112, 4778, 3965, 5116, 1028, 3027, 2715, 4300, 2443, 1574, 2519, 3947, 1649, 144, 1088, 5854, 2792, 5885, 2382, 3163, 1912, 765, 3023, 4013, 2022, 527, 5980, 4482, 3636, 3283, 2067, 2106, 2071, 3242, 1999, 2449, 5893, 1376, 3025, 966, 1204, 5245, 1470, 4185, 4410, 5693, 4614, 3943, 4034, 3384, 4460, 5199, 3759, 1116, 3902, 2861, 3836, 2585, 179, 1697, 5994, 1702, 2817, 4519, 4330, 4480, 4882, 555, 5081, 1279, 3429, 545, 4154, 590, 4135, 5887, 970, 363, 3032, 2419, 4956, 5642, 1345, 1845, 1438, 4775, 6096, 878, 1600, 425, 1335, 1914, 2016, 5523, 3277, 4051, 6021, 1980, 5862, 4566, 450, 3350, 175, 1665, 1969, 524, 1095, 4363, 3118, 3097, 4669, 5460, 3126, 4242, 5702, 5552, 52, 3733, 4142, 2547, 1839, 271, 674, 511, 4248]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
The device used will be: 
True
cuda:0
The device used will be: 
True
cuda:0
chosen_head:  {'blocks.4.layer.bias': tensor([ 0.0099, -0.0016, -0.0140, -0.0056,  0.0170,  0.0083,  0.0102,  0.0137,
        -0.0154,  0.0123], device='cuda:0'), 'blocks.4.layer.weight': tensor([[ 0.1073, -0.0009,  0.0072,  ...,  0.2254,  0.0617,  0.0462],
        [-0.0917, -0.0167, -0.0427,  ..., -0.0005,  0.0644,  0.1190],
        [-0.0364, -0.1114,  0.0774,  ..., -0.0271, -0.0134, -0.0558],
        ...,
        [-0.1048,  0.0533,  0.1243,  ..., -0.0253,  0.0444, -0.1016],
        [-0.0325,  0.1420,  0.0787,  ..., -0.0134,  0.0540,  0.0946],
        [-0.0129, -0.1319, -0.2293,  ..., -0.1601, -0.1963, -0.1403]],
       device='cuda:0')}

 Model STL10_CIFAR10_CLbe1cb3cb-b5f8-4a9d-bda0-16cc8cbeda29 loaded successfuly with best perf



 ----- Architecture Block BatchNorm2dSK3962(5, 5)1.0reflect, number 0 -----
- BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- AvgPool2d(kernel_size=4, stride=2, padding=1)

 ----- Architecture Block BatchNorm2dSK963842(3, 3)0.6499999761581421reflect, number 1 -----
- BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK38415362(3, 3)0.44999998807907104reflect, number 2 -----
- BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.4)
- AvgPool2d(kernel_size=4, stride=2, padding=1)

 ----- Architecture Block BatchNorm2dSK153661442(3, 3)0.25reflect, number 3 -----
- BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 4 -----
- Flatten(start_dim=1, end_dim=-1)
- Dropout(p=0.5, inplace=False)
- Linear(in_features=221184, out_features=10, bias=True)
model.heads:  [{'blocks.4.layer.bias': tensor([ 0.0016, -0.0015,  0.0004, -0.0005,  0.0020,  0.0031,  0.0054, -0.0016,
         0.0026,  0.0032], device='cuda:0'), 'blocks.4.layer.weight': tensor([[ 0.0400,  0.0110,  0.0159,  ..., -0.0202, -0.0415, -0.0304],
        [ 0.0107, -0.0007,  0.0315,  ..., -0.0158,  0.0113,  0.0158],
        [ 0.0364, -0.0160,  0.0407,  ...,  0.0141,  0.0031,  0.0035],
        ...,
        [ 0.0219,  0.0111,  0.0037,  ...,  0.0494,  0.0133, -0.0125],
        [-0.0186,  0.0274, -0.0178,  ..., -0.0034, -0.0105,  0.0038],
        [-0.0212, -0.0315, -0.0387,  ..., -0.0497,  0.0010, -0.0065]],
       device='cuda:0')}, {'blocks.4.layer.bias': tensor([ 0.0099, -0.0016, -0.0140, -0.0056,  0.0170,  0.0083,  0.0102,  0.0137,
        -0.0154,  0.0123], device='cuda:0'), 'blocks.4.layer.weight': tensor([[ 0.1073, -0.0009,  0.0072,  ...,  0.2254,  0.0617,  0.0462],
        [-0.0917, -0.0167, -0.0427,  ..., -0.0005,  0.0644,  0.1190],
        [-0.0364, -0.1114,  0.0774,  ..., -0.0271, -0.0134, -0.0558],
        ...,
        [-0.1048,  0.0533,  0.1243,  ..., -0.0253,  0.0444, -0.1016],
        [-0.0325,  0.1420,  0.0787,  ..., -0.0134,  0.0540,  0.0946],
        [-0.0129, -0.1319, -0.2293,  ..., -0.1601, -0.1963, -0.1403]],
       device='cuda:0')}]
[Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=1.4)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=221184, out_features=10, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)]
LAYER NAME:  Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=1.4)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=221184, out_features=10, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)
LAYER CHILDREN:  [BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
  (activation): Triangle(power=1.4)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1)
), BasicBlock(
  (operations): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
  )
  (layer): Linear(in_features=221184, out_features=10, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)]
subsubl NAME:  Sequential(
  (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=4, stride=2, padding=1)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=4, stride=2, padding=1)
subsubl NAME:  Triangle(power=1.4)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1)
subsubl NAME:  Sequential(
  (0): Flatten(start_dim=1, end_dim=-1)
)
subsubl NAME:  Linear(in_features=221184, out_features=10, bias=True)
subsubl NAME:  Dropout(p=0.5, inplace=False)
CONFIG MODE:  unsupervised
CONFIG MODE:  supervised
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
INDICES:  5000
IMAGE SIZE: torch.Size([64, 3, 96, 96])
The device used will be: 
True
cuda:0
tot_sum:  tensor(217149.9688, device='cuda:0') 0
The device used will be: 
True
cuda:0
tot_sum:  tensor(533084.3125, device='cuda:0') 1
max_key : 1
Accuracy of the network on the 1st dataset: 21.325 %
Test loss on the 1st dataset: 48.583
results:  {'count': 3, 'R0': {'train_loss': 0.03978398069739342, 'train_acc': 98.27600121498108, 'test_loss': 3.0680172443389893, 'test_acc': 66.5625, 'convergence': 23.72207260131836, 'R1': 1, 'dataset_sup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 5000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 4, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': True, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 100, 'print_freq': 10, 'validation': False, 'continual_learning': False}, 'dataset_unsup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 100000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 32, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'unlabeled', 'nb_epoch': 1, 'print_freq': 25, 'validation': False, 'continual_learning': False}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 32, 'nb_epoch': 1, 'print_freq': 25, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 100, 'print_freq': 10, 'lr': 0.001}}}, 'model_config': {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.036828478186799345, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 221184, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}, 'R1': {'train_loss': 0.12020615488290787, 'train_acc': 97.23520278930664, 'test_loss': 0.14057114720344543, 'test_acc': 81.9000015258789, 'convergence': 17.5535945892334, 'R1': 49, 'dataset_sup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 96}, 'dataset_unsup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 96}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 10, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}}, 'cl_hyper': {'training_mode': 'consecutive', 'cf_sol': False, 'head_sol': True, 'top_k': 0.15, 'topk_lock': False, 'high_lr': 0.15, 'low_lr': 1.0, 't_criteria': 'activations', 'delta_w_interval': 50.0, 'heads_basis_t': 0.8595000076293946, 'n_tasks': 2}, 'eval_0': {'test_loss': 48.58317947387695, 'test_acc': 21.325000762939453, 'convergence': 24.0014705657959, 'R1': 0, 'dataset_sup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 5000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 4, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': True, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 100, 'print_freq': 10, 'validation': False, 'continual_learning': False}, 'dataset_unsup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 100000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 32, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'unlabeled', 'nb_epoch': 1, 'print_freq': 25, 'validation': False, 'continual_learning': False}}, 'model_name': 'STL10_CIFAR10_CLbe1cb3cb-b5f8-4a9d-bda0-16cc8cbeda29'}
MULTD_CL_STL10__1_CIFAR10/STL10_CIFAR10_CLbe1cb3cb-b5f8-4a9d-bda0-16cc8cbeda29.json

The device used will be: 
True
cuda:0
BLOCKS:  {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.25, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10}, 'pool': None, 'activation': None}}
[[0,3],[5,7]]


 ########### WARNING ############


 Invalid combination of parameters, provide either: [--classes, --dataset-sup, --dataset-unsup] or [--dataset-sup-1, --dataset-unsup-1, --dataset-sup-2, --dataset-unsup-2]
The continual learning is implemented per tasks where each task is made up of different classes 
 of the same dataset, so only one dataset will be considered.


 ################################


{'training_mode': 'consecutive', 'cf_sol': True, 'head_sol': False, 'top_k': 0.15, 'topk_lock': False, 'high_lr': 0.15, 'low_lr': 1.0, 't_criteria': 'activations', 'delta_w_interval': 50.0, 'heads_basis_t': 0.9, 'n_tasks': 2}
CL:  True
{'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 5000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': True, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 100, 'print_freq': 10, 'validation': False, 'continual_learning': True}
CL:  False
{'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 5000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': True, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 100, 'print_freq': 10, 'validation': False, 'continual_learning': False}
CL:  False
{'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 100000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 32, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'unlabeled', 'nb_epoch': 1, 'print_freq': 25, 'validation': False, 'continual_learning': False}
SEED:  0
block 0, size : 96 48 48
96
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.25, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10}, 'pool': None, 'activation': None}}
range = 2.886751345948129
block 1, size : 384 24 24
384
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.25, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10}, 'pool': None, 'activation': None}}
range = 0.8505172717997146
block 2, size : 1536 12 12
1536
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.25, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10}, 'pool': None, 'activation': None}}
range = 0.4252586358998573
block 3, size : 6144 6 6
6144
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.25, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10}, 'pool': None, 'activation': None}}
range = 0.21262931794992865
NOWWWW
6 6
10
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.25, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 221184, 'old_channels': 6144}, 'pool': None, 'activation': None}}
blocks['b%s' % 4] 221184 221184
range = 0.036828478186799345
train_layer_order: consecutive {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 32, 'nb_epoch': 1, 'print_freq': 25, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 100, 'print_freq': 10, 'lr': 0.001}}
The device used will be: 
True
cuda:0
avg_deltas:  {}
topk_layer:  None
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}

 ----- Architecture Block BatchNorm2dSK3962(5, 5)1.0reflect, number 0 -----
- BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- AvgPool2d(kernel_size=4, stride=2, padding=1)

 ----- Architecture Block BatchNorm2dSK963842(3, 3)0.6499999761581421reflect, number 1 -----
- BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK38415362(3, 3)0.44999998807907104reflect, number 2 -----
- BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.4)
- AvgPool2d(kernel_size=4, stride=2, padding=1)

 ----- Architecture Block BatchNorm2dSK153661442(3, 3)0.25reflect, number 3 -----
- BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 4 -----
- Flatten(start_dim=1, end_dim=-1)
- Dropout(p=0.5, inplace=False)
- Linear(in_features=221184, out_features=10, bias=True)
model.heads:  []
[Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=1.4)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=221184, out_features=10, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)]
LAYER NAME:  Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=1.4)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=221184, out_features=10, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)
LAYER CHILDREN:  [BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
  (activation): Triangle(power=1.4)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1)
), BasicBlock(
  (operations): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
  )
  (layer): Linear(in_features=221184, out_features=10, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)]
subsubl NAME:  Sequential(
  (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=4, stride=2, padding=1)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=4, stride=2, padding=1)
subsubl NAME:  Triangle(power=1.4)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1)
subsubl NAME:  Sequential(
  (0): Flatten(start_dim=1, end_dim=-1)
)
subsubl NAME:  Linear(in_features=221184, out_features=10, bias=True)
subsubl NAME:  Dropout(p=0.5, inplace=False)
CONFIG MODE:  unsupervised

 ********** Hebbian Unsupervised learning of blocks [0, 1, 2, 3] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
INDICES:  5000
IMAGE SIZE: torch.Size([32, 3, 96, 96])
SAVING FOLDER FOR UNSUP:  STL10_CIFAR10_CL3f610ba2-1bdb-42f9-b463-ea87fb3856be
prev_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
activations_sum.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
DEPTH:  4
WTA IN delta_weight:  tensor([[[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         ...,
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00]],

        [[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         ...,
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00]],

        [[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         ...,
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00]],

        ...,

        [[-9.4754e-16, -9.4754e-16, -9.4754e-16,  ..., -9.4754e-16,
          -9.4754e-16, -9.4754e-16],
         [-9.4754e-16, -9.4754e-16, -9.4754e-16,  ..., -9.4754e-16,
          -9.4754e-16, -9.4754e-16],
         [-9.4754e-16, -9.4754e-16, -9.4754e-16,  ..., -9.4754e-16,
          -9.4754e-16, -9.4754e-16],
         ...,
         [-9.4754e-16, -9.4754e-16, -9.4754e-16,  ..., -9.4754e-16,
          -9.4754e-16, -9.4754e-16],
         [-9.4754e-16, -9.4754e-16, -9.4754e-16,  ..., -9.4754e-16,
          -9.4754e-16, -9.4754e-16],
         [-9.4754e-16, -9.4754e-16, -9.4754e-16,  ..., -9.4754e-16,
          -9.4754e-16, -9.4754e-16]],

        [[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         ...,
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00]],

        [[-5.5351e-43, -5.5351e-43, -5.5351e-43,  ..., -5.5351e-43,
          -5.5351e-43, -5.5351e-43],
         [-5.5351e-43, -5.5351e-43, -5.5351e-43,  ..., -5.5351e-43,
          -5.5351e-43, -5.5351e-43],
         [-5.5351e-43, -5.5351e-43, -5.5351e-43,  ..., -5.5351e-43,
          -5.5351e-43, -5.5351e-43],
         ...,
         [-5.5351e-43, -5.5351e-43, -5.5351e-43,  ..., -5.5351e-43,
          -5.5351e-43, -5.5351e-43],
         [-5.5351e-43, -5.5351e-43, -5.5351e-43,  ..., -5.5351e-43,
          -5.5351e-43, -5.5351e-43],
         [-5.5351e-43, -5.5351e-43, -5.5351e-43,  ..., -5.5351e-43,
          -5.5351e-43, -5.5351e-43]]], device='cuda:0')
self.avg_deltas_layer:  <class 'NoneType'>
self.topk_layer:  <class 'NoneType'>
self.avg_deltas_layer:  <class 'NoneType'>
self.topk_layer:  <class 'NoneType'>
self.avg_deltas_layer:  <class 'NoneType'>
self.topk_layer:  <class 'NoneType'>
self.avg_deltas_layer:  <class 'NoneType'>
self.topk_layer:  <class 'NoneType'>
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
activations_sum[k] 96
activations_sum[k]:  [68, 25, 59, 30, 77, 93, 69, 55, 35, 14]
topk_kernels len:  1
topk_kernels keys:  ['conv0']
topk_kernels:  {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38]}
activations_sum[k] len:  96
activations_sum[k] 384
activations_sum[k]:  [345, 365, 75, 354, 190, 100, 56, 180, 191, 105]
topk_kernels len:  2
topk_kernels keys:  ['conv0', 'conv1']
topk_kernels:  {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286]}
activations_sum[k] len:  384
activations_sum[k] 1536
activations_sum[k]:  [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35]
topk_kernels len:  3
topk_kernels keys:  ['conv0', 'conv1', 'conv2']
topk_kernels:  {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276]}
activations_sum[k] len:  1536
activations_sum[k] 6144
activations_sum[k]:  [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822]
topk_kernels len:  4
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3']
topk_kernels:  {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
activations_sum[k] len:  6144
['conv0', 'conv1', 'conv2', 'conv3']
delta_weights INFO: 
NUM OF TRACKED COV LAYERS:  4
NUM OF TRACKED WEIGHTS CHANGES PER LAYER:  4
avg_deltas INFO:  <class 'dict'>
avg_deltas keys:  ['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight']
################################################
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 4.52e-02	time: 00:00:37	Acc_train 0.00	Acc_test 0.00	convergence: 2.37e+01	R1: 1	Info MB:0.000e+00/SB:0.000e+00/MW:4.460e-06/SW:2.112e-01/MR:2.472e+01/SR:2.331e+00/MeD:1.064e+00/MaD:2.372e+01/MW:0.543/MAW:0.457
|       0 |       1 |        2 |        3 |        4 |        5 |        6 |        7 |        8 |        9 |       10 |       11 |       12 |       13 |       14 |       15 |       16 |       17 |       18 |       19 |       20 |       21 |       22 |       23 |       24 |       25 |       26 |      27 |       28 |       29 |
|---------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------|
|   0.049 |   0.049 |   0.0496 |   0.0495 |   0.0494 |   0.0489 |   0.0501 |   0.0491 |   0.0384 |   0.0488 |   0.0495 |   0.0491 |   0.0489 |   0.0495 |   0.0484 |   0.0497 |   0.0494 |   0.0495 |   0.0492 |   0.0493 |   0.0492 |   0.0463 |   0.0491 |   0.0487 |   0.0484 |   0.0491 |   0.0499 |   0.049 |   0.0495 |   0.0496 |
|  24.97  |  24.96  |  25.6    |  25.52   |  25.43   |  24.9    |  26.1    |  25.07   |  15.72   |  24.85   |  25.52   |  25.09   |  24.91   |  25.5    |  24.4    |  25.67   |  25.36   |  25.47   |  25.17   |  25.29   |  25.17   |  22.48   |  25.11   |  24.76   |  24.47   |  25.11   |  25.91   |  25.05  |  25.49   |  25.62   |
|   0.01  |   0.01  |   0      |   0      |   0.01   |   0.02   |   0.01   |   0      |   0.12   |   0      |   0.01   |   0.01   |   0      |   0      |   0.02   |   0      |   0      |   0      |   0      |   0      |   0      |   0.04   |   0      |   0.01   |   0.01   |   0.01   |   0.02   |   0     |   0.01   |   0      |
| nan     | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      |
| nan     | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      |
| nan     | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:1.522e-04/SW:3.711e-01/MR:2.156e+01/SR:3.332e+00/MeD:2.742e+00/MaD:1.700e+01/MW:0.596/MAW:0.404
|        0 |        1 |        2 |        3 |        4 |        5 |        6 |        7 |        8 |       9 |       10 |       11 |       12 |       13 |       14 |       15 |       16 |       17 |       18 |       19 |      20 |       21 |       22 |       23 |       24 |       25 |       26 |       27 |       28 |       29 |
|----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------|
|   0.0491 |   0.0483 |   0.0451 |   0.0462 |   0.0428 |   0.0428 |   0.0492 |   0.0405 |   0.0453 |   0.049 |   0.0398 |   0.0475 |   0.0434 |   0.0357 |   0.0474 |   0.0448 |   0.0387 |   0.0497 |   0.0469 |   0.0492 |   0.041 |   0.0467 |   0.0404 |   0.0478 |   0.0432 |   0.0429 |   0.0488 |   0.0457 |   0.0397 |   0.0486 |
|  25.08   |  24.36   |  21.36   |  22.34   |  19.33   |  19.35   |  25.22   |  17.37   |  21.51   |  25.05  |  16.81   |  23.55   |  19.85   |  13.75   |  23.48   |  21.08   |  16.01   |  25.68   |  22.97   |  25.2    |  17.82  |  22.81   |  17.32   |  23.87   |  19.69   |  19.39   |  24.77   |  21.84   |  16.78   |  24.63   |
|   0      |   0.01   |   0.06   |   0.06   |   0.06   |   0.1    |   0      |   0.07   |   0.06   |   0     |   0.1    |   0.03   |   0.07   |   0.21   |   0.03   |   0.04   |   0.22   |   0      |   0.02   |   0      |   0.14  |   0.03   |   0.2    |   0.02   |   0.05   |   0.05   |   0      |   0.04   |   0.11   |   0      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:-3.013e-03/SW:7.925e-01/MR:2.315e+01/SR:2.575e+00/MeD:1.984e+00/MaD:1.146e+01/MW:0.695/MAW:0.305
|        0 |        1 |        2 |        3 |        4 |        5 |        6 |        7 |        8 |        9 |       10 |       11 |       12 |       13 |       14 |       15 |       16 |       17 |       18 |       19 |       20 |       21 |       22 |       23 |       24 |       25 |       26 |      27 |      28 |       29 |
|----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+---------+----------|
|   0.0245 |   0.0246 |   0.0242 |   0.0241 |   0.0199 |   0.0235 |   0.0244 |   0.0249 |   0.0244 |   0.0234 |   0.0248 |   0.0239 |   0.0242 |   0.0242 |   0.0242 |   0.0241 |   0.0241 |   0.0219 |   0.0243 |   0.0237 |   0.0231 |   0.0248 |   0.0244 |   0.0247 |   0.0233 |   0.0226 |   0.0213 |   0.024 |   0.024 |   0.0251 |
|  25.05   |  25.12   |  24.39   |  24.27   |  16.82   |  23.03   |  24.87   |  25.72   |  24.84   |  22.85   |  25.62   |  23.77   |  24.33   |  24.47   |  24.36   |  24.18   |  24.22   |  20.12   |  24.69   |  23.54   |  22.28   |  25.7    |  24.78   |  25.37   |  22.75   |  21.47   |  19.17   |  24.11  |  24.04  |  26.12   |
|   0.04   |   0      |   0.02   |   0.01   |   1.6    |   0.19   |   0.02   |   0.05   |   0.01   |   0.2    |   0      |   0.07   |   0.16   |   0.12   |   0.02   |   0.06   |   0      |   0.66   |   0.01   |   0.16   |   0.16   |   0      |   0.12   |   0.11   |   0.28   |   0.49   |   1.15   |   0.03  |   0.07  |   0.02   |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan     | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan     | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan     | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:-1.311e-02/SW:1.508e+00/MR:1.280e+01/SR:2.610e+00/MeD:2.095e+00/MaD:7.090e+00/MW:0.547/MAW:0.453
|       0 |      1 |       2 |      3 |      4 |       5 |       6 |       7 |       8 |       9 |      10 |      11 |      12 |      13 |      14 |      15 |      16 |      17 |      18 |     19 |      20 |     21 |      22 |      23 |      24 |      25 |      26 |      27 |      28 |      29 |
|---------+--------+---------+--------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------|
|   0.256 |   0.33 |   0.295 |   0.31 |   0.24 |   0.219 |   0.277 |   0.329 |   0.245 |   0.348 |   0.254 |   0.272 |   0.289 |   0.305 |   0.269 |   0.309 |   0.203 |   0.276 |   0.275 |   0.26 |   0.303 |   0.27 |   0.287 |   0.324 |   0.305 |   0.236 |   0.303 |   0.292 |   0.276 |   0.328 |
|  11.22  |  17.99 |  14.58  |  16.02 |  10.03 |   8.5   |  12.95  |  17.88  |  10.42  |  19.89  |  11.08  |  12.58  |  14.04  |  15.56  |  12.34  |  15.91  |   7.44  |  12.89  |  12.84  |  11.56 |  15.33  |  12.4  |  13.85  |  17.37  |  15.56  |   9.67  |  15.39  |  14.35  |  12.92  |  17.79  |
|   0.64  |   0.18 |   0.17  |   0.26 |   1.98 |   3.7   |   0.76  |   0.33  |   1.3   |   0.13  |   1.05  |   0.49  |   0.54  |   0.25  |   1.82  |   0.22  |   3.77  |   0.5   |   0.66  |   1.12 |   0.45  |   0.84 |   0.42  |   0.27  |   0.39  |   2.91  |   0.37  |   0.58  |   0.34  |   0.18  |
| nan     | nan    | nan     | nan    | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     |
| nan     | nan    | nan     | nan    | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     |
| nan     | nan    | nan     | nan    | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL3f610ba2-1bdb-42f9-b463-ea87fb3856be/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
CONFIG MODE:  supervised

 ********** Supervised learning of blocks [4] **********
SAVING FOLDER FOR SUP:  STL10_CIFAR10_CL3f610ba2-1bdb-42f9-b463-ea87fb3856be
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
INDICES:  5000
IMAGE SIZE: torch.Size([64, 3, 96, 96])
num_blocks:  5
Epoch: [1/100]	lr: 1.00e-03	time: 00:01:05	Loss_train 7.66232	Acc_train 36.62	/	Loss_test 3.46408	Acc_test 46.84
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL3f610ba2-1bdb-42f9-b463-ea87fb3856be/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [10/100]	lr: 1.00e-03	time: 00:01:45	Loss_train 2.96399	Acc_train 61.89	/	Loss_test 4.18236	Acc_test 58.64
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL3f610ba2-1bdb-42f9-b463-ea87fb3856be/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [20/100]	lr: 1.00e-03	time: 00:02:29	Loss_train 1.76923	Acc_train 76.53	/	Loss_test 5.84089	Acc_test 55.47
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL3f610ba2-1bdb-42f9-b463-ea87fb3856be/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [30/100]	lr: 5.00e-04	time: 00:03:12	Loss_train 0.56039	Acc_train 88.94	/	Loss_test 3.81810	Acc_test 62.55
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL3f610ba2-1bdb-42f9-b463-ea87fb3856be/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [40/100]	lr: 2.50e-04	time: 00:03:56	Loss_train 0.33715	Acc_train 92.10	/	Loss_test 3.39348	Acc_test 64.84
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL3f610ba2-1bdb-42f9-b463-ea87fb3856be/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [50/100]	lr: 2.50e-04	time: 00:04:39	Loss_train 0.18524	Acc_train 94.79	/	Loss_test 3.65853	Acc_test 64.54
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL3f610ba2-1bdb-42f9-b463-ea87fb3856be/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [60/100]	lr: 1.25e-04	time: 00:05:23	Loss_train 0.10238	Acc_train 96.51	/	Loss_test 3.27746	Acc_test 65.28
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL3f610ba2-1bdb-42f9-b463-ea87fb3856be/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [70/100]	lr: 6.25e-05	time: 00:06:06	Loss_train 0.06835	Acc_train 97.37	/	Loss_test 3.15753	Acc_test 66.12
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL3f610ba2-1bdb-42f9-b463-ea87fb3856be/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [80/100]	lr: 3.13e-05	time: 00:06:49	Loss_train 0.05019	Acc_train 97.93	/	Loss_test 3.11265	Acc_test 66.12
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL3f610ba2-1bdb-42f9-b463-ea87fb3856be/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [90/100]	lr: 1.56e-05	time: 00:07:33	Loss_train 0.04391	Acc_train 98.04	/	Loss_test 3.07585	Acc_test 66.49
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL3f610ba2-1bdb-42f9-b463-ea87fb3856be/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [100/100]	lr: 7.81e-06	time: 00:08:17	Loss_train 0.03978	Acc_train 98.28	/	Loss_test 3.06802	Acc_test 66.56
new_head:  {'blocks.4.layer.bias': tensor([ 0.0016, -0.0015,  0.0004, -0.0005,  0.0020,  0.0031,  0.0054, -0.0016,
         0.0026,  0.0032], device='cuda:0'), 'blocks.4.layer.weight': tensor([[ 0.0400,  0.0110,  0.0159,  ..., -0.0202, -0.0415, -0.0304],
        [ 0.0107, -0.0007,  0.0315,  ..., -0.0158,  0.0113,  0.0158],
        [ 0.0364, -0.0160,  0.0407,  ...,  0.0141,  0.0031,  0.0035],
        ...,
        [ 0.0219,  0.0111,  0.0037,  ...,  0.0494,  0.0133, -0.0125],
        [-0.0186,  0.0274, -0.0178,  ..., -0.0034, -0.0105,  0.0038],
        [-0.0212, -0.0315, -0.0387,  ..., -0.0497,  0.0010, -0.0065]],
       device='cuda:0')}
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL3f610ba2-1bdb-42f9-b463-ea87fb3856be/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
RESULT:  {'train_loss': 0.03978398069739342, 'train_acc': 98.27600121498108, 'test_loss': 3.0680172443389893, 'test_acc': 66.5625, 'convergence': 23.72207260131836, 'R1': 1, 'dataset_sup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 5000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 4, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': True, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 100, 'print_freq': 10, 'validation': False, 'continual_learning': False}, 'dataset_unsup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 100000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 32, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'unlabeled', 'nb_epoch': 1, 'print_freq': 25, 'validation': False, 'continual_learning': False}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 32, 'nb_epoch': 1, 'print_freq': 25, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 100, 'print_freq': 10, 'lr': 0.001}}}
IN R0:  {'count': 0, 'R0': {'train_loss': 0.03978398069739342, 'train_acc': 98.27600121498108, 'test_loss': 3.0680172443389893, 'test_acc': 66.5625, 'convergence': 23.72207260131836, 'R1': 1, 'dataset_sup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 5000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 4, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': True, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 100, 'print_freq': 10, 'validation': False, 'continual_learning': False}, 'dataset_unsup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 100000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 32, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'unlabeled', 'nb_epoch': 1, 'print_freq': 25, 'validation': False, 'continual_learning': False}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 32, 'nb_epoch': 1, 'print_freq': 25, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 100, 'print_freq': 10, 'lr': 0.001}}}}
CL:  True
{'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True}
CL:  True
{'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': True}
SEED:  0
block 0, size : 96 16 16
96
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.036828478186799345, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 221184, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 2.886751345948129
block 1, size : 384 8 8
384
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.036828478186799345, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 221184, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.8505172717997146
block 2, size : 1536 4 4
1536
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.036828478186799345, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 221184, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.4252586358998573
block 3, size : 6144 2 2
6144
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.036828478186799345, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 221184, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.21262931794992865
NOWWWW
2 2
10
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.036828478186799345, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 24576, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
blocks['b%s' % 4] 24576 24576
range = 0.11048543456039805
train_layer_order: consecutive {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 10, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}
The device used will be: 
True
cuda:0
PARAMSSSS IN LOAD:  {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.036828478186799345, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 221184, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
avg_deltas:  {'blocks.0.layer.weight': tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')}
topk_layer:  None
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.0.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')}
topk_layer inside model after retrieval:  [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.1.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')}
topk_layer inside model after retrieval:  [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.2.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')}
topk_layer inside model after retrieval:  [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.3.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')}
topk_layer inside model after retrieval:  [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}

 Model STL10_CIFAR10_CL3f610ba2-1bdb-42f9-b463-ea87fb3856be loaded successfuly with best perf



 ----- Architecture Block BatchNorm2dSK3962(5, 5)1.0reflect, number 0 -----
- BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- AvgPool2d(kernel_size=4, stride=2, padding=1)

 ----- Architecture Block BatchNorm2dSK963842(3, 3)0.6499999761581421reflect, number 1 -----
- BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK38415362(3, 3)0.44999998807907104reflect, number 2 -----
- BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.4)
- AvgPool2d(kernel_size=4, stride=2, padding=1)

 ----- Architecture Block BatchNorm2dSK153661442(3, 3)0.25reflect, number 3 -----
- BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 4 -----
- Flatten(start_dim=1, end_dim=-1)
- Dropout(p=0.5, inplace=False)
- Linear(in_features=221184, out_features=10, bias=True)
model.heads:  [{'blocks.4.layer.bias': tensor([ 0.0016, -0.0015,  0.0004, -0.0005,  0.0020,  0.0031,  0.0054, -0.0016,
         0.0026,  0.0032], device='cuda:0'), 'blocks.4.layer.weight': tensor([[ 0.0400,  0.0110,  0.0159,  ..., -0.0202, -0.0415, -0.0304],
        [ 0.0107, -0.0007,  0.0315,  ..., -0.0158,  0.0113,  0.0158],
        [ 0.0364, -0.0160,  0.0407,  ...,  0.0141,  0.0031,  0.0035],
        ...,
        [ 0.0219,  0.0111,  0.0037,  ...,  0.0494,  0.0133, -0.0125],
        [-0.0186,  0.0274, -0.0178,  ..., -0.0034, -0.0105,  0.0038],
        [-0.0212, -0.0315, -0.0387,  ..., -0.0497,  0.0010, -0.0065]],
       device='cuda:0')}]
[Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=1.4)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=221184, out_features=10, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)]
LAYER NAME:  Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=1.4)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=221184, out_features=10, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)
LAYER CHILDREN:  [BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
  (activation): Triangle(power=1.4)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1)
), BasicBlock(
  (operations): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
  )
  (layer): Linear(in_features=221184, out_features=10, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)]
subsubl NAME:  Sequential(
  (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=4, stride=2, padding=1)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=4, stride=2, padding=1)
subsubl NAME:  Triangle(power=1.4)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1)
subsubl NAME:  Sequential(
  (0): Flatten(start_dim=1, end_dim=-1)
)
subsubl NAME:  Linear(in_features=221184, out_features=10, bias=True)
subsubl NAME:  Dropout(p=0.5, inplace=False)
CONFIG MODE:  unsupervised

 ********** Hebbian Unsupervised learning of blocks [0, 1, 2, 3] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
INDICES:  50000
IMAGE SIZE: torch.Size([10, 3, 96, 96])
SAVING FOLDER FOR UNSUP:  STL10_CIFAR10_CL3f610ba2-1bdb-42f9-b463-ea87fb3856be
prev_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
activations_sum.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
DEPTH:  4
ACTIVATIONS SHAPE:  tensor([[ 7.0837e+00, -1.7342e+02, -5.2790e+01,  ..., -3.0243e+02,
         -3.1626e+02, -8.0347e+01],
        [-8.1204e+01, -2.1264e+02, -1.1070e+02,  ..., -3.5674e+02,
         -3.7442e+02, -1.3079e+02],
        [-1.0510e+01, -2.5627e+02, -1.4349e+02,  ..., -2.6999e+02,
         -3.1838e+02, -2.5646e+01],
        ...,
        [ 2.0937e+02,  2.8768e-01,  5.9503e+01,  ..., -6.2056e+01,
         -1.4965e+02,  1.2563e+02],
        [ 4.0981e+02,  2.2179e+02,  2.8451e+02,  ...,  1.7146e+02,
          3.1691e+01,  3.1062e+02],
        [ 1.0649e+02, -1.2042e+02, -1.1781e+02,  ..., -1.8817e+02,
         -2.5834e+02,  1.3789e+00]], device='cuda:0')
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.topk_layer:  <class 'list'>
CHECK_THRESHOLD:  tensor(1, device='cuda:0', dtype=torch.uint8) tensor(-0.0004) tensor(-0.2085)
topk_mask  tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0.])
threshold_mask  tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0],
       dtype=torch.uint8)
final_mask tensor([1., 0., 0., 0., 1., 2., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 2., 0.,
        1., 1., 0., 0., 1., 0., 0., 2., 1., 1., 0., 1., 1., 0., 1., 1., 0., 2.,
        1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 2., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0.])
lower_lr_mask  tensor([-0., -0., -0., -0., -0., -1., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -1., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -1., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -1.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.])
higher_lr_mask  tensor([0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.0000, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000,
        0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.0000, 0.0000, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500])
lr_mask  tensor([[[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[0.0000]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]]])
torch.Size([96, 3, 5, 5])
self.lr tensor([[[0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066]],

        [[0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066]],

        [[0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066]]], device='cuda:0')
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.topk_layer:  <class 'list'>
CHECK_THRESHOLD:  tensor(0, device='cuda:0', dtype=torch.uint8) tensor(0.0010) tensor(0.0029)
topk_mask  tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0.])
threshold_mask  tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1],
       dtype=torch.uint8)
final_mask tensor([0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 1., 0., 1., 1., 0., 1., 2., 1., 1., 0., 0., 0., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 2., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 2., 0.,
        2., 0., 1., 0., 1., 1., 0., 0., 1., 2., 1., 0., 0., 1., 2., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 2., 0., 0., 1., 1.,
        1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0.,
        1., 0., 1., 0., 0., 1., 2., 1., 1., 1., 1., 1., 1., 0., 1., 2., 1., 0.,
        0., 2., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1.,
        2., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 2., 0., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 2., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0.,
        0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 1., 2., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.,
        0., 1., 2., 0., 0., 0., 1., 0., 1., 1., 0., 0., 2., 0., 1., 0., 0., 0.,
        1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 2., 1., 1., 0., 1., 1., 0., 1., 0., 0.,
        0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0.,
        2., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0.,
        1., 0., 1., 2., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        0., 0., 1., 0., 0., 2., 1., 0., 0., 1., 0., 0., 0., 2., 1., 0., 1., 0.,
        0., 1., 0., 0., 2., 1.])
lower_lr_mask  tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -1., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -1., -0., -1., -0., -0., -0., -0., -0., -0., -0.,
        -0., -1., -0., -0., -0., -0., -1., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0., -0.,
        -0., -0., -0., -0., -0., -1., -0., -0., -0., -1., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -1., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -1., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -1., -0., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -1., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -1., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -1., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0., -0., -0.,
        -0., -0., -0., -0., -1., -0.])
higher_lr_mask  tensor([0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500,
        0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.0000,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500,
        0.1500, 0.0000, 0.1500, 0.0000, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500,
        0.0000, 0.1500, 0.0000, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500,
        0.0000, 0.0000, 0.1500, 0.1500, 0.1500, 0.0000, 0.0000, 0.1500, 0.1500,
        0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.0000, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.0000, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.0000, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.0000,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.0000, 0.1500, 0.1500,
        0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000,
        0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.0000, 0.0000, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.0000,
        0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.0000,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500])
lr_mask  tensor([[[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]]])
torch.Size([384, 96, 3, 3])
self.lr tensor([[[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]]], device='cuda:0')
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.topk_layer:  <class 'list'>
CHECK_THRESHOLD:  tensor(0, device='cuda:0', dtype=torch.uint8) tensor(0.0006) tensor(0.0017)
topk_mask  tensor([0., 0., 0.,  ..., 0., 0., 0.])
threshold_mask  tensor([0, 1, 0,  ..., 0, 1, 0], dtype=torch.uint8)
final_mask tensor([0., 1., 0.,  ..., 0., 1., 0.])
lower_lr_mask  tensor([-0., -0., -0.,  ..., -0., -0., -0.])
higher_lr_mask  tensor([0.1500, 0.1500, 0.1500,  ..., 0.1500, 0.1500, 0.1500])
lr_mask  tensor([[[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        ...,


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]]])
torch.Size([1536, 384, 3, 3])
self.lr tensor([[[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]],

        [[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]],

        [[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]],

        ...,

        [[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]],

        [[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]],

        [[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]]], device='cuda:0')
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.topk_layer:  <class 'list'>
CHECK_THRESHOLD:  tensor(0, device='cuda:0', dtype=torch.uint8) tensor(0.0006) tensor(0.0113)
topk_mask  tensor([0., 0., 1.,  ..., 0., 1., 0.])
threshold_mask  tensor([0, 0, 1,  ..., 1, 1, 0], dtype=torch.uint8)
final_mask tensor([0., 0., 2.,  ..., 1., 2., 0.])
lower_lr_mask  tensor([-0., -0., -1.,  ..., -0., -1., -0.])
higher_lr_mask  tensor([0.1500, 0.1500, 0.0000,  ..., 0.1500, 0.0000, 0.1500])
lr_mask  tensor([[[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        ...,


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]]])
torch.Size([6144, 1536, 3, 3])
self.lr tensor([[[0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488]],

        [[0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488]],

        [[0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488]],

        ...,

        [[0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488]],

        [[0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488]],

        [[0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488]]], device='cuda:0')
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
activations_sum[k] 96
activations_sum[k]:  [42, 59, 25, 30, 14, 70, 35, 69, 68, 93]
topk_kernels len:  4
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3']
topk_kernels:  {'conv0': [42, 59, 25, 30, 14, 70, 35, 69, 68, 93, 53, 77, 55, 21, 5], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
activations_sum[k] len:  96
activations_sum[k] 384
activations_sum[k]:  [23, 382, 345, 191, 75, 104, 365, 190, 144, 99]
topk_kernels len:  4
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3']
topk_kernels:  {'conv0': [42, 59, 25, 30, 14, 70, 35, 69, 68, 93, 53, 77, 55, 21, 5], 'conv1': [23, 382, 345, 191, 75, 104, 365, 190, 144, 99, 100, 264, 56, 324, 158, 92, 180, 152, 150, 296, 133, 140, 121, 163, 105, 146, 88, 349, 94, 179, 33, 36, 41, 201, 202, 159, 26, 13, 39, 354, 336, 373, 219, 335, 254, 374, 206, 344, 76, 186, 50, 274, 30, 314, 166, 70, 73, 225, 247], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
activations_sum[k] len:  384
activations_sum[k] 1536
activations_sum[k]:  [445, 875, 1317, 235, 1279, 859, 1307, 119, 187, 662]
topk_kernels len:  4
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3']
topk_kernels:  {'conv0': [42, 59, 25, 30, 14, 70, 35, 69, 68, 93, 53, 77, 55, 21, 5], 'conv1': [23, 382, 345, 191, 75, 104, 365, 190, 144, 99, 100, 264, 56, 324, 158, 92, 180, 152, 150, 296, 133, 140, 121, 163, 105, 146, 88, 349, 94, 179, 33, 36, 41, 201, 202, 159, 26, 13, 39, 354, 336, 373, 219, 335, 254, 374, 206, 344, 76, 186, 50, 274, 30, 314, 166, 70, 73, 225, 247], 'conv2': [445, 875, 1317, 235, 1279, 859, 1307, 119, 187, 662, 784, 1243, 760, 711, 276, 735, 335, 427, 80, 1395, 647, 454, 915, 1086, 356, 247, 1441, 590, 106, 167, 141, 1011, 409, 41, 786, 423, 726, 1464, 914, 817, 1494, 613, 68, 827, 1195, 1184, 835, 801, 519, 1412, 812, 943, 1400, 896, 849, 746, 764, 484, 1126, 1247, 599, 295, 752, 1230, 341, 869, 868, 935, 313, 1532, 1233, 857, 1375, 227, 447, 1297, 1006, 1132, 1271, 782, 22, 731, 425, 472, 1042, 350, 891, 144, 1071, 612, 1474, 1186, 159, 964, 381, 959, 450, 408, 683, 70, 1436, 651, 1113, 1486, 426, 1100, 667, 906, 958, 1012, 456, 1069, 1460, 822, 449, 266, 617, 1269, 121, 1429, 1282, 1530, 1357, 1220, 25, 1320, 461, 474, 917, 953, 937, 243, 971, 441, 635, 644, 614, 854, 1372, 304, 1053, 353, 1032, 79, 289, 305, 1465, 839, 601, 244, 524, 1360, 1471, 1266, 1210, 180, 268, 619, 700, 405, 532, 225, 209, 993, 252, 1358, 440, 229, 28, 207, 545, 346, 326, 421, 501, 847, 1134, 846, 241, 142, 340, 1116, 1046, 214, 640, 1361, 537, 161, 380, 366, 1522, 1211, 1481, 1000, 557, 673, 1454, 627, 430, 1252, 865, 900, 5, 58, 130, 414, 135, 1407, 577, 976, 1500, 942, 294, 981, 78, 1442, 781, 195, 314, 636, 818, 264, 1083, 1110, 204, 1489, 579, 660, 1238, 246, 838], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
activations_sum[k] len:  1536
activations_sum[k] 6144
activations_sum[k]:  [4076, 2572, 1544, 944, 1163, 5694, 6119, 5015, 4144, 2067]
topk_kernels len:  4
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3']
topk_kernels:  {'conv0': [42, 59, 25, 30, 14, 70, 35, 69, 68, 93, 53, 77, 55, 21, 5], 'conv1': [23, 382, 345, 191, 75, 104, 365, 190, 144, 99, 100, 264, 56, 324, 158, 92, 180, 152, 150, 296, 133, 140, 121, 163, 105, 146, 88, 349, 94, 179, 33, 36, 41, 201, 202, 159, 26, 13, 39, 354, 336, 373, 219, 335, 254, 374, 206, 344, 76, 186, 50, 274, 30, 314, 166, 70, 73, 225, 247], 'conv2': [445, 875, 1317, 235, 1279, 859, 1307, 119, 187, 662, 784, 1243, 760, 711, 276, 735, 335, 427, 80, 1395, 647, 454, 915, 1086, 356, 247, 1441, 590, 106, 167, 141, 1011, 409, 41, 786, 423, 726, 1464, 914, 817, 1494, 613, 68, 827, 1195, 1184, 835, 801, 519, 1412, 812, 943, 1400, 896, 849, 746, 764, 484, 1126, 1247, 599, 295, 752, 1230, 341, 869, 868, 935, 313, 1532, 1233, 857, 1375, 227, 447, 1297, 1006, 1132, 1271, 782, 22, 731, 425, 472, 1042, 350, 891, 144, 1071, 612, 1474, 1186, 159, 964, 381, 959, 450, 408, 683, 70, 1436, 651, 1113, 1486, 426, 1100, 667, 906, 958, 1012, 456, 1069, 1460, 822, 449, 266, 617, 1269, 121, 1429, 1282, 1530, 1357, 1220, 25, 1320, 461, 474, 917, 953, 937, 243, 971, 441, 635, 644, 614, 854, 1372, 304, 1053, 353, 1032, 79, 289, 305, 1465, 839, 601, 244, 524, 1360, 1471, 1266, 1210, 180, 268, 619, 700, 405, 532, 225, 209, 993, 252, 1358, 440, 229, 28, 207, 545, 346, 326, 421, 501, 847, 1134, 846, 241, 142, 340, 1116, 1046, 214, 640, 1361, 537, 161, 380, 366, 1522, 1211, 1481, 1000, 557, 673, 1454, 627, 430, 1252, 865, 900, 5, 58, 130, 414, 135, 1407, 577, 976, 1500, 942, 294, 981, 78, 1442, 781, 195, 314, 636, 818, 264, 1083, 1110, 204, 1489, 579, 660, 1238, 246, 838], 'conv3': [4076, 2572, 1544, 944, 1163, 5694, 6119, 5015, 4144, 2067, 4382, 1894, 780, 3980, 3710, 5338, 863, 287, 1439, 2101, 425, 5027, 4431, 2323, 534, 2736, 2070, 3165, 309, 5799, 3559, 2598, 1564, 2460, 3947, 3984, 4394, 4244, 5201, 43, 6088, 1641, 26, 178, 775, 2576, 4299, 2607, 5143, 3827, 3399, 5871, 2835, 2752, 5396, 5890, 4821, 3577, 3783, 4312, 1097, 4876, 5910, 3035, 1312, 3828, 2613, 911, 3383, 3187, 5054, 4537, 2213, 1083, 4651, 4817, 3734, 371, 1845, 1257, 2000, 1949, 3897, 2976, 515, 3986, 6094, 2335, 2244, 5631, 5286, 2491, 3348, 4467, 4992, 1877, 514, 4699, 2559, 1428, 5150, 5277, 3561, 4814, 5003, 750, 5222, 5272, 4979, 1960, 2342, 3181, 1753, 894, 2986, 4614, 1653, 8, 5491, 2138, 4906, 1422, 1950, 5908, 5645, 4791, 2966, 465, 2426, 2315, 1532, 5448, 2185, 4824, 1575, 3964, 1080, 1385, 1873, 2891, 5542, 2471, 985, 2679, 1631, 95, 5846, 2221, 4598, 1730, 2152, 3221, 1685, 294, 632, 2656, 3036, 4339, 61, 5048, 5698, 4097, 3226, 5318, 2261, 4186, 2917, 3429, 1988, 1346, 2089, 2515, 4395, 1749, 493, 5421, 948, 4451, 4899, 1501, 527, 3420, 5014, 3722, 5642, 63, 4227, 2013, 3466, 5349, 3662, 4044, 4908, 2693, 4702, 2950, 3362, 1335, 1438, 1709, 2270, 4110, 4482, 4476, 2069, 4134, 4376, 2026, 4502, 6125, 4163, 1622, 4118, 184, 1938, 825, 3788, 5493, 5580, 3733, 2019, 4884, 1539, 5212, 2787, 2975, 3938, 859, 4034, 4819, 2246, 4269, 2580, 1390, 4606, 2367, 2470, 3537, 4135, 1615, 2083, 1696, 2281, 415, 2585, 5407, 4566, 5839, 6032, 5124, 6021, 451, 5626, 3852, 3698, 4735, 679, 2248, 806, 2533, 1798, 5651, 105, 4181, 4358, 1833, 3718, 2996, 3606, 6084, 4352, 5795, 6070, 2420, 6041, 4000, 361, 1037, 5752, 1131, 5000, 471, 599, 4688, 881, 551, 5466, 3375, 1623, 4081, 4247, 796, 3097, 3571, 1009, 167, 2747, 3950, 2814, 1888, 2447, 1357, 3749, 3775, 706, 1349, 999, 3916, 4323, 2242, 2146, 1897, 1307, 4572, 1055, 686, 2581, 2415, 4341, 2049, 6095, 4120, 2850, 3070, 2095, 1022, 5004, 4301, 3495, 2290, 1138, 1973, 4261, 5753, 1978, 2488, 1115, 4100, 205, 3670, 6072, 4778, 4001, 363, 2001, 3991, 430, 3005, 1368, 4602, 3252, 4980, 257, 2172, 1406, 278, 4662, 4900, 2797, 2075, 4025, 5067, 4455, 827, 5876, 5312, 3145, 1853, 5109, 4962, 3448, 3067, 4222, 3839, 5005, 1519, 132, 5984, 836, 3615, 5589, 2328, 1155, 3818, 3384, 4385, 1168, 3745, 1857, 4007, 4639, 3387, 4143, 4277, 6043, 2608, 1222, 1806, 6100, 4363, 2691, 461, 1909, 5796, 4009, 477, 1220, 1778, 676, 5422, 5023, 2056, 261, 468, 2597, 4218, 4885, 2697, 1158, 730, 4357, 5791, 1104, 2785, 4266, 5811, 2962, 1887, 584, 5376, 3242, 2106, 4004, 5423, 4963, 709, 1513, 4719, 2452, 1252, 4570, 5590, 1028, 4392, 2079, 2769, 14, 516, 822, 5809, 6098, 2555, 3253, 1526, 3641, 3740, 4066, 2262, 4845, 2841, 1361, 1337, 5834, 4292, 5414, 1508, 5669, 2698, 5321, 3717, 5792, 1530, 3084, 3013, 4925, 3959, 4349, 3548, 569, 4080, 3106, 4402, 3009, 1597, 732, 4638, 2865, 2800, 2647, 4739, 248, 1306, 2494, 5477, 667, 5072, 2668, 1130, 4460, 3193, 3470, 3170, 2847, 5363, 2979, 3177, 5103, 1799, 5773, 144, 1831, 1289, 5591, 52, 1680, 3334, 3153, 3173, 5078, 5558, 5658, 1594, 4159, 1822, 5254, 646, 4896, 175, 5582, 3107, 3178, 4457, 2799, 2378, 2863, 943, 40, 6131, 2569, 383, 4954, 3279, 2611, 883, 4475, 3761, 1608, 5070, 4415, 4554, 5035, 6024, 2122, 5961, 4780, 5445, 1336, 117, 3822, 5416, 2817, 4029, 6142, 4306, 2666, 2941, 3750, 993, 5013, 5473, 3435, 5442, 4869, 4445, 5790, 4681, 3981, 4077, 2393, 3658, 774, 6118, 3367, 394, 4072, 1274, 3061, 5069, 6090, 2126, 1677, 5127, 583, 21, 3993, 165, 5504, 4407, 1911, 3027, 5055, 1913, 1000, 2030, 338, 5276, 1402, 152, 1424, 2154, 2355, 2792, 3337, 1036, 5292, 1292, 5507, 3523, 3136, 1226, 3050, 4692, 2282, 5866, 850, 3532, 4350, 5243, 5332, 3323, 2522, 3449, 3034, 508, 5451, 326, 4724, 1296, 4843, 842, 5623, 5643, 292, 5918, 3040, 4807, 426, 1996, 1930, 5283, 767, 5024, 3191, 2418, 211, 5322, 5121, 5065, 909, 526, 2305, 3126, 485, 3054, 2804, 5146, 5381, 5956, 3643, 3474, 1011, 378, 5180, 509, 2860, 2645, 3594, 5308, 2336, 4848, 728, 1181, 4682, 4882, 2317, 4052, 511, 221, 5368, 2675, 3653, 5966, 3294, 2037, 4190, 3480, 3802, 2320, 5671, 2427, 2677, 1095, 4073, 799, 3403, 1936, 5852, 1644, 4704, 5993, 2398, 3873, 45, 3502, 1363, 3998, 2538, 3563, 4637, 1854, 3454, 5929, 2051, 3227, 2345, 5625, 3853, 2694, 4544, 5239, 3361, 976, 1706, 3488, 6133, 1294, 3114, 327, 4640, 1636, 5314, 4551, 2978, 5976, 5188, 660, 5316, 555, 716, 2334, 878, 6044, 3041, 6037, 480, 3860, 1789, 5096, 5169, 1401, 443, 3062, 5256, 1086, 581, 1403, 153, 5776, 5772, 46, 3324, 3680, 4958, 2778, 4924, 6025, 3086, 4142, 1520, 4418, 3347, 3621, 3229, 4027, 2751, 3727, 134, 1453, 873, 2112, 4974, 997, 1903, 5205, 3450, 5629, 194, 1515, 2762, 5130, 3801, 1144, 1096, 5836, 159, 547, 1256, 3069, 3830, 2183, 3464, 3275, 5861, 4203, 2682, 450, 4308, 1183, 2714, 3244, 2989, 2702, 1642, 3175, 5151, 544, 505, 2331, 4653, 3388, 2245, 409, 3657, 1280, 860, 1787, 2173, 3533, 1649, 1194, 4141, 2506, 225, 3108, 4217, 1499, 5568, 3197, 2918, 5806, 4102, 1610, 1459, 3540, 5502, 5936, 1998, 5647, 3736, 5398, 5579, 5419, 4987, 5851, 1042, 1153, 2615, 1904, 2222, 2703, 41, 5190, 2625, 1451, 601, 5635, 5661, 122, 273, 1545, 5922, 888, 1712, 1968, 1112, 3649, 4265, 180, 954, 4151, 1073, 5973, 3936, 3941, 530, 3251, 2859, 2914, 1525, 634, 5401, 5051, 5452, 5409, 4556, 2983, 2005, 5108, 5406, 4298, 3469, 1231, 2885, 446, 776, 3033, 2277, 755, 2824, 1191, 3183, 5393, 3720]}
activations_sum[k] len:  6144
['conv0', 'conv1', 'conv2', 'conv3']
delta_weights INFO: 
NUM OF TRACKED COV LAYERS:  4
NUM OF TRACKED WEIGHTS CHANGES PER LAYER:  100
avg_deltas INFO:  <class 'dict'>
avg_deltas keys:  ['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight']
################################################
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 3.84e-02	time: 00:16:12	Acc_train 0.00	Acc_test 0.00	convergence: 2.56e+01	R1: 15	Info MB:0.000e+00/SB:0.000e+00/MW:1.622e-05/SW:2.305e-01/MR:2.655e+01/SR:5.435e+00/MeD:2.986e+00/MaD:3.914e+01/MW:0.686/MAW:0.314
|        0 |        1 |      2 |        3 |        4 |        5 |        6 |        7 |       8 |        9 |       10 |       11 |       12 |       13 |       14 |       15 |       16 |       17 |       18 |       19 |       20 |      21 |       22 |       23 |      24 |       25 |       26 |       27 |       28 |       29 |
|----------+----------+--------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+---------+----------+----------+----------+----------+----------|
|   0.0502 |   0.0515 |   0.05 |   0.0518 |   0.0509 |   0.0482 |   0.0552 |   0.0507 |   0.036 |   0.0496 |   0.0501 |   0.0502 |   0.0497 |   0.0503 |   0.0525 |   0.0511 |   0.0521 |   0.0534 |   0.0498 |   0.0501 |   0.0497 |   0.044 |   0.0506 |   0.0532 |   0.047 |   0.0439 |   0.0804 |   0.0498 |   0.0527 |   0.0531 |
|  26.16   |  27.48   |  26.03 |  27.88   |  26.88   |  24.21   |  31.51   |  26.66   |  13.95  |  25.62   |  26.05   |  26.2    |  25.69   |  26.26   |  28.61   |  27.15   |  28.16   |  29.56   |  25.82   |  26.1    |  25.71   |  20.36  |  26.62   |  29.27   |  23.09  |  20.23   |  65.69   |  25.78   |  28.81   |  29.24   |
|   0.01   |   0      |   0    |   0      |   0.01   |   0.01   |   0.01   |   0      |   0.11  |   0      |   0      |   0      |   0      |   0      |   0.01   |   0      |   0      |   0      |   0      |   0      |   0      |   0.01  |   0      |   0      |   0     |   0.01   |   0.4    |   0      |   0      |   0      |
| nan      | nan      | nan    | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan    | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan    | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:1.073e-03/SW:2.849e-01/MR:1.614e+01/SR:4.449e+00/MeD:3.389e+00/MaD:2.557e+01/MW:0.590/MAW:0.410
|        0 |        1 |        2 |        3 |        4 |        5 |        6 |        7 |        8 |        9 |       10 |       11 |       12 |       13 |       14 |       15 |       16 |       17 |       18 |       19 |       20 |       21 |       22 |       23 |       24 |       25 |      26 |       27 |       28 |       29 |
|----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------|
|   0.0446 |   0.0398 |   0.0419 |   0.0406 |   0.0335 |   0.0422 |   0.0464 |   0.0328 |   0.0385 |   0.0477 |   0.0159 |   0.0398 |   0.0328 |   0.0301 |   0.0384 |   0.0366 |   0.0318 |   0.0501 |   0.0382 |   0.0451 |   0.0355 |   0.0394 |   0.0276 |   0.0395 |   0.0333 |   0.0361 |   0.044 |   0.0394 |   0.0322 |   0.0409 |
|  20.85   |  16.86   |  18.57   |  17.5    |  12.22   |  18.81   |  22.53   |  11.79   |  15.84   |  23.76   |   3.52   |  16.87   |  11.76   |  10.08   |  15.74   |  14.4    |  11.12   |  26.12   |  15.63   |  21.37   |  13.59   |  16.56   |   8.61   |  16.59   |  12.09   |  14.03   |  20.33  |  16.5    |  11.34   |  17.76   |
|   0.02   |   0.02   |   0.06   |   0.06   |   0.03   |   0.12   |   0.01   |   0.05   |   0.04   |   0.02   |   0.16   |   0.03   |   0.03   |   0.04   |   0.04   |   0.02   |   0.09   |   0.01   |   0.03   |   0.01   |   0.08   |   0.02   |   0.32   |   0.03   |   0.04   |   0.09   |   0.01  |   0.04   |   0.04   |   0.01   |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:-1.696e-03/SW:5.886e-01/MR:1.679e+01/SR:4.168e+00/MeD:3.420e+00/MaD:1.259e+01/MW:0.616/MAW:0.384
|        0 |        1 |        2 |        3 |        4 |        5 |        6 |        7 |        8 |        9 |       10 |       11 |       12 |       13 |       14 |       15 |       16 |       17 |       18 |       19 |       20 |       21 |       22 |      23 |      24 |       25 |       26 |       27 |       28 |       29 |
|----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+---------+----------+----------+----------+----------+----------|
|   0.0211 |   0.0232 |   0.0221 |   0.0207 |   0.0143 |   0.0165 |   0.0215 |   0.0192 |   0.0212 |   0.0191 |   0.0246 |   0.0203 |   0.0196 |   0.0207 |   0.0194 |   0.0211 |   0.0214 |   0.0161 |   0.0206 |   0.0165 |   0.0171 |   0.0235 |   0.0196 |   0.025 |   0.018 |   0.0172 |   0.0172 |   0.0208 |   0.0191 |   0.0229 |
|  18.88   |  22.57   |  20.61   |  18.11   |   9.22   |  11.92   |  19.41   |  15.82   |  18.91   |  15.65   |  25.21   |  17.49   |  16.31   |  18.16   |  16.06   |  18.8    |  19.38   |  11.42   |  17.91   |  11.9    |  12.68   |  23.11   |  16.33   |  26.09  |  13.92  |  12.85   |  12.77   |  18.27   |  15.52   |  22.01   |
|   0.08   |   0.05   |   0.06   |   0.08   |   0.25   |   0.11   |   0.12   |   0.11   |   0.07   |   0.14   |   0.01   |   0.14   |   0.15   |   0.25   |   0.12   |   0.09   |   0.2    |   0.33   |   0.05   |   0.19   |   0.13   |   0.2    |   0.49   |   6.63  |   0.22  |   0.34   |   1.78   |   0.05   |   0.08   |   0.2    |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan     | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan     | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan     | nan      | nan      | nan      | nan      | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:-1.965e-02/SW:1.544e+00/MR:1.291e+01/SR:3.499e+00/MeD:2.833e+00/MaD:1.011e+01/MW:0.593/MAW:0.407
|       0 |       1 |       2 |       3 |       4 |       5 |       6 |      7 |       8 |       9 |      10 |      11 |      12 |      13 |      14 |      15 |      16 |     17 |      18 |      19 |      20 |      21 |     22 |      23 |      24 |      25 |     26 |      27 |      28 |     29 |
|---------+---------+---------+---------+---------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------+---------+---------+---------+---------+--------+---------+---------+---------+--------+---------+---------+--------|
|   0.243 |   0.324 |   0.316 |   0.322 |   0.232 |   0.211 |   0.265 |   0.33 |   0.226 |   0.368 |   0.232 |   0.277 |   0.266 |   0.297 |   0.276 |   0.317 |   0.192 |   0.25 |   0.311 |   0.261 |   0.285 |   0.272 |   0.32 |   0.351 |   0.338 |   0.239 |   0.28 |   0.258 |   0.272 |   0.33 |
|  10.2   |  17.42  |  16.64  |  17.17  |   9.44  |   7.99  |  11.98  |  18.03 |   8.98  |  22.15  |   9.38  |  13.03  |  12.1   |  14.75  |  12.91  |  16.7   |   6.75  |  10.8  |  16.09  |  11.64  |  13.7   |  12.53  |  17.02 |  20.26  |  18.81  |   9.96  |  13.26 |  11.39  |  12.55  |  17.99 |
|   0.27  |   0.16  |   0.09  |   0.22  |   1.11  |   7.64  |   0.49  |   0.38 |   0.58  |   0.04  |   0.73  |   0.33  |   0.21  |   0.17  |   1.1   |   0.21  |   6.55  |   0.21 |   0.62  |   1.14  |   0.33  |   0.65  |   0.21 |   0.24  |   0.12  |   3.16  |   0.37 |   0.55  |   0.18  |   0.17 |
| nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan    | nan     | nan     | nan    |
| nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan    | nan     | nan     | nan    |
| nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan    | nan     | nan     | nan    |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL3f610ba2-1bdb-42f9-b463-ea87fb3856be/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [42, 59, 25, 30, 14, 70, 35, 69, 68, 93, 53, 77, 55, 21, 5], 'conv1': [23, 382, 345, 191, 75, 104, 365, 190, 144, 99, 100, 264, 56, 324, 158, 92, 180, 152, 150, 296, 133, 140, 121, 163, 105, 146, 88, 349, 94, 179, 33, 36, 41, 201, 202, 159, 26, 13, 39, 354, 336, 373, 219, 335, 254, 374, 206, 344, 76, 186, 50, 274, 30, 314, 166, 70, 73, 225, 247], 'conv2': [445, 875, 1317, 235, 1279, 859, 1307, 119, 187, 662, 784, 1243, 760, 711, 276, 735, 335, 427, 80, 1395, 647, 454, 915, 1086, 356, 247, 1441, 590, 106, 167, 141, 1011, 409, 41, 786, 423, 726, 1464, 914, 817, 1494, 613, 68, 827, 1195, 1184, 835, 801, 519, 1412, 812, 943, 1400, 896, 849, 746, 764, 484, 1126, 1247, 599, 295, 752, 1230, 341, 869, 868, 935, 313, 1532, 1233, 857, 1375, 227, 447, 1297, 1006, 1132, 1271, 782, 22, 731, 425, 472, 1042, 350, 891, 144, 1071, 612, 1474, 1186, 159, 964, 381, 959, 450, 408, 683, 70, 1436, 651, 1113, 1486, 426, 1100, 667, 906, 958, 1012, 456, 1069, 1460, 822, 449, 266, 617, 1269, 121, 1429, 1282, 1530, 1357, 1220, 25, 1320, 461, 474, 917, 953, 937, 243, 971, 441, 635, 644, 614, 854, 1372, 304, 1053, 353, 1032, 79, 289, 305, 1465, 839, 601, 244, 524, 1360, 1471, 1266, 1210, 180, 268, 619, 700, 405, 532, 225, 209, 993, 252, 1358, 440, 229, 28, 207, 545, 346, 326, 421, 501, 847, 1134, 846, 241, 142, 340, 1116, 1046, 214, 640, 1361, 537, 161, 380, 366, 1522, 1211, 1481, 1000, 557, 673, 1454, 627, 430, 1252, 865, 900, 5, 58, 130, 414, 135, 1407, 577, 976, 1500, 942, 294, 981, 78, 1442, 781, 195, 314, 636, 818, 264, 1083, 1110, 204, 1489, 579, 660, 1238, 246, 838], 'conv3': [4076, 2572, 1544, 944, 1163, 5694, 6119, 5015, 4144, 2067, 4382, 1894, 780, 3980, 3710, 5338, 863, 287, 1439, 2101, 425, 5027, 4431, 2323, 534, 2736, 2070, 3165, 309, 5799, 3559, 2598, 1564, 2460, 3947, 3984, 4394, 4244, 5201, 43, 6088, 1641, 26, 178, 775, 2576, 4299, 2607, 5143, 3827, 3399, 5871, 2835, 2752, 5396, 5890, 4821, 3577, 3783, 4312, 1097, 4876, 5910, 3035, 1312, 3828, 2613, 911, 3383, 3187, 5054, 4537, 2213, 1083, 4651, 4817, 3734, 371, 1845, 1257, 2000, 1949, 3897, 2976, 515, 3986, 6094, 2335, 2244, 5631, 5286, 2491, 3348, 4467, 4992, 1877, 514, 4699, 2559, 1428, 5150, 5277, 3561, 4814, 5003, 750, 5222, 5272, 4979, 1960, 2342, 3181, 1753, 894, 2986, 4614, 1653, 8, 5491, 2138, 4906, 1422, 1950, 5908, 5645, 4791, 2966, 465, 2426, 2315, 1532, 5448, 2185, 4824, 1575, 3964, 1080, 1385, 1873, 2891, 5542, 2471, 985, 2679, 1631, 95, 5846, 2221, 4598, 1730, 2152, 3221, 1685, 294, 632, 2656, 3036, 4339, 61, 5048, 5698, 4097, 3226, 5318, 2261, 4186, 2917, 3429, 1988, 1346, 2089, 2515, 4395, 1749, 493, 5421, 948, 4451, 4899, 1501, 527, 3420, 5014, 3722, 5642, 63, 4227, 2013, 3466, 5349, 3662, 4044, 4908, 2693, 4702, 2950, 3362, 1335, 1438, 1709, 2270, 4110, 4482, 4476, 2069, 4134, 4376, 2026, 4502, 6125, 4163, 1622, 4118, 184, 1938, 825, 3788, 5493, 5580, 3733, 2019, 4884, 1539, 5212, 2787, 2975, 3938, 859, 4034, 4819, 2246, 4269, 2580, 1390, 4606, 2367, 2470, 3537, 4135, 1615, 2083, 1696, 2281, 415, 2585, 5407, 4566, 5839, 6032, 5124, 6021, 451, 5626, 3852, 3698, 4735, 679, 2248, 806, 2533, 1798, 5651, 105, 4181, 4358, 1833, 3718, 2996, 3606, 6084, 4352, 5795, 6070, 2420, 6041, 4000, 361, 1037, 5752, 1131, 5000, 471, 599, 4688, 881, 551, 5466, 3375, 1623, 4081, 4247, 796, 3097, 3571, 1009, 167, 2747, 3950, 2814, 1888, 2447, 1357, 3749, 3775, 706, 1349, 999, 3916, 4323, 2242, 2146, 1897, 1307, 4572, 1055, 686, 2581, 2415, 4341, 2049, 6095, 4120, 2850, 3070, 2095, 1022, 5004, 4301, 3495, 2290, 1138, 1973, 4261, 5753, 1978, 2488, 1115, 4100, 205, 3670, 6072, 4778, 4001, 363, 2001, 3991, 430, 3005, 1368, 4602, 3252, 4980, 257, 2172, 1406, 278, 4662, 4900, 2797, 2075, 4025, 5067, 4455, 827, 5876, 5312, 3145, 1853, 5109, 4962, 3448, 3067, 4222, 3839, 5005, 1519, 132, 5984, 836, 3615, 5589, 2328, 1155, 3818, 3384, 4385, 1168, 3745, 1857, 4007, 4639, 3387, 4143, 4277, 6043, 2608, 1222, 1806, 6100, 4363, 2691, 461, 1909, 5796, 4009, 477, 1220, 1778, 676, 5422, 5023, 2056, 261, 468, 2597, 4218, 4885, 2697, 1158, 730, 4357, 5791, 1104, 2785, 4266, 5811, 2962, 1887, 584, 5376, 3242, 2106, 4004, 5423, 4963, 709, 1513, 4719, 2452, 1252, 4570, 5590, 1028, 4392, 2079, 2769, 14, 516, 822, 5809, 6098, 2555, 3253, 1526, 3641, 3740, 4066, 2262, 4845, 2841, 1361, 1337, 5834, 4292, 5414, 1508, 5669, 2698, 5321, 3717, 5792, 1530, 3084, 3013, 4925, 3959, 4349, 3548, 569, 4080, 3106, 4402, 3009, 1597, 732, 4638, 2865, 2800, 2647, 4739, 248, 1306, 2494, 5477, 667, 5072, 2668, 1130, 4460, 3193, 3470, 3170, 2847, 5363, 2979, 3177, 5103, 1799, 5773, 144, 1831, 1289, 5591, 52, 1680, 3334, 3153, 3173, 5078, 5558, 5658, 1594, 4159, 1822, 5254, 646, 4896, 175, 5582, 3107, 3178, 4457, 2799, 2378, 2863, 943, 40, 6131, 2569, 383, 4954, 3279, 2611, 883, 4475, 3761, 1608, 5070, 4415, 4554, 5035, 6024, 2122, 5961, 4780, 5445, 1336, 117, 3822, 5416, 2817, 4029, 6142, 4306, 2666, 2941, 3750, 993, 5013, 5473, 3435, 5442, 4869, 4445, 5790, 4681, 3981, 4077, 2393, 3658, 774, 6118, 3367, 394, 4072, 1274, 3061, 5069, 6090, 2126, 1677, 5127, 583, 21, 3993, 165, 5504, 4407, 1911, 3027, 5055, 1913, 1000, 2030, 338, 5276, 1402, 152, 1424, 2154, 2355, 2792, 3337, 1036, 5292, 1292, 5507, 3523, 3136, 1226, 3050, 4692, 2282, 5866, 850, 3532, 4350, 5243, 5332, 3323, 2522, 3449, 3034, 508, 5451, 326, 4724, 1296, 4843, 842, 5623, 5643, 292, 5918, 3040, 4807, 426, 1996, 1930, 5283, 767, 5024, 3191, 2418, 211, 5322, 5121, 5065, 909, 526, 2305, 3126, 485, 3054, 2804, 5146, 5381, 5956, 3643, 3474, 1011, 378, 5180, 509, 2860, 2645, 3594, 5308, 2336, 4848, 728, 1181, 4682, 4882, 2317, 4052, 511, 221, 5368, 2675, 3653, 5966, 3294, 2037, 4190, 3480, 3802, 2320, 5671, 2427, 2677, 1095, 4073, 799, 3403, 1936, 5852, 1644, 4704, 5993, 2398, 3873, 45, 3502, 1363, 3998, 2538, 3563, 4637, 1854, 3454, 5929, 2051, 3227, 2345, 5625, 3853, 2694, 4544, 5239, 3361, 976, 1706, 3488, 6133, 1294, 3114, 327, 4640, 1636, 5314, 4551, 2978, 5976, 5188, 660, 5316, 555, 716, 2334, 878, 6044, 3041, 6037, 480, 3860, 1789, 5096, 5169, 1401, 443, 3062, 5256, 1086, 581, 1403, 153, 5776, 5772, 46, 3324, 3680, 4958, 2778, 4924, 6025, 3086, 4142, 1520, 4418, 3347, 3621, 3229, 4027, 2751, 3727, 134, 1453, 873, 2112, 4974, 997, 1903, 5205, 3450, 5629, 194, 1515, 2762, 5130, 3801, 1144, 1096, 5836, 159, 547, 1256, 3069, 3830, 2183, 3464, 3275, 5861, 4203, 2682, 450, 4308, 1183, 2714, 3244, 2989, 2702, 1642, 3175, 5151, 544, 505, 2331, 4653, 3388, 2245, 409, 3657, 1280, 860, 1787, 2173, 3533, 1649, 1194, 4141, 2506, 225, 3108, 4217, 1499, 5568, 3197, 2918, 5806, 4102, 1610, 1459, 3540, 5502, 5936, 1998, 5647, 3736, 5398, 5579, 5419, 4987, 5851, 1042, 1153, 2615, 1904, 2222, 2703, 41, 5190, 2625, 1451, 601, 5635, 5661, 122, 273, 1545, 5922, 888, 1712, 1968, 1112, 3649, 4265, 180, 954, 4151, 1073, 5973, 3936, 3941, 530, 3251, 2859, 2914, 1525, 634, 5401, 5051, 5452, 5409, 4556, 2983, 2005, 5108, 5406, 4298, 3469, 1231, 2885, 446, 776, 3033, 2277, 755, 2824, 1191, 3183, 5393, 3720]}
SAVED model.heads_thresh 0.9
CONFIG MODE:  supervised

 ********** Supervised learning of blocks [4] **********
SAVING FOLDER FOR SUP:  STL10_CIFAR10_CL3f610ba2-1bdb-42f9-b463-ea87fb3856be
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
INDICES:  50000
IMAGE SIZE: torch.Size([64, 3, 96, 96])
num_blocks:  5
Epoch: [1/50]	lr: 1.00e-03	time: 00:16:52	Loss_train 7.09177	Acc_train 50.63	/	Loss_test 0.41713	Acc_test 52.89
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL3f610ba2-1bdb-42f9-b463-ea87fb3856be/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [42, 59, 25, 30, 14, 70, 35, 69, 68, 93, 53, 77, 55, 21, 5], 'conv1': [23, 382, 345, 191, 75, 104, 365, 190, 144, 99, 100, 264, 56, 324, 158, 92, 180, 152, 150, 296, 133, 140, 121, 163, 105, 146, 88, 349, 94, 179, 33, 36, 41, 201, 202, 159, 26, 13, 39, 354, 336, 373, 219, 335, 254, 374, 206, 344, 76, 186, 50, 274, 30, 314, 166, 70, 73, 225, 247], 'conv2': [445, 875, 1317, 235, 1279, 859, 1307, 119, 187, 662, 784, 1243, 760, 711, 276, 735, 335, 427, 80, 1395, 647, 454, 915, 1086, 356, 247, 1441, 590, 106, 167, 141, 1011, 409, 41, 786, 423, 726, 1464, 914, 817, 1494, 613, 68, 827, 1195, 1184, 835, 801, 519, 1412, 812, 943, 1400, 896, 849, 746, 764, 484, 1126, 1247, 599, 295, 752, 1230, 341, 869, 868, 935, 313, 1532, 1233, 857, 1375, 227, 447, 1297, 1006, 1132, 1271, 782, 22, 731, 425, 472, 1042, 350, 891, 144, 1071, 612, 1474, 1186, 159, 964, 381, 959, 450, 408, 683, 70, 1436, 651, 1113, 1486, 426, 1100, 667, 906, 958, 1012, 456, 1069, 1460, 822, 449, 266, 617, 1269, 121, 1429, 1282, 1530, 1357, 1220, 25, 1320, 461, 474, 917, 953, 937, 243, 971, 441, 635, 644, 614, 854, 1372, 304, 1053, 353, 1032, 79, 289, 305, 1465, 839, 601, 244, 524, 1360, 1471, 1266, 1210, 180, 268, 619, 700, 405, 532, 225, 209, 993, 252, 1358, 440, 229, 28, 207, 545, 346, 326, 421, 501, 847, 1134, 846, 241, 142, 340, 1116, 1046, 214, 640, 1361, 537, 161, 380, 366, 1522, 1211, 1481, 1000, 557, 673, 1454, 627, 430, 1252, 865, 900, 5, 58, 130, 414, 135, 1407, 577, 976, 1500, 942, 294, 981, 78, 1442, 781, 195, 314, 636, 818, 264, 1083, 1110, 204, 1489, 579, 660, 1238, 246, 838], 'conv3': [4076, 2572, 1544, 944, 1163, 5694, 6119, 5015, 4144, 2067, 4382, 1894, 780, 3980, 3710, 5338, 863, 287, 1439, 2101, 425, 5027, 4431, 2323, 534, 2736, 2070, 3165, 309, 5799, 3559, 2598, 1564, 2460, 3947, 3984, 4394, 4244, 5201, 43, 6088, 1641, 26, 178, 775, 2576, 4299, 2607, 5143, 3827, 3399, 5871, 2835, 2752, 5396, 5890, 4821, 3577, 3783, 4312, 1097, 4876, 5910, 3035, 1312, 3828, 2613, 911, 3383, 3187, 5054, 4537, 2213, 1083, 4651, 4817, 3734, 371, 1845, 1257, 2000, 1949, 3897, 2976, 515, 3986, 6094, 2335, 2244, 5631, 5286, 2491, 3348, 4467, 4992, 1877, 514, 4699, 2559, 1428, 5150, 5277, 3561, 4814, 5003, 750, 5222, 5272, 4979, 1960, 2342, 3181, 1753, 894, 2986, 4614, 1653, 8, 5491, 2138, 4906, 1422, 1950, 5908, 5645, 4791, 2966, 465, 2426, 2315, 1532, 5448, 2185, 4824, 1575, 3964, 1080, 1385, 1873, 2891, 5542, 2471, 985, 2679, 1631, 95, 5846, 2221, 4598, 1730, 2152, 3221, 1685, 294, 632, 2656, 3036, 4339, 61, 5048, 5698, 4097, 3226, 5318, 2261, 4186, 2917, 3429, 1988, 1346, 2089, 2515, 4395, 1749, 493, 5421, 948, 4451, 4899, 1501, 527, 3420, 5014, 3722, 5642, 63, 4227, 2013, 3466, 5349, 3662, 4044, 4908, 2693, 4702, 2950, 3362, 1335, 1438, 1709, 2270, 4110, 4482, 4476, 2069, 4134, 4376, 2026, 4502, 6125, 4163, 1622, 4118, 184, 1938, 825, 3788, 5493, 5580, 3733, 2019, 4884, 1539, 5212, 2787, 2975, 3938, 859, 4034, 4819, 2246, 4269, 2580, 1390, 4606, 2367, 2470, 3537, 4135, 1615, 2083, 1696, 2281, 415, 2585, 5407, 4566, 5839, 6032, 5124, 6021, 451, 5626, 3852, 3698, 4735, 679, 2248, 806, 2533, 1798, 5651, 105, 4181, 4358, 1833, 3718, 2996, 3606, 6084, 4352, 5795, 6070, 2420, 6041, 4000, 361, 1037, 5752, 1131, 5000, 471, 599, 4688, 881, 551, 5466, 3375, 1623, 4081, 4247, 796, 3097, 3571, 1009, 167, 2747, 3950, 2814, 1888, 2447, 1357, 3749, 3775, 706, 1349, 999, 3916, 4323, 2242, 2146, 1897, 1307, 4572, 1055, 686, 2581, 2415, 4341, 2049, 6095, 4120, 2850, 3070, 2095, 1022, 5004, 4301, 3495, 2290, 1138, 1973, 4261, 5753, 1978, 2488, 1115, 4100, 205, 3670, 6072, 4778, 4001, 363, 2001, 3991, 430, 3005, 1368, 4602, 3252, 4980, 257, 2172, 1406, 278, 4662, 4900, 2797, 2075, 4025, 5067, 4455, 827, 5876, 5312, 3145, 1853, 5109, 4962, 3448, 3067, 4222, 3839, 5005, 1519, 132, 5984, 836, 3615, 5589, 2328, 1155, 3818, 3384, 4385, 1168, 3745, 1857, 4007, 4639, 3387, 4143, 4277, 6043, 2608, 1222, 1806, 6100, 4363, 2691, 461, 1909, 5796, 4009, 477, 1220, 1778, 676, 5422, 5023, 2056, 261, 468, 2597, 4218, 4885, 2697, 1158, 730, 4357, 5791, 1104, 2785, 4266, 5811, 2962, 1887, 584, 5376, 3242, 2106, 4004, 5423, 4963, 709, 1513, 4719, 2452, 1252, 4570, 5590, 1028, 4392, 2079, 2769, 14, 516, 822, 5809, 6098, 2555, 3253, 1526, 3641, 3740, 4066, 2262, 4845, 2841, 1361, 1337, 5834, 4292, 5414, 1508, 5669, 2698, 5321, 3717, 5792, 1530, 3084, 3013, 4925, 3959, 4349, 3548, 569, 4080, 3106, 4402, 3009, 1597, 732, 4638, 2865, 2800, 2647, 4739, 248, 1306, 2494, 5477, 667, 5072, 2668, 1130, 4460, 3193, 3470, 3170, 2847, 5363, 2979, 3177, 5103, 1799, 5773, 144, 1831, 1289, 5591, 52, 1680, 3334, 3153, 3173, 5078, 5558, 5658, 1594, 4159, 1822, 5254, 646, 4896, 175, 5582, 3107, 3178, 4457, 2799, 2378, 2863, 943, 40, 6131, 2569, 383, 4954, 3279, 2611, 883, 4475, 3761, 1608, 5070, 4415, 4554, 5035, 6024, 2122, 5961, 4780, 5445, 1336, 117, 3822, 5416, 2817, 4029, 6142, 4306, 2666, 2941, 3750, 993, 5013, 5473, 3435, 5442, 4869, 4445, 5790, 4681, 3981, 4077, 2393, 3658, 774, 6118, 3367, 394, 4072, 1274, 3061, 5069, 6090, 2126, 1677, 5127, 583, 21, 3993, 165, 5504, 4407, 1911, 3027, 5055, 1913, 1000, 2030, 338, 5276, 1402, 152, 1424, 2154, 2355, 2792, 3337, 1036, 5292, 1292, 5507, 3523, 3136, 1226, 3050, 4692, 2282, 5866, 850, 3532, 4350, 5243, 5332, 3323, 2522, 3449, 3034, 508, 5451, 326, 4724, 1296, 4843, 842, 5623, 5643, 292, 5918, 3040, 4807, 426, 1996, 1930, 5283, 767, 5024, 3191, 2418, 211, 5322, 5121, 5065, 909, 526, 2305, 3126, 485, 3054, 2804, 5146, 5381, 5956, 3643, 3474, 1011, 378, 5180, 509, 2860, 2645, 3594, 5308, 2336, 4848, 728, 1181, 4682, 4882, 2317, 4052, 511, 221, 5368, 2675, 3653, 5966, 3294, 2037, 4190, 3480, 3802, 2320, 5671, 2427, 2677, 1095, 4073, 799, 3403, 1936, 5852, 1644, 4704, 5993, 2398, 3873, 45, 3502, 1363, 3998, 2538, 3563, 4637, 1854, 3454, 5929, 2051, 3227, 2345, 5625, 3853, 2694, 4544, 5239, 3361, 976, 1706, 3488, 6133, 1294, 3114, 327, 4640, 1636, 5314, 4551, 2978, 5976, 5188, 660, 5316, 555, 716, 2334, 878, 6044, 3041, 6037, 480, 3860, 1789, 5096, 5169, 1401, 443, 3062, 5256, 1086, 581, 1403, 153, 5776, 5772, 46, 3324, 3680, 4958, 2778, 4924, 6025, 3086, 4142, 1520, 4418, 3347, 3621, 3229, 4027, 2751, 3727, 134, 1453, 873, 2112, 4974, 997, 1903, 5205, 3450, 5629, 194, 1515, 2762, 5130, 3801, 1144, 1096, 5836, 159, 547, 1256, 3069, 3830, 2183, 3464, 3275, 5861, 4203, 2682, 450, 4308, 1183, 2714, 3244, 2989, 2702, 1642, 3175, 5151, 544, 505, 2331, 4653, 3388, 2245, 409, 3657, 1280, 860, 1787, 2173, 3533, 1649, 1194, 4141, 2506, 225, 3108, 4217, 1499, 5568, 3197, 2918, 5806, 4102, 1610, 1459, 3540, 5502, 5936, 1998, 5647, 3736, 5398, 5579, 5419, 4987, 5851, 1042, 1153, 2615, 1904, 2222, 2703, 41, 5190, 2625, 1451, 601, 5635, 5661, 122, 273, 1545, 5922, 888, 1712, 1968, 1112, 3649, 4265, 180, 954, 4151, 1073, 5973, 3936, 3941, 530, 3251, 2859, 2914, 1525, 634, 5401, 5051, 5452, 5409, 4556, 2983, 2005, 5108, 5406, 4298, 3469, 1231, 2885, 446, 776, 3033, 2277, 755, 2824, 1191, 3183, 5393, 3720]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [10/50]	lr: 1.00e-03	time: 00:20:36	Loss_train 4.23107	Acc_train 68.90	/	Loss_test 0.43428	Acc_test 64.87
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL3f610ba2-1bdb-42f9-b463-ea87fb3856be/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [42, 59, 25, 30, 14, 70, 35, 69, 68, 93, 53, 77, 55, 21, 5], 'conv1': [23, 382, 345, 191, 75, 104, 365, 190, 144, 99, 100, 264, 56, 324, 158, 92, 180, 152, 150, 296, 133, 140, 121, 163, 105, 146, 88, 349, 94, 179, 33, 36, 41, 201, 202, 159, 26, 13, 39, 354, 336, 373, 219, 335, 254, 374, 206, 344, 76, 186, 50, 274, 30, 314, 166, 70, 73, 225, 247], 'conv2': [445, 875, 1317, 235, 1279, 859, 1307, 119, 187, 662, 784, 1243, 760, 711, 276, 735, 335, 427, 80, 1395, 647, 454, 915, 1086, 356, 247, 1441, 590, 106, 167, 141, 1011, 409, 41, 786, 423, 726, 1464, 914, 817, 1494, 613, 68, 827, 1195, 1184, 835, 801, 519, 1412, 812, 943, 1400, 896, 849, 746, 764, 484, 1126, 1247, 599, 295, 752, 1230, 341, 869, 868, 935, 313, 1532, 1233, 857, 1375, 227, 447, 1297, 1006, 1132, 1271, 782, 22, 731, 425, 472, 1042, 350, 891, 144, 1071, 612, 1474, 1186, 159, 964, 381, 959, 450, 408, 683, 70, 1436, 651, 1113, 1486, 426, 1100, 667, 906, 958, 1012, 456, 1069, 1460, 822, 449, 266, 617, 1269, 121, 1429, 1282, 1530, 1357, 1220, 25, 1320, 461, 474, 917, 953, 937, 243, 971, 441, 635, 644, 614, 854, 1372, 304, 1053, 353, 1032, 79, 289, 305, 1465, 839, 601, 244, 524, 1360, 1471, 1266, 1210, 180, 268, 619, 700, 405, 532, 225, 209, 993, 252, 1358, 440, 229, 28, 207, 545, 346, 326, 421, 501, 847, 1134, 846, 241, 142, 340, 1116, 1046, 214, 640, 1361, 537, 161, 380, 366, 1522, 1211, 1481, 1000, 557, 673, 1454, 627, 430, 1252, 865, 900, 5, 58, 130, 414, 135, 1407, 577, 976, 1500, 942, 294, 981, 78, 1442, 781, 195, 314, 636, 818, 264, 1083, 1110, 204, 1489, 579, 660, 1238, 246, 838], 'conv3': [4076, 2572, 1544, 944, 1163, 5694, 6119, 5015, 4144, 2067, 4382, 1894, 780, 3980, 3710, 5338, 863, 287, 1439, 2101, 425, 5027, 4431, 2323, 534, 2736, 2070, 3165, 309, 5799, 3559, 2598, 1564, 2460, 3947, 3984, 4394, 4244, 5201, 43, 6088, 1641, 26, 178, 775, 2576, 4299, 2607, 5143, 3827, 3399, 5871, 2835, 2752, 5396, 5890, 4821, 3577, 3783, 4312, 1097, 4876, 5910, 3035, 1312, 3828, 2613, 911, 3383, 3187, 5054, 4537, 2213, 1083, 4651, 4817, 3734, 371, 1845, 1257, 2000, 1949, 3897, 2976, 515, 3986, 6094, 2335, 2244, 5631, 5286, 2491, 3348, 4467, 4992, 1877, 514, 4699, 2559, 1428, 5150, 5277, 3561, 4814, 5003, 750, 5222, 5272, 4979, 1960, 2342, 3181, 1753, 894, 2986, 4614, 1653, 8, 5491, 2138, 4906, 1422, 1950, 5908, 5645, 4791, 2966, 465, 2426, 2315, 1532, 5448, 2185, 4824, 1575, 3964, 1080, 1385, 1873, 2891, 5542, 2471, 985, 2679, 1631, 95, 5846, 2221, 4598, 1730, 2152, 3221, 1685, 294, 632, 2656, 3036, 4339, 61, 5048, 5698, 4097, 3226, 5318, 2261, 4186, 2917, 3429, 1988, 1346, 2089, 2515, 4395, 1749, 493, 5421, 948, 4451, 4899, 1501, 527, 3420, 5014, 3722, 5642, 63, 4227, 2013, 3466, 5349, 3662, 4044, 4908, 2693, 4702, 2950, 3362, 1335, 1438, 1709, 2270, 4110, 4482, 4476, 2069, 4134, 4376, 2026, 4502, 6125, 4163, 1622, 4118, 184, 1938, 825, 3788, 5493, 5580, 3733, 2019, 4884, 1539, 5212, 2787, 2975, 3938, 859, 4034, 4819, 2246, 4269, 2580, 1390, 4606, 2367, 2470, 3537, 4135, 1615, 2083, 1696, 2281, 415, 2585, 5407, 4566, 5839, 6032, 5124, 6021, 451, 5626, 3852, 3698, 4735, 679, 2248, 806, 2533, 1798, 5651, 105, 4181, 4358, 1833, 3718, 2996, 3606, 6084, 4352, 5795, 6070, 2420, 6041, 4000, 361, 1037, 5752, 1131, 5000, 471, 599, 4688, 881, 551, 5466, 3375, 1623, 4081, 4247, 796, 3097, 3571, 1009, 167, 2747, 3950, 2814, 1888, 2447, 1357, 3749, 3775, 706, 1349, 999, 3916, 4323, 2242, 2146, 1897, 1307, 4572, 1055, 686, 2581, 2415, 4341, 2049, 6095, 4120, 2850, 3070, 2095, 1022, 5004, 4301, 3495, 2290, 1138, 1973, 4261, 5753, 1978, 2488, 1115, 4100, 205, 3670, 6072, 4778, 4001, 363, 2001, 3991, 430, 3005, 1368, 4602, 3252, 4980, 257, 2172, 1406, 278, 4662, 4900, 2797, 2075, 4025, 5067, 4455, 827, 5876, 5312, 3145, 1853, 5109, 4962, 3448, 3067, 4222, 3839, 5005, 1519, 132, 5984, 836, 3615, 5589, 2328, 1155, 3818, 3384, 4385, 1168, 3745, 1857, 4007, 4639, 3387, 4143, 4277, 6043, 2608, 1222, 1806, 6100, 4363, 2691, 461, 1909, 5796, 4009, 477, 1220, 1778, 676, 5422, 5023, 2056, 261, 468, 2597, 4218, 4885, 2697, 1158, 730, 4357, 5791, 1104, 2785, 4266, 5811, 2962, 1887, 584, 5376, 3242, 2106, 4004, 5423, 4963, 709, 1513, 4719, 2452, 1252, 4570, 5590, 1028, 4392, 2079, 2769, 14, 516, 822, 5809, 6098, 2555, 3253, 1526, 3641, 3740, 4066, 2262, 4845, 2841, 1361, 1337, 5834, 4292, 5414, 1508, 5669, 2698, 5321, 3717, 5792, 1530, 3084, 3013, 4925, 3959, 4349, 3548, 569, 4080, 3106, 4402, 3009, 1597, 732, 4638, 2865, 2800, 2647, 4739, 248, 1306, 2494, 5477, 667, 5072, 2668, 1130, 4460, 3193, 3470, 3170, 2847, 5363, 2979, 3177, 5103, 1799, 5773, 144, 1831, 1289, 5591, 52, 1680, 3334, 3153, 3173, 5078, 5558, 5658, 1594, 4159, 1822, 5254, 646, 4896, 175, 5582, 3107, 3178, 4457, 2799, 2378, 2863, 943, 40, 6131, 2569, 383, 4954, 3279, 2611, 883, 4475, 3761, 1608, 5070, 4415, 4554, 5035, 6024, 2122, 5961, 4780, 5445, 1336, 117, 3822, 5416, 2817, 4029, 6142, 4306, 2666, 2941, 3750, 993, 5013, 5473, 3435, 5442, 4869, 4445, 5790, 4681, 3981, 4077, 2393, 3658, 774, 6118, 3367, 394, 4072, 1274, 3061, 5069, 6090, 2126, 1677, 5127, 583, 21, 3993, 165, 5504, 4407, 1911, 3027, 5055, 1913, 1000, 2030, 338, 5276, 1402, 152, 1424, 2154, 2355, 2792, 3337, 1036, 5292, 1292, 5507, 3523, 3136, 1226, 3050, 4692, 2282, 5866, 850, 3532, 4350, 5243, 5332, 3323, 2522, 3449, 3034, 508, 5451, 326, 4724, 1296, 4843, 842, 5623, 5643, 292, 5918, 3040, 4807, 426, 1996, 1930, 5283, 767, 5024, 3191, 2418, 211, 5322, 5121, 5065, 909, 526, 2305, 3126, 485, 3054, 2804, 5146, 5381, 5956, 3643, 3474, 1011, 378, 5180, 509, 2860, 2645, 3594, 5308, 2336, 4848, 728, 1181, 4682, 4882, 2317, 4052, 511, 221, 5368, 2675, 3653, 5966, 3294, 2037, 4190, 3480, 3802, 2320, 5671, 2427, 2677, 1095, 4073, 799, 3403, 1936, 5852, 1644, 4704, 5993, 2398, 3873, 45, 3502, 1363, 3998, 2538, 3563, 4637, 1854, 3454, 5929, 2051, 3227, 2345, 5625, 3853, 2694, 4544, 5239, 3361, 976, 1706, 3488, 6133, 1294, 3114, 327, 4640, 1636, 5314, 4551, 2978, 5976, 5188, 660, 5316, 555, 716, 2334, 878, 6044, 3041, 6037, 480, 3860, 1789, 5096, 5169, 1401, 443, 3062, 5256, 1086, 581, 1403, 153, 5776, 5772, 46, 3324, 3680, 4958, 2778, 4924, 6025, 3086, 4142, 1520, 4418, 3347, 3621, 3229, 4027, 2751, 3727, 134, 1453, 873, 2112, 4974, 997, 1903, 5205, 3450, 5629, 194, 1515, 2762, 5130, 3801, 1144, 1096, 5836, 159, 547, 1256, 3069, 3830, 2183, 3464, 3275, 5861, 4203, 2682, 450, 4308, 1183, 2714, 3244, 2989, 2702, 1642, 3175, 5151, 544, 505, 2331, 4653, 3388, 2245, 409, 3657, 1280, 860, 1787, 2173, 3533, 1649, 1194, 4141, 2506, 225, 3108, 4217, 1499, 5568, 3197, 2918, 5806, 4102, 1610, 1459, 3540, 5502, 5936, 1998, 5647, 3736, 5398, 5579, 5419, 4987, 5851, 1042, 1153, 2615, 1904, 2222, 2703, 41, 5190, 2625, 1451, 601, 5635, 5661, 122, 273, 1545, 5922, 888, 1712, 1968, 1112, 3649, 4265, 180, 954, 4151, 1073, 5973, 3936, 3941, 530, 3251, 2859, 2914, 1525, 634, 5401, 5051, 5452, 5409, 4556, 2983, 2005, 5108, 5406, 4298, 3469, 1231, 2885, 446, 776, 3033, 2277, 755, 2824, 1191, 3183, 5393, 3720]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [20/50]	lr: 2.50e-04	time: 00:24:44	Loss_train 1.92844	Acc_train 81.99	/	Loss_test 0.24184	Acc_test 72.88
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL3f610ba2-1bdb-42f9-b463-ea87fb3856be/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [42, 59, 25, 30, 14, 70, 35, 69, 68, 93, 53, 77, 55, 21, 5], 'conv1': [23, 382, 345, 191, 75, 104, 365, 190, 144, 99, 100, 264, 56, 324, 158, 92, 180, 152, 150, 296, 133, 140, 121, 163, 105, 146, 88, 349, 94, 179, 33, 36, 41, 201, 202, 159, 26, 13, 39, 354, 336, 373, 219, 335, 254, 374, 206, 344, 76, 186, 50, 274, 30, 314, 166, 70, 73, 225, 247], 'conv2': [445, 875, 1317, 235, 1279, 859, 1307, 119, 187, 662, 784, 1243, 760, 711, 276, 735, 335, 427, 80, 1395, 647, 454, 915, 1086, 356, 247, 1441, 590, 106, 167, 141, 1011, 409, 41, 786, 423, 726, 1464, 914, 817, 1494, 613, 68, 827, 1195, 1184, 835, 801, 519, 1412, 812, 943, 1400, 896, 849, 746, 764, 484, 1126, 1247, 599, 295, 752, 1230, 341, 869, 868, 935, 313, 1532, 1233, 857, 1375, 227, 447, 1297, 1006, 1132, 1271, 782, 22, 731, 425, 472, 1042, 350, 891, 144, 1071, 612, 1474, 1186, 159, 964, 381, 959, 450, 408, 683, 70, 1436, 651, 1113, 1486, 426, 1100, 667, 906, 958, 1012, 456, 1069, 1460, 822, 449, 266, 617, 1269, 121, 1429, 1282, 1530, 1357, 1220, 25, 1320, 461, 474, 917, 953, 937, 243, 971, 441, 635, 644, 614, 854, 1372, 304, 1053, 353, 1032, 79, 289, 305, 1465, 839, 601, 244, 524, 1360, 1471, 1266, 1210, 180, 268, 619, 700, 405, 532, 225, 209, 993, 252, 1358, 440, 229, 28, 207, 545, 346, 326, 421, 501, 847, 1134, 846, 241, 142, 340, 1116, 1046, 214, 640, 1361, 537, 161, 380, 366, 1522, 1211, 1481, 1000, 557, 673, 1454, 627, 430, 1252, 865, 900, 5, 58, 130, 414, 135, 1407, 577, 976, 1500, 942, 294, 981, 78, 1442, 781, 195, 314, 636, 818, 264, 1083, 1110, 204, 1489, 579, 660, 1238, 246, 838], 'conv3': [4076, 2572, 1544, 944, 1163, 5694, 6119, 5015, 4144, 2067, 4382, 1894, 780, 3980, 3710, 5338, 863, 287, 1439, 2101, 425, 5027, 4431, 2323, 534, 2736, 2070, 3165, 309, 5799, 3559, 2598, 1564, 2460, 3947, 3984, 4394, 4244, 5201, 43, 6088, 1641, 26, 178, 775, 2576, 4299, 2607, 5143, 3827, 3399, 5871, 2835, 2752, 5396, 5890, 4821, 3577, 3783, 4312, 1097, 4876, 5910, 3035, 1312, 3828, 2613, 911, 3383, 3187, 5054, 4537, 2213, 1083, 4651, 4817, 3734, 371, 1845, 1257, 2000, 1949, 3897, 2976, 515, 3986, 6094, 2335, 2244, 5631, 5286, 2491, 3348, 4467, 4992, 1877, 514, 4699, 2559, 1428, 5150, 5277, 3561, 4814, 5003, 750, 5222, 5272, 4979, 1960, 2342, 3181, 1753, 894, 2986, 4614, 1653, 8, 5491, 2138, 4906, 1422, 1950, 5908, 5645, 4791, 2966, 465, 2426, 2315, 1532, 5448, 2185, 4824, 1575, 3964, 1080, 1385, 1873, 2891, 5542, 2471, 985, 2679, 1631, 95, 5846, 2221, 4598, 1730, 2152, 3221, 1685, 294, 632, 2656, 3036, 4339, 61, 5048, 5698, 4097, 3226, 5318, 2261, 4186, 2917, 3429, 1988, 1346, 2089, 2515, 4395, 1749, 493, 5421, 948, 4451, 4899, 1501, 527, 3420, 5014, 3722, 5642, 63, 4227, 2013, 3466, 5349, 3662, 4044, 4908, 2693, 4702, 2950, 3362, 1335, 1438, 1709, 2270, 4110, 4482, 4476, 2069, 4134, 4376, 2026, 4502, 6125, 4163, 1622, 4118, 184, 1938, 825, 3788, 5493, 5580, 3733, 2019, 4884, 1539, 5212, 2787, 2975, 3938, 859, 4034, 4819, 2246, 4269, 2580, 1390, 4606, 2367, 2470, 3537, 4135, 1615, 2083, 1696, 2281, 415, 2585, 5407, 4566, 5839, 6032, 5124, 6021, 451, 5626, 3852, 3698, 4735, 679, 2248, 806, 2533, 1798, 5651, 105, 4181, 4358, 1833, 3718, 2996, 3606, 6084, 4352, 5795, 6070, 2420, 6041, 4000, 361, 1037, 5752, 1131, 5000, 471, 599, 4688, 881, 551, 5466, 3375, 1623, 4081, 4247, 796, 3097, 3571, 1009, 167, 2747, 3950, 2814, 1888, 2447, 1357, 3749, 3775, 706, 1349, 999, 3916, 4323, 2242, 2146, 1897, 1307, 4572, 1055, 686, 2581, 2415, 4341, 2049, 6095, 4120, 2850, 3070, 2095, 1022, 5004, 4301, 3495, 2290, 1138, 1973, 4261, 5753, 1978, 2488, 1115, 4100, 205, 3670, 6072, 4778, 4001, 363, 2001, 3991, 430, 3005, 1368, 4602, 3252, 4980, 257, 2172, 1406, 278, 4662, 4900, 2797, 2075, 4025, 5067, 4455, 827, 5876, 5312, 3145, 1853, 5109, 4962, 3448, 3067, 4222, 3839, 5005, 1519, 132, 5984, 836, 3615, 5589, 2328, 1155, 3818, 3384, 4385, 1168, 3745, 1857, 4007, 4639, 3387, 4143, 4277, 6043, 2608, 1222, 1806, 6100, 4363, 2691, 461, 1909, 5796, 4009, 477, 1220, 1778, 676, 5422, 5023, 2056, 261, 468, 2597, 4218, 4885, 2697, 1158, 730, 4357, 5791, 1104, 2785, 4266, 5811, 2962, 1887, 584, 5376, 3242, 2106, 4004, 5423, 4963, 709, 1513, 4719, 2452, 1252, 4570, 5590, 1028, 4392, 2079, 2769, 14, 516, 822, 5809, 6098, 2555, 3253, 1526, 3641, 3740, 4066, 2262, 4845, 2841, 1361, 1337, 5834, 4292, 5414, 1508, 5669, 2698, 5321, 3717, 5792, 1530, 3084, 3013, 4925, 3959, 4349, 3548, 569, 4080, 3106, 4402, 3009, 1597, 732, 4638, 2865, 2800, 2647, 4739, 248, 1306, 2494, 5477, 667, 5072, 2668, 1130, 4460, 3193, 3470, 3170, 2847, 5363, 2979, 3177, 5103, 1799, 5773, 144, 1831, 1289, 5591, 52, 1680, 3334, 3153, 3173, 5078, 5558, 5658, 1594, 4159, 1822, 5254, 646, 4896, 175, 5582, 3107, 3178, 4457, 2799, 2378, 2863, 943, 40, 6131, 2569, 383, 4954, 3279, 2611, 883, 4475, 3761, 1608, 5070, 4415, 4554, 5035, 6024, 2122, 5961, 4780, 5445, 1336, 117, 3822, 5416, 2817, 4029, 6142, 4306, 2666, 2941, 3750, 993, 5013, 5473, 3435, 5442, 4869, 4445, 5790, 4681, 3981, 4077, 2393, 3658, 774, 6118, 3367, 394, 4072, 1274, 3061, 5069, 6090, 2126, 1677, 5127, 583, 21, 3993, 165, 5504, 4407, 1911, 3027, 5055, 1913, 1000, 2030, 338, 5276, 1402, 152, 1424, 2154, 2355, 2792, 3337, 1036, 5292, 1292, 5507, 3523, 3136, 1226, 3050, 4692, 2282, 5866, 850, 3532, 4350, 5243, 5332, 3323, 2522, 3449, 3034, 508, 5451, 326, 4724, 1296, 4843, 842, 5623, 5643, 292, 5918, 3040, 4807, 426, 1996, 1930, 5283, 767, 5024, 3191, 2418, 211, 5322, 5121, 5065, 909, 526, 2305, 3126, 485, 3054, 2804, 5146, 5381, 5956, 3643, 3474, 1011, 378, 5180, 509, 2860, 2645, 3594, 5308, 2336, 4848, 728, 1181, 4682, 4882, 2317, 4052, 511, 221, 5368, 2675, 3653, 5966, 3294, 2037, 4190, 3480, 3802, 2320, 5671, 2427, 2677, 1095, 4073, 799, 3403, 1936, 5852, 1644, 4704, 5993, 2398, 3873, 45, 3502, 1363, 3998, 2538, 3563, 4637, 1854, 3454, 5929, 2051, 3227, 2345, 5625, 3853, 2694, 4544, 5239, 3361, 976, 1706, 3488, 6133, 1294, 3114, 327, 4640, 1636, 5314, 4551, 2978, 5976, 5188, 660, 5316, 555, 716, 2334, 878, 6044, 3041, 6037, 480, 3860, 1789, 5096, 5169, 1401, 443, 3062, 5256, 1086, 581, 1403, 153, 5776, 5772, 46, 3324, 3680, 4958, 2778, 4924, 6025, 3086, 4142, 1520, 4418, 3347, 3621, 3229, 4027, 2751, 3727, 134, 1453, 873, 2112, 4974, 997, 1903, 5205, 3450, 5629, 194, 1515, 2762, 5130, 3801, 1144, 1096, 5836, 159, 547, 1256, 3069, 3830, 2183, 3464, 3275, 5861, 4203, 2682, 450, 4308, 1183, 2714, 3244, 2989, 2702, 1642, 3175, 5151, 544, 505, 2331, 4653, 3388, 2245, 409, 3657, 1280, 860, 1787, 2173, 3533, 1649, 1194, 4141, 2506, 225, 3108, 4217, 1499, 5568, 3197, 2918, 5806, 4102, 1610, 1459, 3540, 5502, 5936, 1998, 5647, 3736, 5398, 5579, 5419, 4987, 5851, 1042, 1153, 2615, 1904, 2222, 2703, 41, 5190, 2625, 1451, 601, 5635, 5661, 122, 273, 1545, 5922, 888, 1712, 1968, 1112, 3649, 4265, 180, 954, 4151, 1073, 5973, 3936, 3941, 530, 3251, 2859, 2914, 1525, 634, 5401, 5051, 5452, 5409, 4556, 2983, 2005, 5108, 5406, 4298, 3469, 1231, 2885, 446, 776, 3033, 2277, 755, 2824, 1191, 3183, 5393, 3720]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [30/50]	lr: 1.25e-04	time: 00:28:53	Loss_train 1.14601	Acc_train 86.61	/	Loss_test 0.22495	Acc_test 73.86
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL3f610ba2-1bdb-42f9-b463-ea87fb3856be/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [42, 59, 25, 30, 14, 70, 35, 69, 68, 93, 53, 77, 55, 21, 5], 'conv1': [23, 382, 345, 191, 75, 104, 365, 190, 144, 99, 100, 264, 56, 324, 158, 92, 180, 152, 150, 296, 133, 140, 121, 163, 105, 146, 88, 349, 94, 179, 33, 36, 41, 201, 202, 159, 26, 13, 39, 354, 336, 373, 219, 335, 254, 374, 206, 344, 76, 186, 50, 274, 30, 314, 166, 70, 73, 225, 247], 'conv2': [445, 875, 1317, 235, 1279, 859, 1307, 119, 187, 662, 784, 1243, 760, 711, 276, 735, 335, 427, 80, 1395, 647, 454, 915, 1086, 356, 247, 1441, 590, 106, 167, 141, 1011, 409, 41, 786, 423, 726, 1464, 914, 817, 1494, 613, 68, 827, 1195, 1184, 835, 801, 519, 1412, 812, 943, 1400, 896, 849, 746, 764, 484, 1126, 1247, 599, 295, 752, 1230, 341, 869, 868, 935, 313, 1532, 1233, 857, 1375, 227, 447, 1297, 1006, 1132, 1271, 782, 22, 731, 425, 472, 1042, 350, 891, 144, 1071, 612, 1474, 1186, 159, 964, 381, 959, 450, 408, 683, 70, 1436, 651, 1113, 1486, 426, 1100, 667, 906, 958, 1012, 456, 1069, 1460, 822, 449, 266, 617, 1269, 121, 1429, 1282, 1530, 1357, 1220, 25, 1320, 461, 474, 917, 953, 937, 243, 971, 441, 635, 644, 614, 854, 1372, 304, 1053, 353, 1032, 79, 289, 305, 1465, 839, 601, 244, 524, 1360, 1471, 1266, 1210, 180, 268, 619, 700, 405, 532, 225, 209, 993, 252, 1358, 440, 229, 28, 207, 545, 346, 326, 421, 501, 847, 1134, 846, 241, 142, 340, 1116, 1046, 214, 640, 1361, 537, 161, 380, 366, 1522, 1211, 1481, 1000, 557, 673, 1454, 627, 430, 1252, 865, 900, 5, 58, 130, 414, 135, 1407, 577, 976, 1500, 942, 294, 981, 78, 1442, 781, 195, 314, 636, 818, 264, 1083, 1110, 204, 1489, 579, 660, 1238, 246, 838], 'conv3': [4076, 2572, 1544, 944, 1163, 5694, 6119, 5015, 4144, 2067, 4382, 1894, 780, 3980, 3710, 5338, 863, 287, 1439, 2101, 425, 5027, 4431, 2323, 534, 2736, 2070, 3165, 309, 5799, 3559, 2598, 1564, 2460, 3947, 3984, 4394, 4244, 5201, 43, 6088, 1641, 26, 178, 775, 2576, 4299, 2607, 5143, 3827, 3399, 5871, 2835, 2752, 5396, 5890, 4821, 3577, 3783, 4312, 1097, 4876, 5910, 3035, 1312, 3828, 2613, 911, 3383, 3187, 5054, 4537, 2213, 1083, 4651, 4817, 3734, 371, 1845, 1257, 2000, 1949, 3897, 2976, 515, 3986, 6094, 2335, 2244, 5631, 5286, 2491, 3348, 4467, 4992, 1877, 514, 4699, 2559, 1428, 5150, 5277, 3561, 4814, 5003, 750, 5222, 5272, 4979, 1960, 2342, 3181, 1753, 894, 2986, 4614, 1653, 8, 5491, 2138, 4906, 1422, 1950, 5908, 5645, 4791, 2966, 465, 2426, 2315, 1532, 5448, 2185, 4824, 1575, 3964, 1080, 1385, 1873, 2891, 5542, 2471, 985, 2679, 1631, 95, 5846, 2221, 4598, 1730, 2152, 3221, 1685, 294, 632, 2656, 3036, 4339, 61, 5048, 5698, 4097, 3226, 5318, 2261, 4186, 2917, 3429, 1988, 1346, 2089, 2515, 4395, 1749, 493, 5421, 948, 4451, 4899, 1501, 527, 3420, 5014, 3722, 5642, 63, 4227, 2013, 3466, 5349, 3662, 4044, 4908, 2693, 4702, 2950, 3362, 1335, 1438, 1709, 2270, 4110, 4482, 4476, 2069, 4134, 4376, 2026, 4502, 6125, 4163, 1622, 4118, 184, 1938, 825, 3788, 5493, 5580, 3733, 2019, 4884, 1539, 5212, 2787, 2975, 3938, 859, 4034, 4819, 2246, 4269, 2580, 1390, 4606, 2367, 2470, 3537, 4135, 1615, 2083, 1696, 2281, 415, 2585, 5407, 4566, 5839, 6032, 5124, 6021, 451, 5626, 3852, 3698, 4735, 679, 2248, 806, 2533, 1798, 5651, 105, 4181, 4358, 1833, 3718, 2996, 3606, 6084, 4352, 5795, 6070, 2420, 6041, 4000, 361, 1037, 5752, 1131, 5000, 471, 599, 4688, 881, 551, 5466, 3375, 1623, 4081, 4247, 796, 3097, 3571, 1009, 167, 2747, 3950, 2814, 1888, 2447, 1357, 3749, 3775, 706, 1349, 999, 3916, 4323, 2242, 2146, 1897, 1307, 4572, 1055, 686, 2581, 2415, 4341, 2049, 6095, 4120, 2850, 3070, 2095, 1022, 5004, 4301, 3495, 2290, 1138, 1973, 4261, 5753, 1978, 2488, 1115, 4100, 205, 3670, 6072, 4778, 4001, 363, 2001, 3991, 430, 3005, 1368, 4602, 3252, 4980, 257, 2172, 1406, 278, 4662, 4900, 2797, 2075, 4025, 5067, 4455, 827, 5876, 5312, 3145, 1853, 5109, 4962, 3448, 3067, 4222, 3839, 5005, 1519, 132, 5984, 836, 3615, 5589, 2328, 1155, 3818, 3384, 4385, 1168, 3745, 1857, 4007, 4639, 3387, 4143, 4277, 6043, 2608, 1222, 1806, 6100, 4363, 2691, 461, 1909, 5796, 4009, 477, 1220, 1778, 676, 5422, 5023, 2056, 261, 468, 2597, 4218, 4885, 2697, 1158, 730, 4357, 5791, 1104, 2785, 4266, 5811, 2962, 1887, 584, 5376, 3242, 2106, 4004, 5423, 4963, 709, 1513, 4719, 2452, 1252, 4570, 5590, 1028, 4392, 2079, 2769, 14, 516, 822, 5809, 6098, 2555, 3253, 1526, 3641, 3740, 4066, 2262, 4845, 2841, 1361, 1337, 5834, 4292, 5414, 1508, 5669, 2698, 5321, 3717, 5792, 1530, 3084, 3013, 4925, 3959, 4349, 3548, 569, 4080, 3106, 4402, 3009, 1597, 732, 4638, 2865, 2800, 2647, 4739, 248, 1306, 2494, 5477, 667, 5072, 2668, 1130, 4460, 3193, 3470, 3170, 2847, 5363, 2979, 3177, 5103, 1799, 5773, 144, 1831, 1289, 5591, 52, 1680, 3334, 3153, 3173, 5078, 5558, 5658, 1594, 4159, 1822, 5254, 646, 4896, 175, 5582, 3107, 3178, 4457, 2799, 2378, 2863, 943, 40, 6131, 2569, 383, 4954, 3279, 2611, 883, 4475, 3761, 1608, 5070, 4415, 4554, 5035, 6024, 2122, 5961, 4780, 5445, 1336, 117, 3822, 5416, 2817, 4029, 6142, 4306, 2666, 2941, 3750, 993, 5013, 5473, 3435, 5442, 4869, 4445, 5790, 4681, 3981, 4077, 2393, 3658, 774, 6118, 3367, 394, 4072, 1274, 3061, 5069, 6090, 2126, 1677, 5127, 583, 21, 3993, 165, 5504, 4407, 1911, 3027, 5055, 1913, 1000, 2030, 338, 5276, 1402, 152, 1424, 2154, 2355, 2792, 3337, 1036, 5292, 1292, 5507, 3523, 3136, 1226, 3050, 4692, 2282, 5866, 850, 3532, 4350, 5243, 5332, 3323, 2522, 3449, 3034, 508, 5451, 326, 4724, 1296, 4843, 842, 5623, 5643, 292, 5918, 3040, 4807, 426, 1996, 1930, 5283, 767, 5024, 3191, 2418, 211, 5322, 5121, 5065, 909, 526, 2305, 3126, 485, 3054, 2804, 5146, 5381, 5956, 3643, 3474, 1011, 378, 5180, 509, 2860, 2645, 3594, 5308, 2336, 4848, 728, 1181, 4682, 4882, 2317, 4052, 511, 221, 5368, 2675, 3653, 5966, 3294, 2037, 4190, 3480, 3802, 2320, 5671, 2427, 2677, 1095, 4073, 799, 3403, 1936, 5852, 1644, 4704, 5993, 2398, 3873, 45, 3502, 1363, 3998, 2538, 3563, 4637, 1854, 3454, 5929, 2051, 3227, 2345, 5625, 3853, 2694, 4544, 5239, 3361, 976, 1706, 3488, 6133, 1294, 3114, 327, 4640, 1636, 5314, 4551, 2978, 5976, 5188, 660, 5316, 555, 716, 2334, 878, 6044, 3041, 6037, 480, 3860, 1789, 5096, 5169, 1401, 443, 3062, 5256, 1086, 581, 1403, 153, 5776, 5772, 46, 3324, 3680, 4958, 2778, 4924, 6025, 3086, 4142, 1520, 4418, 3347, 3621, 3229, 4027, 2751, 3727, 134, 1453, 873, 2112, 4974, 997, 1903, 5205, 3450, 5629, 194, 1515, 2762, 5130, 3801, 1144, 1096, 5836, 159, 547, 1256, 3069, 3830, 2183, 3464, 3275, 5861, 4203, 2682, 450, 4308, 1183, 2714, 3244, 2989, 2702, 1642, 3175, 5151, 544, 505, 2331, 4653, 3388, 2245, 409, 3657, 1280, 860, 1787, 2173, 3533, 1649, 1194, 4141, 2506, 225, 3108, 4217, 1499, 5568, 3197, 2918, 5806, 4102, 1610, 1459, 3540, 5502, 5936, 1998, 5647, 3736, 5398, 5579, 5419, 4987, 5851, 1042, 1153, 2615, 1904, 2222, 2703, 41, 5190, 2625, 1451, 601, 5635, 5661, 122, 273, 1545, 5922, 888, 1712, 1968, 1112, 3649, 4265, 180, 954, 4151, 1073, 5973, 3936, 3941, 530, 3251, 2859, 2914, 1525, 634, 5401, 5051, 5452, 5409, 4556, 2983, 2005, 5108, 5406, 4298, 3469, 1231, 2885, 446, 776, 3033, 2277, 755, 2824, 1191, 3183, 5393, 3720]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [40/50]	lr: 3.13e-05	time: 00:33:01	Loss_train 0.80884	Acc_train 89.11	/	Loss_test 0.21880	Acc_test 74.18
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL3f610ba2-1bdb-42f9-b463-ea87fb3856be/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [42, 59, 25, 30, 14, 70, 35, 69, 68, 93, 53, 77, 55, 21, 5], 'conv1': [23, 382, 345, 191, 75, 104, 365, 190, 144, 99, 100, 264, 56, 324, 158, 92, 180, 152, 150, 296, 133, 140, 121, 163, 105, 146, 88, 349, 94, 179, 33, 36, 41, 201, 202, 159, 26, 13, 39, 354, 336, 373, 219, 335, 254, 374, 206, 344, 76, 186, 50, 274, 30, 314, 166, 70, 73, 225, 247], 'conv2': [445, 875, 1317, 235, 1279, 859, 1307, 119, 187, 662, 784, 1243, 760, 711, 276, 735, 335, 427, 80, 1395, 647, 454, 915, 1086, 356, 247, 1441, 590, 106, 167, 141, 1011, 409, 41, 786, 423, 726, 1464, 914, 817, 1494, 613, 68, 827, 1195, 1184, 835, 801, 519, 1412, 812, 943, 1400, 896, 849, 746, 764, 484, 1126, 1247, 599, 295, 752, 1230, 341, 869, 868, 935, 313, 1532, 1233, 857, 1375, 227, 447, 1297, 1006, 1132, 1271, 782, 22, 731, 425, 472, 1042, 350, 891, 144, 1071, 612, 1474, 1186, 159, 964, 381, 959, 450, 408, 683, 70, 1436, 651, 1113, 1486, 426, 1100, 667, 906, 958, 1012, 456, 1069, 1460, 822, 449, 266, 617, 1269, 121, 1429, 1282, 1530, 1357, 1220, 25, 1320, 461, 474, 917, 953, 937, 243, 971, 441, 635, 644, 614, 854, 1372, 304, 1053, 353, 1032, 79, 289, 305, 1465, 839, 601, 244, 524, 1360, 1471, 1266, 1210, 180, 268, 619, 700, 405, 532, 225, 209, 993, 252, 1358, 440, 229, 28, 207, 545, 346, 326, 421, 501, 847, 1134, 846, 241, 142, 340, 1116, 1046, 214, 640, 1361, 537, 161, 380, 366, 1522, 1211, 1481, 1000, 557, 673, 1454, 627, 430, 1252, 865, 900, 5, 58, 130, 414, 135, 1407, 577, 976, 1500, 942, 294, 981, 78, 1442, 781, 195, 314, 636, 818, 264, 1083, 1110, 204, 1489, 579, 660, 1238, 246, 838], 'conv3': [4076, 2572, 1544, 944, 1163, 5694, 6119, 5015, 4144, 2067, 4382, 1894, 780, 3980, 3710, 5338, 863, 287, 1439, 2101, 425, 5027, 4431, 2323, 534, 2736, 2070, 3165, 309, 5799, 3559, 2598, 1564, 2460, 3947, 3984, 4394, 4244, 5201, 43, 6088, 1641, 26, 178, 775, 2576, 4299, 2607, 5143, 3827, 3399, 5871, 2835, 2752, 5396, 5890, 4821, 3577, 3783, 4312, 1097, 4876, 5910, 3035, 1312, 3828, 2613, 911, 3383, 3187, 5054, 4537, 2213, 1083, 4651, 4817, 3734, 371, 1845, 1257, 2000, 1949, 3897, 2976, 515, 3986, 6094, 2335, 2244, 5631, 5286, 2491, 3348, 4467, 4992, 1877, 514, 4699, 2559, 1428, 5150, 5277, 3561, 4814, 5003, 750, 5222, 5272, 4979, 1960, 2342, 3181, 1753, 894, 2986, 4614, 1653, 8, 5491, 2138, 4906, 1422, 1950, 5908, 5645, 4791, 2966, 465, 2426, 2315, 1532, 5448, 2185, 4824, 1575, 3964, 1080, 1385, 1873, 2891, 5542, 2471, 985, 2679, 1631, 95, 5846, 2221, 4598, 1730, 2152, 3221, 1685, 294, 632, 2656, 3036, 4339, 61, 5048, 5698, 4097, 3226, 5318, 2261, 4186, 2917, 3429, 1988, 1346, 2089, 2515, 4395, 1749, 493, 5421, 948, 4451, 4899, 1501, 527, 3420, 5014, 3722, 5642, 63, 4227, 2013, 3466, 5349, 3662, 4044, 4908, 2693, 4702, 2950, 3362, 1335, 1438, 1709, 2270, 4110, 4482, 4476, 2069, 4134, 4376, 2026, 4502, 6125, 4163, 1622, 4118, 184, 1938, 825, 3788, 5493, 5580, 3733, 2019, 4884, 1539, 5212, 2787, 2975, 3938, 859, 4034, 4819, 2246, 4269, 2580, 1390, 4606, 2367, 2470, 3537, 4135, 1615, 2083, 1696, 2281, 415, 2585, 5407, 4566, 5839, 6032, 5124, 6021, 451, 5626, 3852, 3698, 4735, 679, 2248, 806, 2533, 1798, 5651, 105, 4181, 4358, 1833, 3718, 2996, 3606, 6084, 4352, 5795, 6070, 2420, 6041, 4000, 361, 1037, 5752, 1131, 5000, 471, 599, 4688, 881, 551, 5466, 3375, 1623, 4081, 4247, 796, 3097, 3571, 1009, 167, 2747, 3950, 2814, 1888, 2447, 1357, 3749, 3775, 706, 1349, 999, 3916, 4323, 2242, 2146, 1897, 1307, 4572, 1055, 686, 2581, 2415, 4341, 2049, 6095, 4120, 2850, 3070, 2095, 1022, 5004, 4301, 3495, 2290, 1138, 1973, 4261, 5753, 1978, 2488, 1115, 4100, 205, 3670, 6072, 4778, 4001, 363, 2001, 3991, 430, 3005, 1368, 4602, 3252, 4980, 257, 2172, 1406, 278, 4662, 4900, 2797, 2075, 4025, 5067, 4455, 827, 5876, 5312, 3145, 1853, 5109, 4962, 3448, 3067, 4222, 3839, 5005, 1519, 132, 5984, 836, 3615, 5589, 2328, 1155, 3818, 3384, 4385, 1168, 3745, 1857, 4007, 4639, 3387, 4143, 4277, 6043, 2608, 1222, 1806, 6100, 4363, 2691, 461, 1909, 5796, 4009, 477, 1220, 1778, 676, 5422, 5023, 2056, 261, 468, 2597, 4218, 4885, 2697, 1158, 730, 4357, 5791, 1104, 2785, 4266, 5811, 2962, 1887, 584, 5376, 3242, 2106, 4004, 5423, 4963, 709, 1513, 4719, 2452, 1252, 4570, 5590, 1028, 4392, 2079, 2769, 14, 516, 822, 5809, 6098, 2555, 3253, 1526, 3641, 3740, 4066, 2262, 4845, 2841, 1361, 1337, 5834, 4292, 5414, 1508, 5669, 2698, 5321, 3717, 5792, 1530, 3084, 3013, 4925, 3959, 4349, 3548, 569, 4080, 3106, 4402, 3009, 1597, 732, 4638, 2865, 2800, 2647, 4739, 248, 1306, 2494, 5477, 667, 5072, 2668, 1130, 4460, 3193, 3470, 3170, 2847, 5363, 2979, 3177, 5103, 1799, 5773, 144, 1831, 1289, 5591, 52, 1680, 3334, 3153, 3173, 5078, 5558, 5658, 1594, 4159, 1822, 5254, 646, 4896, 175, 5582, 3107, 3178, 4457, 2799, 2378, 2863, 943, 40, 6131, 2569, 383, 4954, 3279, 2611, 883, 4475, 3761, 1608, 5070, 4415, 4554, 5035, 6024, 2122, 5961, 4780, 5445, 1336, 117, 3822, 5416, 2817, 4029, 6142, 4306, 2666, 2941, 3750, 993, 5013, 5473, 3435, 5442, 4869, 4445, 5790, 4681, 3981, 4077, 2393, 3658, 774, 6118, 3367, 394, 4072, 1274, 3061, 5069, 6090, 2126, 1677, 5127, 583, 21, 3993, 165, 5504, 4407, 1911, 3027, 5055, 1913, 1000, 2030, 338, 5276, 1402, 152, 1424, 2154, 2355, 2792, 3337, 1036, 5292, 1292, 5507, 3523, 3136, 1226, 3050, 4692, 2282, 5866, 850, 3532, 4350, 5243, 5332, 3323, 2522, 3449, 3034, 508, 5451, 326, 4724, 1296, 4843, 842, 5623, 5643, 292, 5918, 3040, 4807, 426, 1996, 1930, 5283, 767, 5024, 3191, 2418, 211, 5322, 5121, 5065, 909, 526, 2305, 3126, 485, 3054, 2804, 5146, 5381, 5956, 3643, 3474, 1011, 378, 5180, 509, 2860, 2645, 3594, 5308, 2336, 4848, 728, 1181, 4682, 4882, 2317, 4052, 511, 221, 5368, 2675, 3653, 5966, 3294, 2037, 4190, 3480, 3802, 2320, 5671, 2427, 2677, 1095, 4073, 799, 3403, 1936, 5852, 1644, 4704, 5993, 2398, 3873, 45, 3502, 1363, 3998, 2538, 3563, 4637, 1854, 3454, 5929, 2051, 3227, 2345, 5625, 3853, 2694, 4544, 5239, 3361, 976, 1706, 3488, 6133, 1294, 3114, 327, 4640, 1636, 5314, 4551, 2978, 5976, 5188, 660, 5316, 555, 716, 2334, 878, 6044, 3041, 6037, 480, 3860, 1789, 5096, 5169, 1401, 443, 3062, 5256, 1086, 581, 1403, 153, 5776, 5772, 46, 3324, 3680, 4958, 2778, 4924, 6025, 3086, 4142, 1520, 4418, 3347, 3621, 3229, 4027, 2751, 3727, 134, 1453, 873, 2112, 4974, 997, 1903, 5205, 3450, 5629, 194, 1515, 2762, 5130, 3801, 1144, 1096, 5836, 159, 547, 1256, 3069, 3830, 2183, 3464, 3275, 5861, 4203, 2682, 450, 4308, 1183, 2714, 3244, 2989, 2702, 1642, 3175, 5151, 544, 505, 2331, 4653, 3388, 2245, 409, 3657, 1280, 860, 1787, 2173, 3533, 1649, 1194, 4141, 2506, 225, 3108, 4217, 1499, 5568, 3197, 2918, 5806, 4102, 1610, 1459, 3540, 5502, 5936, 1998, 5647, 3736, 5398, 5579, 5419, 4987, 5851, 1042, 1153, 2615, 1904, 2222, 2703, 41, 5190, 2625, 1451, 601, 5635, 5661, 122, 273, 1545, 5922, 888, 1712, 1968, 1112, 3649, 4265, 180, 954, 4151, 1073, 5973, 3936, 3941, 530, 3251, 2859, 2914, 1525, 634, 5401, 5051, 5452, 5409, 4556, 2983, 2005, 5108, 5406, 4298, 3469, 1231, 2885, 446, 776, 3033, 2277, 755, 2824, 1191, 3183, 5393, 3720]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [50/50]	lr: 7.81e-06	time: 00:37:10	Loss_train 0.72359	Acc_train 89.78	/	Loss_test 0.21046	Acc_test 74.74
new_head:  {'blocks.4.layer.bias': tensor([ 0.0095,  0.0022, -0.0014,  0.0006,  0.0038,  0.0088, -0.0177,  0.0123,
        -0.0043,  0.0127], device='cuda:0'), 'blocks.4.layer.weight': tensor([[ 0.0753,  0.0216,  0.1031,  ..., -0.0119, -0.0026, -0.0461],
        [-0.0021, -0.0158, -0.0219,  ..., -0.1085,  0.1136,  0.0426],
        [-0.0567, -0.0940,  0.1015,  ..., -0.0155, -0.0621,  0.0444],
        ...,
        [-0.0177,  0.0519,  0.0008,  ...,  0.0218,  0.0824, -0.0101],
        [-0.0454,  0.0595, -0.0771,  ...,  0.0664, -0.1257, -0.1442],
        [-0.0302,  0.0074, -0.0728,  ...,  0.0438, -0.0112, -0.0909]],
       device='cuda:0')}
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL3f610ba2-1bdb-42f9-b463-ea87fb3856be/models
SAVED HEADS THRESHOLD:  0.8236999893188477
SAVED model.topk_kernels {'conv0': [42, 59, 25, 30, 14, 70, 35, 69, 68, 93, 53, 77, 55, 21, 5], 'conv1': [23, 382, 345, 191, 75, 104, 365, 190, 144, 99, 100, 264, 56, 324, 158, 92, 180, 152, 150, 296, 133, 140, 121, 163, 105, 146, 88, 349, 94, 179, 33, 36, 41, 201, 202, 159, 26, 13, 39, 354, 336, 373, 219, 335, 254, 374, 206, 344, 76, 186, 50, 274, 30, 314, 166, 70, 73, 225, 247], 'conv2': [445, 875, 1317, 235, 1279, 859, 1307, 119, 187, 662, 784, 1243, 760, 711, 276, 735, 335, 427, 80, 1395, 647, 454, 915, 1086, 356, 247, 1441, 590, 106, 167, 141, 1011, 409, 41, 786, 423, 726, 1464, 914, 817, 1494, 613, 68, 827, 1195, 1184, 835, 801, 519, 1412, 812, 943, 1400, 896, 849, 746, 764, 484, 1126, 1247, 599, 295, 752, 1230, 341, 869, 868, 935, 313, 1532, 1233, 857, 1375, 227, 447, 1297, 1006, 1132, 1271, 782, 22, 731, 425, 472, 1042, 350, 891, 144, 1071, 612, 1474, 1186, 159, 964, 381, 959, 450, 408, 683, 70, 1436, 651, 1113, 1486, 426, 1100, 667, 906, 958, 1012, 456, 1069, 1460, 822, 449, 266, 617, 1269, 121, 1429, 1282, 1530, 1357, 1220, 25, 1320, 461, 474, 917, 953, 937, 243, 971, 441, 635, 644, 614, 854, 1372, 304, 1053, 353, 1032, 79, 289, 305, 1465, 839, 601, 244, 524, 1360, 1471, 1266, 1210, 180, 268, 619, 700, 405, 532, 225, 209, 993, 252, 1358, 440, 229, 28, 207, 545, 346, 326, 421, 501, 847, 1134, 846, 241, 142, 340, 1116, 1046, 214, 640, 1361, 537, 161, 380, 366, 1522, 1211, 1481, 1000, 557, 673, 1454, 627, 430, 1252, 865, 900, 5, 58, 130, 414, 135, 1407, 577, 976, 1500, 942, 294, 981, 78, 1442, 781, 195, 314, 636, 818, 264, 1083, 1110, 204, 1489, 579, 660, 1238, 246, 838], 'conv3': [4076, 2572, 1544, 944, 1163, 5694, 6119, 5015, 4144, 2067, 4382, 1894, 780, 3980, 3710, 5338, 863, 287, 1439, 2101, 425, 5027, 4431, 2323, 534, 2736, 2070, 3165, 309, 5799, 3559, 2598, 1564, 2460, 3947, 3984, 4394, 4244, 5201, 43, 6088, 1641, 26, 178, 775, 2576, 4299, 2607, 5143, 3827, 3399, 5871, 2835, 2752, 5396, 5890, 4821, 3577, 3783, 4312, 1097, 4876, 5910, 3035, 1312, 3828, 2613, 911, 3383, 3187, 5054, 4537, 2213, 1083, 4651, 4817, 3734, 371, 1845, 1257, 2000, 1949, 3897, 2976, 515, 3986, 6094, 2335, 2244, 5631, 5286, 2491, 3348, 4467, 4992, 1877, 514, 4699, 2559, 1428, 5150, 5277, 3561, 4814, 5003, 750, 5222, 5272, 4979, 1960, 2342, 3181, 1753, 894, 2986, 4614, 1653, 8, 5491, 2138, 4906, 1422, 1950, 5908, 5645, 4791, 2966, 465, 2426, 2315, 1532, 5448, 2185, 4824, 1575, 3964, 1080, 1385, 1873, 2891, 5542, 2471, 985, 2679, 1631, 95, 5846, 2221, 4598, 1730, 2152, 3221, 1685, 294, 632, 2656, 3036, 4339, 61, 5048, 5698, 4097, 3226, 5318, 2261, 4186, 2917, 3429, 1988, 1346, 2089, 2515, 4395, 1749, 493, 5421, 948, 4451, 4899, 1501, 527, 3420, 5014, 3722, 5642, 63, 4227, 2013, 3466, 5349, 3662, 4044, 4908, 2693, 4702, 2950, 3362, 1335, 1438, 1709, 2270, 4110, 4482, 4476, 2069, 4134, 4376, 2026, 4502, 6125, 4163, 1622, 4118, 184, 1938, 825, 3788, 5493, 5580, 3733, 2019, 4884, 1539, 5212, 2787, 2975, 3938, 859, 4034, 4819, 2246, 4269, 2580, 1390, 4606, 2367, 2470, 3537, 4135, 1615, 2083, 1696, 2281, 415, 2585, 5407, 4566, 5839, 6032, 5124, 6021, 451, 5626, 3852, 3698, 4735, 679, 2248, 806, 2533, 1798, 5651, 105, 4181, 4358, 1833, 3718, 2996, 3606, 6084, 4352, 5795, 6070, 2420, 6041, 4000, 361, 1037, 5752, 1131, 5000, 471, 599, 4688, 881, 551, 5466, 3375, 1623, 4081, 4247, 796, 3097, 3571, 1009, 167, 2747, 3950, 2814, 1888, 2447, 1357, 3749, 3775, 706, 1349, 999, 3916, 4323, 2242, 2146, 1897, 1307, 4572, 1055, 686, 2581, 2415, 4341, 2049, 6095, 4120, 2850, 3070, 2095, 1022, 5004, 4301, 3495, 2290, 1138, 1973, 4261, 5753, 1978, 2488, 1115, 4100, 205, 3670, 6072, 4778, 4001, 363, 2001, 3991, 430, 3005, 1368, 4602, 3252, 4980, 257, 2172, 1406, 278, 4662, 4900, 2797, 2075, 4025, 5067, 4455, 827, 5876, 5312, 3145, 1853, 5109, 4962, 3448, 3067, 4222, 3839, 5005, 1519, 132, 5984, 836, 3615, 5589, 2328, 1155, 3818, 3384, 4385, 1168, 3745, 1857, 4007, 4639, 3387, 4143, 4277, 6043, 2608, 1222, 1806, 6100, 4363, 2691, 461, 1909, 5796, 4009, 477, 1220, 1778, 676, 5422, 5023, 2056, 261, 468, 2597, 4218, 4885, 2697, 1158, 730, 4357, 5791, 1104, 2785, 4266, 5811, 2962, 1887, 584, 5376, 3242, 2106, 4004, 5423, 4963, 709, 1513, 4719, 2452, 1252, 4570, 5590, 1028, 4392, 2079, 2769, 14, 516, 822, 5809, 6098, 2555, 3253, 1526, 3641, 3740, 4066, 2262, 4845, 2841, 1361, 1337, 5834, 4292, 5414, 1508, 5669, 2698, 5321, 3717, 5792, 1530, 3084, 3013, 4925, 3959, 4349, 3548, 569, 4080, 3106, 4402, 3009, 1597, 732, 4638, 2865, 2800, 2647, 4739, 248, 1306, 2494, 5477, 667, 5072, 2668, 1130, 4460, 3193, 3470, 3170, 2847, 5363, 2979, 3177, 5103, 1799, 5773, 144, 1831, 1289, 5591, 52, 1680, 3334, 3153, 3173, 5078, 5558, 5658, 1594, 4159, 1822, 5254, 646, 4896, 175, 5582, 3107, 3178, 4457, 2799, 2378, 2863, 943, 40, 6131, 2569, 383, 4954, 3279, 2611, 883, 4475, 3761, 1608, 5070, 4415, 4554, 5035, 6024, 2122, 5961, 4780, 5445, 1336, 117, 3822, 5416, 2817, 4029, 6142, 4306, 2666, 2941, 3750, 993, 5013, 5473, 3435, 5442, 4869, 4445, 5790, 4681, 3981, 4077, 2393, 3658, 774, 6118, 3367, 394, 4072, 1274, 3061, 5069, 6090, 2126, 1677, 5127, 583, 21, 3993, 165, 5504, 4407, 1911, 3027, 5055, 1913, 1000, 2030, 338, 5276, 1402, 152, 1424, 2154, 2355, 2792, 3337, 1036, 5292, 1292, 5507, 3523, 3136, 1226, 3050, 4692, 2282, 5866, 850, 3532, 4350, 5243, 5332, 3323, 2522, 3449, 3034, 508, 5451, 326, 4724, 1296, 4843, 842, 5623, 5643, 292, 5918, 3040, 4807, 426, 1996, 1930, 5283, 767, 5024, 3191, 2418, 211, 5322, 5121, 5065, 909, 526, 2305, 3126, 485, 3054, 2804, 5146, 5381, 5956, 3643, 3474, 1011, 378, 5180, 509, 2860, 2645, 3594, 5308, 2336, 4848, 728, 1181, 4682, 4882, 2317, 4052, 511, 221, 5368, 2675, 3653, 5966, 3294, 2037, 4190, 3480, 3802, 2320, 5671, 2427, 2677, 1095, 4073, 799, 3403, 1936, 5852, 1644, 4704, 5993, 2398, 3873, 45, 3502, 1363, 3998, 2538, 3563, 4637, 1854, 3454, 5929, 2051, 3227, 2345, 5625, 3853, 2694, 4544, 5239, 3361, 976, 1706, 3488, 6133, 1294, 3114, 327, 4640, 1636, 5314, 4551, 2978, 5976, 5188, 660, 5316, 555, 716, 2334, 878, 6044, 3041, 6037, 480, 3860, 1789, 5096, 5169, 1401, 443, 3062, 5256, 1086, 581, 1403, 153, 5776, 5772, 46, 3324, 3680, 4958, 2778, 4924, 6025, 3086, 4142, 1520, 4418, 3347, 3621, 3229, 4027, 2751, 3727, 134, 1453, 873, 2112, 4974, 997, 1903, 5205, 3450, 5629, 194, 1515, 2762, 5130, 3801, 1144, 1096, 5836, 159, 547, 1256, 3069, 3830, 2183, 3464, 3275, 5861, 4203, 2682, 450, 4308, 1183, 2714, 3244, 2989, 2702, 1642, 3175, 5151, 544, 505, 2331, 4653, 3388, 2245, 409, 3657, 1280, 860, 1787, 2173, 3533, 1649, 1194, 4141, 2506, 225, 3108, 4217, 1499, 5568, 3197, 2918, 5806, 4102, 1610, 1459, 3540, 5502, 5936, 1998, 5647, 3736, 5398, 5579, 5419, 4987, 5851, 1042, 1153, 2615, 1904, 2222, 2703, 41, 5190, 2625, 1451, 601, 5635, 5661, 122, 273, 1545, 5922, 888, 1712, 1968, 1112, 3649, 4265, 180, 954, 4151, 1073, 5973, 3936, 3941, 530, 3251, 2859, 2914, 1525, 634, 5401, 5051, 5452, 5409, 4556, 2983, 2005, 5108, 5406, 4298, 3469, 1231, 2885, 446, 776, 3033, 2277, 755, 2824, 1191, 3183, 5393, 3720]}
SAVED model.heads_thresh 0.8236999893188477
RESULT:  {'train_loss': 0.7235910296440125, 'train_acc': 89.78360295295715, 'test_loss': 0.2104601263999939, 'test_acc': 74.73999786376953, 'convergence': 25.553024291992188, 'R1': 15, 'dataset_sup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 96}, 'dataset_unsup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 96}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 10, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}}
IN R1:  {'count': 1, 'R0': {'train_loss': 0.03978398069739342, 'train_acc': 98.27600121498108, 'test_loss': 3.0680172443389893, 'test_acc': 66.5625, 'convergence': 23.72207260131836, 'R1': 1, 'dataset_sup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 5000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 4, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': True, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 100, 'print_freq': 10, 'validation': False, 'continual_learning': False}, 'dataset_unsup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 100000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 32, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'unlabeled', 'nb_epoch': 1, 'print_freq': 25, 'validation': False, 'continual_learning': False}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 32, 'nb_epoch': 1, 'print_freq': 25, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 100, 'print_freq': 10, 'lr': 0.001}}}, 'model_config': {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.11048543456039805, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 24576, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}, 'R1': {'train_loss': 0.7235910296440125, 'train_acc': 89.78360295295715, 'test_loss': 0.2104601263999939, 'test_acc': 74.73999786376953, 'convergence': 25.553024291992188, 'R1': 15, 'dataset_sup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 96}, 'dataset_unsup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 96}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 10, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}}}
SEED:  0
block 0, size : 96 48 48
96
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.11048543456039805, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 24576, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 2.886751345948129
block 1, size : 384 24 24
384
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.11048543456039805, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 24576, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.8505172717997146
block 2, size : 1536 12 12
1536
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.11048543456039805, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 24576, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.4252586358998573
block 3, size : 6144 6 6
6144
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.11048543456039805, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 24576, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.21262931794992865
NOWWWW
6 6
10
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.11048543456039805, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 221184, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
blocks['b%s' % 4] 221184 221184
range = 0.036828478186799345
train_layer_order: consecutive {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 32, 'nb_epoch': 1, 'print_freq': 25, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 100, 'print_freq': 10, 'lr': 0.001}}
The device used will be: 
True
cuda:0
PARAMSSSS IN LOAD:  {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.036828478186799345, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 221184, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
avg_deltas:  {'blocks.0.layer.weight': tensor([-0.2630,  0.0083, -0.0191,  0.0142, -0.7412, -0.1396,  0.1026, -0.1063,
        -0.7140, -0.0898,  0.4283,  0.0795,  0.0033, -0.0165,  0.3822, -0.0873,
        -0.2309, -0.0044, -0.3653, -0.7079,  0.1934,  0.3337,  0.0140, -0.0333,
        -0.1300,  0.3305, -0.1130, -0.0865,  0.0273, -0.0352,  0.1947,  0.2420,
         0.0477, -0.8077,  0.2824,  0.7175, -0.8485, -0.2755,  0.4823, -0.0096,
         0.2895,  0.0272, -0.9085, -0.5497,  0.2544, -0.8254, -0.3190, -0.0294,
        -0.0317,  0.2093, -0.4211,  0.4183,  0.2042,  0.4424, -0.0057,  1.0000,
        -0.2801,  0.3765, -0.5709,  0.1538, -0.5018, -0.0155, -0.3676, -0.0315,
        -0.8681, -0.0184, -0.5272, -0.1788,  0.6942,  0.6900,  0.4434,  0.0252,
        -0.0239, -0.6783, -0.0572, -0.6914,  0.0731,  0.4868, -0.9390,  0.1802,
        -0.3681,  0.2990, -1.1148,  0.4157,  0.0550,  0.0094, -0.6950, -0.2249,
        -0.2276, -0.9358, -0.0143, -0.1122, -0.0390,  0.4208, -0.3224, -0.1714],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.6237e-02,  7.0086e-02,  1.5289e-01,  1.1156e-01,  9.7574e-01,
        -6.2613e-01,  1.2767e-01, -4.3946e-01, -1.3801e-01,  1.2916e-01,
        -1.2124e-02,  4.0093e-01,  4.4163e-01,  2.0606e-01,  3.8869e-01,
         1.6200e-01, -4.5216e-02,  4.4844e-01, -5.0046e-01, -5.1407e-01,
         6.5582e-02,  2.1479e-01,  6.4345e-01, -4.1762e-02,  8.5719e-01,
         8.6206e-01, -2.6951e-02, -2.8461e-01, -1.8812e-01,  2.5312e-01,
         1.2315e-03, -8.4117e-01, -6.1376e-01, -6.5717e-02, -4.6042e-02,
        -2.4405e-03,  1.1480e-01,  7.1823e-02,  7.0447e-03,  1.5133e-02,
         2.4647e-01,  5.7565e-02, -3.1824e-02,  1.6643e-02,  5.1735e-01,
         1.1909e-03, -6.6917e-01,  7.0948e-02, -2.4563e-01, -4.2519e-01,
         3.6524e-01,  1.1463e-01, -1.0976e-01, -1.3186e-01, -3.2995e-01,
         3.2732e-01,  2.7510e-01, -3.3060e-01,  1.4748e-01, -4.5586e-02,
         2.5030e-02, -7.9605e-01,  4.9660e-01, -1.8930e-01, -4.6544e-01,
        -4.7806e-03, -2.9966e-01,  4.6741e-01,  2.5794e-01, -1.8034e-01,
        -1.3379e-01, -8.3788e-02, -1.4426e-01,  4.5113e-01, -1.3760e-01,
         1.7828e-01,  3.4841e-01, -6.4871e-01, -6.6265e-01,  3.1118e-01,
        -1.6078e-01,  4.1702e-02,  9.2368e-02, -1.0922e-03,  3.9919e-01,
        -6.0747e-03, -3.3797e-03, -3.0017e-01, -2.9790e-02,  3.8402e-01,
        -3.3105e-02, -5.7794e-03, -5.1916e-02,  1.1503e-01,  3.3566e-01,
        -2.9883e-01,  2.3668e-01,  1.1334e-02, -2.7307e-01, -2.1446e-02,
         5.3822e-01,  1.0365e-01,  1.8027e-02, -2.6713e-01, -5.3410e-02,
         1.3655e-01, -7.8492e-02, -7.2273e-02, -6.1290e-02,  3.5888e-01,
        -1.2939e-01,  1.6825e-01, -8.3204e-04, -1.5619e-01,  9.2945e-01,
        -2.2071e-01, -6.4073e-02,  3.3851e-01,  4.8310e-01, -2.1324e-01,
        -5.7841e-01, -1.1894e-01,  5.6586e-02,  4.4958e-01,  2.0666e-01,
        -4.2389e-01, -1.1529e-01, -2.1057e-01,  6.1187e-01,  9.5866e-02,
         1.6815e-01,  3.4471e-01,  3.2532e-01, -1.7916e-02,  5.6826e-01,
         1.5608e-01,  4.3487e-02,  8.7703e-01,  2.4791e-01,  9.2718e-02,
         2.6397e-01,  2.0243e-01,  6.6764e-01, -1.7141e-01, -2.6422e-02,
         2.2226e-01,  5.5692e-01, -1.0889e-01,  2.3757e-01, -5.3926e-03,
        -5.4804e-02, -1.5549e-01, -2.7168e-03,  3.6941e-02, -1.8294e-02,
        -1.0627e-01, -4.0721e-01, -7.4082e-01,  4.8937e-02, -4.0754e-01,
        -2.0716e-01, -3.1342e-02,  2.1061e-01, -2.6118e-01, -3.0793e-01,
         1.2309e-01, -3.7757e-01, -1.2043e-02,  4.5154e-01, -1.8111e-01,
        -9.9635e-02,  2.6839e-01,  2.7588e-01,  2.7528e-01,  1.8049e-01,
        -2.5277e-01,  1.6421e-01, -2.7796e-02,  6.6651e-02,  1.2785e-01,
        -1.3338e-01,  2.7490e-01,  9.9449e-02,  7.7394e-02, -7.4721e-02,
        -1.3671e-02,  3.2771e-01,  1.9736e-02,  2.6656e-01, -5.5675e-01,
         3.9457e-01, -9.9514e-02,  2.0779e-02,  1.1948e-01,  3.9379e-02,
         6.0977e-01,  4.1385e-02,  7.0908e-01,  9.7083e-04, -1.7671e-01,
         8.6830e-01, -1.0178e-01, -2.5451e-01,  7.2607e-01, -2.2524e-01,
         1.7009e-01,  3.0318e-01, -2.5535e-02, -2.8519e-01,  7.4481e-01,
        -1.6137e-01,  1.7782e-02,  4.3352e-02, -1.9098e-01, -4.4963e-02,
         2.1432e-01,  1.4275e-01,  5.5266e-01, -6.3516e-01,  2.3370e-01,
         3.1676e-02,  8.7186e-01, -9.1121e-02, -1.8329e-01,  4.6815e-01,
        -1.2057e-01,  4.8097e-01, -7.0653e-01,  4.6713e-03,  4.1877e-01,
        -6.2196e-02,  2.9108e-04, -7.8863e-03,  5.2587e-02,  3.4637e-01,
        -4.2772e-01,  5.3634e-01,  2.8193e-02,  2.4816e-01,  4.7977e-01,
        -2.0887e-01, -1.2205e-01,  1.5709e-03, -4.2056e-01, -1.3009e-03,
        -3.0306e-01,  5.4963e-02, -2.3276e-01,  6.1841e-01, -3.2771e-02,
         6.3384e-01,  3.6256e-02, -2.4193e-01,  1.0148e-02, -6.5338e-01,
         1.5213e-01,  9.0067e-02,  4.0155e-04, -1.8124e-01,  4.1780e-02,
        -3.7421e-02, -1.3815e-01,  9.5144e-01,  3.8597e-01, -4.8607e-02,
         2.0470e-01, -9.4395e-01,  2.5681e-01,  6.9425e-02, -9.6314e-03,
        -2.4465e-01,  6.3043e-01,  2.2519e-02,  5.5401e-04,  4.6864e-01,
        -2.1432e-01,  5.3940e-01,  2.9607e-01,  1.8892e-01, -2.2233e-01,
        -1.5041e-01,  6.5015e-01,  3.2351e-01,  2.9961e-03,  3.2108e-02,
         4.6496e-01, -1.5831e-01, -2.1360e-01,  1.2254e-01, -1.5451e-01,
        -9.9106e-02, -6.4133e-01,  9.8545e-02, -4.8964e-02, -9.1282e-02,
        -5.1380e-01, -3.1101e-01, -1.5879e-01,  6.3941e-01,  1.0695e-01,
        -5.6661e-01, -4.9584e-01,  2.5982e-01, -4.4674e-02,  7.8044e-01,
         1.1890e-01,  5.4891e-01, -8.7752e-03,  1.1075e-01, -1.4531e-01,
        -1.1849e-01, -6.8604e-02,  5.4391e-01, -7.3539e-02,  5.0097e-01,
        -2.7288e-02,  8.6610e-02,  3.8844e-01, -5.5026e-02,  5.7245e-02,
        -4.7057e-03,  1.0283e-01,  7.3576e-01, -3.1376e-01, -1.7154e-01,
         1.2800e-01,  8.4984e-01,  5.2382e-01, -1.6816e-01,  8.0808e-01,
        -4.4904e-01,  6.0777e-01, -6.4261e-01,  2.2741e-01,  1.3904e-02,
         6.8419e-02,  1.1787e-01, -1.2100e-01,  3.1964e-01,  1.3321e-01,
         2.1692e-02, -1.2558e-02, -1.2013e-01,  8.2346e-01, -6.0424e-03,
        -8.0741e-02,  8.8737e-02,  1.0000e+00,  1.2711e-01, -5.0877e-01,
         6.1398e-02, -8.3259e-02, -1.4273e-01,  2.3667e-01,  5.1263e-01,
        -5.8611e-01, -6.0547e-02,  6.6582e-01,  4.7099e-01, -3.1921e-04,
         3.1895e-02,  8.0227e-03, -1.0633e-01,  3.9896e-02,  4.2195e-01,
        -8.1213e-02, -1.1851e-01,  3.1089e-01,  5.8907e-02,  6.1397e-04,
         9.3546e-02,  5.8575e-01,  2.2937e-01, -1.5115e-01, -2.3659e-01,
         5.5872e-02, -2.1307e-01,  1.5340e-01, -1.1432e-02,  2.2660e-01,
         5.7918e-01,  2.6871e-01, -6.8558e-02,  1.5709e-01], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0922,  0.0959,  0.3645,  ...,  0.2347, -0.0171,  0.2686],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0578, -0.0195, -0.0196,  ..., -0.0142, -0.0183,  0.0011],
       device='cuda:0')}
topk_layer:  None
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-0.2630,  0.0083, -0.0191,  0.0142, -0.7412, -0.1396,  0.1026, -0.1063,
        -0.7140, -0.0898,  0.4283,  0.0795,  0.0033, -0.0165,  0.3822, -0.0873,
        -0.2309, -0.0044, -0.3653, -0.7079,  0.1934,  0.3337,  0.0140, -0.0333,
        -0.1300,  0.3305, -0.1130, -0.0865,  0.0273, -0.0352,  0.1947,  0.2420,
         0.0477, -0.8077,  0.2824,  0.7175, -0.8485, -0.2755,  0.4823, -0.0096,
         0.2895,  0.0272, -0.9085, -0.5497,  0.2544, -0.8254, -0.3190, -0.0294,
        -0.0317,  0.2093, -0.4211,  0.4183,  0.2042,  0.4424, -0.0057,  1.0000,
        -0.2801,  0.3765, -0.5709,  0.1538, -0.5018, -0.0155, -0.3676, -0.0315,
        -0.8681, -0.0184, -0.5272, -0.1788,  0.6942,  0.6900,  0.4434,  0.0252,
        -0.0239, -0.6783, -0.0572, -0.6914,  0.0731,  0.4868, -0.9390,  0.1802,
        -0.3681,  0.2990, -1.1148,  0.4157,  0.0550,  0.0094, -0.6950, -0.2249,
        -0.2276, -0.9358, -0.0143, -0.1122, -0.0390,  0.4208, -0.3224, -0.1714],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.6237e-02,  7.0086e-02,  1.5289e-01,  1.1156e-01,  9.7574e-01,
        -6.2613e-01,  1.2767e-01, -4.3946e-01, -1.3801e-01,  1.2916e-01,
        -1.2124e-02,  4.0093e-01,  4.4163e-01,  2.0606e-01,  3.8869e-01,
         1.6200e-01, -4.5216e-02,  4.4844e-01, -5.0046e-01, -5.1407e-01,
         6.5582e-02,  2.1479e-01,  6.4345e-01, -4.1762e-02,  8.5719e-01,
         8.6206e-01, -2.6951e-02, -2.8461e-01, -1.8812e-01,  2.5312e-01,
         1.2315e-03, -8.4117e-01, -6.1376e-01, -6.5717e-02, -4.6042e-02,
        -2.4405e-03,  1.1480e-01,  7.1823e-02,  7.0447e-03,  1.5133e-02,
         2.4647e-01,  5.7565e-02, -3.1824e-02,  1.6643e-02,  5.1735e-01,
         1.1909e-03, -6.6917e-01,  7.0948e-02, -2.4563e-01, -4.2519e-01,
         3.6524e-01,  1.1463e-01, -1.0976e-01, -1.3186e-01, -3.2995e-01,
         3.2732e-01,  2.7510e-01, -3.3060e-01,  1.4748e-01, -4.5586e-02,
         2.5030e-02, -7.9605e-01,  4.9660e-01, -1.8930e-01, -4.6544e-01,
        -4.7806e-03, -2.9966e-01,  4.6741e-01,  2.5794e-01, -1.8034e-01,
        -1.3379e-01, -8.3788e-02, -1.4426e-01,  4.5113e-01, -1.3760e-01,
         1.7828e-01,  3.4841e-01, -6.4871e-01, -6.6265e-01,  3.1118e-01,
        -1.6078e-01,  4.1702e-02,  9.2368e-02, -1.0922e-03,  3.9919e-01,
        -6.0747e-03, -3.3797e-03, -3.0017e-01, -2.9790e-02,  3.8402e-01,
        -3.3105e-02, -5.7794e-03, -5.1916e-02,  1.1503e-01,  3.3566e-01,
        -2.9883e-01,  2.3668e-01,  1.1334e-02, -2.7307e-01, -2.1446e-02,
         5.3822e-01,  1.0365e-01,  1.8027e-02, -2.6713e-01, -5.3410e-02,
         1.3655e-01, -7.8492e-02, -7.2273e-02, -6.1290e-02,  3.5888e-01,
        -1.2939e-01,  1.6825e-01, -8.3204e-04, -1.5619e-01,  9.2945e-01,
        -2.2071e-01, -6.4073e-02,  3.3851e-01,  4.8310e-01, -2.1324e-01,
        -5.7841e-01, -1.1894e-01,  5.6586e-02,  4.4958e-01,  2.0666e-01,
        -4.2389e-01, -1.1529e-01, -2.1057e-01,  6.1187e-01,  9.5866e-02,
         1.6815e-01,  3.4471e-01,  3.2532e-01, -1.7916e-02,  5.6826e-01,
         1.5608e-01,  4.3487e-02,  8.7703e-01,  2.4791e-01,  9.2718e-02,
         2.6397e-01,  2.0243e-01,  6.6764e-01, -1.7141e-01, -2.6422e-02,
         2.2226e-01,  5.5692e-01, -1.0889e-01,  2.3757e-01, -5.3926e-03,
        -5.4804e-02, -1.5549e-01, -2.7168e-03,  3.6941e-02, -1.8294e-02,
        -1.0627e-01, -4.0721e-01, -7.4082e-01,  4.8937e-02, -4.0754e-01,
        -2.0716e-01, -3.1342e-02,  2.1061e-01, -2.6118e-01, -3.0793e-01,
         1.2309e-01, -3.7757e-01, -1.2043e-02,  4.5154e-01, -1.8111e-01,
        -9.9635e-02,  2.6839e-01,  2.7588e-01,  2.7528e-01,  1.8049e-01,
        -2.5277e-01,  1.6421e-01, -2.7796e-02,  6.6651e-02,  1.2785e-01,
        -1.3338e-01,  2.7490e-01,  9.9449e-02,  7.7394e-02, -7.4721e-02,
        -1.3671e-02,  3.2771e-01,  1.9736e-02,  2.6656e-01, -5.5675e-01,
         3.9457e-01, -9.9514e-02,  2.0779e-02,  1.1948e-01,  3.9379e-02,
         6.0977e-01,  4.1385e-02,  7.0908e-01,  9.7083e-04, -1.7671e-01,
         8.6830e-01, -1.0178e-01, -2.5451e-01,  7.2607e-01, -2.2524e-01,
         1.7009e-01,  3.0318e-01, -2.5535e-02, -2.8519e-01,  7.4481e-01,
        -1.6137e-01,  1.7782e-02,  4.3352e-02, -1.9098e-01, -4.4963e-02,
         2.1432e-01,  1.4275e-01,  5.5266e-01, -6.3516e-01,  2.3370e-01,
         3.1676e-02,  8.7186e-01, -9.1121e-02, -1.8329e-01,  4.6815e-01,
        -1.2057e-01,  4.8097e-01, -7.0653e-01,  4.6713e-03,  4.1877e-01,
        -6.2196e-02,  2.9108e-04, -7.8863e-03,  5.2587e-02,  3.4637e-01,
        -4.2772e-01,  5.3634e-01,  2.8193e-02,  2.4816e-01,  4.7977e-01,
        -2.0887e-01, -1.2205e-01,  1.5709e-03, -4.2056e-01, -1.3009e-03,
        -3.0306e-01,  5.4963e-02, -2.3276e-01,  6.1841e-01, -3.2771e-02,
         6.3384e-01,  3.6256e-02, -2.4193e-01,  1.0148e-02, -6.5338e-01,
         1.5213e-01,  9.0067e-02,  4.0155e-04, -1.8124e-01,  4.1780e-02,
        -3.7421e-02, -1.3815e-01,  9.5144e-01,  3.8597e-01, -4.8607e-02,
         2.0470e-01, -9.4395e-01,  2.5681e-01,  6.9425e-02, -9.6314e-03,
        -2.4465e-01,  6.3043e-01,  2.2519e-02,  5.5401e-04,  4.6864e-01,
        -2.1432e-01,  5.3940e-01,  2.9607e-01,  1.8892e-01, -2.2233e-01,
        -1.5041e-01,  6.5015e-01,  3.2351e-01,  2.9961e-03,  3.2108e-02,
         4.6496e-01, -1.5831e-01, -2.1360e-01,  1.2254e-01, -1.5451e-01,
        -9.9106e-02, -6.4133e-01,  9.8545e-02, -4.8964e-02, -9.1282e-02,
        -5.1380e-01, -3.1101e-01, -1.5879e-01,  6.3941e-01,  1.0695e-01,
        -5.6661e-01, -4.9584e-01,  2.5982e-01, -4.4674e-02,  7.8044e-01,
         1.1890e-01,  5.4891e-01, -8.7752e-03,  1.1075e-01, -1.4531e-01,
        -1.1849e-01, -6.8604e-02,  5.4391e-01, -7.3539e-02,  5.0097e-01,
        -2.7288e-02,  8.6610e-02,  3.8844e-01, -5.5026e-02,  5.7245e-02,
        -4.7057e-03,  1.0283e-01,  7.3576e-01, -3.1376e-01, -1.7154e-01,
         1.2800e-01,  8.4984e-01,  5.2382e-01, -1.6816e-01,  8.0808e-01,
        -4.4904e-01,  6.0777e-01, -6.4261e-01,  2.2741e-01,  1.3904e-02,
         6.8419e-02,  1.1787e-01, -1.2100e-01,  3.1964e-01,  1.3321e-01,
         2.1692e-02, -1.2558e-02, -1.2013e-01,  8.2346e-01, -6.0424e-03,
        -8.0741e-02,  8.8737e-02,  1.0000e+00,  1.2711e-01, -5.0877e-01,
         6.1398e-02, -8.3259e-02, -1.4273e-01,  2.3667e-01,  5.1263e-01,
        -5.8611e-01, -6.0547e-02,  6.6582e-01,  4.7099e-01, -3.1921e-04,
         3.1895e-02,  8.0227e-03, -1.0633e-01,  3.9896e-02,  4.2195e-01,
        -8.1213e-02, -1.1851e-01,  3.1089e-01,  5.8907e-02,  6.1397e-04,
         9.3546e-02,  5.8575e-01,  2.2937e-01, -1.5115e-01, -2.3659e-01,
         5.5872e-02, -2.1307e-01,  1.5340e-01, -1.1432e-02,  2.2660e-01,
         5.7918e-01,  2.6871e-01, -6.8558e-02,  1.5709e-01], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0922,  0.0959,  0.3645,  ...,  0.2347, -0.0171,  0.2686],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0578, -0.0195, -0.0196,  ..., -0.0142, -0.0183,  0.0011],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.0.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([-0.2630,  0.0083, -0.0191,  0.0142, -0.7412, -0.1396,  0.1026, -0.1063,
        -0.7140, -0.0898,  0.4283,  0.0795,  0.0033, -0.0165,  0.3822, -0.0873,
        -0.2309, -0.0044, -0.3653, -0.7079,  0.1934,  0.3337,  0.0140, -0.0333,
        -0.1300,  0.3305, -0.1130, -0.0865,  0.0273, -0.0352,  0.1947,  0.2420,
         0.0477, -0.8077,  0.2824,  0.7175, -0.8485, -0.2755,  0.4823, -0.0096,
         0.2895,  0.0272, -0.9085, -0.5497,  0.2544, -0.8254, -0.3190, -0.0294,
        -0.0317,  0.2093, -0.4211,  0.4183,  0.2042,  0.4424, -0.0057,  1.0000,
        -0.2801,  0.3765, -0.5709,  0.1538, -0.5018, -0.0155, -0.3676, -0.0315,
        -0.8681, -0.0184, -0.5272, -0.1788,  0.6942,  0.6900,  0.4434,  0.0252,
        -0.0239, -0.6783, -0.0572, -0.6914,  0.0731,  0.4868, -0.9390,  0.1802,
        -0.3681,  0.2990, -1.1148,  0.4157,  0.0550,  0.0094, -0.6950, -0.2249,
        -0.2276, -0.9358, -0.0143, -0.1122, -0.0390,  0.4208, -0.3224, -0.1714],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-0.2630,  0.0083, -0.0191,  0.0142, -0.7412, -0.1396,  0.1026, -0.1063,
        -0.7140, -0.0898,  0.4283,  0.0795,  0.0033, -0.0165,  0.3822, -0.0873,
        -0.2309, -0.0044, -0.3653, -0.7079,  0.1934,  0.3337,  0.0140, -0.0333,
        -0.1300,  0.3305, -0.1130, -0.0865,  0.0273, -0.0352,  0.1947,  0.2420,
         0.0477, -0.8077,  0.2824,  0.7175, -0.8485, -0.2755,  0.4823, -0.0096,
         0.2895,  0.0272, -0.9085, -0.5497,  0.2544, -0.8254, -0.3190, -0.0294,
        -0.0317,  0.2093, -0.4211,  0.4183,  0.2042,  0.4424, -0.0057,  1.0000,
        -0.2801,  0.3765, -0.5709,  0.1538, -0.5018, -0.0155, -0.3676, -0.0315,
        -0.8681, -0.0184, -0.5272, -0.1788,  0.6942,  0.6900,  0.4434,  0.0252,
        -0.0239, -0.6783, -0.0572, -0.6914,  0.0731,  0.4868, -0.9390,  0.1802,
        -0.3681,  0.2990, -1.1148,  0.4157,  0.0550,  0.0094, -0.6950, -0.2249,
        -0.2276, -0.9358, -0.0143, -0.1122, -0.0390,  0.4208, -0.3224, -0.1714],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.6237e-02,  7.0086e-02,  1.5289e-01,  1.1156e-01,  9.7574e-01,
        -6.2613e-01,  1.2767e-01, -4.3946e-01, -1.3801e-01,  1.2916e-01,
        -1.2124e-02,  4.0093e-01,  4.4163e-01,  2.0606e-01,  3.8869e-01,
         1.6200e-01, -4.5216e-02,  4.4844e-01, -5.0046e-01, -5.1407e-01,
         6.5582e-02,  2.1479e-01,  6.4345e-01, -4.1762e-02,  8.5719e-01,
         8.6206e-01, -2.6951e-02, -2.8461e-01, -1.8812e-01,  2.5312e-01,
         1.2315e-03, -8.4117e-01, -6.1376e-01, -6.5717e-02, -4.6042e-02,
        -2.4405e-03,  1.1480e-01,  7.1823e-02,  7.0447e-03,  1.5133e-02,
         2.4647e-01,  5.7565e-02, -3.1824e-02,  1.6643e-02,  5.1735e-01,
         1.1909e-03, -6.6917e-01,  7.0948e-02, -2.4563e-01, -4.2519e-01,
         3.6524e-01,  1.1463e-01, -1.0976e-01, -1.3186e-01, -3.2995e-01,
         3.2732e-01,  2.7510e-01, -3.3060e-01,  1.4748e-01, -4.5586e-02,
         2.5030e-02, -7.9605e-01,  4.9660e-01, -1.8930e-01, -4.6544e-01,
        -4.7806e-03, -2.9966e-01,  4.6741e-01,  2.5794e-01, -1.8034e-01,
        -1.3379e-01, -8.3788e-02, -1.4426e-01,  4.5113e-01, -1.3760e-01,
         1.7828e-01,  3.4841e-01, -6.4871e-01, -6.6265e-01,  3.1118e-01,
        -1.6078e-01,  4.1702e-02,  9.2368e-02, -1.0922e-03,  3.9919e-01,
        -6.0747e-03, -3.3797e-03, -3.0017e-01, -2.9790e-02,  3.8402e-01,
        -3.3105e-02, -5.7794e-03, -5.1916e-02,  1.1503e-01,  3.3566e-01,
        -2.9883e-01,  2.3668e-01,  1.1334e-02, -2.7307e-01, -2.1446e-02,
         5.3822e-01,  1.0365e-01,  1.8027e-02, -2.6713e-01, -5.3410e-02,
         1.3655e-01, -7.8492e-02, -7.2273e-02, -6.1290e-02,  3.5888e-01,
        -1.2939e-01,  1.6825e-01, -8.3204e-04, -1.5619e-01,  9.2945e-01,
        -2.2071e-01, -6.4073e-02,  3.3851e-01,  4.8310e-01, -2.1324e-01,
        -5.7841e-01, -1.1894e-01,  5.6586e-02,  4.4958e-01,  2.0666e-01,
        -4.2389e-01, -1.1529e-01, -2.1057e-01,  6.1187e-01,  9.5866e-02,
         1.6815e-01,  3.4471e-01,  3.2532e-01, -1.7916e-02,  5.6826e-01,
         1.5608e-01,  4.3487e-02,  8.7703e-01,  2.4791e-01,  9.2718e-02,
         2.6397e-01,  2.0243e-01,  6.6764e-01, -1.7141e-01, -2.6422e-02,
         2.2226e-01,  5.5692e-01, -1.0889e-01,  2.3757e-01, -5.3926e-03,
        -5.4804e-02, -1.5549e-01, -2.7168e-03,  3.6941e-02, -1.8294e-02,
        -1.0627e-01, -4.0721e-01, -7.4082e-01,  4.8937e-02, -4.0754e-01,
        -2.0716e-01, -3.1342e-02,  2.1061e-01, -2.6118e-01, -3.0793e-01,
         1.2309e-01, -3.7757e-01, -1.2043e-02,  4.5154e-01, -1.8111e-01,
        -9.9635e-02,  2.6839e-01,  2.7588e-01,  2.7528e-01,  1.8049e-01,
        -2.5277e-01,  1.6421e-01, -2.7796e-02,  6.6651e-02,  1.2785e-01,
        -1.3338e-01,  2.7490e-01,  9.9449e-02,  7.7394e-02, -7.4721e-02,
        -1.3671e-02,  3.2771e-01,  1.9736e-02,  2.6656e-01, -5.5675e-01,
         3.9457e-01, -9.9514e-02,  2.0779e-02,  1.1948e-01,  3.9379e-02,
         6.0977e-01,  4.1385e-02,  7.0908e-01,  9.7083e-04, -1.7671e-01,
         8.6830e-01, -1.0178e-01, -2.5451e-01,  7.2607e-01, -2.2524e-01,
         1.7009e-01,  3.0318e-01, -2.5535e-02, -2.8519e-01,  7.4481e-01,
        -1.6137e-01,  1.7782e-02,  4.3352e-02, -1.9098e-01, -4.4963e-02,
         2.1432e-01,  1.4275e-01,  5.5266e-01, -6.3516e-01,  2.3370e-01,
         3.1676e-02,  8.7186e-01, -9.1121e-02, -1.8329e-01,  4.6815e-01,
        -1.2057e-01,  4.8097e-01, -7.0653e-01,  4.6713e-03,  4.1877e-01,
        -6.2196e-02,  2.9108e-04, -7.8863e-03,  5.2587e-02,  3.4637e-01,
        -4.2772e-01,  5.3634e-01,  2.8193e-02,  2.4816e-01,  4.7977e-01,
        -2.0887e-01, -1.2205e-01,  1.5709e-03, -4.2056e-01, -1.3009e-03,
        -3.0306e-01,  5.4963e-02, -2.3276e-01,  6.1841e-01, -3.2771e-02,
         6.3384e-01,  3.6256e-02, -2.4193e-01,  1.0148e-02, -6.5338e-01,
         1.5213e-01,  9.0067e-02,  4.0155e-04, -1.8124e-01,  4.1780e-02,
        -3.7421e-02, -1.3815e-01,  9.5144e-01,  3.8597e-01, -4.8607e-02,
         2.0470e-01, -9.4395e-01,  2.5681e-01,  6.9425e-02, -9.6314e-03,
        -2.4465e-01,  6.3043e-01,  2.2519e-02,  5.5401e-04,  4.6864e-01,
        -2.1432e-01,  5.3940e-01,  2.9607e-01,  1.8892e-01, -2.2233e-01,
        -1.5041e-01,  6.5015e-01,  3.2351e-01,  2.9961e-03,  3.2108e-02,
         4.6496e-01, -1.5831e-01, -2.1360e-01,  1.2254e-01, -1.5451e-01,
        -9.9106e-02, -6.4133e-01,  9.8545e-02, -4.8964e-02, -9.1282e-02,
        -5.1380e-01, -3.1101e-01, -1.5879e-01,  6.3941e-01,  1.0695e-01,
        -5.6661e-01, -4.9584e-01,  2.5982e-01, -4.4674e-02,  7.8044e-01,
         1.1890e-01,  5.4891e-01, -8.7752e-03,  1.1075e-01, -1.4531e-01,
        -1.1849e-01, -6.8604e-02,  5.4391e-01, -7.3539e-02,  5.0097e-01,
        -2.7288e-02,  8.6610e-02,  3.8844e-01, -5.5026e-02,  5.7245e-02,
        -4.7057e-03,  1.0283e-01,  7.3576e-01, -3.1376e-01, -1.7154e-01,
         1.2800e-01,  8.4984e-01,  5.2382e-01, -1.6816e-01,  8.0808e-01,
        -4.4904e-01,  6.0777e-01, -6.4261e-01,  2.2741e-01,  1.3904e-02,
         6.8419e-02,  1.1787e-01, -1.2100e-01,  3.1964e-01,  1.3321e-01,
         2.1692e-02, -1.2558e-02, -1.2013e-01,  8.2346e-01, -6.0424e-03,
        -8.0741e-02,  8.8737e-02,  1.0000e+00,  1.2711e-01, -5.0877e-01,
         6.1398e-02, -8.3259e-02, -1.4273e-01,  2.3667e-01,  5.1263e-01,
        -5.8611e-01, -6.0547e-02,  6.6582e-01,  4.7099e-01, -3.1921e-04,
         3.1895e-02,  8.0227e-03, -1.0633e-01,  3.9896e-02,  4.2195e-01,
        -8.1213e-02, -1.1851e-01,  3.1089e-01,  5.8907e-02,  6.1397e-04,
         9.3546e-02,  5.8575e-01,  2.2937e-01, -1.5115e-01, -2.3659e-01,
         5.5872e-02, -2.1307e-01,  1.5340e-01, -1.1432e-02,  2.2660e-01,
         5.7918e-01,  2.6871e-01, -6.8558e-02,  1.5709e-01], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0922,  0.0959,  0.3645,  ...,  0.2347, -0.0171,  0.2686],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0578, -0.0195, -0.0196,  ..., -0.0142, -0.0183,  0.0011],
       device='cuda:0')}
topk_layer inside model after retrieval:  [42, 59, 25, 30, 14, 70, 35, 69, 68, 93, 53, 77, 55, 21, 5]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-0.2630,  0.0083, -0.0191,  0.0142, -0.7412, -0.1396,  0.1026, -0.1063,
        -0.7140, -0.0898,  0.4283,  0.0795,  0.0033, -0.0165,  0.3822, -0.0873,
        -0.2309, -0.0044, -0.3653, -0.7079,  0.1934,  0.3337,  0.0140, -0.0333,
        -0.1300,  0.3305, -0.1130, -0.0865,  0.0273, -0.0352,  0.1947,  0.2420,
         0.0477, -0.8077,  0.2824,  0.7175, -0.8485, -0.2755,  0.4823, -0.0096,
         0.2895,  0.0272, -0.9085, -0.5497,  0.2544, -0.8254, -0.3190, -0.0294,
        -0.0317,  0.2093, -0.4211,  0.4183,  0.2042,  0.4424, -0.0057,  1.0000,
        -0.2801,  0.3765, -0.5709,  0.1538, -0.5018, -0.0155, -0.3676, -0.0315,
        -0.8681, -0.0184, -0.5272, -0.1788,  0.6942,  0.6900,  0.4434,  0.0252,
        -0.0239, -0.6783, -0.0572, -0.6914,  0.0731,  0.4868, -0.9390,  0.1802,
        -0.3681,  0.2990, -1.1148,  0.4157,  0.0550,  0.0094, -0.6950, -0.2249,
        -0.2276, -0.9358, -0.0143, -0.1122, -0.0390,  0.4208, -0.3224, -0.1714],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.6237e-02,  7.0086e-02,  1.5289e-01,  1.1156e-01,  9.7574e-01,
        -6.2613e-01,  1.2767e-01, -4.3946e-01, -1.3801e-01,  1.2916e-01,
        -1.2124e-02,  4.0093e-01,  4.4163e-01,  2.0606e-01,  3.8869e-01,
         1.6200e-01, -4.5216e-02,  4.4844e-01, -5.0046e-01, -5.1407e-01,
         6.5582e-02,  2.1479e-01,  6.4345e-01, -4.1762e-02,  8.5719e-01,
         8.6206e-01, -2.6951e-02, -2.8461e-01, -1.8812e-01,  2.5312e-01,
         1.2315e-03, -8.4117e-01, -6.1376e-01, -6.5717e-02, -4.6042e-02,
        -2.4405e-03,  1.1480e-01,  7.1823e-02,  7.0447e-03,  1.5133e-02,
         2.4647e-01,  5.7565e-02, -3.1824e-02,  1.6643e-02,  5.1735e-01,
         1.1909e-03, -6.6917e-01,  7.0948e-02, -2.4563e-01, -4.2519e-01,
         3.6524e-01,  1.1463e-01, -1.0976e-01, -1.3186e-01, -3.2995e-01,
         3.2732e-01,  2.7510e-01, -3.3060e-01,  1.4748e-01, -4.5586e-02,
         2.5030e-02, -7.9605e-01,  4.9660e-01, -1.8930e-01, -4.6544e-01,
        -4.7806e-03, -2.9966e-01,  4.6741e-01,  2.5794e-01, -1.8034e-01,
        -1.3379e-01, -8.3788e-02, -1.4426e-01,  4.5113e-01, -1.3760e-01,
         1.7828e-01,  3.4841e-01, -6.4871e-01, -6.6265e-01,  3.1118e-01,
        -1.6078e-01,  4.1702e-02,  9.2368e-02, -1.0922e-03,  3.9919e-01,
        -6.0747e-03, -3.3797e-03, -3.0017e-01, -2.9790e-02,  3.8402e-01,
        -3.3105e-02, -5.7794e-03, -5.1916e-02,  1.1503e-01,  3.3566e-01,
        -2.9883e-01,  2.3668e-01,  1.1334e-02, -2.7307e-01, -2.1446e-02,
         5.3822e-01,  1.0365e-01,  1.8027e-02, -2.6713e-01, -5.3410e-02,
         1.3655e-01, -7.8492e-02, -7.2273e-02, -6.1290e-02,  3.5888e-01,
        -1.2939e-01,  1.6825e-01, -8.3204e-04, -1.5619e-01,  9.2945e-01,
        -2.2071e-01, -6.4073e-02,  3.3851e-01,  4.8310e-01, -2.1324e-01,
        -5.7841e-01, -1.1894e-01,  5.6586e-02,  4.4958e-01,  2.0666e-01,
        -4.2389e-01, -1.1529e-01, -2.1057e-01,  6.1187e-01,  9.5866e-02,
         1.6815e-01,  3.4471e-01,  3.2532e-01, -1.7916e-02,  5.6826e-01,
         1.5608e-01,  4.3487e-02,  8.7703e-01,  2.4791e-01,  9.2718e-02,
         2.6397e-01,  2.0243e-01,  6.6764e-01, -1.7141e-01, -2.6422e-02,
         2.2226e-01,  5.5692e-01, -1.0889e-01,  2.3757e-01, -5.3926e-03,
        -5.4804e-02, -1.5549e-01, -2.7168e-03,  3.6941e-02, -1.8294e-02,
        -1.0627e-01, -4.0721e-01, -7.4082e-01,  4.8937e-02, -4.0754e-01,
        -2.0716e-01, -3.1342e-02,  2.1061e-01, -2.6118e-01, -3.0793e-01,
         1.2309e-01, -3.7757e-01, -1.2043e-02,  4.5154e-01, -1.8111e-01,
        -9.9635e-02,  2.6839e-01,  2.7588e-01,  2.7528e-01,  1.8049e-01,
        -2.5277e-01,  1.6421e-01, -2.7796e-02,  6.6651e-02,  1.2785e-01,
        -1.3338e-01,  2.7490e-01,  9.9449e-02,  7.7394e-02, -7.4721e-02,
        -1.3671e-02,  3.2771e-01,  1.9736e-02,  2.6656e-01, -5.5675e-01,
         3.9457e-01, -9.9514e-02,  2.0779e-02,  1.1948e-01,  3.9379e-02,
         6.0977e-01,  4.1385e-02,  7.0908e-01,  9.7083e-04, -1.7671e-01,
         8.6830e-01, -1.0178e-01, -2.5451e-01,  7.2607e-01, -2.2524e-01,
         1.7009e-01,  3.0318e-01, -2.5535e-02, -2.8519e-01,  7.4481e-01,
        -1.6137e-01,  1.7782e-02,  4.3352e-02, -1.9098e-01, -4.4963e-02,
         2.1432e-01,  1.4275e-01,  5.5266e-01, -6.3516e-01,  2.3370e-01,
         3.1676e-02,  8.7186e-01, -9.1121e-02, -1.8329e-01,  4.6815e-01,
        -1.2057e-01,  4.8097e-01, -7.0653e-01,  4.6713e-03,  4.1877e-01,
        -6.2196e-02,  2.9108e-04, -7.8863e-03,  5.2587e-02,  3.4637e-01,
        -4.2772e-01,  5.3634e-01,  2.8193e-02,  2.4816e-01,  4.7977e-01,
        -2.0887e-01, -1.2205e-01,  1.5709e-03, -4.2056e-01, -1.3009e-03,
        -3.0306e-01,  5.4963e-02, -2.3276e-01,  6.1841e-01, -3.2771e-02,
         6.3384e-01,  3.6256e-02, -2.4193e-01,  1.0148e-02, -6.5338e-01,
         1.5213e-01,  9.0067e-02,  4.0155e-04, -1.8124e-01,  4.1780e-02,
        -3.7421e-02, -1.3815e-01,  9.5144e-01,  3.8597e-01, -4.8607e-02,
         2.0470e-01, -9.4395e-01,  2.5681e-01,  6.9425e-02, -9.6314e-03,
        -2.4465e-01,  6.3043e-01,  2.2519e-02,  5.5401e-04,  4.6864e-01,
        -2.1432e-01,  5.3940e-01,  2.9607e-01,  1.8892e-01, -2.2233e-01,
        -1.5041e-01,  6.5015e-01,  3.2351e-01,  2.9961e-03,  3.2108e-02,
         4.6496e-01, -1.5831e-01, -2.1360e-01,  1.2254e-01, -1.5451e-01,
        -9.9106e-02, -6.4133e-01,  9.8545e-02, -4.8964e-02, -9.1282e-02,
        -5.1380e-01, -3.1101e-01, -1.5879e-01,  6.3941e-01,  1.0695e-01,
        -5.6661e-01, -4.9584e-01,  2.5982e-01, -4.4674e-02,  7.8044e-01,
         1.1890e-01,  5.4891e-01, -8.7752e-03,  1.1075e-01, -1.4531e-01,
        -1.1849e-01, -6.8604e-02,  5.4391e-01, -7.3539e-02,  5.0097e-01,
        -2.7288e-02,  8.6610e-02,  3.8844e-01, -5.5026e-02,  5.7245e-02,
        -4.7057e-03,  1.0283e-01,  7.3576e-01, -3.1376e-01, -1.7154e-01,
         1.2800e-01,  8.4984e-01,  5.2382e-01, -1.6816e-01,  8.0808e-01,
        -4.4904e-01,  6.0777e-01, -6.4261e-01,  2.2741e-01,  1.3904e-02,
         6.8419e-02,  1.1787e-01, -1.2100e-01,  3.1964e-01,  1.3321e-01,
         2.1692e-02, -1.2558e-02, -1.2013e-01,  8.2346e-01, -6.0424e-03,
        -8.0741e-02,  8.8737e-02,  1.0000e+00,  1.2711e-01, -5.0877e-01,
         6.1398e-02, -8.3259e-02, -1.4273e-01,  2.3667e-01,  5.1263e-01,
        -5.8611e-01, -6.0547e-02,  6.6582e-01,  4.7099e-01, -3.1921e-04,
         3.1895e-02,  8.0227e-03, -1.0633e-01,  3.9896e-02,  4.2195e-01,
        -8.1213e-02, -1.1851e-01,  3.1089e-01,  5.8907e-02,  6.1397e-04,
         9.3546e-02,  5.8575e-01,  2.2937e-01, -1.5115e-01, -2.3659e-01,
         5.5872e-02, -2.1307e-01,  1.5340e-01, -1.1432e-02,  2.2660e-01,
         5.7918e-01,  2.6871e-01, -6.8558e-02,  1.5709e-01], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0922,  0.0959,  0.3645,  ...,  0.2347, -0.0171,  0.2686],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0578, -0.0195, -0.0196,  ..., -0.0142, -0.0183,  0.0011],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.1.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 3.6237e-02,  7.0086e-02,  1.5289e-01,  1.1156e-01,  9.7574e-01,
        -6.2613e-01,  1.2767e-01, -4.3946e-01, -1.3801e-01,  1.2916e-01,
        -1.2124e-02,  4.0093e-01,  4.4163e-01,  2.0606e-01,  3.8869e-01,
         1.6200e-01, -4.5216e-02,  4.4844e-01, -5.0046e-01, -5.1407e-01,
         6.5582e-02,  2.1479e-01,  6.4345e-01, -4.1762e-02,  8.5719e-01,
         8.6206e-01, -2.6951e-02, -2.8461e-01, -1.8812e-01,  2.5312e-01,
         1.2315e-03, -8.4117e-01, -6.1376e-01, -6.5717e-02, -4.6042e-02,
        -2.4405e-03,  1.1480e-01,  7.1823e-02,  7.0447e-03,  1.5133e-02,
         2.4647e-01,  5.7565e-02, -3.1824e-02,  1.6643e-02,  5.1735e-01,
         1.1909e-03, -6.6917e-01,  7.0948e-02, -2.4563e-01, -4.2519e-01,
         3.6524e-01,  1.1463e-01, -1.0976e-01, -1.3186e-01, -3.2995e-01,
         3.2732e-01,  2.7510e-01, -3.3060e-01,  1.4748e-01, -4.5586e-02,
         2.5030e-02, -7.9605e-01,  4.9660e-01, -1.8930e-01, -4.6544e-01,
        -4.7806e-03, -2.9966e-01,  4.6741e-01,  2.5794e-01, -1.8034e-01,
        -1.3379e-01, -8.3788e-02, -1.4426e-01,  4.5113e-01, -1.3760e-01,
         1.7828e-01,  3.4841e-01, -6.4871e-01, -6.6265e-01,  3.1118e-01,
        -1.6078e-01,  4.1702e-02,  9.2368e-02, -1.0922e-03,  3.9919e-01,
        -6.0747e-03, -3.3797e-03, -3.0017e-01, -2.9790e-02,  3.8402e-01,
        -3.3105e-02, -5.7794e-03, -5.1916e-02,  1.1503e-01,  3.3566e-01,
        -2.9883e-01,  2.3668e-01,  1.1334e-02, -2.7307e-01, -2.1446e-02,
         5.3822e-01,  1.0365e-01,  1.8027e-02, -2.6713e-01, -5.3410e-02,
         1.3655e-01, -7.8492e-02, -7.2273e-02, -6.1290e-02,  3.5888e-01,
        -1.2939e-01,  1.6825e-01, -8.3204e-04, -1.5619e-01,  9.2945e-01,
        -2.2071e-01, -6.4073e-02,  3.3851e-01,  4.8310e-01, -2.1324e-01,
        -5.7841e-01, -1.1894e-01,  5.6586e-02,  4.4958e-01,  2.0666e-01,
        -4.2389e-01, -1.1529e-01, -2.1057e-01,  6.1187e-01,  9.5866e-02,
         1.6815e-01,  3.4471e-01,  3.2532e-01, -1.7916e-02,  5.6826e-01,
         1.5608e-01,  4.3487e-02,  8.7703e-01,  2.4791e-01,  9.2718e-02,
         2.6397e-01,  2.0243e-01,  6.6764e-01, -1.7141e-01, -2.6422e-02,
         2.2226e-01,  5.5692e-01, -1.0889e-01,  2.3757e-01, -5.3926e-03,
        -5.4804e-02, -1.5549e-01, -2.7168e-03,  3.6941e-02, -1.8294e-02,
        -1.0627e-01, -4.0721e-01, -7.4082e-01,  4.8937e-02, -4.0754e-01,
        -2.0716e-01, -3.1342e-02,  2.1061e-01, -2.6118e-01, -3.0793e-01,
         1.2309e-01, -3.7757e-01, -1.2043e-02,  4.5154e-01, -1.8111e-01,
        -9.9635e-02,  2.6839e-01,  2.7588e-01,  2.7528e-01,  1.8049e-01,
        -2.5277e-01,  1.6421e-01, -2.7796e-02,  6.6651e-02,  1.2785e-01,
        -1.3338e-01,  2.7490e-01,  9.9449e-02,  7.7394e-02, -7.4721e-02,
        -1.3671e-02,  3.2771e-01,  1.9736e-02,  2.6656e-01, -5.5675e-01,
         3.9457e-01, -9.9514e-02,  2.0779e-02,  1.1948e-01,  3.9379e-02,
         6.0977e-01,  4.1385e-02,  7.0908e-01,  9.7083e-04, -1.7671e-01,
         8.6830e-01, -1.0178e-01, -2.5451e-01,  7.2607e-01, -2.2524e-01,
         1.7009e-01,  3.0318e-01, -2.5535e-02, -2.8519e-01,  7.4481e-01,
        -1.6137e-01,  1.7782e-02,  4.3352e-02, -1.9098e-01, -4.4963e-02,
         2.1432e-01,  1.4275e-01,  5.5266e-01, -6.3516e-01,  2.3370e-01,
         3.1676e-02,  8.7186e-01, -9.1121e-02, -1.8329e-01,  4.6815e-01,
        -1.2057e-01,  4.8097e-01, -7.0653e-01,  4.6713e-03,  4.1877e-01,
        -6.2196e-02,  2.9108e-04, -7.8863e-03,  5.2587e-02,  3.4637e-01,
        -4.2772e-01,  5.3634e-01,  2.8193e-02,  2.4816e-01,  4.7977e-01,
        -2.0887e-01, -1.2205e-01,  1.5709e-03, -4.2056e-01, -1.3009e-03,
        -3.0306e-01,  5.4963e-02, -2.3276e-01,  6.1841e-01, -3.2771e-02,
         6.3384e-01,  3.6256e-02, -2.4193e-01,  1.0148e-02, -6.5338e-01,
         1.5213e-01,  9.0067e-02,  4.0155e-04, -1.8124e-01,  4.1780e-02,
        -3.7421e-02, -1.3815e-01,  9.5144e-01,  3.8597e-01, -4.8607e-02,
         2.0470e-01, -9.4395e-01,  2.5681e-01,  6.9425e-02, -9.6314e-03,
        -2.4465e-01,  6.3043e-01,  2.2519e-02,  5.5401e-04,  4.6864e-01,
        -2.1432e-01,  5.3940e-01,  2.9607e-01,  1.8892e-01, -2.2233e-01,
        -1.5041e-01,  6.5015e-01,  3.2351e-01,  2.9961e-03,  3.2108e-02,
         4.6496e-01, -1.5831e-01, -2.1360e-01,  1.2254e-01, -1.5451e-01,
        -9.9106e-02, -6.4133e-01,  9.8545e-02, -4.8964e-02, -9.1282e-02,
        -5.1380e-01, -3.1101e-01, -1.5879e-01,  6.3941e-01,  1.0695e-01,
        -5.6661e-01, -4.9584e-01,  2.5982e-01, -4.4674e-02,  7.8044e-01,
         1.1890e-01,  5.4891e-01, -8.7752e-03,  1.1075e-01, -1.4531e-01,
        -1.1849e-01, -6.8604e-02,  5.4391e-01, -7.3539e-02,  5.0097e-01,
        -2.7288e-02,  8.6610e-02,  3.8844e-01, -5.5026e-02,  5.7245e-02,
        -4.7057e-03,  1.0283e-01,  7.3576e-01, -3.1376e-01, -1.7154e-01,
         1.2800e-01,  8.4984e-01,  5.2382e-01, -1.6816e-01,  8.0808e-01,
        -4.4904e-01,  6.0777e-01, -6.4261e-01,  2.2741e-01,  1.3904e-02,
         6.8419e-02,  1.1787e-01, -1.2100e-01,  3.1964e-01,  1.3321e-01,
         2.1692e-02, -1.2558e-02, -1.2013e-01,  8.2346e-01, -6.0424e-03,
        -8.0741e-02,  8.8737e-02,  1.0000e+00,  1.2711e-01, -5.0877e-01,
         6.1398e-02, -8.3259e-02, -1.4273e-01,  2.3667e-01,  5.1263e-01,
        -5.8611e-01, -6.0547e-02,  6.6582e-01,  4.7099e-01, -3.1921e-04,
         3.1895e-02,  8.0227e-03, -1.0633e-01,  3.9896e-02,  4.2195e-01,
        -8.1213e-02, -1.1851e-01,  3.1089e-01,  5.8907e-02,  6.1397e-04,
         9.3546e-02,  5.8575e-01,  2.2937e-01, -1.5115e-01, -2.3659e-01,
         5.5872e-02, -2.1307e-01,  1.5340e-01, -1.1432e-02,  2.2660e-01,
         5.7918e-01,  2.6871e-01, -6.8558e-02,  1.5709e-01], device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-0.2630,  0.0083, -0.0191,  0.0142, -0.7412, -0.1396,  0.1026, -0.1063,
        -0.7140, -0.0898,  0.4283,  0.0795,  0.0033, -0.0165,  0.3822, -0.0873,
        -0.2309, -0.0044, -0.3653, -0.7079,  0.1934,  0.3337,  0.0140, -0.0333,
        -0.1300,  0.3305, -0.1130, -0.0865,  0.0273, -0.0352,  0.1947,  0.2420,
         0.0477, -0.8077,  0.2824,  0.7175, -0.8485, -0.2755,  0.4823, -0.0096,
         0.2895,  0.0272, -0.9085, -0.5497,  0.2544, -0.8254, -0.3190, -0.0294,
        -0.0317,  0.2093, -0.4211,  0.4183,  0.2042,  0.4424, -0.0057,  1.0000,
        -0.2801,  0.3765, -0.5709,  0.1538, -0.5018, -0.0155, -0.3676, -0.0315,
        -0.8681, -0.0184, -0.5272, -0.1788,  0.6942,  0.6900,  0.4434,  0.0252,
        -0.0239, -0.6783, -0.0572, -0.6914,  0.0731,  0.4868, -0.9390,  0.1802,
        -0.3681,  0.2990, -1.1148,  0.4157,  0.0550,  0.0094, -0.6950, -0.2249,
        -0.2276, -0.9358, -0.0143, -0.1122, -0.0390,  0.4208, -0.3224, -0.1714],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.6237e-02,  7.0086e-02,  1.5289e-01,  1.1156e-01,  9.7574e-01,
        -6.2613e-01,  1.2767e-01, -4.3946e-01, -1.3801e-01,  1.2916e-01,
        -1.2124e-02,  4.0093e-01,  4.4163e-01,  2.0606e-01,  3.8869e-01,
         1.6200e-01, -4.5216e-02,  4.4844e-01, -5.0046e-01, -5.1407e-01,
         6.5582e-02,  2.1479e-01,  6.4345e-01, -4.1762e-02,  8.5719e-01,
         8.6206e-01, -2.6951e-02, -2.8461e-01, -1.8812e-01,  2.5312e-01,
         1.2315e-03, -8.4117e-01, -6.1376e-01, -6.5717e-02, -4.6042e-02,
        -2.4405e-03,  1.1480e-01,  7.1823e-02,  7.0447e-03,  1.5133e-02,
         2.4647e-01,  5.7565e-02, -3.1824e-02,  1.6643e-02,  5.1735e-01,
         1.1909e-03, -6.6917e-01,  7.0948e-02, -2.4563e-01, -4.2519e-01,
         3.6524e-01,  1.1463e-01, -1.0976e-01, -1.3186e-01, -3.2995e-01,
         3.2732e-01,  2.7510e-01, -3.3060e-01,  1.4748e-01, -4.5586e-02,
         2.5030e-02, -7.9605e-01,  4.9660e-01, -1.8930e-01, -4.6544e-01,
        -4.7806e-03, -2.9966e-01,  4.6741e-01,  2.5794e-01, -1.8034e-01,
        -1.3379e-01, -8.3788e-02, -1.4426e-01,  4.5113e-01, -1.3760e-01,
         1.7828e-01,  3.4841e-01, -6.4871e-01, -6.6265e-01,  3.1118e-01,
        -1.6078e-01,  4.1702e-02,  9.2368e-02, -1.0922e-03,  3.9919e-01,
        -6.0747e-03, -3.3797e-03, -3.0017e-01, -2.9790e-02,  3.8402e-01,
        -3.3105e-02, -5.7794e-03, -5.1916e-02,  1.1503e-01,  3.3566e-01,
        -2.9883e-01,  2.3668e-01,  1.1334e-02, -2.7307e-01, -2.1446e-02,
         5.3822e-01,  1.0365e-01,  1.8027e-02, -2.6713e-01, -5.3410e-02,
         1.3655e-01, -7.8492e-02, -7.2273e-02, -6.1290e-02,  3.5888e-01,
        -1.2939e-01,  1.6825e-01, -8.3204e-04, -1.5619e-01,  9.2945e-01,
        -2.2071e-01, -6.4073e-02,  3.3851e-01,  4.8310e-01, -2.1324e-01,
        -5.7841e-01, -1.1894e-01,  5.6586e-02,  4.4958e-01,  2.0666e-01,
        -4.2389e-01, -1.1529e-01, -2.1057e-01,  6.1187e-01,  9.5866e-02,
         1.6815e-01,  3.4471e-01,  3.2532e-01, -1.7916e-02,  5.6826e-01,
         1.5608e-01,  4.3487e-02,  8.7703e-01,  2.4791e-01,  9.2718e-02,
         2.6397e-01,  2.0243e-01,  6.6764e-01, -1.7141e-01, -2.6422e-02,
         2.2226e-01,  5.5692e-01, -1.0889e-01,  2.3757e-01, -5.3926e-03,
        -5.4804e-02, -1.5549e-01, -2.7168e-03,  3.6941e-02, -1.8294e-02,
        -1.0627e-01, -4.0721e-01, -7.4082e-01,  4.8937e-02, -4.0754e-01,
        -2.0716e-01, -3.1342e-02,  2.1061e-01, -2.6118e-01, -3.0793e-01,
         1.2309e-01, -3.7757e-01, -1.2043e-02,  4.5154e-01, -1.8111e-01,
        -9.9635e-02,  2.6839e-01,  2.7588e-01,  2.7528e-01,  1.8049e-01,
        -2.5277e-01,  1.6421e-01, -2.7796e-02,  6.6651e-02,  1.2785e-01,
        -1.3338e-01,  2.7490e-01,  9.9449e-02,  7.7394e-02, -7.4721e-02,
        -1.3671e-02,  3.2771e-01,  1.9736e-02,  2.6656e-01, -5.5675e-01,
         3.9457e-01, -9.9514e-02,  2.0779e-02,  1.1948e-01,  3.9379e-02,
         6.0977e-01,  4.1385e-02,  7.0908e-01,  9.7083e-04, -1.7671e-01,
         8.6830e-01, -1.0178e-01, -2.5451e-01,  7.2607e-01, -2.2524e-01,
         1.7009e-01,  3.0318e-01, -2.5535e-02, -2.8519e-01,  7.4481e-01,
        -1.6137e-01,  1.7782e-02,  4.3352e-02, -1.9098e-01, -4.4963e-02,
         2.1432e-01,  1.4275e-01,  5.5266e-01, -6.3516e-01,  2.3370e-01,
         3.1676e-02,  8.7186e-01, -9.1121e-02, -1.8329e-01,  4.6815e-01,
        -1.2057e-01,  4.8097e-01, -7.0653e-01,  4.6713e-03,  4.1877e-01,
        -6.2196e-02,  2.9108e-04, -7.8863e-03,  5.2587e-02,  3.4637e-01,
        -4.2772e-01,  5.3634e-01,  2.8193e-02,  2.4816e-01,  4.7977e-01,
        -2.0887e-01, -1.2205e-01,  1.5709e-03, -4.2056e-01, -1.3009e-03,
        -3.0306e-01,  5.4963e-02, -2.3276e-01,  6.1841e-01, -3.2771e-02,
         6.3384e-01,  3.6256e-02, -2.4193e-01,  1.0148e-02, -6.5338e-01,
         1.5213e-01,  9.0067e-02,  4.0155e-04, -1.8124e-01,  4.1780e-02,
        -3.7421e-02, -1.3815e-01,  9.5144e-01,  3.8597e-01, -4.8607e-02,
         2.0470e-01, -9.4395e-01,  2.5681e-01,  6.9425e-02, -9.6314e-03,
        -2.4465e-01,  6.3043e-01,  2.2519e-02,  5.5401e-04,  4.6864e-01,
        -2.1432e-01,  5.3940e-01,  2.9607e-01,  1.8892e-01, -2.2233e-01,
        -1.5041e-01,  6.5015e-01,  3.2351e-01,  2.9961e-03,  3.2108e-02,
         4.6496e-01, -1.5831e-01, -2.1360e-01,  1.2254e-01, -1.5451e-01,
        -9.9106e-02, -6.4133e-01,  9.8545e-02, -4.8964e-02, -9.1282e-02,
        -5.1380e-01, -3.1101e-01, -1.5879e-01,  6.3941e-01,  1.0695e-01,
        -5.6661e-01, -4.9584e-01,  2.5982e-01, -4.4674e-02,  7.8044e-01,
         1.1890e-01,  5.4891e-01, -8.7752e-03,  1.1075e-01, -1.4531e-01,
        -1.1849e-01, -6.8604e-02,  5.4391e-01, -7.3539e-02,  5.0097e-01,
        -2.7288e-02,  8.6610e-02,  3.8844e-01, -5.5026e-02,  5.7245e-02,
        -4.7057e-03,  1.0283e-01,  7.3576e-01, -3.1376e-01, -1.7154e-01,
         1.2800e-01,  8.4984e-01,  5.2382e-01, -1.6816e-01,  8.0808e-01,
        -4.4904e-01,  6.0777e-01, -6.4261e-01,  2.2741e-01,  1.3904e-02,
         6.8419e-02,  1.1787e-01, -1.2100e-01,  3.1964e-01,  1.3321e-01,
         2.1692e-02, -1.2558e-02, -1.2013e-01,  8.2346e-01, -6.0424e-03,
        -8.0741e-02,  8.8737e-02,  1.0000e+00,  1.2711e-01, -5.0877e-01,
         6.1398e-02, -8.3259e-02, -1.4273e-01,  2.3667e-01,  5.1263e-01,
        -5.8611e-01, -6.0547e-02,  6.6582e-01,  4.7099e-01, -3.1921e-04,
         3.1895e-02,  8.0227e-03, -1.0633e-01,  3.9896e-02,  4.2195e-01,
        -8.1213e-02, -1.1851e-01,  3.1089e-01,  5.8907e-02,  6.1397e-04,
         9.3546e-02,  5.8575e-01,  2.2937e-01, -1.5115e-01, -2.3659e-01,
         5.5872e-02, -2.1307e-01,  1.5340e-01, -1.1432e-02,  2.2660e-01,
         5.7918e-01,  2.6871e-01, -6.8558e-02,  1.5709e-01], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0922,  0.0959,  0.3645,  ...,  0.2347, -0.0171,  0.2686],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0578, -0.0195, -0.0196,  ..., -0.0142, -0.0183,  0.0011],
       device='cuda:0')}
topk_layer inside model after retrieval:  [23, 382, 345, 191, 75, 104, 365, 190, 144, 99, 100, 264, 56, 324, 158, 92, 180, 152, 150, 296, 133, 140, 121, 163, 105, 146, 88, 349, 94, 179, 33, 36, 41, 201, 202, 159, 26, 13, 39, 354, 336, 373, 219, 335, 254, 374, 206, 344, 76, 186, 50, 274, 30, 314, 166, 70, 73, 225, 247]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-0.2630,  0.0083, -0.0191,  0.0142, -0.7412, -0.1396,  0.1026, -0.1063,
        -0.7140, -0.0898,  0.4283,  0.0795,  0.0033, -0.0165,  0.3822, -0.0873,
        -0.2309, -0.0044, -0.3653, -0.7079,  0.1934,  0.3337,  0.0140, -0.0333,
        -0.1300,  0.3305, -0.1130, -0.0865,  0.0273, -0.0352,  0.1947,  0.2420,
         0.0477, -0.8077,  0.2824,  0.7175, -0.8485, -0.2755,  0.4823, -0.0096,
         0.2895,  0.0272, -0.9085, -0.5497,  0.2544, -0.8254, -0.3190, -0.0294,
        -0.0317,  0.2093, -0.4211,  0.4183,  0.2042,  0.4424, -0.0057,  1.0000,
        -0.2801,  0.3765, -0.5709,  0.1538, -0.5018, -0.0155, -0.3676, -0.0315,
        -0.8681, -0.0184, -0.5272, -0.1788,  0.6942,  0.6900,  0.4434,  0.0252,
        -0.0239, -0.6783, -0.0572, -0.6914,  0.0731,  0.4868, -0.9390,  0.1802,
        -0.3681,  0.2990, -1.1148,  0.4157,  0.0550,  0.0094, -0.6950, -0.2249,
        -0.2276, -0.9358, -0.0143, -0.1122, -0.0390,  0.4208, -0.3224, -0.1714],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.6237e-02,  7.0086e-02,  1.5289e-01,  1.1156e-01,  9.7574e-01,
        -6.2613e-01,  1.2767e-01, -4.3946e-01, -1.3801e-01,  1.2916e-01,
        -1.2124e-02,  4.0093e-01,  4.4163e-01,  2.0606e-01,  3.8869e-01,
         1.6200e-01, -4.5216e-02,  4.4844e-01, -5.0046e-01, -5.1407e-01,
         6.5582e-02,  2.1479e-01,  6.4345e-01, -4.1762e-02,  8.5719e-01,
         8.6206e-01, -2.6951e-02, -2.8461e-01, -1.8812e-01,  2.5312e-01,
         1.2315e-03, -8.4117e-01, -6.1376e-01, -6.5717e-02, -4.6042e-02,
        -2.4405e-03,  1.1480e-01,  7.1823e-02,  7.0447e-03,  1.5133e-02,
         2.4647e-01,  5.7565e-02, -3.1824e-02,  1.6643e-02,  5.1735e-01,
         1.1909e-03, -6.6917e-01,  7.0948e-02, -2.4563e-01, -4.2519e-01,
         3.6524e-01,  1.1463e-01, -1.0976e-01, -1.3186e-01, -3.2995e-01,
         3.2732e-01,  2.7510e-01, -3.3060e-01,  1.4748e-01, -4.5586e-02,
         2.5030e-02, -7.9605e-01,  4.9660e-01, -1.8930e-01, -4.6544e-01,
        -4.7806e-03, -2.9966e-01,  4.6741e-01,  2.5794e-01, -1.8034e-01,
        -1.3379e-01, -8.3788e-02, -1.4426e-01,  4.5113e-01, -1.3760e-01,
         1.7828e-01,  3.4841e-01, -6.4871e-01, -6.6265e-01,  3.1118e-01,
        -1.6078e-01,  4.1702e-02,  9.2368e-02, -1.0922e-03,  3.9919e-01,
        -6.0747e-03, -3.3797e-03, -3.0017e-01, -2.9790e-02,  3.8402e-01,
        -3.3105e-02, -5.7794e-03, -5.1916e-02,  1.1503e-01,  3.3566e-01,
        -2.9883e-01,  2.3668e-01,  1.1334e-02, -2.7307e-01, -2.1446e-02,
         5.3822e-01,  1.0365e-01,  1.8027e-02, -2.6713e-01, -5.3410e-02,
         1.3655e-01, -7.8492e-02, -7.2273e-02, -6.1290e-02,  3.5888e-01,
        -1.2939e-01,  1.6825e-01, -8.3204e-04, -1.5619e-01,  9.2945e-01,
        -2.2071e-01, -6.4073e-02,  3.3851e-01,  4.8310e-01, -2.1324e-01,
        -5.7841e-01, -1.1894e-01,  5.6586e-02,  4.4958e-01,  2.0666e-01,
        -4.2389e-01, -1.1529e-01, -2.1057e-01,  6.1187e-01,  9.5866e-02,
         1.6815e-01,  3.4471e-01,  3.2532e-01, -1.7916e-02,  5.6826e-01,
         1.5608e-01,  4.3487e-02,  8.7703e-01,  2.4791e-01,  9.2718e-02,
         2.6397e-01,  2.0243e-01,  6.6764e-01, -1.7141e-01, -2.6422e-02,
         2.2226e-01,  5.5692e-01, -1.0889e-01,  2.3757e-01, -5.3926e-03,
        -5.4804e-02, -1.5549e-01, -2.7168e-03,  3.6941e-02, -1.8294e-02,
        -1.0627e-01, -4.0721e-01, -7.4082e-01,  4.8937e-02, -4.0754e-01,
        -2.0716e-01, -3.1342e-02,  2.1061e-01, -2.6118e-01, -3.0793e-01,
         1.2309e-01, -3.7757e-01, -1.2043e-02,  4.5154e-01, -1.8111e-01,
        -9.9635e-02,  2.6839e-01,  2.7588e-01,  2.7528e-01,  1.8049e-01,
        -2.5277e-01,  1.6421e-01, -2.7796e-02,  6.6651e-02,  1.2785e-01,
        -1.3338e-01,  2.7490e-01,  9.9449e-02,  7.7394e-02, -7.4721e-02,
        -1.3671e-02,  3.2771e-01,  1.9736e-02,  2.6656e-01, -5.5675e-01,
         3.9457e-01, -9.9514e-02,  2.0779e-02,  1.1948e-01,  3.9379e-02,
         6.0977e-01,  4.1385e-02,  7.0908e-01,  9.7083e-04, -1.7671e-01,
         8.6830e-01, -1.0178e-01, -2.5451e-01,  7.2607e-01, -2.2524e-01,
         1.7009e-01,  3.0318e-01, -2.5535e-02, -2.8519e-01,  7.4481e-01,
        -1.6137e-01,  1.7782e-02,  4.3352e-02, -1.9098e-01, -4.4963e-02,
         2.1432e-01,  1.4275e-01,  5.5266e-01, -6.3516e-01,  2.3370e-01,
         3.1676e-02,  8.7186e-01, -9.1121e-02, -1.8329e-01,  4.6815e-01,
        -1.2057e-01,  4.8097e-01, -7.0653e-01,  4.6713e-03,  4.1877e-01,
        -6.2196e-02,  2.9108e-04, -7.8863e-03,  5.2587e-02,  3.4637e-01,
        -4.2772e-01,  5.3634e-01,  2.8193e-02,  2.4816e-01,  4.7977e-01,
        -2.0887e-01, -1.2205e-01,  1.5709e-03, -4.2056e-01, -1.3009e-03,
        -3.0306e-01,  5.4963e-02, -2.3276e-01,  6.1841e-01, -3.2771e-02,
         6.3384e-01,  3.6256e-02, -2.4193e-01,  1.0148e-02, -6.5338e-01,
         1.5213e-01,  9.0067e-02,  4.0155e-04, -1.8124e-01,  4.1780e-02,
        -3.7421e-02, -1.3815e-01,  9.5144e-01,  3.8597e-01, -4.8607e-02,
         2.0470e-01, -9.4395e-01,  2.5681e-01,  6.9425e-02, -9.6314e-03,
        -2.4465e-01,  6.3043e-01,  2.2519e-02,  5.5401e-04,  4.6864e-01,
        -2.1432e-01,  5.3940e-01,  2.9607e-01,  1.8892e-01, -2.2233e-01,
        -1.5041e-01,  6.5015e-01,  3.2351e-01,  2.9961e-03,  3.2108e-02,
         4.6496e-01, -1.5831e-01, -2.1360e-01,  1.2254e-01, -1.5451e-01,
        -9.9106e-02, -6.4133e-01,  9.8545e-02, -4.8964e-02, -9.1282e-02,
        -5.1380e-01, -3.1101e-01, -1.5879e-01,  6.3941e-01,  1.0695e-01,
        -5.6661e-01, -4.9584e-01,  2.5982e-01, -4.4674e-02,  7.8044e-01,
         1.1890e-01,  5.4891e-01, -8.7752e-03,  1.1075e-01, -1.4531e-01,
        -1.1849e-01, -6.8604e-02,  5.4391e-01, -7.3539e-02,  5.0097e-01,
        -2.7288e-02,  8.6610e-02,  3.8844e-01, -5.5026e-02,  5.7245e-02,
        -4.7057e-03,  1.0283e-01,  7.3576e-01, -3.1376e-01, -1.7154e-01,
         1.2800e-01,  8.4984e-01,  5.2382e-01, -1.6816e-01,  8.0808e-01,
        -4.4904e-01,  6.0777e-01, -6.4261e-01,  2.2741e-01,  1.3904e-02,
         6.8419e-02,  1.1787e-01, -1.2100e-01,  3.1964e-01,  1.3321e-01,
         2.1692e-02, -1.2558e-02, -1.2013e-01,  8.2346e-01, -6.0424e-03,
        -8.0741e-02,  8.8737e-02,  1.0000e+00,  1.2711e-01, -5.0877e-01,
         6.1398e-02, -8.3259e-02, -1.4273e-01,  2.3667e-01,  5.1263e-01,
        -5.8611e-01, -6.0547e-02,  6.6582e-01,  4.7099e-01, -3.1921e-04,
         3.1895e-02,  8.0227e-03, -1.0633e-01,  3.9896e-02,  4.2195e-01,
        -8.1213e-02, -1.1851e-01,  3.1089e-01,  5.8907e-02,  6.1397e-04,
         9.3546e-02,  5.8575e-01,  2.2937e-01, -1.5115e-01, -2.3659e-01,
         5.5872e-02, -2.1307e-01,  1.5340e-01, -1.1432e-02,  2.2660e-01,
         5.7918e-01,  2.6871e-01, -6.8558e-02,  1.5709e-01], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0922,  0.0959,  0.3645,  ...,  0.2347, -0.0171,  0.2686],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0578, -0.0195, -0.0196,  ..., -0.0142, -0.0183,  0.0011],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.2.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 0.0922,  0.0959,  0.3645,  ...,  0.2347, -0.0171,  0.2686],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-0.2630,  0.0083, -0.0191,  0.0142, -0.7412, -0.1396,  0.1026, -0.1063,
        -0.7140, -0.0898,  0.4283,  0.0795,  0.0033, -0.0165,  0.3822, -0.0873,
        -0.2309, -0.0044, -0.3653, -0.7079,  0.1934,  0.3337,  0.0140, -0.0333,
        -0.1300,  0.3305, -0.1130, -0.0865,  0.0273, -0.0352,  0.1947,  0.2420,
         0.0477, -0.8077,  0.2824,  0.7175, -0.8485, -0.2755,  0.4823, -0.0096,
         0.2895,  0.0272, -0.9085, -0.5497,  0.2544, -0.8254, -0.3190, -0.0294,
        -0.0317,  0.2093, -0.4211,  0.4183,  0.2042,  0.4424, -0.0057,  1.0000,
        -0.2801,  0.3765, -0.5709,  0.1538, -0.5018, -0.0155, -0.3676, -0.0315,
        -0.8681, -0.0184, -0.5272, -0.1788,  0.6942,  0.6900,  0.4434,  0.0252,
        -0.0239, -0.6783, -0.0572, -0.6914,  0.0731,  0.4868, -0.9390,  0.1802,
        -0.3681,  0.2990, -1.1148,  0.4157,  0.0550,  0.0094, -0.6950, -0.2249,
        -0.2276, -0.9358, -0.0143, -0.1122, -0.0390,  0.4208, -0.3224, -0.1714],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.6237e-02,  7.0086e-02,  1.5289e-01,  1.1156e-01,  9.7574e-01,
        -6.2613e-01,  1.2767e-01, -4.3946e-01, -1.3801e-01,  1.2916e-01,
        -1.2124e-02,  4.0093e-01,  4.4163e-01,  2.0606e-01,  3.8869e-01,
         1.6200e-01, -4.5216e-02,  4.4844e-01, -5.0046e-01, -5.1407e-01,
         6.5582e-02,  2.1479e-01,  6.4345e-01, -4.1762e-02,  8.5719e-01,
         8.6206e-01, -2.6951e-02, -2.8461e-01, -1.8812e-01,  2.5312e-01,
         1.2315e-03, -8.4117e-01, -6.1376e-01, -6.5717e-02, -4.6042e-02,
        -2.4405e-03,  1.1480e-01,  7.1823e-02,  7.0447e-03,  1.5133e-02,
         2.4647e-01,  5.7565e-02, -3.1824e-02,  1.6643e-02,  5.1735e-01,
         1.1909e-03, -6.6917e-01,  7.0948e-02, -2.4563e-01, -4.2519e-01,
         3.6524e-01,  1.1463e-01, -1.0976e-01, -1.3186e-01, -3.2995e-01,
         3.2732e-01,  2.7510e-01, -3.3060e-01,  1.4748e-01, -4.5586e-02,
         2.5030e-02, -7.9605e-01,  4.9660e-01, -1.8930e-01, -4.6544e-01,
        -4.7806e-03, -2.9966e-01,  4.6741e-01,  2.5794e-01, -1.8034e-01,
        -1.3379e-01, -8.3788e-02, -1.4426e-01,  4.5113e-01, -1.3760e-01,
         1.7828e-01,  3.4841e-01, -6.4871e-01, -6.6265e-01,  3.1118e-01,
        -1.6078e-01,  4.1702e-02,  9.2368e-02, -1.0922e-03,  3.9919e-01,
        -6.0747e-03, -3.3797e-03, -3.0017e-01, -2.9790e-02,  3.8402e-01,
        -3.3105e-02, -5.7794e-03, -5.1916e-02,  1.1503e-01,  3.3566e-01,
        -2.9883e-01,  2.3668e-01,  1.1334e-02, -2.7307e-01, -2.1446e-02,
         5.3822e-01,  1.0365e-01,  1.8027e-02, -2.6713e-01, -5.3410e-02,
         1.3655e-01, -7.8492e-02, -7.2273e-02, -6.1290e-02,  3.5888e-01,
        -1.2939e-01,  1.6825e-01, -8.3204e-04, -1.5619e-01,  9.2945e-01,
        -2.2071e-01, -6.4073e-02,  3.3851e-01,  4.8310e-01, -2.1324e-01,
        -5.7841e-01, -1.1894e-01,  5.6586e-02,  4.4958e-01,  2.0666e-01,
        -4.2389e-01, -1.1529e-01, -2.1057e-01,  6.1187e-01,  9.5866e-02,
         1.6815e-01,  3.4471e-01,  3.2532e-01, -1.7916e-02,  5.6826e-01,
         1.5608e-01,  4.3487e-02,  8.7703e-01,  2.4791e-01,  9.2718e-02,
         2.6397e-01,  2.0243e-01,  6.6764e-01, -1.7141e-01, -2.6422e-02,
         2.2226e-01,  5.5692e-01, -1.0889e-01,  2.3757e-01, -5.3926e-03,
        -5.4804e-02, -1.5549e-01, -2.7168e-03,  3.6941e-02, -1.8294e-02,
        -1.0627e-01, -4.0721e-01, -7.4082e-01,  4.8937e-02, -4.0754e-01,
        -2.0716e-01, -3.1342e-02,  2.1061e-01, -2.6118e-01, -3.0793e-01,
         1.2309e-01, -3.7757e-01, -1.2043e-02,  4.5154e-01, -1.8111e-01,
        -9.9635e-02,  2.6839e-01,  2.7588e-01,  2.7528e-01,  1.8049e-01,
        -2.5277e-01,  1.6421e-01, -2.7796e-02,  6.6651e-02,  1.2785e-01,
        -1.3338e-01,  2.7490e-01,  9.9449e-02,  7.7394e-02, -7.4721e-02,
        -1.3671e-02,  3.2771e-01,  1.9736e-02,  2.6656e-01, -5.5675e-01,
         3.9457e-01, -9.9514e-02,  2.0779e-02,  1.1948e-01,  3.9379e-02,
         6.0977e-01,  4.1385e-02,  7.0908e-01,  9.7083e-04, -1.7671e-01,
         8.6830e-01, -1.0178e-01, -2.5451e-01,  7.2607e-01, -2.2524e-01,
         1.7009e-01,  3.0318e-01, -2.5535e-02, -2.8519e-01,  7.4481e-01,
        -1.6137e-01,  1.7782e-02,  4.3352e-02, -1.9098e-01, -4.4963e-02,
         2.1432e-01,  1.4275e-01,  5.5266e-01, -6.3516e-01,  2.3370e-01,
         3.1676e-02,  8.7186e-01, -9.1121e-02, -1.8329e-01,  4.6815e-01,
        -1.2057e-01,  4.8097e-01, -7.0653e-01,  4.6713e-03,  4.1877e-01,
        -6.2196e-02,  2.9108e-04, -7.8863e-03,  5.2587e-02,  3.4637e-01,
        -4.2772e-01,  5.3634e-01,  2.8193e-02,  2.4816e-01,  4.7977e-01,
        -2.0887e-01, -1.2205e-01,  1.5709e-03, -4.2056e-01, -1.3009e-03,
        -3.0306e-01,  5.4963e-02, -2.3276e-01,  6.1841e-01, -3.2771e-02,
         6.3384e-01,  3.6256e-02, -2.4193e-01,  1.0148e-02, -6.5338e-01,
         1.5213e-01,  9.0067e-02,  4.0155e-04, -1.8124e-01,  4.1780e-02,
        -3.7421e-02, -1.3815e-01,  9.5144e-01,  3.8597e-01, -4.8607e-02,
         2.0470e-01, -9.4395e-01,  2.5681e-01,  6.9425e-02, -9.6314e-03,
        -2.4465e-01,  6.3043e-01,  2.2519e-02,  5.5401e-04,  4.6864e-01,
        -2.1432e-01,  5.3940e-01,  2.9607e-01,  1.8892e-01, -2.2233e-01,
        -1.5041e-01,  6.5015e-01,  3.2351e-01,  2.9961e-03,  3.2108e-02,
         4.6496e-01, -1.5831e-01, -2.1360e-01,  1.2254e-01, -1.5451e-01,
        -9.9106e-02, -6.4133e-01,  9.8545e-02, -4.8964e-02, -9.1282e-02,
        -5.1380e-01, -3.1101e-01, -1.5879e-01,  6.3941e-01,  1.0695e-01,
        -5.6661e-01, -4.9584e-01,  2.5982e-01, -4.4674e-02,  7.8044e-01,
         1.1890e-01,  5.4891e-01, -8.7752e-03,  1.1075e-01, -1.4531e-01,
        -1.1849e-01, -6.8604e-02,  5.4391e-01, -7.3539e-02,  5.0097e-01,
        -2.7288e-02,  8.6610e-02,  3.8844e-01, -5.5026e-02,  5.7245e-02,
        -4.7057e-03,  1.0283e-01,  7.3576e-01, -3.1376e-01, -1.7154e-01,
         1.2800e-01,  8.4984e-01,  5.2382e-01, -1.6816e-01,  8.0808e-01,
        -4.4904e-01,  6.0777e-01, -6.4261e-01,  2.2741e-01,  1.3904e-02,
         6.8419e-02,  1.1787e-01, -1.2100e-01,  3.1964e-01,  1.3321e-01,
         2.1692e-02, -1.2558e-02, -1.2013e-01,  8.2346e-01, -6.0424e-03,
        -8.0741e-02,  8.8737e-02,  1.0000e+00,  1.2711e-01, -5.0877e-01,
         6.1398e-02, -8.3259e-02, -1.4273e-01,  2.3667e-01,  5.1263e-01,
        -5.8611e-01, -6.0547e-02,  6.6582e-01,  4.7099e-01, -3.1921e-04,
         3.1895e-02,  8.0227e-03, -1.0633e-01,  3.9896e-02,  4.2195e-01,
        -8.1213e-02, -1.1851e-01,  3.1089e-01,  5.8907e-02,  6.1397e-04,
         9.3546e-02,  5.8575e-01,  2.2937e-01, -1.5115e-01, -2.3659e-01,
         5.5872e-02, -2.1307e-01,  1.5340e-01, -1.1432e-02,  2.2660e-01,
         5.7918e-01,  2.6871e-01, -6.8558e-02,  1.5709e-01], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0922,  0.0959,  0.3645,  ...,  0.2347, -0.0171,  0.2686],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0578, -0.0195, -0.0196,  ..., -0.0142, -0.0183,  0.0011],
       device='cuda:0')}
topk_layer inside model after retrieval:  [445, 875, 1317, 235, 1279, 859, 1307, 119, 187, 662, 784, 1243, 760, 711, 276, 735, 335, 427, 80, 1395, 647, 454, 915, 1086, 356, 247, 1441, 590, 106, 167, 141, 1011, 409, 41, 786, 423, 726, 1464, 914, 817, 1494, 613, 68, 827, 1195, 1184, 835, 801, 519, 1412, 812, 943, 1400, 896, 849, 746, 764, 484, 1126, 1247, 599, 295, 752, 1230, 341, 869, 868, 935, 313, 1532, 1233, 857, 1375, 227, 447, 1297, 1006, 1132, 1271, 782, 22, 731, 425, 472, 1042, 350, 891, 144, 1071, 612, 1474, 1186, 159, 964, 381, 959, 450, 408, 683, 70, 1436, 651, 1113, 1486, 426, 1100, 667, 906, 958, 1012, 456, 1069, 1460, 822, 449, 266, 617, 1269, 121, 1429, 1282, 1530, 1357, 1220, 25, 1320, 461, 474, 917, 953, 937, 243, 971, 441, 635, 644, 614, 854, 1372, 304, 1053, 353, 1032, 79, 289, 305, 1465, 839, 601, 244, 524, 1360, 1471, 1266, 1210, 180, 268, 619, 700, 405, 532, 225, 209, 993, 252, 1358, 440, 229, 28, 207, 545, 346, 326, 421, 501, 847, 1134, 846, 241, 142, 340, 1116, 1046, 214, 640, 1361, 537, 161, 380, 366, 1522, 1211, 1481, 1000, 557, 673, 1454, 627, 430, 1252, 865, 900, 5, 58, 130, 414, 135, 1407, 577, 976, 1500, 942, 294, 981, 78, 1442, 781, 195, 314, 636, 818, 264, 1083, 1110, 204, 1489, 579, 660, 1238, 246, 838]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-0.2630,  0.0083, -0.0191,  0.0142, -0.7412, -0.1396,  0.1026, -0.1063,
        -0.7140, -0.0898,  0.4283,  0.0795,  0.0033, -0.0165,  0.3822, -0.0873,
        -0.2309, -0.0044, -0.3653, -0.7079,  0.1934,  0.3337,  0.0140, -0.0333,
        -0.1300,  0.3305, -0.1130, -0.0865,  0.0273, -0.0352,  0.1947,  0.2420,
         0.0477, -0.8077,  0.2824,  0.7175, -0.8485, -0.2755,  0.4823, -0.0096,
         0.2895,  0.0272, -0.9085, -0.5497,  0.2544, -0.8254, -0.3190, -0.0294,
        -0.0317,  0.2093, -0.4211,  0.4183,  0.2042,  0.4424, -0.0057,  1.0000,
        -0.2801,  0.3765, -0.5709,  0.1538, -0.5018, -0.0155, -0.3676, -0.0315,
        -0.8681, -0.0184, -0.5272, -0.1788,  0.6942,  0.6900,  0.4434,  0.0252,
        -0.0239, -0.6783, -0.0572, -0.6914,  0.0731,  0.4868, -0.9390,  0.1802,
        -0.3681,  0.2990, -1.1148,  0.4157,  0.0550,  0.0094, -0.6950, -0.2249,
        -0.2276, -0.9358, -0.0143, -0.1122, -0.0390,  0.4208, -0.3224, -0.1714],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.6237e-02,  7.0086e-02,  1.5289e-01,  1.1156e-01,  9.7574e-01,
        -6.2613e-01,  1.2767e-01, -4.3946e-01, -1.3801e-01,  1.2916e-01,
        -1.2124e-02,  4.0093e-01,  4.4163e-01,  2.0606e-01,  3.8869e-01,
         1.6200e-01, -4.5216e-02,  4.4844e-01, -5.0046e-01, -5.1407e-01,
         6.5582e-02,  2.1479e-01,  6.4345e-01, -4.1762e-02,  8.5719e-01,
         8.6206e-01, -2.6951e-02, -2.8461e-01, -1.8812e-01,  2.5312e-01,
         1.2315e-03, -8.4117e-01, -6.1376e-01, -6.5717e-02, -4.6042e-02,
        -2.4405e-03,  1.1480e-01,  7.1823e-02,  7.0447e-03,  1.5133e-02,
         2.4647e-01,  5.7565e-02, -3.1824e-02,  1.6643e-02,  5.1735e-01,
         1.1909e-03, -6.6917e-01,  7.0948e-02, -2.4563e-01, -4.2519e-01,
         3.6524e-01,  1.1463e-01, -1.0976e-01, -1.3186e-01, -3.2995e-01,
         3.2732e-01,  2.7510e-01, -3.3060e-01,  1.4748e-01, -4.5586e-02,
         2.5030e-02, -7.9605e-01,  4.9660e-01, -1.8930e-01, -4.6544e-01,
        -4.7806e-03, -2.9966e-01,  4.6741e-01,  2.5794e-01, -1.8034e-01,
        -1.3379e-01, -8.3788e-02, -1.4426e-01,  4.5113e-01, -1.3760e-01,
         1.7828e-01,  3.4841e-01, -6.4871e-01, -6.6265e-01,  3.1118e-01,
        -1.6078e-01,  4.1702e-02,  9.2368e-02, -1.0922e-03,  3.9919e-01,
        -6.0747e-03, -3.3797e-03, -3.0017e-01, -2.9790e-02,  3.8402e-01,
        -3.3105e-02, -5.7794e-03, -5.1916e-02,  1.1503e-01,  3.3566e-01,
        -2.9883e-01,  2.3668e-01,  1.1334e-02, -2.7307e-01, -2.1446e-02,
         5.3822e-01,  1.0365e-01,  1.8027e-02, -2.6713e-01, -5.3410e-02,
         1.3655e-01, -7.8492e-02, -7.2273e-02, -6.1290e-02,  3.5888e-01,
        -1.2939e-01,  1.6825e-01, -8.3204e-04, -1.5619e-01,  9.2945e-01,
        -2.2071e-01, -6.4073e-02,  3.3851e-01,  4.8310e-01, -2.1324e-01,
        -5.7841e-01, -1.1894e-01,  5.6586e-02,  4.4958e-01,  2.0666e-01,
        -4.2389e-01, -1.1529e-01, -2.1057e-01,  6.1187e-01,  9.5866e-02,
         1.6815e-01,  3.4471e-01,  3.2532e-01, -1.7916e-02,  5.6826e-01,
         1.5608e-01,  4.3487e-02,  8.7703e-01,  2.4791e-01,  9.2718e-02,
         2.6397e-01,  2.0243e-01,  6.6764e-01, -1.7141e-01, -2.6422e-02,
         2.2226e-01,  5.5692e-01, -1.0889e-01,  2.3757e-01, -5.3926e-03,
        -5.4804e-02, -1.5549e-01, -2.7168e-03,  3.6941e-02, -1.8294e-02,
        -1.0627e-01, -4.0721e-01, -7.4082e-01,  4.8937e-02, -4.0754e-01,
        -2.0716e-01, -3.1342e-02,  2.1061e-01, -2.6118e-01, -3.0793e-01,
         1.2309e-01, -3.7757e-01, -1.2043e-02,  4.5154e-01, -1.8111e-01,
        -9.9635e-02,  2.6839e-01,  2.7588e-01,  2.7528e-01,  1.8049e-01,
        -2.5277e-01,  1.6421e-01, -2.7796e-02,  6.6651e-02,  1.2785e-01,
        -1.3338e-01,  2.7490e-01,  9.9449e-02,  7.7394e-02, -7.4721e-02,
        -1.3671e-02,  3.2771e-01,  1.9736e-02,  2.6656e-01, -5.5675e-01,
         3.9457e-01, -9.9514e-02,  2.0779e-02,  1.1948e-01,  3.9379e-02,
         6.0977e-01,  4.1385e-02,  7.0908e-01,  9.7083e-04, -1.7671e-01,
         8.6830e-01, -1.0178e-01, -2.5451e-01,  7.2607e-01, -2.2524e-01,
         1.7009e-01,  3.0318e-01, -2.5535e-02, -2.8519e-01,  7.4481e-01,
        -1.6137e-01,  1.7782e-02,  4.3352e-02, -1.9098e-01, -4.4963e-02,
         2.1432e-01,  1.4275e-01,  5.5266e-01, -6.3516e-01,  2.3370e-01,
         3.1676e-02,  8.7186e-01, -9.1121e-02, -1.8329e-01,  4.6815e-01,
        -1.2057e-01,  4.8097e-01, -7.0653e-01,  4.6713e-03,  4.1877e-01,
        -6.2196e-02,  2.9108e-04, -7.8863e-03,  5.2587e-02,  3.4637e-01,
        -4.2772e-01,  5.3634e-01,  2.8193e-02,  2.4816e-01,  4.7977e-01,
        -2.0887e-01, -1.2205e-01,  1.5709e-03, -4.2056e-01, -1.3009e-03,
        -3.0306e-01,  5.4963e-02, -2.3276e-01,  6.1841e-01, -3.2771e-02,
         6.3384e-01,  3.6256e-02, -2.4193e-01,  1.0148e-02, -6.5338e-01,
         1.5213e-01,  9.0067e-02,  4.0155e-04, -1.8124e-01,  4.1780e-02,
        -3.7421e-02, -1.3815e-01,  9.5144e-01,  3.8597e-01, -4.8607e-02,
         2.0470e-01, -9.4395e-01,  2.5681e-01,  6.9425e-02, -9.6314e-03,
        -2.4465e-01,  6.3043e-01,  2.2519e-02,  5.5401e-04,  4.6864e-01,
        -2.1432e-01,  5.3940e-01,  2.9607e-01,  1.8892e-01, -2.2233e-01,
        -1.5041e-01,  6.5015e-01,  3.2351e-01,  2.9961e-03,  3.2108e-02,
         4.6496e-01, -1.5831e-01, -2.1360e-01,  1.2254e-01, -1.5451e-01,
        -9.9106e-02, -6.4133e-01,  9.8545e-02, -4.8964e-02, -9.1282e-02,
        -5.1380e-01, -3.1101e-01, -1.5879e-01,  6.3941e-01,  1.0695e-01,
        -5.6661e-01, -4.9584e-01,  2.5982e-01, -4.4674e-02,  7.8044e-01,
         1.1890e-01,  5.4891e-01, -8.7752e-03,  1.1075e-01, -1.4531e-01,
        -1.1849e-01, -6.8604e-02,  5.4391e-01, -7.3539e-02,  5.0097e-01,
        -2.7288e-02,  8.6610e-02,  3.8844e-01, -5.5026e-02,  5.7245e-02,
        -4.7057e-03,  1.0283e-01,  7.3576e-01, -3.1376e-01, -1.7154e-01,
         1.2800e-01,  8.4984e-01,  5.2382e-01, -1.6816e-01,  8.0808e-01,
        -4.4904e-01,  6.0777e-01, -6.4261e-01,  2.2741e-01,  1.3904e-02,
         6.8419e-02,  1.1787e-01, -1.2100e-01,  3.1964e-01,  1.3321e-01,
         2.1692e-02, -1.2558e-02, -1.2013e-01,  8.2346e-01, -6.0424e-03,
        -8.0741e-02,  8.8737e-02,  1.0000e+00,  1.2711e-01, -5.0877e-01,
         6.1398e-02, -8.3259e-02, -1.4273e-01,  2.3667e-01,  5.1263e-01,
        -5.8611e-01, -6.0547e-02,  6.6582e-01,  4.7099e-01, -3.1921e-04,
         3.1895e-02,  8.0227e-03, -1.0633e-01,  3.9896e-02,  4.2195e-01,
        -8.1213e-02, -1.1851e-01,  3.1089e-01,  5.8907e-02,  6.1397e-04,
         9.3546e-02,  5.8575e-01,  2.2937e-01, -1.5115e-01, -2.3659e-01,
         5.5872e-02, -2.1307e-01,  1.5340e-01, -1.1432e-02,  2.2660e-01,
         5.7918e-01,  2.6871e-01, -6.8558e-02,  1.5709e-01], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0922,  0.0959,  0.3645,  ...,  0.2347, -0.0171,  0.2686],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0578, -0.0195, -0.0196,  ..., -0.0142, -0.0183,  0.0011],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.3.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 0.0578, -0.0195, -0.0196,  ..., -0.0142, -0.0183,  0.0011],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-0.2630,  0.0083, -0.0191,  0.0142, -0.7412, -0.1396,  0.1026, -0.1063,
        -0.7140, -0.0898,  0.4283,  0.0795,  0.0033, -0.0165,  0.3822, -0.0873,
        -0.2309, -0.0044, -0.3653, -0.7079,  0.1934,  0.3337,  0.0140, -0.0333,
        -0.1300,  0.3305, -0.1130, -0.0865,  0.0273, -0.0352,  0.1947,  0.2420,
         0.0477, -0.8077,  0.2824,  0.7175, -0.8485, -0.2755,  0.4823, -0.0096,
         0.2895,  0.0272, -0.9085, -0.5497,  0.2544, -0.8254, -0.3190, -0.0294,
        -0.0317,  0.2093, -0.4211,  0.4183,  0.2042,  0.4424, -0.0057,  1.0000,
        -0.2801,  0.3765, -0.5709,  0.1538, -0.5018, -0.0155, -0.3676, -0.0315,
        -0.8681, -0.0184, -0.5272, -0.1788,  0.6942,  0.6900,  0.4434,  0.0252,
        -0.0239, -0.6783, -0.0572, -0.6914,  0.0731,  0.4868, -0.9390,  0.1802,
        -0.3681,  0.2990, -1.1148,  0.4157,  0.0550,  0.0094, -0.6950, -0.2249,
        -0.2276, -0.9358, -0.0143, -0.1122, -0.0390,  0.4208, -0.3224, -0.1714],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.6237e-02,  7.0086e-02,  1.5289e-01,  1.1156e-01,  9.7574e-01,
        -6.2613e-01,  1.2767e-01, -4.3946e-01, -1.3801e-01,  1.2916e-01,
        -1.2124e-02,  4.0093e-01,  4.4163e-01,  2.0606e-01,  3.8869e-01,
         1.6200e-01, -4.5216e-02,  4.4844e-01, -5.0046e-01, -5.1407e-01,
         6.5582e-02,  2.1479e-01,  6.4345e-01, -4.1762e-02,  8.5719e-01,
         8.6206e-01, -2.6951e-02, -2.8461e-01, -1.8812e-01,  2.5312e-01,
         1.2315e-03, -8.4117e-01, -6.1376e-01, -6.5717e-02, -4.6042e-02,
        -2.4405e-03,  1.1480e-01,  7.1823e-02,  7.0447e-03,  1.5133e-02,
         2.4647e-01,  5.7565e-02, -3.1824e-02,  1.6643e-02,  5.1735e-01,
         1.1909e-03, -6.6917e-01,  7.0948e-02, -2.4563e-01, -4.2519e-01,
         3.6524e-01,  1.1463e-01, -1.0976e-01, -1.3186e-01, -3.2995e-01,
         3.2732e-01,  2.7510e-01, -3.3060e-01,  1.4748e-01, -4.5586e-02,
         2.5030e-02, -7.9605e-01,  4.9660e-01, -1.8930e-01, -4.6544e-01,
        -4.7806e-03, -2.9966e-01,  4.6741e-01,  2.5794e-01, -1.8034e-01,
        -1.3379e-01, -8.3788e-02, -1.4426e-01,  4.5113e-01, -1.3760e-01,
         1.7828e-01,  3.4841e-01, -6.4871e-01, -6.6265e-01,  3.1118e-01,
        -1.6078e-01,  4.1702e-02,  9.2368e-02, -1.0922e-03,  3.9919e-01,
        -6.0747e-03, -3.3797e-03, -3.0017e-01, -2.9790e-02,  3.8402e-01,
        -3.3105e-02, -5.7794e-03, -5.1916e-02,  1.1503e-01,  3.3566e-01,
        -2.9883e-01,  2.3668e-01,  1.1334e-02, -2.7307e-01, -2.1446e-02,
         5.3822e-01,  1.0365e-01,  1.8027e-02, -2.6713e-01, -5.3410e-02,
         1.3655e-01, -7.8492e-02, -7.2273e-02, -6.1290e-02,  3.5888e-01,
        -1.2939e-01,  1.6825e-01, -8.3204e-04, -1.5619e-01,  9.2945e-01,
        -2.2071e-01, -6.4073e-02,  3.3851e-01,  4.8310e-01, -2.1324e-01,
        -5.7841e-01, -1.1894e-01,  5.6586e-02,  4.4958e-01,  2.0666e-01,
        -4.2389e-01, -1.1529e-01, -2.1057e-01,  6.1187e-01,  9.5866e-02,
         1.6815e-01,  3.4471e-01,  3.2532e-01, -1.7916e-02,  5.6826e-01,
         1.5608e-01,  4.3487e-02,  8.7703e-01,  2.4791e-01,  9.2718e-02,
         2.6397e-01,  2.0243e-01,  6.6764e-01, -1.7141e-01, -2.6422e-02,
         2.2226e-01,  5.5692e-01, -1.0889e-01,  2.3757e-01, -5.3926e-03,
        -5.4804e-02, -1.5549e-01, -2.7168e-03,  3.6941e-02, -1.8294e-02,
        -1.0627e-01, -4.0721e-01, -7.4082e-01,  4.8937e-02, -4.0754e-01,
        -2.0716e-01, -3.1342e-02,  2.1061e-01, -2.6118e-01, -3.0793e-01,
         1.2309e-01, -3.7757e-01, -1.2043e-02,  4.5154e-01, -1.8111e-01,
        -9.9635e-02,  2.6839e-01,  2.7588e-01,  2.7528e-01,  1.8049e-01,
        -2.5277e-01,  1.6421e-01, -2.7796e-02,  6.6651e-02,  1.2785e-01,
        -1.3338e-01,  2.7490e-01,  9.9449e-02,  7.7394e-02, -7.4721e-02,
        -1.3671e-02,  3.2771e-01,  1.9736e-02,  2.6656e-01, -5.5675e-01,
         3.9457e-01, -9.9514e-02,  2.0779e-02,  1.1948e-01,  3.9379e-02,
         6.0977e-01,  4.1385e-02,  7.0908e-01,  9.7083e-04, -1.7671e-01,
         8.6830e-01, -1.0178e-01, -2.5451e-01,  7.2607e-01, -2.2524e-01,
         1.7009e-01,  3.0318e-01, -2.5535e-02, -2.8519e-01,  7.4481e-01,
        -1.6137e-01,  1.7782e-02,  4.3352e-02, -1.9098e-01, -4.4963e-02,
         2.1432e-01,  1.4275e-01,  5.5266e-01, -6.3516e-01,  2.3370e-01,
         3.1676e-02,  8.7186e-01, -9.1121e-02, -1.8329e-01,  4.6815e-01,
        -1.2057e-01,  4.8097e-01, -7.0653e-01,  4.6713e-03,  4.1877e-01,
        -6.2196e-02,  2.9108e-04, -7.8863e-03,  5.2587e-02,  3.4637e-01,
        -4.2772e-01,  5.3634e-01,  2.8193e-02,  2.4816e-01,  4.7977e-01,
        -2.0887e-01, -1.2205e-01,  1.5709e-03, -4.2056e-01, -1.3009e-03,
        -3.0306e-01,  5.4963e-02, -2.3276e-01,  6.1841e-01, -3.2771e-02,
         6.3384e-01,  3.6256e-02, -2.4193e-01,  1.0148e-02, -6.5338e-01,
         1.5213e-01,  9.0067e-02,  4.0155e-04, -1.8124e-01,  4.1780e-02,
        -3.7421e-02, -1.3815e-01,  9.5144e-01,  3.8597e-01, -4.8607e-02,
         2.0470e-01, -9.4395e-01,  2.5681e-01,  6.9425e-02, -9.6314e-03,
        -2.4465e-01,  6.3043e-01,  2.2519e-02,  5.5401e-04,  4.6864e-01,
        -2.1432e-01,  5.3940e-01,  2.9607e-01,  1.8892e-01, -2.2233e-01,
        -1.5041e-01,  6.5015e-01,  3.2351e-01,  2.9961e-03,  3.2108e-02,
         4.6496e-01, -1.5831e-01, -2.1360e-01,  1.2254e-01, -1.5451e-01,
        -9.9106e-02, -6.4133e-01,  9.8545e-02, -4.8964e-02, -9.1282e-02,
        -5.1380e-01, -3.1101e-01, -1.5879e-01,  6.3941e-01,  1.0695e-01,
        -5.6661e-01, -4.9584e-01,  2.5982e-01, -4.4674e-02,  7.8044e-01,
         1.1890e-01,  5.4891e-01, -8.7752e-03,  1.1075e-01, -1.4531e-01,
        -1.1849e-01, -6.8604e-02,  5.4391e-01, -7.3539e-02,  5.0097e-01,
        -2.7288e-02,  8.6610e-02,  3.8844e-01, -5.5026e-02,  5.7245e-02,
        -4.7057e-03,  1.0283e-01,  7.3576e-01, -3.1376e-01, -1.7154e-01,
         1.2800e-01,  8.4984e-01,  5.2382e-01, -1.6816e-01,  8.0808e-01,
        -4.4904e-01,  6.0777e-01, -6.4261e-01,  2.2741e-01,  1.3904e-02,
         6.8419e-02,  1.1787e-01, -1.2100e-01,  3.1964e-01,  1.3321e-01,
         2.1692e-02, -1.2558e-02, -1.2013e-01,  8.2346e-01, -6.0424e-03,
        -8.0741e-02,  8.8737e-02,  1.0000e+00,  1.2711e-01, -5.0877e-01,
         6.1398e-02, -8.3259e-02, -1.4273e-01,  2.3667e-01,  5.1263e-01,
        -5.8611e-01, -6.0547e-02,  6.6582e-01,  4.7099e-01, -3.1921e-04,
         3.1895e-02,  8.0227e-03, -1.0633e-01,  3.9896e-02,  4.2195e-01,
        -8.1213e-02, -1.1851e-01,  3.1089e-01,  5.8907e-02,  6.1397e-04,
         9.3546e-02,  5.8575e-01,  2.2937e-01, -1.5115e-01, -2.3659e-01,
         5.5872e-02, -2.1307e-01,  1.5340e-01, -1.1432e-02,  2.2660e-01,
         5.7918e-01,  2.6871e-01, -6.8558e-02,  1.5709e-01], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0922,  0.0959,  0.3645,  ...,  0.2347, -0.0171,  0.2686],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0578, -0.0195, -0.0196,  ..., -0.0142, -0.0183,  0.0011],
       device='cuda:0')}
topk_layer inside model after retrieval:  [4076, 2572, 1544, 944, 1163, 5694, 6119, 5015, 4144, 2067, 4382, 1894, 780, 3980, 3710, 5338, 863, 287, 1439, 2101, 425, 5027, 4431, 2323, 534, 2736, 2070, 3165, 309, 5799, 3559, 2598, 1564, 2460, 3947, 3984, 4394, 4244, 5201, 43, 6088, 1641, 26, 178, 775, 2576, 4299, 2607, 5143, 3827, 3399, 5871, 2835, 2752, 5396, 5890, 4821, 3577, 3783, 4312, 1097, 4876, 5910, 3035, 1312, 3828, 2613, 911, 3383, 3187, 5054, 4537, 2213, 1083, 4651, 4817, 3734, 371, 1845, 1257, 2000, 1949, 3897, 2976, 515, 3986, 6094, 2335, 2244, 5631, 5286, 2491, 3348, 4467, 4992, 1877, 514, 4699, 2559, 1428, 5150, 5277, 3561, 4814, 5003, 750, 5222, 5272, 4979, 1960, 2342, 3181, 1753, 894, 2986, 4614, 1653, 8, 5491, 2138, 4906, 1422, 1950, 5908, 5645, 4791, 2966, 465, 2426, 2315, 1532, 5448, 2185, 4824, 1575, 3964, 1080, 1385, 1873, 2891, 5542, 2471, 985, 2679, 1631, 95, 5846, 2221, 4598, 1730, 2152, 3221, 1685, 294, 632, 2656, 3036, 4339, 61, 5048, 5698, 4097, 3226, 5318, 2261, 4186, 2917, 3429, 1988, 1346, 2089, 2515, 4395, 1749, 493, 5421, 948, 4451, 4899, 1501, 527, 3420, 5014, 3722, 5642, 63, 4227, 2013, 3466, 5349, 3662, 4044, 4908, 2693, 4702, 2950, 3362, 1335, 1438, 1709, 2270, 4110, 4482, 4476, 2069, 4134, 4376, 2026, 4502, 6125, 4163, 1622, 4118, 184, 1938, 825, 3788, 5493, 5580, 3733, 2019, 4884, 1539, 5212, 2787, 2975, 3938, 859, 4034, 4819, 2246, 4269, 2580, 1390, 4606, 2367, 2470, 3537, 4135, 1615, 2083, 1696, 2281, 415, 2585, 5407, 4566, 5839, 6032, 5124, 6021, 451, 5626, 3852, 3698, 4735, 679, 2248, 806, 2533, 1798, 5651, 105, 4181, 4358, 1833, 3718, 2996, 3606, 6084, 4352, 5795, 6070, 2420, 6041, 4000, 361, 1037, 5752, 1131, 5000, 471, 599, 4688, 881, 551, 5466, 3375, 1623, 4081, 4247, 796, 3097, 3571, 1009, 167, 2747, 3950, 2814, 1888, 2447, 1357, 3749, 3775, 706, 1349, 999, 3916, 4323, 2242, 2146, 1897, 1307, 4572, 1055, 686, 2581, 2415, 4341, 2049, 6095, 4120, 2850, 3070, 2095, 1022, 5004, 4301, 3495, 2290, 1138, 1973, 4261, 5753, 1978, 2488, 1115, 4100, 205, 3670, 6072, 4778, 4001, 363, 2001, 3991, 430, 3005, 1368, 4602, 3252, 4980, 257, 2172, 1406, 278, 4662, 4900, 2797, 2075, 4025, 5067, 4455, 827, 5876, 5312, 3145, 1853, 5109, 4962, 3448, 3067, 4222, 3839, 5005, 1519, 132, 5984, 836, 3615, 5589, 2328, 1155, 3818, 3384, 4385, 1168, 3745, 1857, 4007, 4639, 3387, 4143, 4277, 6043, 2608, 1222, 1806, 6100, 4363, 2691, 461, 1909, 5796, 4009, 477, 1220, 1778, 676, 5422, 5023, 2056, 261, 468, 2597, 4218, 4885, 2697, 1158, 730, 4357, 5791, 1104, 2785, 4266, 5811, 2962, 1887, 584, 5376, 3242, 2106, 4004, 5423, 4963, 709, 1513, 4719, 2452, 1252, 4570, 5590, 1028, 4392, 2079, 2769, 14, 516, 822, 5809, 6098, 2555, 3253, 1526, 3641, 3740, 4066, 2262, 4845, 2841, 1361, 1337, 5834, 4292, 5414, 1508, 5669, 2698, 5321, 3717, 5792, 1530, 3084, 3013, 4925, 3959, 4349, 3548, 569, 4080, 3106, 4402, 3009, 1597, 732, 4638, 2865, 2800, 2647, 4739, 248, 1306, 2494, 5477, 667, 5072, 2668, 1130, 4460, 3193, 3470, 3170, 2847, 5363, 2979, 3177, 5103, 1799, 5773, 144, 1831, 1289, 5591, 52, 1680, 3334, 3153, 3173, 5078, 5558, 5658, 1594, 4159, 1822, 5254, 646, 4896, 175, 5582, 3107, 3178, 4457, 2799, 2378, 2863, 943, 40, 6131, 2569, 383, 4954, 3279, 2611, 883, 4475, 3761, 1608, 5070, 4415, 4554, 5035, 6024, 2122, 5961, 4780, 5445, 1336, 117, 3822, 5416, 2817, 4029, 6142, 4306, 2666, 2941, 3750, 993, 5013, 5473, 3435, 5442, 4869, 4445, 5790, 4681, 3981, 4077, 2393, 3658, 774, 6118, 3367, 394, 4072, 1274, 3061, 5069, 6090, 2126, 1677, 5127, 583, 21, 3993, 165, 5504, 4407, 1911, 3027, 5055, 1913, 1000, 2030, 338, 5276, 1402, 152, 1424, 2154, 2355, 2792, 3337, 1036, 5292, 1292, 5507, 3523, 3136, 1226, 3050, 4692, 2282, 5866, 850, 3532, 4350, 5243, 5332, 3323, 2522, 3449, 3034, 508, 5451, 326, 4724, 1296, 4843, 842, 5623, 5643, 292, 5918, 3040, 4807, 426, 1996, 1930, 5283, 767, 5024, 3191, 2418, 211, 5322, 5121, 5065, 909, 526, 2305, 3126, 485, 3054, 2804, 5146, 5381, 5956, 3643, 3474, 1011, 378, 5180, 509, 2860, 2645, 3594, 5308, 2336, 4848, 728, 1181, 4682, 4882, 2317, 4052, 511, 221, 5368, 2675, 3653, 5966, 3294, 2037, 4190, 3480, 3802, 2320, 5671, 2427, 2677, 1095, 4073, 799, 3403, 1936, 5852, 1644, 4704, 5993, 2398, 3873, 45, 3502, 1363, 3998, 2538, 3563, 4637, 1854, 3454, 5929, 2051, 3227, 2345, 5625, 3853, 2694, 4544, 5239, 3361, 976, 1706, 3488, 6133, 1294, 3114, 327, 4640, 1636, 5314, 4551, 2978, 5976, 5188, 660, 5316, 555, 716, 2334, 878, 6044, 3041, 6037, 480, 3860, 1789, 5096, 5169, 1401, 443, 3062, 5256, 1086, 581, 1403, 153, 5776, 5772, 46, 3324, 3680, 4958, 2778, 4924, 6025, 3086, 4142, 1520, 4418, 3347, 3621, 3229, 4027, 2751, 3727, 134, 1453, 873, 2112, 4974, 997, 1903, 5205, 3450, 5629, 194, 1515, 2762, 5130, 3801, 1144, 1096, 5836, 159, 547, 1256, 3069, 3830, 2183, 3464, 3275, 5861, 4203, 2682, 450, 4308, 1183, 2714, 3244, 2989, 2702, 1642, 3175, 5151, 544, 505, 2331, 4653, 3388, 2245, 409, 3657, 1280, 860, 1787, 2173, 3533, 1649, 1194, 4141, 2506, 225, 3108, 4217, 1499, 5568, 3197, 2918, 5806, 4102, 1610, 1459, 3540, 5502, 5936, 1998, 5647, 3736, 5398, 5579, 5419, 4987, 5851, 1042, 1153, 2615, 1904, 2222, 2703, 41, 5190, 2625, 1451, 601, 5635, 5661, 122, 273, 1545, 5922, 888, 1712, 1968, 1112, 3649, 4265, 180, 954, 4151, 1073, 5973, 3936, 3941, 530, 3251, 2859, 2914, 1525, 634, 5401, 5051, 5452, 5409, 4556, 2983, 2005, 5108, 5406, 4298, 3469, 1231, 2885, 446, 776, 3033, 2277, 755, 2824, 1191, 3183, 5393, 3720]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}

 Model STL10_CIFAR10_CL3f610ba2-1bdb-42f9-b463-ea87fb3856be loaded successfuly with best perf



 ----- Architecture Block BatchNorm2dSK3962(5, 5)1.0reflect, number 0 -----
- BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- AvgPool2d(kernel_size=4, stride=2, padding=1)

 ----- Architecture Block BatchNorm2dSK963842(3, 3)0.6499999761581421reflect, number 1 -----
- BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK38415362(3, 3)0.44999998807907104reflect, number 2 -----
- BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.4)
- AvgPool2d(kernel_size=4, stride=2, padding=1)

 ----- Architecture Block BatchNorm2dSK153661442(3, 3)0.25reflect, number 3 -----
- BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 4 -----
- Flatten(start_dim=1, end_dim=-1)
- Dropout(p=0.5, inplace=False)
- Linear(in_features=221184, out_features=10, bias=True)
model.heads:  [{'blocks.4.layer.bias': tensor([ 0.0016, -0.0015,  0.0004, -0.0005,  0.0020,  0.0031,  0.0054, -0.0016,
         0.0026,  0.0032], device='cuda:0'), 'blocks.4.layer.weight': tensor([[ 0.0400,  0.0110,  0.0159,  ..., -0.0202, -0.0415, -0.0304],
        [ 0.0107, -0.0007,  0.0315,  ..., -0.0158,  0.0113,  0.0158],
        [ 0.0364, -0.0160,  0.0407,  ...,  0.0141,  0.0031,  0.0035],
        ...,
        [ 0.0219,  0.0111,  0.0037,  ...,  0.0494,  0.0133, -0.0125],
        [-0.0186,  0.0274, -0.0178,  ..., -0.0034, -0.0105,  0.0038],
        [-0.0212, -0.0315, -0.0387,  ..., -0.0497,  0.0010, -0.0065]],
       device='cuda:0')}, {'blocks.4.layer.bias': tensor([ 0.0095,  0.0022, -0.0014,  0.0006,  0.0038,  0.0088, -0.0177,  0.0123,
        -0.0043,  0.0127], device='cuda:0'), 'blocks.4.layer.weight': tensor([[ 0.0753,  0.0216,  0.1031,  ..., -0.0119, -0.0026, -0.0461],
        [-0.0021, -0.0158, -0.0219,  ..., -0.1085,  0.1136,  0.0426],
        [-0.0567, -0.0940,  0.1015,  ..., -0.0155, -0.0621,  0.0444],
        ...,
        [-0.0177,  0.0519,  0.0008,  ...,  0.0218,  0.0824, -0.0101],
        [-0.0454,  0.0595, -0.0771,  ...,  0.0664, -0.1257, -0.1442],
        [-0.0302,  0.0074, -0.0728,  ...,  0.0438, -0.0112, -0.0909]],
       device='cuda:0')}]
[Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=1.4)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=221184, out_features=10, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)]
LAYER NAME:  Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=1.4)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=221184, out_features=10, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)
LAYER CHILDREN:  [BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
  (activation): Triangle(power=1.4)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1)
), BasicBlock(
  (operations): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
  )
  (layer): Linear(in_features=221184, out_features=10, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)]
subsubl NAME:  Sequential(
  (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=4, stride=2, padding=1)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=4, stride=2, padding=1)
subsubl NAME:  Triangle(power=1.4)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1)
subsubl NAME:  Sequential(
  (0): Flatten(start_dim=1, end_dim=-1)
)
subsubl NAME:  Linear(in_features=221184, out_features=10, bias=True)
subsubl NAME:  Dropout(p=0.5, inplace=False)
CONFIG MODE:  unsupervised
CONFIG MODE:  supervised
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
INDICES:  5000
IMAGE SIZE: torch.Size([64, 3, 96, 96])
Accuracy of the network on the 1st dataset: 30.175 %
Test loss on the 1st dataset: 26.732
results:  {'count': 3, 'R0': {'train_loss': 0.03978398069739342, 'train_acc': 98.27600121498108, 'test_loss': 3.0680172443389893, 'test_acc': 66.5625, 'convergence': 23.72207260131836, 'R1': 1, 'dataset_sup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 5000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 4, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': True, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 100, 'print_freq': 10, 'validation': False, 'continual_learning': False}, 'dataset_unsup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 100000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 32, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'unlabeled', 'nb_epoch': 1, 'print_freq': 25, 'validation': False, 'continual_learning': False}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 32, 'nb_epoch': 1, 'print_freq': 25, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 100, 'print_freq': 10, 'lr': 0.001}}}, 'model_config': {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.036828478186799345, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 221184, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}, 'R1': {'train_loss': 0.7235910296440125, 'train_acc': 89.78360295295715, 'test_loss': 0.2104601263999939, 'test_acc': 74.73999786376953, 'convergence': 25.553024291992188, 'R1': 15, 'dataset_sup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 96}, 'dataset_unsup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 96}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 10, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}}, 'cl_hyper': {'training_mode': 'consecutive', 'cf_sol': True, 'head_sol': False, 'top_k': 0.15, 'topk_lock': False, 'high_lr': 0.15, 'low_lr': 1.0, 't_criteria': 'activations', 'delta_w_interval': 50.0, 'heads_basis_t': 0.8236999893188477, 'n_tasks': 2}, 'eval_0': {'test_loss': 26.732187271118164, 'test_acc': 30.174999237060547, 'convergence': 24.0014705657959, 'R1': 0, 'dataset_sup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 5000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 4, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': True, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 100, 'print_freq': 10, 'validation': False, 'continual_learning': False}, 'dataset_unsup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 100000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 32, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'unlabeled', 'nb_epoch': 1, 'print_freq': 25, 'validation': False, 'continual_learning': False}}, 'model_name': 'STL10_CIFAR10_CL3f610ba2-1bdb-42f9-b463-ea87fb3856be'}
MULTD_CL_STL10__1_CIFAR10/STL10_CIFAR10_CL3f610ba2-1bdb-42f9-b463-ea87fb3856be.json

The device used will be: 
True
cuda:0
BLOCKS:  {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.25, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10}, 'pool': None, 'activation': None}}
[[0,3],[5,7]]


 ########### WARNING ############


 Invalid combination of parameters, provide either: [--classes, --dataset-sup, --dataset-unsup] or [--dataset-sup-1, --dataset-unsup-1, --dataset-sup-2, --dataset-unsup-2]
The continual learning is implemented per tasks where each task is made up of different classes 
 of the same dataset, so only one dataset will be considered.


 ################################


{'training_mode': 'consecutive', 'cf_sol': True, 'head_sol': True, 'top_k': 0.15, 'topk_lock': False, 'high_lr': 0.15, 'low_lr': 1.0, 't_criteria': 'activations', 'delta_w_interval': 50.0, 'heads_basis_t': 0.9, 'n_tasks': 2}
CL:  True
{'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 5000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': True, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 100, 'print_freq': 10, 'validation': False, 'continual_learning': True}
CL:  False
{'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 5000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': True, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 100, 'print_freq': 10, 'validation': False, 'continual_learning': False}
CL:  False
{'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 100000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 32, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'unlabeled', 'nb_epoch': 1, 'print_freq': 25, 'validation': False, 'continual_learning': False}
SEED:  0
block 0, size : 96 48 48
96
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.25, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10}, 'pool': None, 'activation': None}}
range = 2.886751345948129
block 1, size : 384 24 24
384
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.25, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10}, 'pool': None, 'activation': None}}
range = 0.8505172717997146
block 2, size : 1536 12 12
1536
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.25, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10}, 'pool': None, 'activation': None}}
range = 0.4252586358998573
block 3, size : 6144 6 6
6144
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.25, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10}, 'pool': None, 'activation': None}}
range = 0.21262931794992865
NOWWWW
6 6
10
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.25, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 221184, 'old_channels': 6144}, 'pool': None, 'activation': None}}
blocks['b%s' % 4] 221184 221184
range = 0.036828478186799345
train_layer_order: consecutive {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 32, 'nb_epoch': 1, 'print_freq': 25, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 100, 'print_freq': 10, 'lr': 0.001}}
The device used will be: 
True
cuda:0
avg_deltas:  {}
topk_layer:  None
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}

 ----- Architecture Block BatchNorm2dSK3962(5, 5)1.0reflect, number 0 -----
- BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- AvgPool2d(kernel_size=4, stride=2, padding=1)

 ----- Architecture Block BatchNorm2dSK963842(3, 3)0.6499999761581421reflect, number 1 -----
- BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK38415362(3, 3)0.44999998807907104reflect, number 2 -----
- BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.4)
- AvgPool2d(kernel_size=4, stride=2, padding=1)

 ----- Architecture Block BatchNorm2dSK153661442(3, 3)0.25reflect, number 3 -----
- BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 4 -----
- Flatten(start_dim=1, end_dim=-1)
- Dropout(p=0.5, inplace=False)
- Linear(in_features=221184, out_features=10, bias=True)
model.heads:  []
[Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=1.4)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=221184, out_features=10, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)]
LAYER NAME:  Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=1.4)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=221184, out_features=10, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)
LAYER CHILDREN:  [BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
  (activation): Triangle(power=1.4)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1)
), BasicBlock(
  (operations): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
  )
  (layer): Linear(in_features=221184, out_features=10, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)]
subsubl NAME:  Sequential(
  (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=4, stride=2, padding=1)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=4, stride=2, padding=1)
subsubl NAME:  Triangle(power=1.4)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1)
subsubl NAME:  Sequential(
  (0): Flatten(start_dim=1, end_dim=-1)
)
subsubl NAME:  Linear(in_features=221184, out_features=10, bias=True)
subsubl NAME:  Dropout(p=0.5, inplace=False)
CONFIG MODE:  unsupervised

 ********** Hebbian Unsupervised learning of blocks [0, 1, 2, 3] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
INDICES:  5000
IMAGE SIZE: torch.Size([32, 3, 96, 96])
SAVING FOLDER FOR UNSUP:  STL10_CIFAR10_CL158728fd-dcb8-497d-8bd4-d5ba2c749836
prev_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
activations_sum.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
DEPTH:  4
WTA IN delta_weight:  tensor([[[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         ...,
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00]],

        [[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         ...,
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00]],

        [[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         ...,
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00]],

        ...,

        [[-9.4754e-16, -9.4754e-16, -9.4754e-16,  ..., -9.4754e-16,
          -9.4754e-16, -9.4754e-16],
         [-9.4754e-16, -9.4754e-16, -9.4754e-16,  ..., -9.4754e-16,
          -9.4754e-16, -9.4754e-16],
         [-9.4754e-16, -9.4754e-16, -9.4754e-16,  ..., -9.4754e-16,
          -9.4754e-16, -9.4754e-16],
         ...,
         [-9.4754e-16, -9.4754e-16, -9.4754e-16,  ..., -9.4754e-16,
          -9.4754e-16, -9.4754e-16],
         [-9.4754e-16, -9.4754e-16, -9.4754e-16,  ..., -9.4754e-16,
          -9.4754e-16, -9.4754e-16],
         [-9.4754e-16, -9.4754e-16, -9.4754e-16,  ..., -9.4754e-16,
          -9.4754e-16, -9.4754e-16]],

        [[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         ...,
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00]],

        [[-5.5351e-43, -5.5351e-43, -5.5351e-43,  ..., -5.5351e-43,
          -5.5351e-43, -5.5351e-43],
         [-5.5351e-43, -5.5351e-43, -5.5351e-43,  ..., -5.5351e-43,
          -5.5351e-43, -5.5351e-43],
         [-5.5351e-43, -5.5351e-43, -5.5351e-43,  ..., -5.5351e-43,
          -5.5351e-43, -5.5351e-43],
         ...,
         [-5.5351e-43, -5.5351e-43, -5.5351e-43,  ..., -5.5351e-43,
          -5.5351e-43, -5.5351e-43],
         [-5.5351e-43, -5.5351e-43, -5.5351e-43,  ..., -5.5351e-43,
          -5.5351e-43, -5.5351e-43],
         [-5.5351e-43, -5.5351e-43, -5.5351e-43,  ..., -5.5351e-43,
          -5.5351e-43, -5.5351e-43]]], device='cuda:0')
self.avg_deltas_layer:  <class 'NoneType'>
self.topk_layer:  <class 'NoneType'>
self.avg_deltas_layer:  <class 'NoneType'>
self.topk_layer:  <class 'NoneType'>
self.avg_deltas_layer:  <class 'NoneType'>
self.topk_layer:  <class 'NoneType'>
self.avg_deltas_layer:  <class 'NoneType'>
self.topk_layer:  <class 'NoneType'>
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
activations_sum[k] 96
activations_sum[k]:  [68, 25, 59, 30, 77, 93, 69, 55, 35, 14]
topk_kernels len:  1
topk_kernels keys:  ['conv0']
topk_kernels:  {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38]}
activations_sum[k] len:  96
activations_sum[k] 384
activations_sum[k]:  [345, 365, 75, 354, 190, 100, 56, 180, 191, 105]
topk_kernels len:  2
topk_kernels keys:  ['conv0', 'conv1']
topk_kernels:  {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286]}
activations_sum[k] len:  384
activations_sum[k] 1536
activations_sum[k]:  [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35]
topk_kernels len:  3
topk_kernels keys:  ['conv0', 'conv1', 'conv2']
topk_kernels:  {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276]}
activations_sum[k] len:  1536
activations_sum[k] 6144
activations_sum[k]:  [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822]
topk_kernels len:  4
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3']
topk_kernels:  {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
activations_sum[k] len:  6144
['conv0', 'conv1', 'conv2', 'conv3']
delta_weights INFO: 
NUM OF TRACKED COV LAYERS:  4
NUM OF TRACKED WEIGHTS CHANGES PER LAYER:  4
avg_deltas INFO:  <class 'dict'>
avg_deltas keys:  ['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight']
################################################
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 4.52e-02	time: 00:00:39	Acc_train 0.00	Acc_test 0.00	convergence: 2.37e+01	R1: 1	Info MB:0.000e+00/SB:0.000e+00/MW:4.460e-06/SW:2.112e-01/MR:2.472e+01/SR:2.331e+00/MeD:1.064e+00/MaD:2.372e+01/MW:0.543/MAW:0.457
|       0 |       1 |        2 |        3 |        4 |        5 |        6 |        7 |        8 |        9 |       10 |       11 |       12 |       13 |       14 |       15 |       16 |       17 |       18 |       19 |       20 |       21 |       22 |       23 |       24 |       25 |       26 |      27 |       28 |       29 |
|---------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------|
|   0.049 |   0.049 |   0.0496 |   0.0495 |   0.0494 |   0.0489 |   0.0501 |   0.0491 |   0.0384 |   0.0488 |   0.0495 |   0.0491 |   0.0489 |   0.0495 |   0.0484 |   0.0497 |   0.0494 |   0.0495 |   0.0492 |   0.0493 |   0.0492 |   0.0463 |   0.0491 |   0.0487 |   0.0484 |   0.0491 |   0.0499 |   0.049 |   0.0495 |   0.0496 |
|  24.97  |  24.96  |  25.6    |  25.52   |  25.43   |  24.9    |  26.1    |  25.07   |  15.72   |  24.85   |  25.52   |  25.09   |  24.91   |  25.5    |  24.4    |  25.67   |  25.36   |  25.47   |  25.17   |  25.29   |  25.17   |  22.48   |  25.11   |  24.76   |  24.47   |  25.11   |  25.91   |  25.05  |  25.49   |  25.62   |
|   0.01  |   0.01  |   0      |   0      |   0.01   |   0.02   |   0.01   |   0      |   0.12   |   0      |   0.01   |   0.01   |   0      |   0      |   0.02   |   0      |   0      |   0      |   0      |   0      |   0      |   0.04   |   0      |   0.01   |   0.01   |   0.01   |   0.02   |   0     |   0.01   |   0      |
| nan     | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      |
| nan     | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      |
| nan     | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:1.522e-04/SW:3.711e-01/MR:2.156e+01/SR:3.332e+00/MeD:2.742e+00/MaD:1.700e+01/MW:0.596/MAW:0.404
|        0 |        1 |        2 |        3 |        4 |        5 |        6 |        7 |        8 |       9 |       10 |       11 |       12 |       13 |       14 |       15 |       16 |       17 |       18 |       19 |      20 |       21 |       22 |       23 |       24 |       25 |       26 |       27 |       28 |       29 |
|----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------|
|   0.0491 |   0.0483 |   0.0451 |   0.0462 |   0.0428 |   0.0428 |   0.0492 |   0.0405 |   0.0453 |   0.049 |   0.0398 |   0.0475 |   0.0434 |   0.0357 |   0.0474 |   0.0448 |   0.0387 |   0.0497 |   0.0469 |   0.0492 |   0.041 |   0.0467 |   0.0404 |   0.0478 |   0.0432 |   0.0429 |   0.0488 |   0.0457 |   0.0397 |   0.0486 |
|  25.08   |  24.36   |  21.36   |  22.34   |  19.33   |  19.35   |  25.22   |  17.37   |  21.51   |  25.05  |  16.81   |  23.55   |  19.85   |  13.75   |  23.48   |  21.08   |  16.01   |  25.68   |  22.97   |  25.2    |  17.82  |  22.81   |  17.32   |  23.87   |  19.69   |  19.39   |  24.77   |  21.84   |  16.78   |  24.63   |
|   0      |   0.01   |   0.06   |   0.06   |   0.06   |   0.1    |   0      |   0.07   |   0.06   |   0     |   0.1    |   0.03   |   0.07   |   0.21   |   0.03   |   0.04   |   0.22   |   0      |   0.02   |   0      |   0.14  |   0.03   |   0.2    |   0.02   |   0.05   |   0.05   |   0      |   0.04   |   0.11   |   0      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:-3.013e-03/SW:7.925e-01/MR:2.315e+01/SR:2.575e+00/MeD:1.984e+00/MaD:1.146e+01/MW:0.695/MAW:0.305
|        0 |        1 |        2 |        3 |        4 |        5 |        6 |        7 |        8 |        9 |       10 |       11 |       12 |       13 |       14 |       15 |       16 |       17 |       18 |       19 |       20 |       21 |       22 |       23 |       24 |       25 |       26 |      27 |      28 |       29 |
|----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+---------+----------|
|   0.0245 |   0.0246 |   0.0242 |   0.0241 |   0.0199 |   0.0235 |   0.0244 |   0.0249 |   0.0244 |   0.0234 |   0.0248 |   0.0239 |   0.0242 |   0.0242 |   0.0242 |   0.0241 |   0.0241 |   0.0219 |   0.0243 |   0.0237 |   0.0231 |   0.0248 |   0.0244 |   0.0247 |   0.0233 |   0.0226 |   0.0213 |   0.024 |   0.024 |   0.0251 |
|  25.05   |  25.12   |  24.39   |  24.27   |  16.82   |  23.03   |  24.87   |  25.72   |  24.84   |  22.85   |  25.62   |  23.77   |  24.33   |  24.47   |  24.36   |  24.18   |  24.22   |  20.12   |  24.69   |  23.54   |  22.28   |  25.7    |  24.78   |  25.37   |  22.75   |  21.47   |  19.17   |  24.11  |  24.04  |  26.12   |
|   0.04   |   0      |   0.02   |   0.01   |   1.6    |   0.19   |   0.02   |   0.05   |   0.01   |   0.2    |   0      |   0.07   |   0.16   |   0.12   |   0.02   |   0.06   |   0      |   0.66   |   0.01   |   0.16   |   0.16   |   0      |   0.12   |   0.11   |   0.28   |   0.49   |   1.15   |   0.03  |   0.07  |   0.02   |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan     | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan     | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan     | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:-1.311e-02/SW:1.508e+00/MR:1.280e+01/SR:2.610e+00/MeD:2.095e+00/MaD:7.090e+00/MW:0.547/MAW:0.453
|       0 |      1 |       2 |      3 |      4 |       5 |       6 |       7 |       8 |       9 |      10 |      11 |      12 |      13 |      14 |      15 |      16 |      17 |      18 |     19 |      20 |     21 |      22 |      23 |      24 |      25 |      26 |      27 |      28 |      29 |
|---------+--------+---------+--------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------|
|   0.256 |   0.33 |   0.295 |   0.31 |   0.24 |   0.219 |   0.277 |   0.329 |   0.245 |   0.348 |   0.254 |   0.272 |   0.289 |   0.305 |   0.269 |   0.309 |   0.203 |   0.276 |   0.275 |   0.26 |   0.303 |   0.27 |   0.287 |   0.324 |   0.305 |   0.236 |   0.303 |   0.292 |   0.276 |   0.328 |
|  11.22  |  17.99 |  14.58  |  16.02 |  10.03 |   8.5   |  12.95  |  17.88  |  10.42  |  19.89  |  11.08  |  12.58  |  14.04  |  15.56  |  12.34  |  15.91  |   7.44  |  12.89  |  12.84  |  11.56 |  15.33  |  12.4  |  13.85  |  17.37  |  15.56  |   9.67  |  15.39  |  14.35  |  12.92  |  17.79  |
|   0.64  |   0.18 |   0.17  |   0.26 |   1.98 |   3.7   |   0.76  |   0.33  |   1.3   |   0.13  |   1.05  |   0.49  |   0.54  |   0.25  |   1.82  |   0.22  |   3.77  |   0.5   |   0.66  |   1.12 |   0.45  |   0.84 |   0.42  |   0.27  |   0.39  |   2.91  |   0.37  |   0.58  |   0.34  |   0.18  |
| nan     | nan    | nan     | nan    | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     |
| nan     | nan    | nan     | nan    | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     |
| nan     | nan    | nan     | nan    | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL158728fd-dcb8-497d-8bd4-d5ba2c749836/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
CONFIG MODE:  supervised

 ********** Supervised learning of blocks [4] **********
SAVING FOLDER FOR SUP:  STL10_CIFAR10_CL158728fd-dcb8-497d-8bd4-d5ba2c749836
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
INDICES:  5000
IMAGE SIZE: torch.Size([64, 3, 96, 96])
num_blocks:  5
Epoch: [1/100]	lr: 1.00e-03	time: 00:01:08	Loss_train 7.66232	Acc_train 36.62	/	Loss_test 3.46408	Acc_test 46.84
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL158728fd-dcb8-497d-8bd4-d5ba2c749836/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [10/100]	lr: 1.00e-03	time: 00:01:48	Loss_train 2.96399	Acc_train 61.89	/	Loss_test 4.18236	Acc_test 58.64
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL158728fd-dcb8-497d-8bd4-d5ba2c749836/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [20/100]	lr: 1.00e-03	time: 00:02:32	Loss_train 1.76923	Acc_train 76.53	/	Loss_test 5.84089	Acc_test 55.47
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL158728fd-dcb8-497d-8bd4-d5ba2c749836/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [30/100]	lr: 5.00e-04	time: 00:03:17	Loss_train 0.56039	Acc_train 88.94	/	Loss_test 3.81810	Acc_test 62.55
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL158728fd-dcb8-497d-8bd4-d5ba2c749836/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [40/100]	lr: 2.50e-04	time: 00:04:01	Loss_train 0.33715	Acc_train 92.10	/	Loss_test 3.39348	Acc_test 64.84
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL158728fd-dcb8-497d-8bd4-d5ba2c749836/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [50/100]	lr: 2.50e-04	time: 00:04:45	Loss_train 0.18524	Acc_train 94.79	/	Loss_test 3.65853	Acc_test 64.54
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL158728fd-dcb8-497d-8bd4-d5ba2c749836/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [60/100]	lr: 1.25e-04	time: 00:05:29	Loss_train 0.10238	Acc_train 96.51	/	Loss_test 3.27746	Acc_test 65.28
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL158728fd-dcb8-497d-8bd4-d5ba2c749836/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [70/100]	lr: 6.25e-05	time: 00:06:13	Loss_train 0.06835	Acc_train 97.37	/	Loss_test 3.15753	Acc_test 66.12
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL158728fd-dcb8-497d-8bd4-d5ba2c749836/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [80/100]	lr: 3.13e-05	time: 00:06:57	Loss_train 0.05019	Acc_train 97.93	/	Loss_test 3.11265	Acc_test 66.12
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL158728fd-dcb8-497d-8bd4-d5ba2c749836/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [90/100]	lr: 1.56e-05	time: 00:07:42	Loss_train 0.04391	Acc_train 98.04	/	Loss_test 3.07585	Acc_test 66.49
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL158728fd-dcb8-497d-8bd4-d5ba2c749836/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [100/100]	lr: 7.81e-06	time: 00:08:26	Loss_train 0.03978	Acc_train 98.28	/	Loss_test 3.06802	Acc_test 66.56
new_head:  {'blocks.4.layer.bias': tensor([ 0.0016, -0.0015,  0.0004, -0.0005,  0.0020,  0.0031,  0.0054, -0.0016,
         0.0026,  0.0032], device='cuda:0'), 'blocks.4.layer.weight': tensor([[ 0.0400,  0.0110,  0.0159,  ..., -0.0202, -0.0415, -0.0304],
        [ 0.0107, -0.0007,  0.0315,  ..., -0.0158,  0.0113,  0.0158],
        [ 0.0364, -0.0160,  0.0407,  ...,  0.0141,  0.0031,  0.0035],
        ...,
        [ 0.0219,  0.0111,  0.0037,  ...,  0.0494,  0.0133, -0.0125],
        [-0.0186,  0.0274, -0.0178,  ..., -0.0034, -0.0105,  0.0038],
        [-0.0212, -0.0315, -0.0387,  ..., -0.0497,  0.0010, -0.0065]],
       device='cuda:0')}
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL158728fd-dcb8-497d-8bd4-d5ba2c749836/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
SAVED model.heads_thresh 0.9
RESULT:  {'train_loss': 0.03978398069739342, 'train_acc': 98.27600121498108, 'test_loss': 3.0680172443389893, 'test_acc': 66.5625, 'convergence': 23.72207260131836, 'R1': 1, 'dataset_sup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 5000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 4, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': True, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 100, 'print_freq': 10, 'validation': False, 'continual_learning': False}, 'dataset_unsup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 100000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 32, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'unlabeled', 'nb_epoch': 1, 'print_freq': 25, 'validation': False, 'continual_learning': False}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 32, 'nb_epoch': 1, 'print_freq': 25, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 100, 'print_freq': 10, 'lr': 0.001}}}
IN R0:  {'count': 0, 'R0': {'train_loss': 0.03978398069739342, 'train_acc': 98.27600121498108, 'test_loss': 3.0680172443389893, 'test_acc': 66.5625, 'convergence': 23.72207260131836, 'R1': 1, 'dataset_sup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 5000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 4, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': True, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 100, 'print_freq': 10, 'validation': False, 'continual_learning': False}, 'dataset_unsup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 100000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 32, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'unlabeled', 'nb_epoch': 1, 'print_freq': 25, 'validation': False, 'continual_learning': False}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 32, 'nb_epoch': 1, 'print_freq': 25, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 100, 'print_freq': 10, 'lr': 0.001}}}}
CL:  True
{'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True}
CL:  True
{'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': True}
SEED:  0
block 0, size : 96 16 16
96
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.036828478186799345, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 221184, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 2.886751345948129
block 1, size : 384 8 8
384
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.036828478186799345, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 221184, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.8505172717997146
block 2, size : 1536 4 4
1536
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.036828478186799345, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 221184, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.4252586358998573
block 3, size : 6144 2 2
6144
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.036828478186799345, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 221184, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.21262931794992865
NOWWWW
2 2
10
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.036828478186799345, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 24576, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
blocks['b%s' % 4] 24576 24576
range = 0.11048543456039805
train_layer_order: consecutive {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 10, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}
The device used will be: 
True
cuda:0
PARAMSSSS IN LOAD:  {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.036828478186799345, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 221184, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
avg_deltas:  {'blocks.0.layer.weight': tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')}
topk_layer:  None
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.0.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')}
topk_layer inside model after retrieval:  [68, 25, 59, 30, 77, 93, 69, 55, 35, 14, 42, 70, 5, 16, 38]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.1.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')}
topk_layer inside model after retrieval:  [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.2.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')}
topk_layer inside model after retrieval:  [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.3.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-2.0850e-01,  1.0141e-02,  3.7897e-03,  1.2087e-02, -4.2626e-01,
        -6.9632e-01,  1.2661e-01, -2.4243e-02, -4.7815e-01,  2.1819e-02,
         2.0428e-01,  1.5160e-01,  5.3976e-05,  1.7926e-02,  3.7270e-01,
        -3.6495e-02, -8.6384e-01,  2.5512e-03, -2.7039e-01, -2.6664e-01,
         9.0813e-02,  2.9871e-01, -3.0370e-02,  2.2152e-03,  8.0428e-02,
         5.2483e-01, -2.1873e-02, -3.0976e-02,  5.7897e-02, -9.6521e-03,
         4.7601e-01,  1.4491e-01, -6.6567e-02, -3.6590e-01,  3.8715e-01,
         3.4064e-01, -3.7787e-01, -2.1896e-02,  3.2052e-01,  1.5280e-02,
         1.7573e-01,  9.9540e-02,  2.0687e-01, -9.8208e-02,  1.0994e-01,
        -3.0720e-01, -4.8367e-02,  6.4110e-02, -1.5191e-02,  2.1114e-01,
        -3.9178e-01,  3.1799e-01,  1.5839e-01,  1.7251e-01, -1.4598e-02,
         4.3733e-01, -1.2984e-01,  2.4802e-01, -2.1282e-01,  1.0000e+00,
        -3.6643e-02, -8.9123e-03, -1.8253e-01,  1.8334e-02, -4.7541e-01,
        -1.2347e-01, -3.0017e-01, -1.9162e-02,  7.9042e-01,  7.1249e-01,
         2.5595e-01,  8.1255e-02, -9.7164e-03, -4.6770e-01, -2.5754e-02,
        -2.0322e-01,  4.4566e-02,  8.0481e-01, -2.4393e-01,  2.5718e-01,
        -1.9591e-01,  4.7296e-01, -3.9645e-01,  2.0320e-01,  2.3972e-02,
        -6.7666e-02, -3.6306e-01, -8.6361e-02, -5.4476e-02, -4.4156e-01,
         7.5531e-03, -1.0399e-01, -1.4780e-01,  6.3307e-01, -9.3987e-02,
         8.3486e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.8831e-03,  4.4693e-05,  3.1384e-03,  1.3022e-03,  1.0000e+00,
        -1.6808e-01,  9.6764e-04, -1.8962e-02, -6.0760e-04,  3.0391e-02,
         6.6584e-06,  3.1156e-02,  6.3664e-02,  1.9562e-02,  4.9538e-03,
         1.2004e-02,  2.0355e-04,  2.1321e-01, -6.5541e-03, -8.5653e-02,
         1.1465e-02, -4.1223e-04,  2.0906e-02,  7.4955e-04,  1.4127e-01,
         2.2068e-01, -2.1797e-02, -1.5719e-02, -1.8498e-02,  1.1960e-03,
         1.0076e-03, -2.2028e-02, -7.4847e-02, -9.9260e-03, -6.7753e-03,
        -1.4613e-02,  2.1240e-01,  7.0459e-03,  9.3225e-05, -3.4410e-05,
         2.2786e-02,  3.4854e-02, -2.7520e-03,  2.5467e-05,  1.3703e-02,
        -3.1509e-06, -1.7625e-01,  1.4792e-02, -1.0303e-02, -4.5249e-02,
         1.4711e-01,  5.9926e-04, -1.4317e-03, -3.8056e-04, -2.0961e-02,
         3.8917e-01,  2.3412e-01, -4.3675e-03,  4.7999e-02, -3.9179e-04,
        -6.0061e-04, -1.1895e-03,  5.0713e-03, -6.9000e-04, -1.3569e-01,
         1.1500e-05, -2.6023e-01, -1.1578e-03,  1.2414e-01, -7.4177e-03,
        -1.9455e-01, -1.4389e-03, -1.9725e-01,  5.2555e-01, -2.5956e-03,
         2.0233e-01,  3.7187e-01, -5.7518e-02, -3.8959e-01,  2.2606e-01,
        -1.1214e-02, -9.5311e-04,  4.5465e-03, -2.4184e-06,  1.9256e-01,
        -9.1186e-05,  3.4184e-08, -3.0409e-01, -2.2900e-02, -3.2758e-04,
        -3.8031e-03,  1.2543e-04, -3.3458e-03,  5.3432e-04,  1.7430e-01,
        -1.7411e-02,  1.0339e-02,  1.5628e-03, -3.9877e-03, -4.3580e-03,
         3.9937e-01,  9.6866e-04,  8.7742e-06, -5.2717e-02, -3.5485e-02,
         4.9890e-02, -9.4919e-03, -1.9720e-03, -3.7293e-02,  1.6881e-01,
        -5.2319e-03,  7.7173e-04, -7.3775e-06, -1.0550e-01,  3.2068e-01,
        -5.8648e-04, -2.3276e-04,  2.7336e-01,  1.9826e-01, -3.5176e-02,
        -1.2565e-01, -4.6731e-01,  3.3817e-03,  1.3880e-01,  9.7203e-02,
        -4.1886e-01, -1.2097e-02, -1.0924e-01,  3.9540e-02,  8.3518e-04,
         1.1569e-03,  1.1371e-02,  2.5416e-03,  1.9541e-04,  7.2080e-02,
         4.6089e-03,  1.0022e-04,  2.8405e-02,  1.2725e-01,  4.7869e-03,
         1.0532e-01, -1.5683e-04,  2.9728e-02, -8.0283e-04,  2.3233e-02,
         1.0156e-03,  1.3281e-01, -3.2782e-04,  1.1640e-02, -4.7958e-04,
        -3.7176e-03,  6.6801e-05,  6.8665e-03,  1.4019e-05, -2.3272e-06,
        -1.2716e-03, -1.3122e-02, -8.6312e-01,  1.6983e-02, -2.2264e-01,
        -7.8657e-04, -2.3517e-04,  9.1756e-02, -2.3929e-01, -2.4755e-02,
         1.5707e-02, -6.2752e-03,  1.0232e-05,  5.1350e-02, -2.9531e-02,
        -6.5448e-02,  1.6286e-02,  1.1663e-03,  4.8492e-03,  4.6873e-02,
        -1.2503e-02,  1.9537e-02, -4.4562e-05,  8.3896e-04,  5.4747e-02,
        -6.2767e-01,  5.5156e-02,  1.3153e-03, -8.7628e-06,  6.8267e-03,
        -1.3091e-05,  3.0393e-02,  9.0337e-05,  2.1258e-02, -1.6369e-01,
         4.6796e-01, -2.5011e-03,  5.2330e-05,  6.4636e-02,  1.3741e-04,
         6.7758e-01,  5.8917e-04,  8.2353e-01, -2.8776e-05, -1.5678e-03,
         7.2723e-02,  2.7946e-04, -2.3258e-01,  3.5024e-01, -2.6454e-02,
         1.7098e-04,  1.5497e-01,  4.3112e-06, -2.0308e-03,  1.3530e-01,
        -2.5868e-04,  5.3722e-03,  4.3828e-06, -1.5398e-01, -6.4083e-06,
         1.8747e-03,  2.4167e-04,  8.3178e-03, -7.0711e-02,  8.5409e-02,
         6.7670e-04,  1.7111e-01, -1.1834e-04, -4.7772e-02,  7.8583e-02,
        -6.5034e-02,  1.8078e-02, -4.2819e-02,  7.6453e-04,  5.9756e-03,
        -2.1853e-03,  2.0467e-08,  3.9350e-04,  3.0751e-02,  1.1598e-02,
        -2.1640e-03,  7.4639e-02, -3.1943e-05,  2.3254e-02,  1.1610e-02,
        -7.6786e-02, -1.0156e-01,  6.0372e-07, -4.2275e-01,  2.3656e-06,
        -7.1841e-02, -1.5612e-04, -5.7086e-02,  2.2834e-01, -1.1531e-04,
         2.5017e-01,  6.4074e-06, -1.3002e-02, -4.7602e-06, -1.9919e-01,
         5.9968e-02, -7.9413e-05,  5.6352e-04, -3.3386e-04,  2.1153e-04,
        -5.7045e-04, -4.7611e-02,  4.4709e-01,  1.0108e-02, -6.7639e-02,
         8.1788e-03, -6.6393e-01,  1.7682e-02,  8.5582e-05,  1.4776e-05,
        -2.4690e-02,  1.6740e-01,  4.0839e-04,  1.8113e-06,  3.4211e-01,
        -4.8903e-03,  5.6738e-02,  1.1174e-02,  3.5182e-02, -4.1038e-05,
        -1.0973e-02,  8.2375e-02,  1.0408e-01,  6.3732e-06,  1.3850e-02,
         1.2594e-01, -2.2929e-02, -9.9479e-03,  1.0495e-03, -9.7291e-04,
        -1.6408e-03, -1.7214e-01,  3.8177e-02, -1.0959e-02, -7.3237e-04,
        -1.1285e-01, -1.4503e-01, -5.7074e-02,  5.7544e-03,  1.5482e-02,
        -2.1555e-01, -9.2510e-02,  2.0760e-02, -6.7862e-04,  3.6181e-01,
         4.0509e-03,  2.1297e-01,  6.9971e-04,  7.1412e-04, -1.5466e-02,
        -1.7287e-03, -7.4907e-05,  3.6467e-02, -3.4289e-03, -1.2092e-03,
         1.0044e-04,  1.0677e-04, -6.9785e-04, -9.6544e-05,  1.9970e-04,
        -3.5668e-04,  1.8275e-04,  1.9252e-02, -2.8384e-03, -3.8757e-01,
         9.2391e-05,  9.5487e-02,  1.3809e-01, -1.1875e-01,  6.6287e-01,
        -2.1670e-02,  3.0698e-02, -3.6498e-01,  4.4805e-03,  5.6883e-05,
         5.9523e-03,  2.4246e-02, -9.7359e-06,  2.2126e-02,  9.2660e-04,
         2.2162e-04,  3.4083e-04, -5.2630e-03,  1.6407e-01, -2.1346e-05,
        -1.9892e-01,  6.0817e-03,  4.8914e-01,  1.7297e-02, -5.6295e-01,
         1.3413e-03, -3.1048e-03, -1.5502e-01,  2.4281e-02,  5.1836e-01,
        -3.1381e-01, -4.2890e-04,  1.9801e-01,  2.5547e-01, -2.0620e-05,
         1.9262e-04,  3.9514e-05, -1.1212e-02,  4.9109e-03,  5.3391e-02,
        -5.5269e-01, -1.2357e-03,  1.0280e-02,  1.2236e-02, -1.4693e-06,
         5.7962e-03,  5.6604e-02,  4.3830e-02, -1.8146e-01, -1.0936e-01,
         3.0645e-03, -2.1037e-03,  2.0572e-03,  2.5947e-04,  3.2721e-03,
         3.7780e-02,  1.0327e-02, -3.7739e-03,  2.8228e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0017,  0.0018,  0.1315,  ...,  0.0263, -0.0631,  0.0074],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0113,  0.0087, -0.0156,  ..., -0.0003, -0.0090,  0.0015],
       device='cuda:0')}
topk_layer inside model after retrieval:  [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
The device used will be: 
True
cuda:0
The device used will be: 
True
cuda:0
chosen_head:  {'blocks.4.layer.bias': tensor([ 0.0016, -0.0015,  0.0004, -0.0005,  0.0020,  0.0031,  0.0054, -0.0016,
         0.0026,  0.0032], device='cuda:0'), 'blocks.4.layer.weight': tensor([[ 0.0400,  0.0110,  0.0159,  ..., -0.0202, -0.0415, -0.0304],
        [ 0.0107, -0.0007,  0.0315,  ..., -0.0158,  0.0113,  0.0158],
        [ 0.0364, -0.0160,  0.0407,  ...,  0.0141,  0.0031,  0.0035],
        ...,
        [ 0.0219,  0.0111,  0.0037,  ...,  0.0494,  0.0133, -0.0125],
        [-0.0186,  0.0274, -0.0178,  ..., -0.0034, -0.0105,  0.0038],
        [-0.0212, -0.0315, -0.0387,  ..., -0.0497,  0.0010, -0.0065]],
       device='cuda:0')}

 Model STL10_CIFAR10_CL158728fd-dcb8-497d-8bd4-d5ba2c749836 loaded successfuly with best perf



 ----- Architecture Block BatchNorm2dSK3962(5, 5)1.0reflect, number 0 -----
- BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- AvgPool2d(kernel_size=4, stride=2, padding=1)

 ----- Architecture Block BatchNorm2dSK963842(3, 3)0.6499999761581421reflect, number 1 -----
- BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK38415362(3, 3)0.44999998807907104reflect, number 2 -----
- BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.4)
- AvgPool2d(kernel_size=4, stride=2, padding=1)

 ----- Architecture Block BatchNorm2dSK153661442(3, 3)0.25reflect, number 3 -----
- BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 4 -----
- Flatten(start_dim=1, end_dim=-1)
- Dropout(p=0.5, inplace=False)
- Linear(in_features=221184, out_features=10, bias=True)
model.heads:  [{'blocks.4.layer.bias': tensor([ 0.0016, -0.0015,  0.0004, -0.0005,  0.0020,  0.0031,  0.0054, -0.0016,
         0.0026,  0.0032], device='cuda:0'), 'blocks.4.layer.weight': tensor([[ 0.0400,  0.0110,  0.0159,  ..., -0.0202, -0.0415, -0.0304],
        [ 0.0107, -0.0007,  0.0315,  ..., -0.0158,  0.0113,  0.0158],
        [ 0.0364, -0.0160,  0.0407,  ...,  0.0141,  0.0031,  0.0035],
        ...,
        [ 0.0219,  0.0111,  0.0037,  ...,  0.0494,  0.0133, -0.0125],
        [-0.0186,  0.0274, -0.0178,  ..., -0.0034, -0.0105,  0.0038],
        [-0.0212, -0.0315, -0.0387,  ..., -0.0497,  0.0010, -0.0065]],
       device='cuda:0')}]
[Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=1.4)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=221184, out_features=10, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)]
LAYER NAME:  Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=1.4)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=221184, out_features=10, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)
LAYER CHILDREN:  [BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
  (activation): Triangle(power=1.4)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1)
), BasicBlock(
  (operations): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
  )
  (layer): Linear(in_features=221184, out_features=10, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)]
subsubl NAME:  Sequential(
  (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=4, stride=2, padding=1)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=4, stride=2, padding=1)
subsubl NAME:  Triangle(power=1.4)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1)
subsubl NAME:  Sequential(
  (0): Flatten(start_dim=1, end_dim=-1)
)
subsubl NAME:  Linear(in_features=221184, out_features=10, bias=True)
subsubl NAME:  Dropout(p=0.5, inplace=False)
CONFIG MODE:  unsupervised

 ********** Hebbian Unsupervised learning of blocks [0, 1, 2, 3] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
INDICES:  50000
IMAGE SIZE: torch.Size([10, 3, 96, 96])
SAVING FOLDER FOR UNSUP:  STL10_CIFAR10_CL158728fd-dcb8-497d-8bd4-d5ba2c749836
prev_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
activations_sum.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
DEPTH:  4
ACTIVATIONS SHAPE:  tensor([[ 7.0837e+00, -1.7342e+02, -5.2790e+01,  ..., -3.0243e+02,
         -3.1626e+02, -8.0347e+01],
        [-8.1204e+01, -2.1264e+02, -1.1070e+02,  ..., -3.5674e+02,
         -3.7442e+02, -1.3079e+02],
        [-1.0510e+01, -2.5627e+02, -1.4349e+02,  ..., -2.6999e+02,
         -3.1838e+02, -2.5646e+01],
        ...,
        [ 2.0937e+02,  2.8768e-01,  5.9503e+01,  ..., -6.2056e+01,
         -1.4965e+02,  1.2563e+02],
        [ 4.0981e+02,  2.2179e+02,  2.8451e+02,  ...,  1.7146e+02,
          3.1691e+01,  3.1062e+02],
        [ 1.0649e+02, -1.2042e+02, -1.1781e+02,  ..., -1.8817e+02,
         -2.5834e+02,  1.3789e+00]], device='cuda:0')
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.topk_layer:  <class 'list'>
CHECK_THRESHOLD:  tensor(1, device='cuda:0', dtype=torch.uint8) tensor(-0.0004) tensor(-0.2085)
topk_mask  tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0.])
threshold_mask  tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0],
       dtype=torch.uint8)
final_mask tensor([1., 0., 0., 0., 1., 2., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 2., 0.,
        1., 1., 0., 0., 1., 0., 0., 2., 1., 1., 0., 1., 1., 0., 1., 1., 0., 2.,
        1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0.,
        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 2., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0.])
lower_lr_mask  tensor([-0., -0., -0., -0., -0., -1., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -1., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -1., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -1.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.])
higher_lr_mask  tensor([0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.0000, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000,
        0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.0000, 0.0000, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500])
lr_mask  tensor([[[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[0.0000]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]]])
torch.Size([96, 3, 5, 5])
self.lr tensor([[[0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066]],

        [[0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066]],

        [[0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066]]], device='cuda:0')
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.topk_layer:  <class 'list'>
CHECK_THRESHOLD:  tensor(0, device='cuda:0', dtype=torch.uint8) tensor(0.0010) tensor(0.0029)
topk_mask  tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0.])
threshold_mask  tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1],
       dtype=torch.uint8)
final_mask tensor([0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 1., 0., 1., 1., 0., 1., 2., 1., 1., 0., 0., 0., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 2., 1.,
        1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 2., 0.,
        2., 0., 1., 0., 1., 1., 0., 0., 1., 2., 1., 0., 0., 1., 2., 1., 1., 1.,
        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 2., 0., 0., 1., 1.,
        1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0.,
        1., 0., 1., 0., 0., 1., 2., 1., 1., 1., 1., 1., 1., 0., 1., 2., 1., 0.,
        0., 2., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1.,
        2., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 2., 0., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 2., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0.,
        0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 1., 2., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.,
        0., 1., 2., 0., 0., 0., 1., 0., 1., 1., 0., 0., 2., 0., 1., 0., 0., 0.,
        1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1.,
        0., 1., 1., 1., 0., 1., 1., 1., 2., 1., 1., 0., 1., 1., 0., 1., 0., 0.,
        0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0.,
        2., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0.,
        1., 0., 1., 2., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,
        0., 0., 1., 0., 0., 2., 1., 0., 0., 1., 0., 0., 0., 2., 1., 0., 1., 0.,
        0., 1., 0., 0., 2., 1.])
lower_lr_mask  tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -1., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -1., -0., -1., -0., -0., -0., -0., -0., -0., -0.,
        -0., -1., -0., -0., -0., -0., -1., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0., -0.,
        -0., -0., -0., -0., -0., -1., -0., -0., -0., -1., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -1., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -1., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -1., -0., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -1., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -1., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -1., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0., -0., -0.,
        -0., -0., -0., -0., -1., -0.])
higher_lr_mask  tensor([0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500,
        0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.0000,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500,
        0.1500, 0.0000, 0.1500, 0.0000, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500,
        0.0000, 0.1500, 0.0000, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500,
        0.0000, 0.0000, 0.1500, 0.1500, 0.1500, 0.0000, 0.0000, 0.1500, 0.1500,
        0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.0000, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.0000, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.0000, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.0000,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.0000, 0.1500, 0.1500,
        0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000,
        0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.0000, 0.0000, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.0000,
        0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.0000,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500])
lr_mask  tensor([[[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]]])
torch.Size([384, 96, 3, 3])
self.lr tensor([[[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]]], device='cuda:0')
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.topk_layer:  <class 'list'>
CHECK_THRESHOLD:  tensor(0, device='cuda:0', dtype=torch.uint8) tensor(0.0006) tensor(0.0017)
topk_mask  tensor([0., 0., 0.,  ..., 0., 0., 0.])
threshold_mask  tensor([0, 1, 0,  ..., 0, 1, 0], dtype=torch.uint8)
final_mask tensor([0., 1., 0.,  ..., 0., 1., 0.])
lower_lr_mask  tensor([-0., -0., -0.,  ..., -0., -0., -0.])
higher_lr_mask  tensor([0.1500, 0.1500, 0.1500,  ..., 0.1500, 0.1500, 0.1500])
lr_mask  tensor([[[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        ...,


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]]])
torch.Size([1536, 384, 3, 3])
self.lr tensor([[[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]],

        [[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]],

        [[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]],

        ...,

        [[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]],

        [[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]],

        [[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]]], device='cuda:0')
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.topk_layer:  <class 'list'>
CHECK_THRESHOLD:  tensor(0, device='cuda:0', dtype=torch.uint8) tensor(0.0006) tensor(0.0113)
topk_mask  tensor([0., 0., 1.,  ..., 0., 1., 0.])
threshold_mask  tensor([0, 0, 1,  ..., 1, 1, 0], dtype=torch.uint8)
final_mask tensor([0., 0., 2.,  ..., 1., 2., 0.])
lower_lr_mask  tensor([-0., -0., -1.,  ..., -0., -1., -0.])
higher_lr_mask  tensor([0.1500, 0.1500, 0.0000,  ..., 0.1500, 0.0000, 0.1500])
lr_mask  tensor([[[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        ...,


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]]])
torch.Size([6144, 1536, 3, 3])
self.lr tensor([[[0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488]],

        [[0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488]],

        [[0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488]],

        ...,

        [[0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488]],

        [[0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488]],

        [[0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488]]], device='cuda:0')
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight'])
activations_sum[k] 96
activations_sum[k]:  [42, 59, 25, 30, 14, 70, 35, 69, 68, 93]
topk_kernels len:  4
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3']
topk_kernels:  {'conv0': [42, 59, 25, 30, 14, 70, 35, 69, 68, 93, 53, 77, 55, 21, 5], 'conv1': [345, 365, 75, 354, 190, 100, 56, 180, 191, 105, 104, 73, 382, 121, 94, 274, 158, 36, 150, 264, 336, 23, 99, 140, 163, 76, 197, 26, 349, 179, 109, 159, 324, 202, 92, 193, 206, 241, 373, 41, 219, 17, 118, 296, 70, 4, 254, 9, 88, 50, 90, 282, 144, 146, 355, 124, 133, 152, 286], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
activations_sum[k] len:  96
activations_sum[k] 384
activations_sum[k]:  [23, 382, 345, 191, 75, 104, 365, 190, 144, 99]
topk_kernels len:  4
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3']
topk_kernels:  {'conv0': [42, 59, 25, 30, 14, 70, 35, 69, 68, 93, 53, 77, 55, 21, 5], 'conv1': [23, 382, 345, 191, 75, 104, 365, 190, 144, 99, 100, 264, 56, 324, 158, 92, 180, 152, 150, 296, 133, 140, 121, 163, 105, 146, 88, 349, 94, 179, 33, 36, 41, 201, 202, 159, 26, 13, 39, 354, 336, 373, 219, 335, 254, 374, 206, 344, 76, 186, 50, 274, 30, 314, 166, 70, 73, 225, 247], 'conv2': [835, 942, 869, 790, 313, 209, 733, 761, 1006, 35, 423, 936, 915, 98, 1455, 68, 441, 735, 644, 590, 1113, 1532, 1395, 484, 1100, 449, 662, 119, 1297, 350, 270, 250, 752, 1441, 857, 711, 868, 599, 959, 1211, 324, 542, 981, 335, 1086, 454, 295, 1011, 667, 1307, 1238, 341, 381, 1357, 1474, 227, 731, 812, 1486, 1470, 746, 1126, 964, 106, 801, 743, 461, 257, 229, 891, 1471, 361, 1048, 760, 456, 79, 632, 622, 1347, 356, 1115, 5, 1071, 519, 636, 896, 647, 958, 1436, 683, 549, 859, 235, 408, 1429, 827, 545, 875, 849, 514, 1046, 405, 1243, 1215, 1134, 846, 1450, 445, 70, 75, 1266, 1143, 1282, 1494, 904, 1269, 304, 726, 1402, 1317, 626, 1230, 142, 427, 614, 1358, 33, 1472, 1184, 834, 1032, 161, 1388, 613, 1195, 764, 1132, 601, 817, 22, 783, 871, 207, 1116, 426, 1464, 534, 978, 627, 41, 1233, 409, 854, 80, 212, 425, 634, 1052, 450, 876, 1380, 1083, 553, 802, 447, 856, 1360, 1252, 914, 935, 1442, 214, 1167, 1172, 289, 839, 342, 255, 1480, 872, 1377, 1412, 532, 943, 1053, 919, 139, 788, 1371, 121, 1225, 1279, 167, 340, 1520, 28, 976, 1522, 141, 1110, 164, 326, 243, 187, 264, 95, 635, 1460, 544, 588, 247, 118, 496, 25, 1508, 404, 1278, 241, 1012, 971, 557, 786, 524, 1400, 980, 906, 784, 900, 1276, 1271, 276], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
activations_sum[k] len:  384
activations_sum[k] 1536
activations_sum[k]:  [445, 875, 1317, 235, 1279, 859, 1307, 119, 187, 662]
topk_kernels len:  4
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3']
topk_kernels:  {'conv0': [42, 59, 25, 30, 14, 70, 35, 69, 68, 93, 53, 77, 55, 21, 5], 'conv1': [23, 382, 345, 191, 75, 104, 365, 190, 144, 99, 100, 264, 56, 324, 158, 92, 180, 152, 150, 296, 133, 140, 121, 163, 105, 146, 88, 349, 94, 179, 33, 36, 41, 201, 202, 159, 26, 13, 39, 354, 336, 373, 219, 335, 254, 374, 206, 344, 76, 186, 50, 274, 30, 314, 166, 70, 73, 225, 247], 'conv2': [445, 875, 1317, 235, 1279, 859, 1307, 119, 187, 662, 784, 1243, 760, 711, 276, 735, 335, 427, 80, 1395, 647, 454, 915, 1086, 356, 247, 1441, 590, 106, 167, 141, 1011, 409, 41, 786, 423, 726, 1464, 914, 817, 1494, 613, 68, 827, 1195, 1184, 835, 801, 519, 1412, 812, 943, 1400, 896, 849, 746, 764, 484, 1126, 1247, 599, 295, 752, 1230, 341, 869, 868, 935, 313, 1532, 1233, 857, 1375, 227, 447, 1297, 1006, 1132, 1271, 782, 22, 731, 425, 472, 1042, 350, 891, 144, 1071, 612, 1474, 1186, 159, 964, 381, 959, 450, 408, 683, 70, 1436, 651, 1113, 1486, 426, 1100, 667, 906, 958, 1012, 456, 1069, 1460, 822, 449, 266, 617, 1269, 121, 1429, 1282, 1530, 1357, 1220, 25, 1320, 461, 474, 917, 953, 937, 243, 971, 441, 635, 644, 614, 854, 1372, 304, 1053, 353, 1032, 79, 289, 305, 1465, 839, 601, 244, 524, 1360, 1471, 1266, 1210, 180, 268, 619, 700, 405, 532, 225, 209, 993, 252, 1358, 440, 229, 28, 207, 545, 346, 326, 421, 501, 847, 1134, 846, 241, 142, 340, 1116, 1046, 214, 640, 1361, 537, 161, 380, 366, 1522, 1211, 1481, 1000, 557, 673, 1454, 627, 430, 1252, 865, 900, 5, 58, 130, 414, 135, 1407, 577, 976, 1500, 942, 294, 981, 78, 1442, 781, 195, 314, 636, 818, 264, 1083, 1110, 204, 1489, 579, 660, 1238, 246, 838], 'conv3': [2460, 4651, 2000, 3540, 514, 4704, 5890, 4502, 4778, 822, 6094, 2572, 5643, 5121, 1798, 2679, 361, 5421, 2966, 4217, 3844, 1504, 1115, 4962, 2613, 4299, 2447, 2771, 5558, 1712, 2075, 4395, 1312, 5368, 1806, 4821, 1564, 3718, 4537, 1544, 309, 3165, 4306, 2736, 4114, 5469, 5318, 534, 1857, 1653, 363, 3828, 4108, 2058, 2019, 2533, 43, 3722, 5096, 4739, 4814, 5582, 4614, 4670, 4186, 5396, 4699, 1641, 3375, 1978, 5146, 1022, 165, 4258, 5349, 5243, 6119, 3734, 2281, 4572, 3329, 2691, 2069, 2056, 33, 5407, 5503, 2074, 4097, 3827, 106, 4402, 3469, 2415, 5150, 3430, 4076, 911, 2342, 2860, 5520, 4908, 6088, 1557, 6142, 1577, 1887, 1112, 312, 4735, 2101, 1138, 2013, 3067, 6131, 6043, 2979, 3877, 796, 4376, 3036, 4924, 4702, 3964, 6030, 3750, 4100, 3716, 2917, 3816, 5035, 2470, 2248, 3383, 5027, 5580, 676, 4703, 3178, 2850, 687, 3420, 2607, 863, 294, 3783, 1709, 767, 4415, 999, 4218, 1885, 2270, 4247, 1131, 4606, 4992, 4357, 706, 1845, 5821, 5778, 5671, 5048, 5187, 5045, 164, 4791, 5799, 894, 1481, 5322, 1680, 4484, 1385, 3418, 4135, 3009, 198, 1501, 1176, 2841, 2522, 2611, 6074, 3740, 3448, 2762, 5205, 3717, 3170, 1753, 5143, 3212, 5707, 2914, 1335, 3227, 1623, 2078, 5491, 4275, 2089, 3523, 1939, 5531, 3454, 4566, 5834, 3615, 2305, 677, 1163, 1946, 780, 194, 5776, 2542, 5752, 167, 1304, 5698, 4906, 1631, 3193, 1158, 3646, 1362, 1294, 5008, 5631, 471, 1866, 569, 505, 5796, 114, 2783, 806, 4007, 3986, 5277, 8, 4971, 4266, 3598, 6139, 5423, 116, 1168, 146, 5083, 1561, 4470, 3916, 5908, 3941, 4475, 3183, 4460, 2047, 4144, 5642, 5876, 4482, 6098, 1853, 2697, 4668, 3662, 5961, 430, 3221, 5482, 3035, 1988, 685, 3086, 3393, 2205, 2483, 4244, 3384, 772, 3998, 4246, 5964, 5451, 5452, 2847, 1443, 4087, 4190, 4554, 827, 6133, 4819, 5795, 1591, 5422, 1439, 3198, 129, 667, 6050, 3880, 5015, 4681, 5967, 2122, 5235, 2716, 732, 5013, 5338, 2462, 3119, 5334, 4165, 3597, 4877, 4261, 21, 2494, 5966, 1096, 3179, 1554, 1066, 1549, 3480, 63, 2032, 4544, 2331, 1685, 3181, 2083, 4780, 292, 4066, 2418, 240, 61, 3141, 3751, 3563, 4637, 2997, 6090, 5473, 5690, 1083, 4958, 943, 4465, 3177, 4726, 221, 3275, 4467, 2213, 3873, 3237, 1830, 3858, 1520, 4163, 2747, 5271, 2203, 5230, 187, 5014, 4349, 5222, 1220, 3392, 2488, 1894, 5448, 4551, 1548, 3450, 66, 2645, 278, 1086, 5918, 5124, 4181, 26, 4222, 493, 1615, 2493, 599, 5442, 1835, 2261, 3108, 5861, 709, 3557, 790, 4431, 3537, 1346, 5470, 287, 5363, 2598, 1865, 2559, 3449, 5072, 52, 2399, 1130, 443, 3033, 4574, 2610, 4451, 1319, 3863, 3459, 2152, 1097, 105, 5651, 424, 1052, 2852, 3152, 3559, 5024, 4278, 5445, 4382, 1877, 5376, 1996, 3543, 5021, 2315, 5694, 1696, 5808, 2471, 4269, 5103, 851, 1825, 3435, 3736, 2576, 1445, 3470, 3561, 978, 701, 2824, 2976, 411, 3744, 2398, 265, 4876, 1539, 3909, 4309, 2804, 5661, 1037, 4602, 4143, 4538, 1909, 4151, 2221, 4212, 5657, 1222, 2950, 2187, 682, 3312, 3897, 5806, 3490, 6095, 4170, 801, 4034, 3761, 2891, 1296, 3013, 4879, 1043, 620, 4899, 1499, 5866, 180, 6100, 1260, 122, 5757, 4081, 3024, 3319, 5003, 1181, 69, 3950, 857, 1679, 4690, 4599, 1575, 5746, 5054, 3187, 261, 3061, 909, 3807, 1042, 30, 1284, 117, 2095, 1028, 4329, 2656, 3991, 2452, 539, 2608, 1799, 3252, 3015, 3839, 3126, 41, 5953, 1804, 326, 2070, 2702, 2022, 1622, 5109, 799, 3981, 1313, 4182, 425, 3733, 4639, 2741, 2246, 2817, 511, 4546, 4652, 5857, 903, 4979, 4262, 4009, 1579, 899, 4358, 5928, 2446, 2515, 3548, 5123, 4513, 1274, 3989, 5493, 6112, 5910, 5984, 5286, 5429, 3145, 4287, 5759, 2336, 4196, 4457, 3153, 1890, 5414, 948, 5500, 371, 3134, 1452, 2138, 3074, 1532, 976, 575, 3323, 3802, 1301, 2693, 4688, 6118, 4547, 5116, 3044, 1232, 1960, 3845, 1942, 1451, 6058, 1055, 584, 4141, 5812, 507, 3301, 745, 3121, 4556, 712, 1644, 5291, 2555, 3942, 1357, 5676, 1425, 5504, 819, 1938, 5234, 5617, 1154, 1489, 3634, 450, 1730, 6024, 750, 5078, 981, 1919, 2849, 3577, 646, 6125, 1854, 448, 5191, 1020, 1824, 2491, 4734, 4485, 2212, 305, 4134, 1182, 381, 4838, 4080, 3399, 2644, 3968, 2752, 985, 1040, 4531, 13, 4341, 933, 5091, 3337, 4027, 1424, 465, 993, 2980, 4271, 5839, 3197, 4738, 2799, 4159, 2993, 2253, 3242, 673, 2814, 4394, 2778, 1904, 5871, 1995, 3987, 842, 2106, 5381, 4000, 947, 1950, 5700, 4388, 4308, 1533, 694, 185, 516, 2846, 4312, 4539, 1349, 5466, 2835, 954, 1306, 686, 4227, 774, 5255, 3649, 675, 4292, 4961, 1257, 4318, 5669, 3388, 5981, 3888, 4103, 3800, 3041, 4496, 3084, 5262, 1221, 178, 5454, 3362, 4118, 5477, 5125, 888, 1280, 5374, 5110, 4824, 2185, 3643, 555, 944, 5962, 1749, 1526, 5090, 881, 1093, 3050, 632, 2527, 239, 301, 1585, 2571, 5051, 5416, 1231, 1519, 5212, 2986, 5706, 2146, 446, 3005, 1422, 1949, 3486, 3348, 5791, 3173, 3516, 5948, 926, 825, 2067, 2283, 2062, 2962, 331, 4270, 1466, 95, 5201, 2, 3984, 742, 1259, 5138, 2666, 1662, 1838, 1741, 4408, 4638, 551, 3980, 1874, 5717, 183, 5067, 4926, 3027, 785, 3745, 2323, 2181, 1859, 2367, 699, 1011, 4753, 5820, 4817, 3191, 1361, 581, 2298, 3436, 5217, 1815, 1731, 2907, 2486, 5040, 3753, 2951, 4693, 1822, 515, 2335, 5817, 5532, 2037, 131, 1566, 5180, 4249, 4631, 3993, 347, 5127, 3234, 2692, 4719, 1000, 3710, 4352, 3096, 68, 358, 1289, 2785, 4696, 2033, 3349, 4120, 1428, 3919, 1913, 3818, 1943, 3300, 2784, 4848, 5626, 2290, 4001, 775, 4277, 4110, 4807, 5272, 3334, 2226, 4986, 5619, 3947, 4359, 436, 4445, 2865, 2378, 4239, 4972, 3852, 4044, 4808, 4555, 5899, 2585, 5647, 5517, 5495, 634, 2426, 3097, 4653, 5108]}
activations_sum[k] len:  1536
activations_sum[k] 6144
activations_sum[k]:  [4076, 2572, 1544, 944, 1163, 5694, 6119, 5015, 4144, 2067]
topk_kernels len:  4
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3']
topk_kernels:  {'conv0': [42, 59, 25, 30, 14, 70, 35, 69, 68, 93, 53, 77, 55, 21, 5], 'conv1': [23, 382, 345, 191, 75, 104, 365, 190, 144, 99, 100, 264, 56, 324, 158, 92, 180, 152, 150, 296, 133, 140, 121, 163, 105, 146, 88, 349, 94, 179, 33, 36, 41, 201, 202, 159, 26, 13, 39, 354, 336, 373, 219, 335, 254, 374, 206, 344, 76, 186, 50, 274, 30, 314, 166, 70, 73, 225, 247], 'conv2': [445, 875, 1317, 235, 1279, 859, 1307, 119, 187, 662, 784, 1243, 760, 711, 276, 735, 335, 427, 80, 1395, 647, 454, 915, 1086, 356, 247, 1441, 590, 106, 167, 141, 1011, 409, 41, 786, 423, 726, 1464, 914, 817, 1494, 613, 68, 827, 1195, 1184, 835, 801, 519, 1412, 812, 943, 1400, 896, 849, 746, 764, 484, 1126, 1247, 599, 295, 752, 1230, 341, 869, 868, 935, 313, 1532, 1233, 857, 1375, 227, 447, 1297, 1006, 1132, 1271, 782, 22, 731, 425, 472, 1042, 350, 891, 144, 1071, 612, 1474, 1186, 159, 964, 381, 959, 450, 408, 683, 70, 1436, 651, 1113, 1486, 426, 1100, 667, 906, 958, 1012, 456, 1069, 1460, 822, 449, 266, 617, 1269, 121, 1429, 1282, 1530, 1357, 1220, 25, 1320, 461, 474, 917, 953, 937, 243, 971, 441, 635, 644, 614, 854, 1372, 304, 1053, 353, 1032, 79, 289, 305, 1465, 839, 601, 244, 524, 1360, 1471, 1266, 1210, 180, 268, 619, 700, 405, 532, 225, 209, 993, 252, 1358, 440, 229, 28, 207, 545, 346, 326, 421, 501, 847, 1134, 846, 241, 142, 340, 1116, 1046, 214, 640, 1361, 537, 161, 380, 366, 1522, 1211, 1481, 1000, 557, 673, 1454, 627, 430, 1252, 865, 900, 5, 58, 130, 414, 135, 1407, 577, 976, 1500, 942, 294, 981, 78, 1442, 781, 195, 314, 636, 818, 264, 1083, 1110, 204, 1489, 579, 660, 1238, 246, 838], 'conv3': [4076, 2572, 1544, 944, 1163, 5694, 6119, 5015, 4144, 2067, 4382, 1894, 780, 3980, 3710, 5338, 863, 287, 1439, 2101, 425, 5027, 4431, 2323, 534, 2736, 2070, 3165, 309, 5799, 3559, 2598, 1564, 2460, 3947, 3984, 4394, 4244, 5201, 43, 6088, 1641, 26, 178, 775, 2576, 4299, 2607, 5143, 3827, 3399, 5871, 2835, 2752, 5396, 5890, 4821, 3577, 3783, 4312, 1097, 4876, 5910, 3035, 1312, 3828, 2613, 911, 3383, 3187, 5054, 4537, 2213, 1083, 4651, 4817, 3734, 371, 1845, 1257, 2000, 1949, 3897, 2976, 515, 3986, 6094, 2335, 2244, 5631, 5286, 2491, 3348, 4467, 4992, 1877, 514, 4699, 2559, 1428, 5150, 5277, 3561, 4814, 5003, 750, 5222, 5272, 4979, 1960, 2342, 3181, 1753, 894, 2986, 4614, 1653, 8, 5491, 2138, 4906, 1422, 1950, 5908, 5645, 4791, 2966, 465, 2426, 2315, 1532, 5448, 2185, 4824, 1575, 3964, 1080, 1385, 1873, 2891, 5542, 2471, 985, 2679, 1631, 95, 5846, 2221, 4598, 1730, 2152, 3221, 1685, 294, 632, 2656, 3036, 4339, 61, 5048, 5698, 4097, 3226, 5318, 2261, 4186, 2917, 3429, 1988, 1346, 2089, 2515, 4395, 1749, 493, 5421, 948, 4451, 4899, 1501, 527, 3420, 5014, 3722, 5642, 63, 4227, 2013, 3466, 5349, 3662, 4044, 4908, 2693, 4702, 2950, 3362, 1335, 1438, 1709, 2270, 4110, 4482, 4476, 2069, 4134, 4376, 2026, 4502, 6125, 4163, 1622, 4118, 184, 1938, 825, 3788, 5493, 5580, 3733, 2019, 4884, 1539, 5212, 2787, 2975, 3938, 859, 4034, 4819, 2246, 4269, 2580, 1390, 4606, 2367, 2470, 3537, 4135, 1615, 2083, 1696, 2281, 415, 2585, 5407, 4566, 5839, 6032, 5124, 6021, 451, 5626, 3852, 3698, 4735, 679, 2248, 806, 2533, 1798, 5651, 105, 4181, 4358, 1833, 3718, 2996, 3606, 6084, 4352, 5795, 6070, 2420, 6041, 4000, 361, 1037, 5752, 1131, 5000, 471, 599, 4688, 881, 551, 5466, 3375, 1623, 4081, 4247, 796, 3097, 3571, 1009, 167, 2747, 3950, 2814, 1888, 2447, 1357, 3749, 3775, 706, 1349, 999, 3916, 4323, 2242, 2146, 1897, 1307, 4572, 1055, 686, 2581, 2415, 4341, 2049, 6095, 4120, 2850, 3070, 2095, 1022, 5004, 4301, 3495, 2290, 1138, 1973, 4261, 5753, 1978, 2488, 1115, 4100, 205, 3670, 6072, 4778, 4001, 363, 2001, 3991, 430, 3005, 1368, 4602, 3252, 4980, 257, 2172, 1406, 278, 4662, 4900, 2797, 2075, 4025, 5067, 4455, 827, 5876, 5312, 3145, 1853, 5109, 4962, 3448, 3067, 4222, 3839, 5005, 1519, 132, 5984, 836, 3615, 5589, 2328, 1155, 3818, 3384, 4385, 1168, 3745, 1857, 4007, 4639, 3387, 4143, 4277, 6043, 2608, 1222, 1806, 6100, 4363, 2691, 461, 1909, 5796, 4009, 477, 1220, 1778, 676, 5422, 5023, 2056, 261, 468, 2597, 4218, 4885, 2697, 1158, 730, 4357, 5791, 1104, 2785, 4266, 5811, 2962, 1887, 584, 5376, 3242, 2106, 4004, 5423, 4963, 709, 1513, 4719, 2452, 1252, 4570, 5590, 1028, 4392, 2079, 2769, 14, 516, 822, 5809, 6098, 2555, 3253, 1526, 3641, 3740, 4066, 2262, 4845, 2841, 1361, 1337, 5834, 4292, 5414, 1508, 5669, 2698, 5321, 3717, 5792, 1530, 3084, 3013, 4925, 3959, 4349, 3548, 569, 4080, 3106, 4402, 3009, 1597, 732, 4638, 2865, 2800, 2647, 4739, 248, 1306, 2494, 5477, 667, 5072, 2668, 1130, 4460, 3193, 3470, 3170, 2847, 5363, 2979, 3177, 5103, 1799, 5773, 144, 1831, 1289, 5591, 52, 1680, 3334, 3153, 3173, 5078, 5558, 5658, 1594, 4159, 1822, 5254, 646, 4896, 175, 5582, 3107, 3178, 4457, 2799, 2378, 2863, 943, 40, 6131, 2569, 383, 4954, 3279, 2611, 883, 4475, 3761, 1608, 5070, 4415, 4554, 5035, 6024, 2122, 5961, 4780, 5445, 1336, 117, 3822, 5416, 2817, 4029, 6142, 4306, 2666, 2941, 3750, 993, 5013, 5473, 3435, 5442, 4869, 4445, 5790, 4681, 3981, 4077, 2393, 3658, 774, 6118, 3367, 394, 4072, 1274, 3061, 5069, 6090, 2126, 1677, 5127, 583, 21, 3993, 165, 5504, 4407, 1911, 3027, 5055, 1913, 1000, 2030, 338, 5276, 1402, 152, 1424, 2154, 2355, 2792, 3337, 1036, 5292, 1292, 5507, 3523, 3136, 1226, 3050, 4692, 2282, 5866, 850, 3532, 4350, 5243, 5332, 3323, 2522, 3449, 3034, 508, 5451, 326, 4724, 1296, 4843, 842, 5623, 5643, 292, 5918, 3040, 4807, 426, 1996, 1930, 5283, 767, 5024, 3191, 2418, 211, 5322, 5121, 5065, 909, 526, 2305, 3126, 485, 3054, 2804, 5146, 5381, 5956, 3643, 3474, 1011, 378, 5180, 509, 2860, 2645, 3594, 5308, 2336, 4848, 728, 1181, 4682, 4882, 2317, 4052, 511, 221, 5368, 2675, 3653, 5966, 3294, 2037, 4190, 3480, 3802, 2320, 5671, 2427, 2677, 1095, 4073, 799, 3403, 1936, 5852, 1644, 4704, 5993, 2398, 3873, 45, 3502, 1363, 3998, 2538, 3563, 4637, 1854, 3454, 5929, 2051, 3227, 2345, 5625, 3853, 2694, 4544, 5239, 3361, 976, 1706, 3488, 6133, 1294, 3114, 327, 4640, 1636, 5314, 4551, 2978, 5976, 5188, 660, 5316, 555, 716, 2334, 878, 6044, 3041, 6037, 480, 3860, 1789, 5096, 5169, 1401, 443, 3062, 5256, 1086, 581, 1403, 153, 5776, 5772, 46, 3324, 3680, 4958, 2778, 4924, 6025, 3086, 4142, 1520, 4418, 3347, 3621, 3229, 4027, 2751, 3727, 134, 1453, 873, 2112, 4974, 997, 1903, 5205, 3450, 5629, 194, 1515, 2762, 5130, 3801, 1144, 1096, 5836, 159, 547, 1256, 3069, 3830, 2183, 3464, 3275, 5861, 4203, 2682, 450, 4308, 1183, 2714, 3244, 2989, 2702, 1642, 3175, 5151, 544, 505, 2331, 4653, 3388, 2245, 409, 3657, 1280, 860, 1787, 2173, 3533, 1649, 1194, 4141, 2506, 225, 3108, 4217, 1499, 5568, 3197, 2918, 5806, 4102, 1610, 1459, 3540, 5502, 5936, 1998, 5647, 3736, 5398, 5579, 5419, 4987, 5851, 1042, 1153, 2615, 1904, 2222, 2703, 41, 5190, 2625, 1451, 601, 5635, 5661, 122, 273, 1545, 5922, 888, 1712, 1968, 1112, 3649, 4265, 180, 954, 4151, 1073, 5973, 3936, 3941, 530, 3251, 2859, 2914, 1525, 634, 5401, 5051, 5452, 5409, 4556, 2983, 2005, 5108, 5406, 4298, 3469, 1231, 2885, 446, 776, 3033, 2277, 755, 2824, 1191, 3183, 5393, 3720]}
activations_sum[k] len:  6144
['conv0', 'conv1', 'conv2', 'conv3']
delta_weights INFO: 
NUM OF TRACKED COV LAYERS:  4
NUM OF TRACKED WEIGHTS CHANGES PER LAYER:  100
avg_deltas INFO:  <class 'dict'>
avg_deltas keys:  ['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight']
################################################
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 3.84e-02	time: 00:16:04	Acc_train 0.00	Acc_test 0.00	convergence: 2.56e+01	R1: 15	Info MB:0.000e+00/SB:0.000e+00/MW:1.622e-05/SW:2.305e-01/MR:2.655e+01/SR:5.435e+00/MeD:2.986e+00/MaD:3.914e+01/MW:0.686/MAW:0.314
|        0 |        1 |      2 |        3 |        4 |        5 |        6 |        7 |       8 |        9 |       10 |       11 |       12 |       13 |       14 |       15 |       16 |       17 |       18 |       19 |       20 |      21 |       22 |       23 |      24 |       25 |       26 |       27 |       28 |       29 |
|----------+----------+--------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+---------+----------+----------+----------+----------+----------|
|   0.0502 |   0.0515 |   0.05 |   0.0518 |   0.0509 |   0.0482 |   0.0552 |   0.0507 |   0.036 |   0.0496 |   0.0501 |   0.0502 |   0.0497 |   0.0503 |   0.0525 |   0.0511 |   0.0521 |   0.0534 |   0.0498 |   0.0501 |   0.0497 |   0.044 |   0.0506 |   0.0532 |   0.047 |   0.0439 |   0.0804 |   0.0498 |   0.0527 |   0.0531 |
|  26.16   |  27.48   |  26.03 |  27.88   |  26.88   |  24.21   |  31.51   |  26.66   |  13.95  |  25.62   |  26.05   |  26.2    |  25.69   |  26.26   |  28.61   |  27.15   |  28.16   |  29.56   |  25.82   |  26.1    |  25.71   |  20.36  |  26.62   |  29.27   |  23.09  |  20.23   |  65.69   |  25.78   |  28.81   |  29.24   |
|   0.01   |   0      |   0    |   0      |   0.01   |   0.01   |   0.01   |   0      |   0.11  |   0      |   0      |   0      |   0      |   0      |   0.01   |   0      |   0      |   0      |   0      |   0      |   0      |   0.01  |   0      |   0      |   0     |   0.01   |   0.4    |   0      |   0      |   0      |
| nan      | nan      | nan    | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan    | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan    | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:1.073e-03/SW:2.849e-01/MR:1.614e+01/SR:4.449e+00/MeD:3.389e+00/MaD:2.557e+01/MW:0.590/MAW:0.410
|        0 |        1 |        2 |        3 |        4 |        5 |        6 |        7 |        8 |        9 |       10 |       11 |       12 |       13 |       14 |       15 |       16 |       17 |       18 |       19 |       20 |       21 |       22 |       23 |       24 |       25 |      26 |       27 |       28 |       29 |
|----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------|
|   0.0446 |   0.0398 |   0.0419 |   0.0406 |   0.0335 |   0.0422 |   0.0464 |   0.0328 |   0.0385 |   0.0477 |   0.0159 |   0.0398 |   0.0328 |   0.0301 |   0.0384 |   0.0366 |   0.0318 |   0.0501 |   0.0382 |   0.0451 |   0.0355 |   0.0394 |   0.0276 |   0.0395 |   0.0333 |   0.0361 |   0.044 |   0.0394 |   0.0322 |   0.0409 |
|  20.85   |  16.86   |  18.57   |  17.5    |  12.22   |  18.81   |  22.53   |  11.79   |  15.84   |  23.76   |   3.52   |  16.87   |  11.76   |  10.08   |  15.74   |  14.4    |  11.12   |  26.12   |  15.63   |  21.37   |  13.59   |  16.56   |   8.61   |  16.59   |  12.09   |  14.03   |  20.33  |  16.5    |  11.34   |  17.76   |
|   0.02   |   0.02   |   0.06   |   0.06   |   0.03   |   0.12   |   0.01   |   0.05   |   0.04   |   0.02   |   0.16   |   0.03   |   0.03   |   0.04   |   0.04   |   0.02   |   0.09   |   0.01   |   0.03   |   0.01   |   0.08   |   0.02   |   0.32   |   0.03   |   0.04   |   0.09   |   0.01  |   0.04   |   0.04   |   0.01   |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:-1.696e-03/SW:5.886e-01/MR:1.679e+01/SR:4.168e+00/MeD:3.420e+00/MaD:1.259e+01/MW:0.616/MAW:0.384
|        0 |        1 |        2 |        3 |        4 |        5 |        6 |        7 |        8 |        9 |       10 |       11 |       12 |       13 |       14 |       15 |       16 |       17 |       18 |       19 |       20 |       21 |       22 |      23 |      24 |       25 |       26 |       27 |       28 |       29 |
|----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+---------+----------+----------+----------+----------+----------|
|   0.0211 |   0.0232 |   0.0221 |   0.0207 |   0.0143 |   0.0165 |   0.0215 |   0.0192 |   0.0212 |   0.0191 |   0.0246 |   0.0203 |   0.0196 |   0.0207 |   0.0194 |   0.0211 |   0.0214 |   0.0161 |   0.0206 |   0.0165 |   0.0171 |   0.0235 |   0.0196 |   0.025 |   0.018 |   0.0172 |   0.0172 |   0.0208 |   0.0191 |   0.0229 |
|  18.88   |  22.57   |  20.61   |  18.11   |   9.22   |  11.92   |  19.41   |  15.82   |  18.91   |  15.65   |  25.21   |  17.49   |  16.31   |  18.16   |  16.06   |  18.8    |  19.38   |  11.42   |  17.91   |  11.9    |  12.68   |  23.11   |  16.33   |  26.09  |  13.92  |  12.85   |  12.77   |  18.27   |  15.52   |  22.01   |
|   0.08   |   0.05   |   0.06   |   0.08   |   0.25   |   0.11   |   0.12   |   0.11   |   0.07   |   0.14   |   0.01   |   0.14   |   0.15   |   0.25   |   0.12   |   0.09   |   0.2    |   0.33   |   0.05   |   0.19   |   0.13   |   0.2    |   0.49   |   6.63  |   0.22  |   0.34   |   1.78   |   0.05   |   0.08   |   0.2    |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan     | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan     | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan     | nan      | nan      | nan      | nan      | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:-1.965e-02/SW:1.544e+00/MR:1.291e+01/SR:3.499e+00/MeD:2.833e+00/MaD:1.011e+01/MW:0.593/MAW:0.407
|       0 |       1 |       2 |       3 |       4 |       5 |       6 |      7 |       8 |       9 |      10 |      11 |      12 |      13 |      14 |      15 |      16 |     17 |      18 |      19 |      20 |      21 |     22 |      23 |      24 |      25 |     26 |      27 |      28 |     29 |
|---------+---------+---------+---------+---------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------+---------+---------+---------+---------+--------+---------+---------+---------+--------+---------+---------+--------|
|   0.243 |   0.324 |   0.316 |   0.322 |   0.232 |   0.211 |   0.265 |   0.33 |   0.226 |   0.368 |   0.232 |   0.277 |   0.266 |   0.297 |   0.276 |   0.317 |   0.192 |   0.25 |   0.311 |   0.261 |   0.285 |   0.272 |   0.32 |   0.351 |   0.338 |   0.239 |   0.28 |   0.258 |   0.272 |   0.33 |
|  10.2   |  17.42  |  16.64  |  17.17  |   9.44  |   7.99  |  11.98  |  18.03 |   8.98  |  22.15  |   9.38  |  13.03  |  12.1   |  14.75  |  12.91  |  16.7   |   6.75  |  10.8  |  16.09  |  11.64  |  13.7   |  12.53  |  17.02 |  20.26  |  18.81  |   9.96  |  13.26 |  11.39  |  12.55  |  17.99 |
|   0.27  |   0.16  |   0.09  |   0.22  |   1.11  |   7.64  |   0.49  |   0.38 |   0.58  |   0.04  |   0.73  |   0.33  |   0.21  |   0.17  |   1.1   |   0.21  |   6.55  |   0.21 |   0.62  |   1.14  |   0.33  |   0.65  |   0.21 |   0.24  |   0.12  |   3.16  |   0.37 |   0.55  |   0.18  |   0.17 |
| nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan    | nan     | nan     | nan    |
| nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan    | nan     | nan     | nan    |
| nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan    | nan     | nan     | nan    |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL158728fd-dcb8-497d-8bd4-d5ba2c749836/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [42, 59, 25, 30, 14, 70, 35, 69, 68, 93, 53, 77, 55, 21, 5], 'conv1': [23, 382, 345, 191, 75, 104, 365, 190, 144, 99, 100, 264, 56, 324, 158, 92, 180, 152, 150, 296, 133, 140, 121, 163, 105, 146, 88, 349, 94, 179, 33, 36, 41, 201, 202, 159, 26, 13, 39, 354, 336, 373, 219, 335, 254, 374, 206, 344, 76, 186, 50, 274, 30, 314, 166, 70, 73, 225, 247], 'conv2': [445, 875, 1317, 235, 1279, 859, 1307, 119, 187, 662, 784, 1243, 760, 711, 276, 735, 335, 427, 80, 1395, 647, 454, 915, 1086, 356, 247, 1441, 590, 106, 167, 141, 1011, 409, 41, 786, 423, 726, 1464, 914, 817, 1494, 613, 68, 827, 1195, 1184, 835, 801, 519, 1412, 812, 943, 1400, 896, 849, 746, 764, 484, 1126, 1247, 599, 295, 752, 1230, 341, 869, 868, 935, 313, 1532, 1233, 857, 1375, 227, 447, 1297, 1006, 1132, 1271, 782, 22, 731, 425, 472, 1042, 350, 891, 144, 1071, 612, 1474, 1186, 159, 964, 381, 959, 450, 408, 683, 70, 1436, 651, 1113, 1486, 426, 1100, 667, 906, 958, 1012, 456, 1069, 1460, 822, 449, 266, 617, 1269, 121, 1429, 1282, 1530, 1357, 1220, 25, 1320, 461, 474, 917, 953, 937, 243, 971, 441, 635, 644, 614, 854, 1372, 304, 1053, 353, 1032, 79, 289, 305, 1465, 839, 601, 244, 524, 1360, 1471, 1266, 1210, 180, 268, 619, 700, 405, 532, 225, 209, 993, 252, 1358, 440, 229, 28, 207, 545, 346, 326, 421, 501, 847, 1134, 846, 241, 142, 340, 1116, 1046, 214, 640, 1361, 537, 161, 380, 366, 1522, 1211, 1481, 1000, 557, 673, 1454, 627, 430, 1252, 865, 900, 5, 58, 130, 414, 135, 1407, 577, 976, 1500, 942, 294, 981, 78, 1442, 781, 195, 314, 636, 818, 264, 1083, 1110, 204, 1489, 579, 660, 1238, 246, 838], 'conv3': [4076, 2572, 1544, 944, 1163, 5694, 6119, 5015, 4144, 2067, 4382, 1894, 780, 3980, 3710, 5338, 863, 287, 1439, 2101, 425, 5027, 4431, 2323, 534, 2736, 2070, 3165, 309, 5799, 3559, 2598, 1564, 2460, 3947, 3984, 4394, 4244, 5201, 43, 6088, 1641, 26, 178, 775, 2576, 4299, 2607, 5143, 3827, 3399, 5871, 2835, 2752, 5396, 5890, 4821, 3577, 3783, 4312, 1097, 4876, 5910, 3035, 1312, 3828, 2613, 911, 3383, 3187, 5054, 4537, 2213, 1083, 4651, 4817, 3734, 371, 1845, 1257, 2000, 1949, 3897, 2976, 515, 3986, 6094, 2335, 2244, 5631, 5286, 2491, 3348, 4467, 4992, 1877, 514, 4699, 2559, 1428, 5150, 5277, 3561, 4814, 5003, 750, 5222, 5272, 4979, 1960, 2342, 3181, 1753, 894, 2986, 4614, 1653, 8, 5491, 2138, 4906, 1422, 1950, 5908, 5645, 4791, 2966, 465, 2426, 2315, 1532, 5448, 2185, 4824, 1575, 3964, 1080, 1385, 1873, 2891, 5542, 2471, 985, 2679, 1631, 95, 5846, 2221, 4598, 1730, 2152, 3221, 1685, 294, 632, 2656, 3036, 4339, 61, 5048, 5698, 4097, 3226, 5318, 2261, 4186, 2917, 3429, 1988, 1346, 2089, 2515, 4395, 1749, 493, 5421, 948, 4451, 4899, 1501, 527, 3420, 5014, 3722, 5642, 63, 4227, 2013, 3466, 5349, 3662, 4044, 4908, 2693, 4702, 2950, 3362, 1335, 1438, 1709, 2270, 4110, 4482, 4476, 2069, 4134, 4376, 2026, 4502, 6125, 4163, 1622, 4118, 184, 1938, 825, 3788, 5493, 5580, 3733, 2019, 4884, 1539, 5212, 2787, 2975, 3938, 859, 4034, 4819, 2246, 4269, 2580, 1390, 4606, 2367, 2470, 3537, 4135, 1615, 2083, 1696, 2281, 415, 2585, 5407, 4566, 5839, 6032, 5124, 6021, 451, 5626, 3852, 3698, 4735, 679, 2248, 806, 2533, 1798, 5651, 105, 4181, 4358, 1833, 3718, 2996, 3606, 6084, 4352, 5795, 6070, 2420, 6041, 4000, 361, 1037, 5752, 1131, 5000, 471, 599, 4688, 881, 551, 5466, 3375, 1623, 4081, 4247, 796, 3097, 3571, 1009, 167, 2747, 3950, 2814, 1888, 2447, 1357, 3749, 3775, 706, 1349, 999, 3916, 4323, 2242, 2146, 1897, 1307, 4572, 1055, 686, 2581, 2415, 4341, 2049, 6095, 4120, 2850, 3070, 2095, 1022, 5004, 4301, 3495, 2290, 1138, 1973, 4261, 5753, 1978, 2488, 1115, 4100, 205, 3670, 6072, 4778, 4001, 363, 2001, 3991, 430, 3005, 1368, 4602, 3252, 4980, 257, 2172, 1406, 278, 4662, 4900, 2797, 2075, 4025, 5067, 4455, 827, 5876, 5312, 3145, 1853, 5109, 4962, 3448, 3067, 4222, 3839, 5005, 1519, 132, 5984, 836, 3615, 5589, 2328, 1155, 3818, 3384, 4385, 1168, 3745, 1857, 4007, 4639, 3387, 4143, 4277, 6043, 2608, 1222, 1806, 6100, 4363, 2691, 461, 1909, 5796, 4009, 477, 1220, 1778, 676, 5422, 5023, 2056, 261, 468, 2597, 4218, 4885, 2697, 1158, 730, 4357, 5791, 1104, 2785, 4266, 5811, 2962, 1887, 584, 5376, 3242, 2106, 4004, 5423, 4963, 709, 1513, 4719, 2452, 1252, 4570, 5590, 1028, 4392, 2079, 2769, 14, 516, 822, 5809, 6098, 2555, 3253, 1526, 3641, 3740, 4066, 2262, 4845, 2841, 1361, 1337, 5834, 4292, 5414, 1508, 5669, 2698, 5321, 3717, 5792, 1530, 3084, 3013, 4925, 3959, 4349, 3548, 569, 4080, 3106, 4402, 3009, 1597, 732, 4638, 2865, 2800, 2647, 4739, 248, 1306, 2494, 5477, 667, 5072, 2668, 1130, 4460, 3193, 3470, 3170, 2847, 5363, 2979, 3177, 5103, 1799, 5773, 144, 1831, 1289, 5591, 52, 1680, 3334, 3153, 3173, 5078, 5558, 5658, 1594, 4159, 1822, 5254, 646, 4896, 175, 5582, 3107, 3178, 4457, 2799, 2378, 2863, 943, 40, 6131, 2569, 383, 4954, 3279, 2611, 883, 4475, 3761, 1608, 5070, 4415, 4554, 5035, 6024, 2122, 5961, 4780, 5445, 1336, 117, 3822, 5416, 2817, 4029, 6142, 4306, 2666, 2941, 3750, 993, 5013, 5473, 3435, 5442, 4869, 4445, 5790, 4681, 3981, 4077, 2393, 3658, 774, 6118, 3367, 394, 4072, 1274, 3061, 5069, 6090, 2126, 1677, 5127, 583, 21, 3993, 165, 5504, 4407, 1911, 3027, 5055, 1913, 1000, 2030, 338, 5276, 1402, 152, 1424, 2154, 2355, 2792, 3337, 1036, 5292, 1292, 5507, 3523, 3136, 1226, 3050, 4692, 2282, 5866, 850, 3532, 4350, 5243, 5332, 3323, 2522, 3449, 3034, 508, 5451, 326, 4724, 1296, 4843, 842, 5623, 5643, 292, 5918, 3040, 4807, 426, 1996, 1930, 5283, 767, 5024, 3191, 2418, 211, 5322, 5121, 5065, 909, 526, 2305, 3126, 485, 3054, 2804, 5146, 5381, 5956, 3643, 3474, 1011, 378, 5180, 509, 2860, 2645, 3594, 5308, 2336, 4848, 728, 1181, 4682, 4882, 2317, 4052, 511, 221, 5368, 2675, 3653, 5966, 3294, 2037, 4190, 3480, 3802, 2320, 5671, 2427, 2677, 1095, 4073, 799, 3403, 1936, 5852, 1644, 4704, 5993, 2398, 3873, 45, 3502, 1363, 3998, 2538, 3563, 4637, 1854, 3454, 5929, 2051, 3227, 2345, 5625, 3853, 2694, 4544, 5239, 3361, 976, 1706, 3488, 6133, 1294, 3114, 327, 4640, 1636, 5314, 4551, 2978, 5976, 5188, 660, 5316, 555, 716, 2334, 878, 6044, 3041, 6037, 480, 3860, 1789, 5096, 5169, 1401, 443, 3062, 5256, 1086, 581, 1403, 153, 5776, 5772, 46, 3324, 3680, 4958, 2778, 4924, 6025, 3086, 4142, 1520, 4418, 3347, 3621, 3229, 4027, 2751, 3727, 134, 1453, 873, 2112, 4974, 997, 1903, 5205, 3450, 5629, 194, 1515, 2762, 5130, 3801, 1144, 1096, 5836, 159, 547, 1256, 3069, 3830, 2183, 3464, 3275, 5861, 4203, 2682, 450, 4308, 1183, 2714, 3244, 2989, 2702, 1642, 3175, 5151, 544, 505, 2331, 4653, 3388, 2245, 409, 3657, 1280, 860, 1787, 2173, 3533, 1649, 1194, 4141, 2506, 225, 3108, 4217, 1499, 5568, 3197, 2918, 5806, 4102, 1610, 1459, 3540, 5502, 5936, 1998, 5647, 3736, 5398, 5579, 5419, 4987, 5851, 1042, 1153, 2615, 1904, 2222, 2703, 41, 5190, 2625, 1451, 601, 5635, 5661, 122, 273, 1545, 5922, 888, 1712, 1968, 1112, 3649, 4265, 180, 954, 4151, 1073, 5973, 3936, 3941, 530, 3251, 2859, 2914, 1525, 634, 5401, 5051, 5452, 5409, 4556, 2983, 2005, 5108, 5406, 4298, 3469, 1231, 2885, 446, 776, 3033, 2277, 755, 2824, 1191, 3183, 5393, 3720]}
SAVED model.heads_thresh 0.9
CONFIG MODE:  supervised

 ********** Supervised learning of blocks [4] **********
SAVING FOLDER FOR SUP:  STL10_CIFAR10_CL158728fd-dcb8-497d-8bd4-d5ba2c749836
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
INDICES:  50000
IMAGE SIZE: torch.Size([64, 3, 96, 96])
num_blocks:  5
Epoch: [1/50]	lr: 1.00e-03	time: 00:16:44	Loss_train 7.09177	Acc_train 50.63	/	Loss_test 0.41713	Acc_test 52.89
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL158728fd-dcb8-497d-8bd4-d5ba2c749836/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [42, 59, 25, 30, 14, 70, 35, 69, 68, 93, 53, 77, 55, 21, 5], 'conv1': [23, 382, 345, 191, 75, 104, 365, 190, 144, 99, 100, 264, 56, 324, 158, 92, 180, 152, 150, 296, 133, 140, 121, 163, 105, 146, 88, 349, 94, 179, 33, 36, 41, 201, 202, 159, 26, 13, 39, 354, 336, 373, 219, 335, 254, 374, 206, 344, 76, 186, 50, 274, 30, 314, 166, 70, 73, 225, 247], 'conv2': [445, 875, 1317, 235, 1279, 859, 1307, 119, 187, 662, 784, 1243, 760, 711, 276, 735, 335, 427, 80, 1395, 647, 454, 915, 1086, 356, 247, 1441, 590, 106, 167, 141, 1011, 409, 41, 786, 423, 726, 1464, 914, 817, 1494, 613, 68, 827, 1195, 1184, 835, 801, 519, 1412, 812, 943, 1400, 896, 849, 746, 764, 484, 1126, 1247, 599, 295, 752, 1230, 341, 869, 868, 935, 313, 1532, 1233, 857, 1375, 227, 447, 1297, 1006, 1132, 1271, 782, 22, 731, 425, 472, 1042, 350, 891, 144, 1071, 612, 1474, 1186, 159, 964, 381, 959, 450, 408, 683, 70, 1436, 651, 1113, 1486, 426, 1100, 667, 906, 958, 1012, 456, 1069, 1460, 822, 449, 266, 617, 1269, 121, 1429, 1282, 1530, 1357, 1220, 25, 1320, 461, 474, 917, 953, 937, 243, 971, 441, 635, 644, 614, 854, 1372, 304, 1053, 353, 1032, 79, 289, 305, 1465, 839, 601, 244, 524, 1360, 1471, 1266, 1210, 180, 268, 619, 700, 405, 532, 225, 209, 993, 252, 1358, 440, 229, 28, 207, 545, 346, 326, 421, 501, 847, 1134, 846, 241, 142, 340, 1116, 1046, 214, 640, 1361, 537, 161, 380, 366, 1522, 1211, 1481, 1000, 557, 673, 1454, 627, 430, 1252, 865, 900, 5, 58, 130, 414, 135, 1407, 577, 976, 1500, 942, 294, 981, 78, 1442, 781, 195, 314, 636, 818, 264, 1083, 1110, 204, 1489, 579, 660, 1238, 246, 838], 'conv3': [4076, 2572, 1544, 944, 1163, 5694, 6119, 5015, 4144, 2067, 4382, 1894, 780, 3980, 3710, 5338, 863, 287, 1439, 2101, 425, 5027, 4431, 2323, 534, 2736, 2070, 3165, 309, 5799, 3559, 2598, 1564, 2460, 3947, 3984, 4394, 4244, 5201, 43, 6088, 1641, 26, 178, 775, 2576, 4299, 2607, 5143, 3827, 3399, 5871, 2835, 2752, 5396, 5890, 4821, 3577, 3783, 4312, 1097, 4876, 5910, 3035, 1312, 3828, 2613, 911, 3383, 3187, 5054, 4537, 2213, 1083, 4651, 4817, 3734, 371, 1845, 1257, 2000, 1949, 3897, 2976, 515, 3986, 6094, 2335, 2244, 5631, 5286, 2491, 3348, 4467, 4992, 1877, 514, 4699, 2559, 1428, 5150, 5277, 3561, 4814, 5003, 750, 5222, 5272, 4979, 1960, 2342, 3181, 1753, 894, 2986, 4614, 1653, 8, 5491, 2138, 4906, 1422, 1950, 5908, 5645, 4791, 2966, 465, 2426, 2315, 1532, 5448, 2185, 4824, 1575, 3964, 1080, 1385, 1873, 2891, 5542, 2471, 985, 2679, 1631, 95, 5846, 2221, 4598, 1730, 2152, 3221, 1685, 294, 632, 2656, 3036, 4339, 61, 5048, 5698, 4097, 3226, 5318, 2261, 4186, 2917, 3429, 1988, 1346, 2089, 2515, 4395, 1749, 493, 5421, 948, 4451, 4899, 1501, 527, 3420, 5014, 3722, 5642, 63, 4227, 2013, 3466, 5349, 3662, 4044, 4908, 2693, 4702, 2950, 3362, 1335, 1438, 1709, 2270, 4110, 4482, 4476, 2069, 4134, 4376, 2026, 4502, 6125, 4163, 1622, 4118, 184, 1938, 825, 3788, 5493, 5580, 3733, 2019, 4884, 1539, 5212, 2787, 2975, 3938, 859, 4034, 4819, 2246, 4269, 2580, 1390, 4606, 2367, 2470, 3537, 4135, 1615, 2083, 1696, 2281, 415, 2585, 5407, 4566, 5839, 6032, 5124, 6021, 451, 5626, 3852, 3698, 4735, 679, 2248, 806, 2533, 1798, 5651, 105, 4181, 4358, 1833, 3718, 2996, 3606, 6084, 4352, 5795, 6070, 2420, 6041, 4000, 361, 1037, 5752, 1131, 5000, 471, 599, 4688, 881, 551, 5466, 3375, 1623, 4081, 4247, 796, 3097, 3571, 1009, 167, 2747, 3950, 2814, 1888, 2447, 1357, 3749, 3775, 706, 1349, 999, 3916, 4323, 2242, 2146, 1897, 1307, 4572, 1055, 686, 2581, 2415, 4341, 2049, 6095, 4120, 2850, 3070, 2095, 1022, 5004, 4301, 3495, 2290, 1138, 1973, 4261, 5753, 1978, 2488, 1115, 4100, 205, 3670, 6072, 4778, 4001, 363, 2001, 3991, 430, 3005, 1368, 4602, 3252, 4980, 257, 2172, 1406, 278, 4662, 4900, 2797, 2075, 4025, 5067, 4455, 827, 5876, 5312, 3145, 1853, 5109, 4962, 3448, 3067, 4222, 3839, 5005, 1519, 132, 5984, 836, 3615, 5589, 2328, 1155, 3818, 3384, 4385, 1168, 3745, 1857, 4007, 4639, 3387, 4143, 4277, 6043, 2608, 1222, 1806, 6100, 4363, 2691, 461, 1909, 5796, 4009, 477, 1220, 1778, 676, 5422, 5023, 2056, 261, 468, 2597, 4218, 4885, 2697, 1158, 730, 4357, 5791, 1104, 2785, 4266, 5811, 2962, 1887, 584, 5376, 3242, 2106, 4004, 5423, 4963, 709, 1513, 4719, 2452, 1252, 4570, 5590, 1028, 4392, 2079, 2769, 14, 516, 822, 5809, 6098, 2555, 3253, 1526, 3641, 3740, 4066, 2262, 4845, 2841, 1361, 1337, 5834, 4292, 5414, 1508, 5669, 2698, 5321, 3717, 5792, 1530, 3084, 3013, 4925, 3959, 4349, 3548, 569, 4080, 3106, 4402, 3009, 1597, 732, 4638, 2865, 2800, 2647, 4739, 248, 1306, 2494, 5477, 667, 5072, 2668, 1130, 4460, 3193, 3470, 3170, 2847, 5363, 2979, 3177, 5103, 1799, 5773, 144, 1831, 1289, 5591, 52, 1680, 3334, 3153, 3173, 5078, 5558, 5658, 1594, 4159, 1822, 5254, 646, 4896, 175, 5582, 3107, 3178, 4457, 2799, 2378, 2863, 943, 40, 6131, 2569, 383, 4954, 3279, 2611, 883, 4475, 3761, 1608, 5070, 4415, 4554, 5035, 6024, 2122, 5961, 4780, 5445, 1336, 117, 3822, 5416, 2817, 4029, 6142, 4306, 2666, 2941, 3750, 993, 5013, 5473, 3435, 5442, 4869, 4445, 5790, 4681, 3981, 4077, 2393, 3658, 774, 6118, 3367, 394, 4072, 1274, 3061, 5069, 6090, 2126, 1677, 5127, 583, 21, 3993, 165, 5504, 4407, 1911, 3027, 5055, 1913, 1000, 2030, 338, 5276, 1402, 152, 1424, 2154, 2355, 2792, 3337, 1036, 5292, 1292, 5507, 3523, 3136, 1226, 3050, 4692, 2282, 5866, 850, 3532, 4350, 5243, 5332, 3323, 2522, 3449, 3034, 508, 5451, 326, 4724, 1296, 4843, 842, 5623, 5643, 292, 5918, 3040, 4807, 426, 1996, 1930, 5283, 767, 5024, 3191, 2418, 211, 5322, 5121, 5065, 909, 526, 2305, 3126, 485, 3054, 2804, 5146, 5381, 5956, 3643, 3474, 1011, 378, 5180, 509, 2860, 2645, 3594, 5308, 2336, 4848, 728, 1181, 4682, 4882, 2317, 4052, 511, 221, 5368, 2675, 3653, 5966, 3294, 2037, 4190, 3480, 3802, 2320, 5671, 2427, 2677, 1095, 4073, 799, 3403, 1936, 5852, 1644, 4704, 5993, 2398, 3873, 45, 3502, 1363, 3998, 2538, 3563, 4637, 1854, 3454, 5929, 2051, 3227, 2345, 5625, 3853, 2694, 4544, 5239, 3361, 976, 1706, 3488, 6133, 1294, 3114, 327, 4640, 1636, 5314, 4551, 2978, 5976, 5188, 660, 5316, 555, 716, 2334, 878, 6044, 3041, 6037, 480, 3860, 1789, 5096, 5169, 1401, 443, 3062, 5256, 1086, 581, 1403, 153, 5776, 5772, 46, 3324, 3680, 4958, 2778, 4924, 6025, 3086, 4142, 1520, 4418, 3347, 3621, 3229, 4027, 2751, 3727, 134, 1453, 873, 2112, 4974, 997, 1903, 5205, 3450, 5629, 194, 1515, 2762, 5130, 3801, 1144, 1096, 5836, 159, 547, 1256, 3069, 3830, 2183, 3464, 3275, 5861, 4203, 2682, 450, 4308, 1183, 2714, 3244, 2989, 2702, 1642, 3175, 5151, 544, 505, 2331, 4653, 3388, 2245, 409, 3657, 1280, 860, 1787, 2173, 3533, 1649, 1194, 4141, 2506, 225, 3108, 4217, 1499, 5568, 3197, 2918, 5806, 4102, 1610, 1459, 3540, 5502, 5936, 1998, 5647, 3736, 5398, 5579, 5419, 4987, 5851, 1042, 1153, 2615, 1904, 2222, 2703, 41, 5190, 2625, 1451, 601, 5635, 5661, 122, 273, 1545, 5922, 888, 1712, 1968, 1112, 3649, 4265, 180, 954, 4151, 1073, 5973, 3936, 3941, 530, 3251, 2859, 2914, 1525, 634, 5401, 5051, 5452, 5409, 4556, 2983, 2005, 5108, 5406, 4298, 3469, 1231, 2885, 446, 776, 3033, 2277, 755, 2824, 1191, 3183, 5393, 3720]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [10/50]	lr: 1.00e-03	time: 00:20:29	Loss_train 4.23107	Acc_train 68.90	/	Loss_test 0.43428	Acc_test 64.87
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL158728fd-dcb8-497d-8bd4-d5ba2c749836/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [42, 59, 25, 30, 14, 70, 35, 69, 68, 93, 53, 77, 55, 21, 5], 'conv1': [23, 382, 345, 191, 75, 104, 365, 190, 144, 99, 100, 264, 56, 324, 158, 92, 180, 152, 150, 296, 133, 140, 121, 163, 105, 146, 88, 349, 94, 179, 33, 36, 41, 201, 202, 159, 26, 13, 39, 354, 336, 373, 219, 335, 254, 374, 206, 344, 76, 186, 50, 274, 30, 314, 166, 70, 73, 225, 247], 'conv2': [445, 875, 1317, 235, 1279, 859, 1307, 119, 187, 662, 784, 1243, 760, 711, 276, 735, 335, 427, 80, 1395, 647, 454, 915, 1086, 356, 247, 1441, 590, 106, 167, 141, 1011, 409, 41, 786, 423, 726, 1464, 914, 817, 1494, 613, 68, 827, 1195, 1184, 835, 801, 519, 1412, 812, 943, 1400, 896, 849, 746, 764, 484, 1126, 1247, 599, 295, 752, 1230, 341, 869, 868, 935, 313, 1532, 1233, 857, 1375, 227, 447, 1297, 1006, 1132, 1271, 782, 22, 731, 425, 472, 1042, 350, 891, 144, 1071, 612, 1474, 1186, 159, 964, 381, 959, 450, 408, 683, 70, 1436, 651, 1113, 1486, 426, 1100, 667, 906, 958, 1012, 456, 1069, 1460, 822, 449, 266, 617, 1269, 121, 1429, 1282, 1530, 1357, 1220, 25, 1320, 461, 474, 917, 953, 937, 243, 971, 441, 635, 644, 614, 854, 1372, 304, 1053, 353, 1032, 79, 289, 305, 1465, 839, 601, 244, 524, 1360, 1471, 1266, 1210, 180, 268, 619, 700, 405, 532, 225, 209, 993, 252, 1358, 440, 229, 28, 207, 545, 346, 326, 421, 501, 847, 1134, 846, 241, 142, 340, 1116, 1046, 214, 640, 1361, 537, 161, 380, 366, 1522, 1211, 1481, 1000, 557, 673, 1454, 627, 430, 1252, 865, 900, 5, 58, 130, 414, 135, 1407, 577, 976, 1500, 942, 294, 981, 78, 1442, 781, 195, 314, 636, 818, 264, 1083, 1110, 204, 1489, 579, 660, 1238, 246, 838], 'conv3': [4076, 2572, 1544, 944, 1163, 5694, 6119, 5015, 4144, 2067, 4382, 1894, 780, 3980, 3710, 5338, 863, 287, 1439, 2101, 425, 5027, 4431, 2323, 534, 2736, 2070, 3165, 309, 5799, 3559, 2598, 1564, 2460, 3947, 3984, 4394, 4244, 5201, 43, 6088, 1641, 26, 178, 775, 2576, 4299, 2607, 5143, 3827, 3399, 5871, 2835, 2752, 5396, 5890, 4821, 3577, 3783, 4312, 1097, 4876, 5910, 3035, 1312, 3828, 2613, 911, 3383, 3187, 5054, 4537, 2213, 1083, 4651, 4817, 3734, 371, 1845, 1257, 2000, 1949, 3897, 2976, 515, 3986, 6094, 2335, 2244, 5631, 5286, 2491, 3348, 4467, 4992, 1877, 514, 4699, 2559, 1428, 5150, 5277, 3561, 4814, 5003, 750, 5222, 5272, 4979, 1960, 2342, 3181, 1753, 894, 2986, 4614, 1653, 8, 5491, 2138, 4906, 1422, 1950, 5908, 5645, 4791, 2966, 465, 2426, 2315, 1532, 5448, 2185, 4824, 1575, 3964, 1080, 1385, 1873, 2891, 5542, 2471, 985, 2679, 1631, 95, 5846, 2221, 4598, 1730, 2152, 3221, 1685, 294, 632, 2656, 3036, 4339, 61, 5048, 5698, 4097, 3226, 5318, 2261, 4186, 2917, 3429, 1988, 1346, 2089, 2515, 4395, 1749, 493, 5421, 948, 4451, 4899, 1501, 527, 3420, 5014, 3722, 5642, 63, 4227, 2013, 3466, 5349, 3662, 4044, 4908, 2693, 4702, 2950, 3362, 1335, 1438, 1709, 2270, 4110, 4482, 4476, 2069, 4134, 4376, 2026, 4502, 6125, 4163, 1622, 4118, 184, 1938, 825, 3788, 5493, 5580, 3733, 2019, 4884, 1539, 5212, 2787, 2975, 3938, 859, 4034, 4819, 2246, 4269, 2580, 1390, 4606, 2367, 2470, 3537, 4135, 1615, 2083, 1696, 2281, 415, 2585, 5407, 4566, 5839, 6032, 5124, 6021, 451, 5626, 3852, 3698, 4735, 679, 2248, 806, 2533, 1798, 5651, 105, 4181, 4358, 1833, 3718, 2996, 3606, 6084, 4352, 5795, 6070, 2420, 6041, 4000, 361, 1037, 5752, 1131, 5000, 471, 599, 4688, 881, 551, 5466, 3375, 1623, 4081, 4247, 796, 3097, 3571, 1009, 167, 2747, 3950, 2814, 1888, 2447, 1357, 3749, 3775, 706, 1349, 999, 3916, 4323, 2242, 2146, 1897, 1307, 4572, 1055, 686, 2581, 2415, 4341, 2049, 6095, 4120, 2850, 3070, 2095, 1022, 5004, 4301, 3495, 2290, 1138, 1973, 4261, 5753, 1978, 2488, 1115, 4100, 205, 3670, 6072, 4778, 4001, 363, 2001, 3991, 430, 3005, 1368, 4602, 3252, 4980, 257, 2172, 1406, 278, 4662, 4900, 2797, 2075, 4025, 5067, 4455, 827, 5876, 5312, 3145, 1853, 5109, 4962, 3448, 3067, 4222, 3839, 5005, 1519, 132, 5984, 836, 3615, 5589, 2328, 1155, 3818, 3384, 4385, 1168, 3745, 1857, 4007, 4639, 3387, 4143, 4277, 6043, 2608, 1222, 1806, 6100, 4363, 2691, 461, 1909, 5796, 4009, 477, 1220, 1778, 676, 5422, 5023, 2056, 261, 468, 2597, 4218, 4885, 2697, 1158, 730, 4357, 5791, 1104, 2785, 4266, 5811, 2962, 1887, 584, 5376, 3242, 2106, 4004, 5423, 4963, 709, 1513, 4719, 2452, 1252, 4570, 5590, 1028, 4392, 2079, 2769, 14, 516, 822, 5809, 6098, 2555, 3253, 1526, 3641, 3740, 4066, 2262, 4845, 2841, 1361, 1337, 5834, 4292, 5414, 1508, 5669, 2698, 5321, 3717, 5792, 1530, 3084, 3013, 4925, 3959, 4349, 3548, 569, 4080, 3106, 4402, 3009, 1597, 732, 4638, 2865, 2800, 2647, 4739, 248, 1306, 2494, 5477, 667, 5072, 2668, 1130, 4460, 3193, 3470, 3170, 2847, 5363, 2979, 3177, 5103, 1799, 5773, 144, 1831, 1289, 5591, 52, 1680, 3334, 3153, 3173, 5078, 5558, 5658, 1594, 4159, 1822, 5254, 646, 4896, 175, 5582, 3107, 3178, 4457, 2799, 2378, 2863, 943, 40, 6131, 2569, 383, 4954, 3279, 2611, 883, 4475, 3761, 1608, 5070, 4415, 4554, 5035, 6024, 2122, 5961, 4780, 5445, 1336, 117, 3822, 5416, 2817, 4029, 6142, 4306, 2666, 2941, 3750, 993, 5013, 5473, 3435, 5442, 4869, 4445, 5790, 4681, 3981, 4077, 2393, 3658, 774, 6118, 3367, 394, 4072, 1274, 3061, 5069, 6090, 2126, 1677, 5127, 583, 21, 3993, 165, 5504, 4407, 1911, 3027, 5055, 1913, 1000, 2030, 338, 5276, 1402, 152, 1424, 2154, 2355, 2792, 3337, 1036, 5292, 1292, 5507, 3523, 3136, 1226, 3050, 4692, 2282, 5866, 850, 3532, 4350, 5243, 5332, 3323, 2522, 3449, 3034, 508, 5451, 326, 4724, 1296, 4843, 842, 5623, 5643, 292, 5918, 3040, 4807, 426, 1996, 1930, 5283, 767, 5024, 3191, 2418, 211, 5322, 5121, 5065, 909, 526, 2305, 3126, 485, 3054, 2804, 5146, 5381, 5956, 3643, 3474, 1011, 378, 5180, 509, 2860, 2645, 3594, 5308, 2336, 4848, 728, 1181, 4682, 4882, 2317, 4052, 511, 221, 5368, 2675, 3653, 5966, 3294, 2037, 4190, 3480, 3802, 2320, 5671, 2427, 2677, 1095, 4073, 799, 3403, 1936, 5852, 1644, 4704, 5993, 2398, 3873, 45, 3502, 1363, 3998, 2538, 3563, 4637, 1854, 3454, 5929, 2051, 3227, 2345, 5625, 3853, 2694, 4544, 5239, 3361, 976, 1706, 3488, 6133, 1294, 3114, 327, 4640, 1636, 5314, 4551, 2978, 5976, 5188, 660, 5316, 555, 716, 2334, 878, 6044, 3041, 6037, 480, 3860, 1789, 5096, 5169, 1401, 443, 3062, 5256, 1086, 581, 1403, 153, 5776, 5772, 46, 3324, 3680, 4958, 2778, 4924, 6025, 3086, 4142, 1520, 4418, 3347, 3621, 3229, 4027, 2751, 3727, 134, 1453, 873, 2112, 4974, 997, 1903, 5205, 3450, 5629, 194, 1515, 2762, 5130, 3801, 1144, 1096, 5836, 159, 547, 1256, 3069, 3830, 2183, 3464, 3275, 5861, 4203, 2682, 450, 4308, 1183, 2714, 3244, 2989, 2702, 1642, 3175, 5151, 544, 505, 2331, 4653, 3388, 2245, 409, 3657, 1280, 860, 1787, 2173, 3533, 1649, 1194, 4141, 2506, 225, 3108, 4217, 1499, 5568, 3197, 2918, 5806, 4102, 1610, 1459, 3540, 5502, 5936, 1998, 5647, 3736, 5398, 5579, 5419, 4987, 5851, 1042, 1153, 2615, 1904, 2222, 2703, 41, 5190, 2625, 1451, 601, 5635, 5661, 122, 273, 1545, 5922, 888, 1712, 1968, 1112, 3649, 4265, 180, 954, 4151, 1073, 5973, 3936, 3941, 530, 3251, 2859, 2914, 1525, 634, 5401, 5051, 5452, 5409, 4556, 2983, 2005, 5108, 5406, 4298, 3469, 1231, 2885, 446, 776, 3033, 2277, 755, 2824, 1191, 3183, 5393, 3720]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [20/50]	lr: 2.50e-04	time: 00:24:39	Loss_train 1.92844	Acc_train 81.99	/	Loss_test 0.24184	Acc_test 72.88
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL158728fd-dcb8-497d-8bd4-d5ba2c749836/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [42, 59, 25, 30, 14, 70, 35, 69, 68, 93, 53, 77, 55, 21, 5], 'conv1': [23, 382, 345, 191, 75, 104, 365, 190, 144, 99, 100, 264, 56, 324, 158, 92, 180, 152, 150, 296, 133, 140, 121, 163, 105, 146, 88, 349, 94, 179, 33, 36, 41, 201, 202, 159, 26, 13, 39, 354, 336, 373, 219, 335, 254, 374, 206, 344, 76, 186, 50, 274, 30, 314, 166, 70, 73, 225, 247], 'conv2': [445, 875, 1317, 235, 1279, 859, 1307, 119, 187, 662, 784, 1243, 760, 711, 276, 735, 335, 427, 80, 1395, 647, 454, 915, 1086, 356, 247, 1441, 590, 106, 167, 141, 1011, 409, 41, 786, 423, 726, 1464, 914, 817, 1494, 613, 68, 827, 1195, 1184, 835, 801, 519, 1412, 812, 943, 1400, 896, 849, 746, 764, 484, 1126, 1247, 599, 295, 752, 1230, 341, 869, 868, 935, 313, 1532, 1233, 857, 1375, 227, 447, 1297, 1006, 1132, 1271, 782, 22, 731, 425, 472, 1042, 350, 891, 144, 1071, 612, 1474, 1186, 159, 964, 381, 959, 450, 408, 683, 70, 1436, 651, 1113, 1486, 426, 1100, 667, 906, 958, 1012, 456, 1069, 1460, 822, 449, 266, 617, 1269, 121, 1429, 1282, 1530, 1357, 1220, 25, 1320, 461, 474, 917, 953, 937, 243, 971, 441, 635, 644, 614, 854, 1372, 304, 1053, 353, 1032, 79, 289, 305, 1465, 839, 601, 244, 524, 1360, 1471, 1266, 1210, 180, 268, 619, 700, 405, 532, 225, 209, 993, 252, 1358, 440, 229, 28, 207, 545, 346, 326, 421, 501, 847, 1134, 846, 241, 142, 340, 1116, 1046, 214, 640, 1361, 537, 161, 380, 366, 1522, 1211, 1481, 1000, 557, 673, 1454, 627, 430, 1252, 865, 900, 5, 58, 130, 414, 135, 1407, 577, 976, 1500, 942, 294, 981, 78, 1442, 781, 195, 314, 636, 818, 264, 1083, 1110, 204, 1489, 579, 660, 1238, 246, 838], 'conv3': [4076, 2572, 1544, 944, 1163, 5694, 6119, 5015, 4144, 2067, 4382, 1894, 780, 3980, 3710, 5338, 863, 287, 1439, 2101, 425, 5027, 4431, 2323, 534, 2736, 2070, 3165, 309, 5799, 3559, 2598, 1564, 2460, 3947, 3984, 4394, 4244, 5201, 43, 6088, 1641, 26, 178, 775, 2576, 4299, 2607, 5143, 3827, 3399, 5871, 2835, 2752, 5396, 5890, 4821, 3577, 3783, 4312, 1097, 4876, 5910, 3035, 1312, 3828, 2613, 911, 3383, 3187, 5054, 4537, 2213, 1083, 4651, 4817, 3734, 371, 1845, 1257, 2000, 1949, 3897, 2976, 515, 3986, 6094, 2335, 2244, 5631, 5286, 2491, 3348, 4467, 4992, 1877, 514, 4699, 2559, 1428, 5150, 5277, 3561, 4814, 5003, 750, 5222, 5272, 4979, 1960, 2342, 3181, 1753, 894, 2986, 4614, 1653, 8, 5491, 2138, 4906, 1422, 1950, 5908, 5645, 4791, 2966, 465, 2426, 2315, 1532, 5448, 2185, 4824, 1575, 3964, 1080, 1385, 1873, 2891, 5542, 2471, 985, 2679, 1631, 95, 5846, 2221, 4598, 1730, 2152, 3221, 1685, 294, 632, 2656, 3036, 4339, 61, 5048, 5698, 4097, 3226, 5318, 2261, 4186, 2917, 3429, 1988, 1346, 2089, 2515, 4395, 1749, 493, 5421, 948, 4451, 4899, 1501, 527, 3420, 5014, 3722, 5642, 63, 4227, 2013, 3466, 5349, 3662, 4044, 4908, 2693, 4702, 2950, 3362, 1335, 1438, 1709, 2270, 4110, 4482, 4476, 2069, 4134, 4376, 2026, 4502, 6125, 4163, 1622, 4118, 184, 1938, 825, 3788, 5493, 5580, 3733, 2019, 4884, 1539, 5212, 2787, 2975, 3938, 859, 4034, 4819, 2246, 4269, 2580, 1390, 4606, 2367, 2470, 3537, 4135, 1615, 2083, 1696, 2281, 415, 2585, 5407, 4566, 5839, 6032, 5124, 6021, 451, 5626, 3852, 3698, 4735, 679, 2248, 806, 2533, 1798, 5651, 105, 4181, 4358, 1833, 3718, 2996, 3606, 6084, 4352, 5795, 6070, 2420, 6041, 4000, 361, 1037, 5752, 1131, 5000, 471, 599, 4688, 881, 551, 5466, 3375, 1623, 4081, 4247, 796, 3097, 3571, 1009, 167, 2747, 3950, 2814, 1888, 2447, 1357, 3749, 3775, 706, 1349, 999, 3916, 4323, 2242, 2146, 1897, 1307, 4572, 1055, 686, 2581, 2415, 4341, 2049, 6095, 4120, 2850, 3070, 2095, 1022, 5004, 4301, 3495, 2290, 1138, 1973, 4261, 5753, 1978, 2488, 1115, 4100, 205, 3670, 6072, 4778, 4001, 363, 2001, 3991, 430, 3005, 1368, 4602, 3252, 4980, 257, 2172, 1406, 278, 4662, 4900, 2797, 2075, 4025, 5067, 4455, 827, 5876, 5312, 3145, 1853, 5109, 4962, 3448, 3067, 4222, 3839, 5005, 1519, 132, 5984, 836, 3615, 5589, 2328, 1155, 3818, 3384, 4385, 1168, 3745, 1857, 4007, 4639, 3387, 4143, 4277, 6043, 2608, 1222, 1806, 6100, 4363, 2691, 461, 1909, 5796, 4009, 477, 1220, 1778, 676, 5422, 5023, 2056, 261, 468, 2597, 4218, 4885, 2697, 1158, 730, 4357, 5791, 1104, 2785, 4266, 5811, 2962, 1887, 584, 5376, 3242, 2106, 4004, 5423, 4963, 709, 1513, 4719, 2452, 1252, 4570, 5590, 1028, 4392, 2079, 2769, 14, 516, 822, 5809, 6098, 2555, 3253, 1526, 3641, 3740, 4066, 2262, 4845, 2841, 1361, 1337, 5834, 4292, 5414, 1508, 5669, 2698, 5321, 3717, 5792, 1530, 3084, 3013, 4925, 3959, 4349, 3548, 569, 4080, 3106, 4402, 3009, 1597, 732, 4638, 2865, 2800, 2647, 4739, 248, 1306, 2494, 5477, 667, 5072, 2668, 1130, 4460, 3193, 3470, 3170, 2847, 5363, 2979, 3177, 5103, 1799, 5773, 144, 1831, 1289, 5591, 52, 1680, 3334, 3153, 3173, 5078, 5558, 5658, 1594, 4159, 1822, 5254, 646, 4896, 175, 5582, 3107, 3178, 4457, 2799, 2378, 2863, 943, 40, 6131, 2569, 383, 4954, 3279, 2611, 883, 4475, 3761, 1608, 5070, 4415, 4554, 5035, 6024, 2122, 5961, 4780, 5445, 1336, 117, 3822, 5416, 2817, 4029, 6142, 4306, 2666, 2941, 3750, 993, 5013, 5473, 3435, 5442, 4869, 4445, 5790, 4681, 3981, 4077, 2393, 3658, 774, 6118, 3367, 394, 4072, 1274, 3061, 5069, 6090, 2126, 1677, 5127, 583, 21, 3993, 165, 5504, 4407, 1911, 3027, 5055, 1913, 1000, 2030, 338, 5276, 1402, 152, 1424, 2154, 2355, 2792, 3337, 1036, 5292, 1292, 5507, 3523, 3136, 1226, 3050, 4692, 2282, 5866, 850, 3532, 4350, 5243, 5332, 3323, 2522, 3449, 3034, 508, 5451, 326, 4724, 1296, 4843, 842, 5623, 5643, 292, 5918, 3040, 4807, 426, 1996, 1930, 5283, 767, 5024, 3191, 2418, 211, 5322, 5121, 5065, 909, 526, 2305, 3126, 485, 3054, 2804, 5146, 5381, 5956, 3643, 3474, 1011, 378, 5180, 509, 2860, 2645, 3594, 5308, 2336, 4848, 728, 1181, 4682, 4882, 2317, 4052, 511, 221, 5368, 2675, 3653, 5966, 3294, 2037, 4190, 3480, 3802, 2320, 5671, 2427, 2677, 1095, 4073, 799, 3403, 1936, 5852, 1644, 4704, 5993, 2398, 3873, 45, 3502, 1363, 3998, 2538, 3563, 4637, 1854, 3454, 5929, 2051, 3227, 2345, 5625, 3853, 2694, 4544, 5239, 3361, 976, 1706, 3488, 6133, 1294, 3114, 327, 4640, 1636, 5314, 4551, 2978, 5976, 5188, 660, 5316, 555, 716, 2334, 878, 6044, 3041, 6037, 480, 3860, 1789, 5096, 5169, 1401, 443, 3062, 5256, 1086, 581, 1403, 153, 5776, 5772, 46, 3324, 3680, 4958, 2778, 4924, 6025, 3086, 4142, 1520, 4418, 3347, 3621, 3229, 4027, 2751, 3727, 134, 1453, 873, 2112, 4974, 997, 1903, 5205, 3450, 5629, 194, 1515, 2762, 5130, 3801, 1144, 1096, 5836, 159, 547, 1256, 3069, 3830, 2183, 3464, 3275, 5861, 4203, 2682, 450, 4308, 1183, 2714, 3244, 2989, 2702, 1642, 3175, 5151, 544, 505, 2331, 4653, 3388, 2245, 409, 3657, 1280, 860, 1787, 2173, 3533, 1649, 1194, 4141, 2506, 225, 3108, 4217, 1499, 5568, 3197, 2918, 5806, 4102, 1610, 1459, 3540, 5502, 5936, 1998, 5647, 3736, 5398, 5579, 5419, 4987, 5851, 1042, 1153, 2615, 1904, 2222, 2703, 41, 5190, 2625, 1451, 601, 5635, 5661, 122, 273, 1545, 5922, 888, 1712, 1968, 1112, 3649, 4265, 180, 954, 4151, 1073, 5973, 3936, 3941, 530, 3251, 2859, 2914, 1525, 634, 5401, 5051, 5452, 5409, 4556, 2983, 2005, 5108, 5406, 4298, 3469, 1231, 2885, 446, 776, 3033, 2277, 755, 2824, 1191, 3183, 5393, 3720]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [30/50]	lr: 1.25e-04	time: 00:28:49	Loss_train 1.14601	Acc_train 86.61	/	Loss_test 0.22495	Acc_test 73.86
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL158728fd-dcb8-497d-8bd4-d5ba2c749836/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [42, 59, 25, 30, 14, 70, 35, 69, 68, 93, 53, 77, 55, 21, 5], 'conv1': [23, 382, 345, 191, 75, 104, 365, 190, 144, 99, 100, 264, 56, 324, 158, 92, 180, 152, 150, 296, 133, 140, 121, 163, 105, 146, 88, 349, 94, 179, 33, 36, 41, 201, 202, 159, 26, 13, 39, 354, 336, 373, 219, 335, 254, 374, 206, 344, 76, 186, 50, 274, 30, 314, 166, 70, 73, 225, 247], 'conv2': [445, 875, 1317, 235, 1279, 859, 1307, 119, 187, 662, 784, 1243, 760, 711, 276, 735, 335, 427, 80, 1395, 647, 454, 915, 1086, 356, 247, 1441, 590, 106, 167, 141, 1011, 409, 41, 786, 423, 726, 1464, 914, 817, 1494, 613, 68, 827, 1195, 1184, 835, 801, 519, 1412, 812, 943, 1400, 896, 849, 746, 764, 484, 1126, 1247, 599, 295, 752, 1230, 341, 869, 868, 935, 313, 1532, 1233, 857, 1375, 227, 447, 1297, 1006, 1132, 1271, 782, 22, 731, 425, 472, 1042, 350, 891, 144, 1071, 612, 1474, 1186, 159, 964, 381, 959, 450, 408, 683, 70, 1436, 651, 1113, 1486, 426, 1100, 667, 906, 958, 1012, 456, 1069, 1460, 822, 449, 266, 617, 1269, 121, 1429, 1282, 1530, 1357, 1220, 25, 1320, 461, 474, 917, 953, 937, 243, 971, 441, 635, 644, 614, 854, 1372, 304, 1053, 353, 1032, 79, 289, 305, 1465, 839, 601, 244, 524, 1360, 1471, 1266, 1210, 180, 268, 619, 700, 405, 532, 225, 209, 993, 252, 1358, 440, 229, 28, 207, 545, 346, 326, 421, 501, 847, 1134, 846, 241, 142, 340, 1116, 1046, 214, 640, 1361, 537, 161, 380, 366, 1522, 1211, 1481, 1000, 557, 673, 1454, 627, 430, 1252, 865, 900, 5, 58, 130, 414, 135, 1407, 577, 976, 1500, 942, 294, 981, 78, 1442, 781, 195, 314, 636, 818, 264, 1083, 1110, 204, 1489, 579, 660, 1238, 246, 838], 'conv3': [4076, 2572, 1544, 944, 1163, 5694, 6119, 5015, 4144, 2067, 4382, 1894, 780, 3980, 3710, 5338, 863, 287, 1439, 2101, 425, 5027, 4431, 2323, 534, 2736, 2070, 3165, 309, 5799, 3559, 2598, 1564, 2460, 3947, 3984, 4394, 4244, 5201, 43, 6088, 1641, 26, 178, 775, 2576, 4299, 2607, 5143, 3827, 3399, 5871, 2835, 2752, 5396, 5890, 4821, 3577, 3783, 4312, 1097, 4876, 5910, 3035, 1312, 3828, 2613, 911, 3383, 3187, 5054, 4537, 2213, 1083, 4651, 4817, 3734, 371, 1845, 1257, 2000, 1949, 3897, 2976, 515, 3986, 6094, 2335, 2244, 5631, 5286, 2491, 3348, 4467, 4992, 1877, 514, 4699, 2559, 1428, 5150, 5277, 3561, 4814, 5003, 750, 5222, 5272, 4979, 1960, 2342, 3181, 1753, 894, 2986, 4614, 1653, 8, 5491, 2138, 4906, 1422, 1950, 5908, 5645, 4791, 2966, 465, 2426, 2315, 1532, 5448, 2185, 4824, 1575, 3964, 1080, 1385, 1873, 2891, 5542, 2471, 985, 2679, 1631, 95, 5846, 2221, 4598, 1730, 2152, 3221, 1685, 294, 632, 2656, 3036, 4339, 61, 5048, 5698, 4097, 3226, 5318, 2261, 4186, 2917, 3429, 1988, 1346, 2089, 2515, 4395, 1749, 493, 5421, 948, 4451, 4899, 1501, 527, 3420, 5014, 3722, 5642, 63, 4227, 2013, 3466, 5349, 3662, 4044, 4908, 2693, 4702, 2950, 3362, 1335, 1438, 1709, 2270, 4110, 4482, 4476, 2069, 4134, 4376, 2026, 4502, 6125, 4163, 1622, 4118, 184, 1938, 825, 3788, 5493, 5580, 3733, 2019, 4884, 1539, 5212, 2787, 2975, 3938, 859, 4034, 4819, 2246, 4269, 2580, 1390, 4606, 2367, 2470, 3537, 4135, 1615, 2083, 1696, 2281, 415, 2585, 5407, 4566, 5839, 6032, 5124, 6021, 451, 5626, 3852, 3698, 4735, 679, 2248, 806, 2533, 1798, 5651, 105, 4181, 4358, 1833, 3718, 2996, 3606, 6084, 4352, 5795, 6070, 2420, 6041, 4000, 361, 1037, 5752, 1131, 5000, 471, 599, 4688, 881, 551, 5466, 3375, 1623, 4081, 4247, 796, 3097, 3571, 1009, 167, 2747, 3950, 2814, 1888, 2447, 1357, 3749, 3775, 706, 1349, 999, 3916, 4323, 2242, 2146, 1897, 1307, 4572, 1055, 686, 2581, 2415, 4341, 2049, 6095, 4120, 2850, 3070, 2095, 1022, 5004, 4301, 3495, 2290, 1138, 1973, 4261, 5753, 1978, 2488, 1115, 4100, 205, 3670, 6072, 4778, 4001, 363, 2001, 3991, 430, 3005, 1368, 4602, 3252, 4980, 257, 2172, 1406, 278, 4662, 4900, 2797, 2075, 4025, 5067, 4455, 827, 5876, 5312, 3145, 1853, 5109, 4962, 3448, 3067, 4222, 3839, 5005, 1519, 132, 5984, 836, 3615, 5589, 2328, 1155, 3818, 3384, 4385, 1168, 3745, 1857, 4007, 4639, 3387, 4143, 4277, 6043, 2608, 1222, 1806, 6100, 4363, 2691, 461, 1909, 5796, 4009, 477, 1220, 1778, 676, 5422, 5023, 2056, 261, 468, 2597, 4218, 4885, 2697, 1158, 730, 4357, 5791, 1104, 2785, 4266, 5811, 2962, 1887, 584, 5376, 3242, 2106, 4004, 5423, 4963, 709, 1513, 4719, 2452, 1252, 4570, 5590, 1028, 4392, 2079, 2769, 14, 516, 822, 5809, 6098, 2555, 3253, 1526, 3641, 3740, 4066, 2262, 4845, 2841, 1361, 1337, 5834, 4292, 5414, 1508, 5669, 2698, 5321, 3717, 5792, 1530, 3084, 3013, 4925, 3959, 4349, 3548, 569, 4080, 3106, 4402, 3009, 1597, 732, 4638, 2865, 2800, 2647, 4739, 248, 1306, 2494, 5477, 667, 5072, 2668, 1130, 4460, 3193, 3470, 3170, 2847, 5363, 2979, 3177, 5103, 1799, 5773, 144, 1831, 1289, 5591, 52, 1680, 3334, 3153, 3173, 5078, 5558, 5658, 1594, 4159, 1822, 5254, 646, 4896, 175, 5582, 3107, 3178, 4457, 2799, 2378, 2863, 943, 40, 6131, 2569, 383, 4954, 3279, 2611, 883, 4475, 3761, 1608, 5070, 4415, 4554, 5035, 6024, 2122, 5961, 4780, 5445, 1336, 117, 3822, 5416, 2817, 4029, 6142, 4306, 2666, 2941, 3750, 993, 5013, 5473, 3435, 5442, 4869, 4445, 5790, 4681, 3981, 4077, 2393, 3658, 774, 6118, 3367, 394, 4072, 1274, 3061, 5069, 6090, 2126, 1677, 5127, 583, 21, 3993, 165, 5504, 4407, 1911, 3027, 5055, 1913, 1000, 2030, 338, 5276, 1402, 152, 1424, 2154, 2355, 2792, 3337, 1036, 5292, 1292, 5507, 3523, 3136, 1226, 3050, 4692, 2282, 5866, 850, 3532, 4350, 5243, 5332, 3323, 2522, 3449, 3034, 508, 5451, 326, 4724, 1296, 4843, 842, 5623, 5643, 292, 5918, 3040, 4807, 426, 1996, 1930, 5283, 767, 5024, 3191, 2418, 211, 5322, 5121, 5065, 909, 526, 2305, 3126, 485, 3054, 2804, 5146, 5381, 5956, 3643, 3474, 1011, 378, 5180, 509, 2860, 2645, 3594, 5308, 2336, 4848, 728, 1181, 4682, 4882, 2317, 4052, 511, 221, 5368, 2675, 3653, 5966, 3294, 2037, 4190, 3480, 3802, 2320, 5671, 2427, 2677, 1095, 4073, 799, 3403, 1936, 5852, 1644, 4704, 5993, 2398, 3873, 45, 3502, 1363, 3998, 2538, 3563, 4637, 1854, 3454, 5929, 2051, 3227, 2345, 5625, 3853, 2694, 4544, 5239, 3361, 976, 1706, 3488, 6133, 1294, 3114, 327, 4640, 1636, 5314, 4551, 2978, 5976, 5188, 660, 5316, 555, 716, 2334, 878, 6044, 3041, 6037, 480, 3860, 1789, 5096, 5169, 1401, 443, 3062, 5256, 1086, 581, 1403, 153, 5776, 5772, 46, 3324, 3680, 4958, 2778, 4924, 6025, 3086, 4142, 1520, 4418, 3347, 3621, 3229, 4027, 2751, 3727, 134, 1453, 873, 2112, 4974, 997, 1903, 5205, 3450, 5629, 194, 1515, 2762, 5130, 3801, 1144, 1096, 5836, 159, 547, 1256, 3069, 3830, 2183, 3464, 3275, 5861, 4203, 2682, 450, 4308, 1183, 2714, 3244, 2989, 2702, 1642, 3175, 5151, 544, 505, 2331, 4653, 3388, 2245, 409, 3657, 1280, 860, 1787, 2173, 3533, 1649, 1194, 4141, 2506, 225, 3108, 4217, 1499, 5568, 3197, 2918, 5806, 4102, 1610, 1459, 3540, 5502, 5936, 1998, 5647, 3736, 5398, 5579, 5419, 4987, 5851, 1042, 1153, 2615, 1904, 2222, 2703, 41, 5190, 2625, 1451, 601, 5635, 5661, 122, 273, 1545, 5922, 888, 1712, 1968, 1112, 3649, 4265, 180, 954, 4151, 1073, 5973, 3936, 3941, 530, 3251, 2859, 2914, 1525, 634, 5401, 5051, 5452, 5409, 4556, 2983, 2005, 5108, 5406, 4298, 3469, 1231, 2885, 446, 776, 3033, 2277, 755, 2824, 1191, 3183, 5393, 3720]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [40/50]	lr: 3.13e-05	time: 00:32:59	Loss_train 0.80884	Acc_train 89.11	/	Loss_test 0.21880	Acc_test 74.18
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL158728fd-dcb8-497d-8bd4-d5ba2c749836/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [42, 59, 25, 30, 14, 70, 35, 69, 68, 93, 53, 77, 55, 21, 5], 'conv1': [23, 382, 345, 191, 75, 104, 365, 190, 144, 99, 100, 264, 56, 324, 158, 92, 180, 152, 150, 296, 133, 140, 121, 163, 105, 146, 88, 349, 94, 179, 33, 36, 41, 201, 202, 159, 26, 13, 39, 354, 336, 373, 219, 335, 254, 374, 206, 344, 76, 186, 50, 274, 30, 314, 166, 70, 73, 225, 247], 'conv2': [445, 875, 1317, 235, 1279, 859, 1307, 119, 187, 662, 784, 1243, 760, 711, 276, 735, 335, 427, 80, 1395, 647, 454, 915, 1086, 356, 247, 1441, 590, 106, 167, 141, 1011, 409, 41, 786, 423, 726, 1464, 914, 817, 1494, 613, 68, 827, 1195, 1184, 835, 801, 519, 1412, 812, 943, 1400, 896, 849, 746, 764, 484, 1126, 1247, 599, 295, 752, 1230, 341, 869, 868, 935, 313, 1532, 1233, 857, 1375, 227, 447, 1297, 1006, 1132, 1271, 782, 22, 731, 425, 472, 1042, 350, 891, 144, 1071, 612, 1474, 1186, 159, 964, 381, 959, 450, 408, 683, 70, 1436, 651, 1113, 1486, 426, 1100, 667, 906, 958, 1012, 456, 1069, 1460, 822, 449, 266, 617, 1269, 121, 1429, 1282, 1530, 1357, 1220, 25, 1320, 461, 474, 917, 953, 937, 243, 971, 441, 635, 644, 614, 854, 1372, 304, 1053, 353, 1032, 79, 289, 305, 1465, 839, 601, 244, 524, 1360, 1471, 1266, 1210, 180, 268, 619, 700, 405, 532, 225, 209, 993, 252, 1358, 440, 229, 28, 207, 545, 346, 326, 421, 501, 847, 1134, 846, 241, 142, 340, 1116, 1046, 214, 640, 1361, 537, 161, 380, 366, 1522, 1211, 1481, 1000, 557, 673, 1454, 627, 430, 1252, 865, 900, 5, 58, 130, 414, 135, 1407, 577, 976, 1500, 942, 294, 981, 78, 1442, 781, 195, 314, 636, 818, 264, 1083, 1110, 204, 1489, 579, 660, 1238, 246, 838], 'conv3': [4076, 2572, 1544, 944, 1163, 5694, 6119, 5015, 4144, 2067, 4382, 1894, 780, 3980, 3710, 5338, 863, 287, 1439, 2101, 425, 5027, 4431, 2323, 534, 2736, 2070, 3165, 309, 5799, 3559, 2598, 1564, 2460, 3947, 3984, 4394, 4244, 5201, 43, 6088, 1641, 26, 178, 775, 2576, 4299, 2607, 5143, 3827, 3399, 5871, 2835, 2752, 5396, 5890, 4821, 3577, 3783, 4312, 1097, 4876, 5910, 3035, 1312, 3828, 2613, 911, 3383, 3187, 5054, 4537, 2213, 1083, 4651, 4817, 3734, 371, 1845, 1257, 2000, 1949, 3897, 2976, 515, 3986, 6094, 2335, 2244, 5631, 5286, 2491, 3348, 4467, 4992, 1877, 514, 4699, 2559, 1428, 5150, 5277, 3561, 4814, 5003, 750, 5222, 5272, 4979, 1960, 2342, 3181, 1753, 894, 2986, 4614, 1653, 8, 5491, 2138, 4906, 1422, 1950, 5908, 5645, 4791, 2966, 465, 2426, 2315, 1532, 5448, 2185, 4824, 1575, 3964, 1080, 1385, 1873, 2891, 5542, 2471, 985, 2679, 1631, 95, 5846, 2221, 4598, 1730, 2152, 3221, 1685, 294, 632, 2656, 3036, 4339, 61, 5048, 5698, 4097, 3226, 5318, 2261, 4186, 2917, 3429, 1988, 1346, 2089, 2515, 4395, 1749, 493, 5421, 948, 4451, 4899, 1501, 527, 3420, 5014, 3722, 5642, 63, 4227, 2013, 3466, 5349, 3662, 4044, 4908, 2693, 4702, 2950, 3362, 1335, 1438, 1709, 2270, 4110, 4482, 4476, 2069, 4134, 4376, 2026, 4502, 6125, 4163, 1622, 4118, 184, 1938, 825, 3788, 5493, 5580, 3733, 2019, 4884, 1539, 5212, 2787, 2975, 3938, 859, 4034, 4819, 2246, 4269, 2580, 1390, 4606, 2367, 2470, 3537, 4135, 1615, 2083, 1696, 2281, 415, 2585, 5407, 4566, 5839, 6032, 5124, 6021, 451, 5626, 3852, 3698, 4735, 679, 2248, 806, 2533, 1798, 5651, 105, 4181, 4358, 1833, 3718, 2996, 3606, 6084, 4352, 5795, 6070, 2420, 6041, 4000, 361, 1037, 5752, 1131, 5000, 471, 599, 4688, 881, 551, 5466, 3375, 1623, 4081, 4247, 796, 3097, 3571, 1009, 167, 2747, 3950, 2814, 1888, 2447, 1357, 3749, 3775, 706, 1349, 999, 3916, 4323, 2242, 2146, 1897, 1307, 4572, 1055, 686, 2581, 2415, 4341, 2049, 6095, 4120, 2850, 3070, 2095, 1022, 5004, 4301, 3495, 2290, 1138, 1973, 4261, 5753, 1978, 2488, 1115, 4100, 205, 3670, 6072, 4778, 4001, 363, 2001, 3991, 430, 3005, 1368, 4602, 3252, 4980, 257, 2172, 1406, 278, 4662, 4900, 2797, 2075, 4025, 5067, 4455, 827, 5876, 5312, 3145, 1853, 5109, 4962, 3448, 3067, 4222, 3839, 5005, 1519, 132, 5984, 836, 3615, 5589, 2328, 1155, 3818, 3384, 4385, 1168, 3745, 1857, 4007, 4639, 3387, 4143, 4277, 6043, 2608, 1222, 1806, 6100, 4363, 2691, 461, 1909, 5796, 4009, 477, 1220, 1778, 676, 5422, 5023, 2056, 261, 468, 2597, 4218, 4885, 2697, 1158, 730, 4357, 5791, 1104, 2785, 4266, 5811, 2962, 1887, 584, 5376, 3242, 2106, 4004, 5423, 4963, 709, 1513, 4719, 2452, 1252, 4570, 5590, 1028, 4392, 2079, 2769, 14, 516, 822, 5809, 6098, 2555, 3253, 1526, 3641, 3740, 4066, 2262, 4845, 2841, 1361, 1337, 5834, 4292, 5414, 1508, 5669, 2698, 5321, 3717, 5792, 1530, 3084, 3013, 4925, 3959, 4349, 3548, 569, 4080, 3106, 4402, 3009, 1597, 732, 4638, 2865, 2800, 2647, 4739, 248, 1306, 2494, 5477, 667, 5072, 2668, 1130, 4460, 3193, 3470, 3170, 2847, 5363, 2979, 3177, 5103, 1799, 5773, 144, 1831, 1289, 5591, 52, 1680, 3334, 3153, 3173, 5078, 5558, 5658, 1594, 4159, 1822, 5254, 646, 4896, 175, 5582, 3107, 3178, 4457, 2799, 2378, 2863, 943, 40, 6131, 2569, 383, 4954, 3279, 2611, 883, 4475, 3761, 1608, 5070, 4415, 4554, 5035, 6024, 2122, 5961, 4780, 5445, 1336, 117, 3822, 5416, 2817, 4029, 6142, 4306, 2666, 2941, 3750, 993, 5013, 5473, 3435, 5442, 4869, 4445, 5790, 4681, 3981, 4077, 2393, 3658, 774, 6118, 3367, 394, 4072, 1274, 3061, 5069, 6090, 2126, 1677, 5127, 583, 21, 3993, 165, 5504, 4407, 1911, 3027, 5055, 1913, 1000, 2030, 338, 5276, 1402, 152, 1424, 2154, 2355, 2792, 3337, 1036, 5292, 1292, 5507, 3523, 3136, 1226, 3050, 4692, 2282, 5866, 850, 3532, 4350, 5243, 5332, 3323, 2522, 3449, 3034, 508, 5451, 326, 4724, 1296, 4843, 842, 5623, 5643, 292, 5918, 3040, 4807, 426, 1996, 1930, 5283, 767, 5024, 3191, 2418, 211, 5322, 5121, 5065, 909, 526, 2305, 3126, 485, 3054, 2804, 5146, 5381, 5956, 3643, 3474, 1011, 378, 5180, 509, 2860, 2645, 3594, 5308, 2336, 4848, 728, 1181, 4682, 4882, 2317, 4052, 511, 221, 5368, 2675, 3653, 5966, 3294, 2037, 4190, 3480, 3802, 2320, 5671, 2427, 2677, 1095, 4073, 799, 3403, 1936, 5852, 1644, 4704, 5993, 2398, 3873, 45, 3502, 1363, 3998, 2538, 3563, 4637, 1854, 3454, 5929, 2051, 3227, 2345, 5625, 3853, 2694, 4544, 5239, 3361, 976, 1706, 3488, 6133, 1294, 3114, 327, 4640, 1636, 5314, 4551, 2978, 5976, 5188, 660, 5316, 555, 716, 2334, 878, 6044, 3041, 6037, 480, 3860, 1789, 5096, 5169, 1401, 443, 3062, 5256, 1086, 581, 1403, 153, 5776, 5772, 46, 3324, 3680, 4958, 2778, 4924, 6025, 3086, 4142, 1520, 4418, 3347, 3621, 3229, 4027, 2751, 3727, 134, 1453, 873, 2112, 4974, 997, 1903, 5205, 3450, 5629, 194, 1515, 2762, 5130, 3801, 1144, 1096, 5836, 159, 547, 1256, 3069, 3830, 2183, 3464, 3275, 5861, 4203, 2682, 450, 4308, 1183, 2714, 3244, 2989, 2702, 1642, 3175, 5151, 544, 505, 2331, 4653, 3388, 2245, 409, 3657, 1280, 860, 1787, 2173, 3533, 1649, 1194, 4141, 2506, 225, 3108, 4217, 1499, 5568, 3197, 2918, 5806, 4102, 1610, 1459, 3540, 5502, 5936, 1998, 5647, 3736, 5398, 5579, 5419, 4987, 5851, 1042, 1153, 2615, 1904, 2222, 2703, 41, 5190, 2625, 1451, 601, 5635, 5661, 122, 273, 1545, 5922, 888, 1712, 1968, 1112, 3649, 4265, 180, 954, 4151, 1073, 5973, 3936, 3941, 530, 3251, 2859, 2914, 1525, 634, 5401, 5051, 5452, 5409, 4556, 2983, 2005, 5108, 5406, 4298, 3469, 1231, 2885, 446, 776, 3033, 2277, 755, 2824, 1191, 3183, 5393, 3720]}
SAVED model.heads_thresh 0.9
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
num_blocks:  5
Epoch: [50/50]	lr: 7.81e-06	time: 00:37:08	Loss_train 0.72359	Acc_train 89.78	/	Loss_test 0.21046	Acc_test 74.74
new_head:  {'blocks.4.layer.bias': tensor([ 0.0095,  0.0022, -0.0014,  0.0006,  0.0038,  0.0088, -0.0177,  0.0123,
        -0.0043,  0.0127], device='cuda:0'), 'blocks.4.layer.weight': tensor([[ 0.0753,  0.0216,  0.1031,  ..., -0.0119, -0.0026, -0.0461],
        [-0.0021, -0.0158, -0.0219,  ..., -0.1085,  0.1136,  0.0426],
        [-0.0567, -0.0940,  0.1015,  ..., -0.0155, -0.0621,  0.0444],
        ...,
        [-0.0177,  0.0519,  0.0008,  ...,  0.0218,  0.0824, -0.0101],
        [-0.0454,  0.0595, -0.0771,  ...,  0.0664, -0.1257, -0.1442],
        [-0.0302,  0.0074, -0.0728,  ...,  0.0438, -0.0112, -0.0909]],
       device='cuda:0')}
BLOCKS IN SUP:  [4]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/STL10_CIFAR10_CL158728fd-dcb8-497d-8bd4-d5ba2c749836/models
SAVED HEADS THRESHOLD:  0.8236999893188477
SAVED model.topk_kernels {'conv0': [42, 59, 25, 30, 14, 70, 35, 69, 68, 93, 53, 77, 55, 21, 5], 'conv1': [23, 382, 345, 191, 75, 104, 365, 190, 144, 99, 100, 264, 56, 324, 158, 92, 180, 152, 150, 296, 133, 140, 121, 163, 105, 146, 88, 349, 94, 179, 33, 36, 41, 201, 202, 159, 26, 13, 39, 354, 336, 373, 219, 335, 254, 374, 206, 344, 76, 186, 50, 274, 30, 314, 166, 70, 73, 225, 247], 'conv2': [445, 875, 1317, 235, 1279, 859, 1307, 119, 187, 662, 784, 1243, 760, 711, 276, 735, 335, 427, 80, 1395, 647, 454, 915, 1086, 356, 247, 1441, 590, 106, 167, 141, 1011, 409, 41, 786, 423, 726, 1464, 914, 817, 1494, 613, 68, 827, 1195, 1184, 835, 801, 519, 1412, 812, 943, 1400, 896, 849, 746, 764, 484, 1126, 1247, 599, 295, 752, 1230, 341, 869, 868, 935, 313, 1532, 1233, 857, 1375, 227, 447, 1297, 1006, 1132, 1271, 782, 22, 731, 425, 472, 1042, 350, 891, 144, 1071, 612, 1474, 1186, 159, 964, 381, 959, 450, 408, 683, 70, 1436, 651, 1113, 1486, 426, 1100, 667, 906, 958, 1012, 456, 1069, 1460, 822, 449, 266, 617, 1269, 121, 1429, 1282, 1530, 1357, 1220, 25, 1320, 461, 474, 917, 953, 937, 243, 971, 441, 635, 644, 614, 854, 1372, 304, 1053, 353, 1032, 79, 289, 305, 1465, 839, 601, 244, 524, 1360, 1471, 1266, 1210, 180, 268, 619, 700, 405, 532, 225, 209, 993, 252, 1358, 440, 229, 28, 207, 545, 346, 326, 421, 501, 847, 1134, 846, 241, 142, 340, 1116, 1046, 214, 640, 1361, 537, 161, 380, 366, 1522, 1211, 1481, 1000, 557, 673, 1454, 627, 430, 1252, 865, 900, 5, 58, 130, 414, 135, 1407, 577, 976, 1500, 942, 294, 981, 78, 1442, 781, 195, 314, 636, 818, 264, 1083, 1110, 204, 1489, 579, 660, 1238, 246, 838], 'conv3': [4076, 2572, 1544, 944, 1163, 5694, 6119, 5015, 4144, 2067, 4382, 1894, 780, 3980, 3710, 5338, 863, 287, 1439, 2101, 425, 5027, 4431, 2323, 534, 2736, 2070, 3165, 309, 5799, 3559, 2598, 1564, 2460, 3947, 3984, 4394, 4244, 5201, 43, 6088, 1641, 26, 178, 775, 2576, 4299, 2607, 5143, 3827, 3399, 5871, 2835, 2752, 5396, 5890, 4821, 3577, 3783, 4312, 1097, 4876, 5910, 3035, 1312, 3828, 2613, 911, 3383, 3187, 5054, 4537, 2213, 1083, 4651, 4817, 3734, 371, 1845, 1257, 2000, 1949, 3897, 2976, 515, 3986, 6094, 2335, 2244, 5631, 5286, 2491, 3348, 4467, 4992, 1877, 514, 4699, 2559, 1428, 5150, 5277, 3561, 4814, 5003, 750, 5222, 5272, 4979, 1960, 2342, 3181, 1753, 894, 2986, 4614, 1653, 8, 5491, 2138, 4906, 1422, 1950, 5908, 5645, 4791, 2966, 465, 2426, 2315, 1532, 5448, 2185, 4824, 1575, 3964, 1080, 1385, 1873, 2891, 5542, 2471, 985, 2679, 1631, 95, 5846, 2221, 4598, 1730, 2152, 3221, 1685, 294, 632, 2656, 3036, 4339, 61, 5048, 5698, 4097, 3226, 5318, 2261, 4186, 2917, 3429, 1988, 1346, 2089, 2515, 4395, 1749, 493, 5421, 948, 4451, 4899, 1501, 527, 3420, 5014, 3722, 5642, 63, 4227, 2013, 3466, 5349, 3662, 4044, 4908, 2693, 4702, 2950, 3362, 1335, 1438, 1709, 2270, 4110, 4482, 4476, 2069, 4134, 4376, 2026, 4502, 6125, 4163, 1622, 4118, 184, 1938, 825, 3788, 5493, 5580, 3733, 2019, 4884, 1539, 5212, 2787, 2975, 3938, 859, 4034, 4819, 2246, 4269, 2580, 1390, 4606, 2367, 2470, 3537, 4135, 1615, 2083, 1696, 2281, 415, 2585, 5407, 4566, 5839, 6032, 5124, 6021, 451, 5626, 3852, 3698, 4735, 679, 2248, 806, 2533, 1798, 5651, 105, 4181, 4358, 1833, 3718, 2996, 3606, 6084, 4352, 5795, 6070, 2420, 6041, 4000, 361, 1037, 5752, 1131, 5000, 471, 599, 4688, 881, 551, 5466, 3375, 1623, 4081, 4247, 796, 3097, 3571, 1009, 167, 2747, 3950, 2814, 1888, 2447, 1357, 3749, 3775, 706, 1349, 999, 3916, 4323, 2242, 2146, 1897, 1307, 4572, 1055, 686, 2581, 2415, 4341, 2049, 6095, 4120, 2850, 3070, 2095, 1022, 5004, 4301, 3495, 2290, 1138, 1973, 4261, 5753, 1978, 2488, 1115, 4100, 205, 3670, 6072, 4778, 4001, 363, 2001, 3991, 430, 3005, 1368, 4602, 3252, 4980, 257, 2172, 1406, 278, 4662, 4900, 2797, 2075, 4025, 5067, 4455, 827, 5876, 5312, 3145, 1853, 5109, 4962, 3448, 3067, 4222, 3839, 5005, 1519, 132, 5984, 836, 3615, 5589, 2328, 1155, 3818, 3384, 4385, 1168, 3745, 1857, 4007, 4639, 3387, 4143, 4277, 6043, 2608, 1222, 1806, 6100, 4363, 2691, 461, 1909, 5796, 4009, 477, 1220, 1778, 676, 5422, 5023, 2056, 261, 468, 2597, 4218, 4885, 2697, 1158, 730, 4357, 5791, 1104, 2785, 4266, 5811, 2962, 1887, 584, 5376, 3242, 2106, 4004, 5423, 4963, 709, 1513, 4719, 2452, 1252, 4570, 5590, 1028, 4392, 2079, 2769, 14, 516, 822, 5809, 6098, 2555, 3253, 1526, 3641, 3740, 4066, 2262, 4845, 2841, 1361, 1337, 5834, 4292, 5414, 1508, 5669, 2698, 5321, 3717, 5792, 1530, 3084, 3013, 4925, 3959, 4349, 3548, 569, 4080, 3106, 4402, 3009, 1597, 732, 4638, 2865, 2800, 2647, 4739, 248, 1306, 2494, 5477, 667, 5072, 2668, 1130, 4460, 3193, 3470, 3170, 2847, 5363, 2979, 3177, 5103, 1799, 5773, 144, 1831, 1289, 5591, 52, 1680, 3334, 3153, 3173, 5078, 5558, 5658, 1594, 4159, 1822, 5254, 646, 4896, 175, 5582, 3107, 3178, 4457, 2799, 2378, 2863, 943, 40, 6131, 2569, 383, 4954, 3279, 2611, 883, 4475, 3761, 1608, 5070, 4415, 4554, 5035, 6024, 2122, 5961, 4780, 5445, 1336, 117, 3822, 5416, 2817, 4029, 6142, 4306, 2666, 2941, 3750, 993, 5013, 5473, 3435, 5442, 4869, 4445, 5790, 4681, 3981, 4077, 2393, 3658, 774, 6118, 3367, 394, 4072, 1274, 3061, 5069, 6090, 2126, 1677, 5127, 583, 21, 3993, 165, 5504, 4407, 1911, 3027, 5055, 1913, 1000, 2030, 338, 5276, 1402, 152, 1424, 2154, 2355, 2792, 3337, 1036, 5292, 1292, 5507, 3523, 3136, 1226, 3050, 4692, 2282, 5866, 850, 3532, 4350, 5243, 5332, 3323, 2522, 3449, 3034, 508, 5451, 326, 4724, 1296, 4843, 842, 5623, 5643, 292, 5918, 3040, 4807, 426, 1996, 1930, 5283, 767, 5024, 3191, 2418, 211, 5322, 5121, 5065, 909, 526, 2305, 3126, 485, 3054, 2804, 5146, 5381, 5956, 3643, 3474, 1011, 378, 5180, 509, 2860, 2645, 3594, 5308, 2336, 4848, 728, 1181, 4682, 4882, 2317, 4052, 511, 221, 5368, 2675, 3653, 5966, 3294, 2037, 4190, 3480, 3802, 2320, 5671, 2427, 2677, 1095, 4073, 799, 3403, 1936, 5852, 1644, 4704, 5993, 2398, 3873, 45, 3502, 1363, 3998, 2538, 3563, 4637, 1854, 3454, 5929, 2051, 3227, 2345, 5625, 3853, 2694, 4544, 5239, 3361, 976, 1706, 3488, 6133, 1294, 3114, 327, 4640, 1636, 5314, 4551, 2978, 5976, 5188, 660, 5316, 555, 716, 2334, 878, 6044, 3041, 6037, 480, 3860, 1789, 5096, 5169, 1401, 443, 3062, 5256, 1086, 581, 1403, 153, 5776, 5772, 46, 3324, 3680, 4958, 2778, 4924, 6025, 3086, 4142, 1520, 4418, 3347, 3621, 3229, 4027, 2751, 3727, 134, 1453, 873, 2112, 4974, 997, 1903, 5205, 3450, 5629, 194, 1515, 2762, 5130, 3801, 1144, 1096, 5836, 159, 547, 1256, 3069, 3830, 2183, 3464, 3275, 5861, 4203, 2682, 450, 4308, 1183, 2714, 3244, 2989, 2702, 1642, 3175, 5151, 544, 505, 2331, 4653, 3388, 2245, 409, 3657, 1280, 860, 1787, 2173, 3533, 1649, 1194, 4141, 2506, 225, 3108, 4217, 1499, 5568, 3197, 2918, 5806, 4102, 1610, 1459, 3540, 5502, 5936, 1998, 5647, 3736, 5398, 5579, 5419, 4987, 5851, 1042, 1153, 2615, 1904, 2222, 2703, 41, 5190, 2625, 1451, 601, 5635, 5661, 122, 273, 1545, 5922, 888, 1712, 1968, 1112, 3649, 4265, 180, 954, 4151, 1073, 5973, 3936, 3941, 530, 3251, 2859, 2914, 1525, 634, 5401, 5051, 5452, 5409, 4556, 2983, 2005, 5108, 5406, 4298, 3469, 1231, 2885, 446, 776, 3033, 2277, 755, 2824, 1191, 3183, 5393, 3720]}
SAVED model.heads_thresh 0.8236999893188477
RESULT:  {'train_loss': 0.7235910296440125, 'train_acc': 89.78360295295715, 'test_loss': 0.2104601263999939, 'test_acc': 74.73999786376953, 'convergence': 25.553024291992188, 'R1': 15, 'dataset_sup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 96}, 'dataset_unsup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 96}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 10, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}}
IN R1:  {'count': 1, 'R0': {'train_loss': 0.03978398069739342, 'train_acc': 98.27600121498108, 'test_loss': 3.0680172443389893, 'test_acc': 66.5625, 'convergence': 23.72207260131836, 'R1': 1, 'dataset_sup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 5000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 4, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': True, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 100, 'print_freq': 10, 'validation': False, 'continual_learning': False}, 'dataset_unsup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 100000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 32, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'unlabeled', 'nb_epoch': 1, 'print_freq': 25, 'validation': False, 'continual_learning': False}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 32, 'nb_epoch': 1, 'print_freq': 25, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 100, 'print_freq': 10, 'lr': 0.001}}}, 'model_config': {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.11048543456039805, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 24576, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}, 'R1': {'train_loss': 0.7235910296440125, 'train_acc': 89.78360295295715, 'test_loss': 0.2104601263999939, 'test_acc': 74.73999786376953, 'convergence': 25.553024291992188, 'R1': 15, 'dataset_sup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 96}, 'dataset_unsup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 96}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 10, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}}}
SEED:  0
block 0, size : 96 48 48
96
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.11048543456039805, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 24576, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 2.886751345948129
block 1, size : 384 24 24
384
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.11048543456039805, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 24576, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.8505172717997146
block 2, size : 1536 12 12
1536
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.11048543456039805, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 24576, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.4252586358998573
block 3, size : 6144 6 6
6144
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.11048543456039805, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 24576, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.21262931794992865
NOWWWW
6 6
10
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.11048543456039805, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 221184, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
blocks['b%s' % 4] 221184 221184
range = 0.036828478186799345
train_layer_order: consecutive {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 32, 'nb_epoch': 1, 'print_freq': 25, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 100, 'print_freq': 10, 'lr': 0.001}}
The device used will be: 
True
cuda:0
PARAMSSSS IN LOAD:  {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.036828478186799345, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 221184, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
avg_deltas:  {'blocks.0.layer.weight': tensor([-0.2630,  0.0083, -0.0191,  0.0142, -0.7412, -0.1396,  0.1026, -0.1063,
        -0.7140, -0.0898,  0.4283,  0.0795,  0.0033, -0.0165,  0.3822, -0.0873,
        -0.2309, -0.0044, -0.3653, -0.7079,  0.1934,  0.3337,  0.0140, -0.0333,
        -0.1300,  0.3305, -0.1130, -0.0865,  0.0273, -0.0352,  0.1947,  0.2420,
         0.0477, -0.8077,  0.2824,  0.7175, -0.8485, -0.2755,  0.4823, -0.0096,
         0.2895,  0.0272, -0.9085, -0.5497,  0.2544, -0.8254, -0.3190, -0.0294,
        -0.0317,  0.2093, -0.4211,  0.4183,  0.2042,  0.4424, -0.0057,  1.0000,
        -0.2801,  0.3765, -0.5709,  0.1538, -0.5018, -0.0155, -0.3676, -0.0315,
        -0.8681, -0.0184, -0.5272, -0.1788,  0.6942,  0.6900,  0.4434,  0.0252,
        -0.0239, -0.6783, -0.0572, -0.6914,  0.0731,  0.4868, -0.9390,  0.1802,
        -0.3681,  0.2990, -1.1148,  0.4157,  0.0550,  0.0094, -0.6950, -0.2249,
        -0.2276, -0.9358, -0.0143, -0.1122, -0.0390,  0.4208, -0.3224, -0.1714],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.6237e-02,  7.0086e-02,  1.5289e-01,  1.1156e-01,  9.7574e-01,
        -6.2613e-01,  1.2767e-01, -4.3946e-01, -1.3801e-01,  1.2916e-01,
        -1.2124e-02,  4.0093e-01,  4.4163e-01,  2.0606e-01,  3.8869e-01,
         1.6200e-01, -4.5216e-02,  4.4844e-01, -5.0046e-01, -5.1407e-01,
         6.5582e-02,  2.1479e-01,  6.4345e-01, -4.1762e-02,  8.5719e-01,
         8.6206e-01, -2.6951e-02, -2.8461e-01, -1.8812e-01,  2.5312e-01,
         1.2315e-03, -8.4117e-01, -6.1376e-01, -6.5717e-02, -4.6042e-02,
        -2.4405e-03,  1.1480e-01,  7.1823e-02,  7.0447e-03,  1.5133e-02,
         2.4647e-01,  5.7565e-02, -3.1824e-02,  1.6643e-02,  5.1735e-01,
         1.1909e-03, -6.6917e-01,  7.0948e-02, -2.4563e-01, -4.2519e-01,
         3.6524e-01,  1.1463e-01, -1.0976e-01, -1.3186e-01, -3.2995e-01,
         3.2732e-01,  2.7510e-01, -3.3060e-01,  1.4748e-01, -4.5586e-02,
         2.5030e-02, -7.9605e-01,  4.9660e-01, -1.8930e-01, -4.6544e-01,
        -4.7806e-03, -2.9966e-01,  4.6741e-01,  2.5794e-01, -1.8034e-01,
        -1.3379e-01, -8.3788e-02, -1.4426e-01,  4.5113e-01, -1.3760e-01,
         1.7828e-01,  3.4841e-01, -6.4871e-01, -6.6265e-01,  3.1118e-01,
        -1.6078e-01,  4.1702e-02,  9.2368e-02, -1.0922e-03,  3.9919e-01,
        -6.0747e-03, -3.3797e-03, -3.0017e-01, -2.9790e-02,  3.8402e-01,
        -3.3105e-02, -5.7794e-03, -5.1916e-02,  1.1503e-01,  3.3566e-01,
        -2.9883e-01,  2.3668e-01,  1.1334e-02, -2.7307e-01, -2.1446e-02,
         5.3822e-01,  1.0365e-01,  1.8027e-02, -2.6713e-01, -5.3410e-02,
         1.3655e-01, -7.8492e-02, -7.2273e-02, -6.1290e-02,  3.5888e-01,
        -1.2939e-01,  1.6825e-01, -8.3204e-04, -1.5619e-01,  9.2945e-01,
        -2.2071e-01, -6.4073e-02,  3.3851e-01,  4.8310e-01, -2.1324e-01,
        -5.7841e-01, -1.1894e-01,  5.6586e-02,  4.4958e-01,  2.0666e-01,
        -4.2389e-01, -1.1529e-01, -2.1057e-01,  6.1187e-01,  9.5866e-02,
         1.6815e-01,  3.4471e-01,  3.2532e-01, -1.7916e-02,  5.6826e-01,
         1.5608e-01,  4.3487e-02,  8.7703e-01,  2.4791e-01,  9.2718e-02,
         2.6397e-01,  2.0243e-01,  6.6764e-01, -1.7141e-01, -2.6422e-02,
         2.2226e-01,  5.5692e-01, -1.0889e-01,  2.3757e-01, -5.3926e-03,
        -5.4804e-02, -1.5549e-01, -2.7168e-03,  3.6941e-02, -1.8294e-02,
        -1.0627e-01, -4.0721e-01, -7.4082e-01,  4.8937e-02, -4.0754e-01,
        -2.0716e-01, -3.1342e-02,  2.1061e-01, -2.6118e-01, -3.0793e-01,
         1.2309e-01, -3.7757e-01, -1.2043e-02,  4.5154e-01, -1.8111e-01,
        -9.9635e-02,  2.6839e-01,  2.7588e-01,  2.7528e-01,  1.8049e-01,
        -2.5277e-01,  1.6421e-01, -2.7796e-02,  6.6651e-02,  1.2785e-01,
        -1.3338e-01,  2.7490e-01,  9.9449e-02,  7.7394e-02, -7.4721e-02,
        -1.3671e-02,  3.2771e-01,  1.9736e-02,  2.6656e-01, -5.5675e-01,
         3.9457e-01, -9.9514e-02,  2.0779e-02,  1.1948e-01,  3.9379e-02,
         6.0977e-01,  4.1385e-02,  7.0908e-01,  9.7083e-04, -1.7671e-01,
         8.6830e-01, -1.0178e-01, -2.5451e-01,  7.2607e-01, -2.2524e-01,
         1.7009e-01,  3.0318e-01, -2.5535e-02, -2.8519e-01,  7.4481e-01,
        -1.6137e-01,  1.7782e-02,  4.3352e-02, -1.9098e-01, -4.4963e-02,
         2.1432e-01,  1.4275e-01,  5.5266e-01, -6.3516e-01,  2.3370e-01,
         3.1676e-02,  8.7186e-01, -9.1121e-02, -1.8329e-01,  4.6815e-01,
        -1.2057e-01,  4.8097e-01, -7.0653e-01,  4.6713e-03,  4.1877e-01,
        -6.2196e-02,  2.9108e-04, -7.8863e-03,  5.2587e-02,  3.4637e-01,
        -4.2772e-01,  5.3634e-01,  2.8193e-02,  2.4816e-01,  4.7977e-01,
        -2.0887e-01, -1.2205e-01,  1.5709e-03, -4.2056e-01, -1.3009e-03,
        -3.0306e-01,  5.4963e-02, -2.3276e-01,  6.1841e-01, -3.2771e-02,
         6.3384e-01,  3.6256e-02, -2.4193e-01,  1.0148e-02, -6.5338e-01,
         1.5213e-01,  9.0067e-02,  4.0155e-04, -1.8124e-01,  4.1780e-02,
        -3.7421e-02, -1.3815e-01,  9.5144e-01,  3.8597e-01, -4.8607e-02,
         2.0470e-01, -9.4395e-01,  2.5681e-01,  6.9425e-02, -9.6314e-03,
        -2.4465e-01,  6.3043e-01,  2.2519e-02,  5.5401e-04,  4.6864e-01,
        -2.1432e-01,  5.3940e-01,  2.9607e-01,  1.8892e-01, -2.2233e-01,
        -1.5041e-01,  6.5015e-01,  3.2351e-01,  2.9961e-03,  3.2108e-02,
         4.6496e-01, -1.5831e-01, -2.1360e-01,  1.2254e-01, -1.5451e-01,
        -9.9106e-02, -6.4133e-01,  9.8545e-02, -4.8964e-02, -9.1282e-02,
        -5.1380e-01, -3.1101e-01, -1.5879e-01,  6.3941e-01,  1.0695e-01,
        -5.6661e-01, -4.9584e-01,  2.5982e-01, -4.4674e-02,  7.8044e-01,
         1.1890e-01,  5.4891e-01, -8.7752e-03,  1.1075e-01, -1.4531e-01,
        -1.1849e-01, -6.8604e-02,  5.4391e-01, -7.3539e-02,  5.0097e-01,
        -2.7288e-02,  8.6610e-02,  3.8844e-01, -5.5026e-02,  5.7245e-02,
        -4.7057e-03,  1.0283e-01,  7.3576e-01, -3.1376e-01, -1.7154e-01,
         1.2800e-01,  8.4984e-01,  5.2382e-01, -1.6816e-01,  8.0808e-01,
        -4.4904e-01,  6.0777e-01, -6.4261e-01,  2.2741e-01,  1.3904e-02,
         6.8419e-02,  1.1787e-01, -1.2100e-01,  3.1964e-01,  1.3321e-01,
         2.1692e-02, -1.2558e-02, -1.2013e-01,  8.2346e-01, -6.0424e-03,
        -8.0741e-02,  8.8737e-02,  1.0000e+00,  1.2711e-01, -5.0877e-01,
         6.1398e-02, -8.3259e-02, -1.4273e-01,  2.3667e-01,  5.1263e-01,
        -5.8611e-01, -6.0547e-02,  6.6582e-01,  4.7099e-01, -3.1921e-04,
         3.1895e-02,  8.0227e-03, -1.0633e-01,  3.9896e-02,  4.2195e-01,
        -8.1213e-02, -1.1851e-01,  3.1089e-01,  5.8907e-02,  6.1397e-04,
         9.3546e-02,  5.8575e-01,  2.2937e-01, -1.5115e-01, -2.3659e-01,
         5.5872e-02, -2.1307e-01,  1.5340e-01, -1.1432e-02,  2.2660e-01,
         5.7918e-01,  2.6871e-01, -6.8558e-02,  1.5709e-01], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0922,  0.0959,  0.3645,  ...,  0.2347, -0.0171,  0.2686],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0578, -0.0195, -0.0196,  ..., -0.0142, -0.0183,  0.0011],
       device='cuda:0')}
topk_layer:  None
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-0.2630,  0.0083, -0.0191,  0.0142, -0.7412, -0.1396,  0.1026, -0.1063,
        -0.7140, -0.0898,  0.4283,  0.0795,  0.0033, -0.0165,  0.3822, -0.0873,
        -0.2309, -0.0044, -0.3653, -0.7079,  0.1934,  0.3337,  0.0140, -0.0333,
        -0.1300,  0.3305, -0.1130, -0.0865,  0.0273, -0.0352,  0.1947,  0.2420,
         0.0477, -0.8077,  0.2824,  0.7175, -0.8485, -0.2755,  0.4823, -0.0096,
         0.2895,  0.0272, -0.9085, -0.5497,  0.2544, -0.8254, -0.3190, -0.0294,
        -0.0317,  0.2093, -0.4211,  0.4183,  0.2042,  0.4424, -0.0057,  1.0000,
        -0.2801,  0.3765, -0.5709,  0.1538, -0.5018, -0.0155, -0.3676, -0.0315,
        -0.8681, -0.0184, -0.5272, -0.1788,  0.6942,  0.6900,  0.4434,  0.0252,
        -0.0239, -0.6783, -0.0572, -0.6914,  0.0731,  0.4868, -0.9390,  0.1802,
        -0.3681,  0.2990, -1.1148,  0.4157,  0.0550,  0.0094, -0.6950, -0.2249,
        -0.2276, -0.9358, -0.0143, -0.1122, -0.0390,  0.4208, -0.3224, -0.1714],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.6237e-02,  7.0086e-02,  1.5289e-01,  1.1156e-01,  9.7574e-01,
        -6.2613e-01,  1.2767e-01, -4.3946e-01, -1.3801e-01,  1.2916e-01,
        -1.2124e-02,  4.0093e-01,  4.4163e-01,  2.0606e-01,  3.8869e-01,
         1.6200e-01, -4.5216e-02,  4.4844e-01, -5.0046e-01, -5.1407e-01,
         6.5582e-02,  2.1479e-01,  6.4345e-01, -4.1762e-02,  8.5719e-01,
         8.6206e-01, -2.6951e-02, -2.8461e-01, -1.8812e-01,  2.5312e-01,
         1.2315e-03, -8.4117e-01, -6.1376e-01, -6.5717e-02, -4.6042e-02,
        -2.4405e-03,  1.1480e-01,  7.1823e-02,  7.0447e-03,  1.5133e-02,
         2.4647e-01,  5.7565e-02, -3.1824e-02,  1.6643e-02,  5.1735e-01,
         1.1909e-03, -6.6917e-01,  7.0948e-02, -2.4563e-01, -4.2519e-01,
         3.6524e-01,  1.1463e-01, -1.0976e-01, -1.3186e-01, -3.2995e-01,
         3.2732e-01,  2.7510e-01, -3.3060e-01,  1.4748e-01, -4.5586e-02,
         2.5030e-02, -7.9605e-01,  4.9660e-01, -1.8930e-01, -4.6544e-01,
        -4.7806e-03, -2.9966e-01,  4.6741e-01,  2.5794e-01, -1.8034e-01,
        -1.3379e-01, -8.3788e-02, -1.4426e-01,  4.5113e-01, -1.3760e-01,
         1.7828e-01,  3.4841e-01, -6.4871e-01, -6.6265e-01,  3.1118e-01,
        -1.6078e-01,  4.1702e-02,  9.2368e-02, -1.0922e-03,  3.9919e-01,
        -6.0747e-03, -3.3797e-03, -3.0017e-01, -2.9790e-02,  3.8402e-01,
        -3.3105e-02, -5.7794e-03, -5.1916e-02,  1.1503e-01,  3.3566e-01,
        -2.9883e-01,  2.3668e-01,  1.1334e-02, -2.7307e-01, -2.1446e-02,
         5.3822e-01,  1.0365e-01,  1.8027e-02, -2.6713e-01, -5.3410e-02,
         1.3655e-01, -7.8492e-02, -7.2273e-02, -6.1290e-02,  3.5888e-01,
        -1.2939e-01,  1.6825e-01, -8.3204e-04, -1.5619e-01,  9.2945e-01,
        -2.2071e-01, -6.4073e-02,  3.3851e-01,  4.8310e-01, -2.1324e-01,
        -5.7841e-01, -1.1894e-01,  5.6586e-02,  4.4958e-01,  2.0666e-01,
        -4.2389e-01, -1.1529e-01, -2.1057e-01,  6.1187e-01,  9.5866e-02,
         1.6815e-01,  3.4471e-01,  3.2532e-01, -1.7916e-02,  5.6826e-01,
         1.5608e-01,  4.3487e-02,  8.7703e-01,  2.4791e-01,  9.2718e-02,
         2.6397e-01,  2.0243e-01,  6.6764e-01, -1.7141e-01, -2.6422e-02,
         2.2226e-01,  5.5692e-01, -1.0889e-01,  2.3757e-01, -5.3926e-03,
        -5.4804e-02, -1.5549e-01, -2.7168e-03,  3.6941e-02, -1.8294e-02,
        -1.0627e-01, -4.0721e-01, -7.4082e-01,  4.8937e-02, -4.0754e-01,
        -2.0716e-01, -3.1342e-02,  2.1061e-01, -2.6118e-01, -3.0793e-01,
         1.2309e-01, -3.7757e-01, -1.2043e-02,  4.5154e-01, -1.8111e-01,
        -9.9635e-02,  2.6839e-01,  2.7588e-01,  2.7528e-01,  1.8049e-01,
        -2.5277e-01,  1.6421e-01, -2.7796e-02,  6.6651e-02,  1.2785e-01,
        -1.3338e-01,  2.7490e-01,  9.9449e-02,  7.7394e-02, -7.4721e-02,
        -1.3671e-02,  3.2771e-01,  1.9736e-02,  2.6656e-01, -5.5675e-01,
         3.9457e-01, -9.9514e-02,  2.0779e-02,  1.1948e-01,  3.9379e-02,
         6.0977e-01,  4.1385e-02,  7.0908e-01,  9.7083e-04, -1.7671e-01,
         8.6830e-01, -1.0178e-01, -2.5451e-01,  7.2607e-01, -2.2524e-01,
         1.7009e-01,  3.0318e-01, -2.5535e-02, -2.8519e-01,  7.4481e-01,
        -1.6137e-01,  1.7782e-02,  4.3352e-02, -1.9098e-01, -4.4963e-02,
         2.1432e-01,  1.4275e-01,  5.5266e-01, -6.3516e-01,  2.3370e-01,
         3.1676e-02,  8.7186e-01, -9.1121e-02, -1.8329e-01,  4.6815e-01,
        -1.2057e-01,  4.8097e-01, -7.0653e-01,  4.6713e-03,  4.1877e-01,
        -6.2196e-02,  2.9108e-04, -7.8863e-03,  5.2587e-02,  3.4637e-01,
        -4.2772e-01,  5.3634e-01,  2.8193e-02,  2.4816e-01,  4.7977e-01,
        -2.0887e-01, -1.2205e-01,  1.5709e-03, -4.2056e-01, -1.3009e-03,
        -3.0306e-01,  5.4963e-02, -2.3276e-01,  6.1841e-01, -3.2771e-02,
         6.3384e-01,  3.6256e-02, -2.4193e-01,  1.0148e-02, -6.5338e-01,
         1.5213e-01,  9.0067e-02,  4.0155e-04, -1.8124e-01,  4.1780e-02,
        -3.7421e-02, -1.3815e-01,  9.5144e-01,  3.8597e-01, -4.8607e-02,
         2.0470e-01, -9.4395e-01,  2.5681e-01,  6.9425e-02, -9.6314e-03,
        -2.4465e-01,  6.3043e-01,  2.2519e-02,  5.5401e-04,  4.6864e-01,
        -2.1432e-01,  5.3940e-01,  2.9607e-01,  1.8892e-01, -2.2233e-01,
        -1.5041e-01,  6.5015e-01,  3.2351e-01,  2.9961e-03,  3.2108e-02,
         4.6496e-01, -1.5831e-01, -2.1360e-01,  1.2254e-01, -1.5451e-01,
        -9.9106e-02, -6.4133e-01,  9.8545e-02, -4.8964e-02, -9.1282e-02,
        -5.1380e-01, -3.1101e-01, -1.5879e-01,  6.3941e-01,  1.0695e-01,
        -5.6661e-01, -4.9584e-01,  2.5982e-01, -4.4674e-02,  7.8044e-01,
         1.1890e-01,  5.4891e-01, -8.7752e-03,  1.1075e-01, -1.4531e-01,
        -1.1849e-01, -6.8604e-02,  5.4391e-01, -7.3539e-02,  5.0097e-01,
        -2.7288e-02,  8.6610e-02,  3.8844e-01, -5.5026e-02,  5.7245e-02,
        -4.7057e-03,  1.0283e-01,  7.3576e-01, -3.1376e-01, -1.7154e-01,
         1.2800e-01,  8.4984e-01,  5.2382e-01, -1.6816e-01,  8.0808e-01,
        -4.4904e-01,  6.0777e-01, -6.4261e-01,  2.2741e-01,  1.3904e-02,
         6.8419e-02,  1.1787e-01, -1.2100e-01,  3.1964e-01,  1.3321e-01,
         2.1692e-02, -1.2558e-02, -1.2013e-01,  8.2346e-01, -6.0424e-03,
        -8.0741e-02,  8.8737e-02,  1.0000e+00,  1.2711e-01, -5.0877e-01,
         6.1398e-02, -8.3259e-02, -1.4273e-01,  2.3667e-01,  5.1263e-01,
        -5.8611e-01, -6.0547e-02,  6.6582e-01,  4.7099e-01, -3.1921e-04,
         3.1895e-02,  8.0227e-03, -1.0633e-01,  3.9896e-02,  4.2195e-01,
        -8.1213e-02, -1.1851e-01,  3.1089e-01,  5.8907e-02,  6.1397e-04,
         9.3546e-02,  5.8575e-01,  2.2937e-01, -1.5115e-01, -2.3659e-01,
         5.5872e-02, -2.1307e-01,  1.5340e-01, -1.1432e-02,  2.2660e-01,
         5.7918e-01,  2.6871e-01, -6.8558e-02,  1.5709e-01], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0922,  0.0959,  0.3645,  ...,  0.2347, -0.0171,  0.2686],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0578, -0.0195, -0.0196,  ..., -0.0142, -0.0183,  0.0011],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.0.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([-0.2630,  0.0083, -0.0191,  0.0142, -0.7412, -0.1396,  0.1026, -0.1063,
        -0.7140, -0.0898,  0.4283,  0.0795,  0.0033, -0.0165,  0.3822, -0.0873,
        -0.2309, -0.0044, -0.3653, -0.7079,  0.1934,  0.3337,  0.0140, -0.0333,
        -0.1300,  0.3305, -0.1130, -0.0865,  0.0273, -0.0352,  0.1947,  0.2420,
         0.0477, -0.8077,  0.2824,  0.7175, -0.8485, -0.2755,  0.4823, -0.0096,
         0.2895,  0.0272, -0.9085, -0.5497,  0.2544, -0.8254, -0.3190, -0.0294,
        -0.0317,  0.2093, -0.4211,  0.4183,  0.2042,  0.4424, -0.0057,  1.0000,
        -0.2801,  0.3765, -0.5709,  0.1538, -0.5018, -0.0155, -0.3676, -0.0315,
        -0.8681, -0.0184, -0.5272, -0.1788,  0.6942,  0.6900,  0.4434,  0.0252,
        -0.0239, -0.6783, -0.0572, -0.6914,  0.0731,  0.4868, -0.9390,  0.1802,
        -0.3681,  0.2990, -1.1148,  0.4157,  0.0550,  0.0094, -0.6950, -0.2249,
        -0.2276, -0.9358, -0.0143, -0.1122, -0.0390,  0.4208, -0.3224, -0.1714],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-0.2630,  0.0083, -0.0191,  0.0142, -0.7412, -0.1396,  0.1026, -0.1063,
        -0.7140, -0.0898,  0.4283,  0.0795,  0.0033, -0.0165,  0.3822, -0.0873,
        -0.2309, -0.0044, -0.3653, -0.7079,  0.1934,  0.3337,  0.0140, -0.0333,
        -0.1300,  0.3305, -0.1130, -0.0865,  0.0273, -0.0352,  0.1947,  0.2420,
         0.0477, -0.8077,  0.2824,  0.7175, -0.8485, -0.2755,  0.4823, -0.0096,
         0.2895,  0.0272, -0.9085, -0.5497,  0.2544, -0.8254, -0.3190, -0.0294,
        -0.0317,  0.2093, -0.4211,  0.4183,  0.2042,  0.4424, -0.0057,  1.0000,
        -0.2801,  0.3765, -0.5709,  0.1538, -0.5018, -0.0155, -0.3676, -0.0315,
        -0.8681, -0.0184, -0.5272, -0.1788,  0.6942,  0.6900,  0.4434,  0.0252,
        -0.0239, -0.6783, -0.0572, -0.6914,  0.0731,  0.4868, -0.9390,  0.1802,
        -0.3681,  0.2990, -1.1148,  0.4157,  0.0550,  0.0094, -0.6950, -0.2249,
        -0.2276, -0.9358, -0.0143, -0.1122, -0.0390,  0.4208, -0.3224, -0.1714],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.6237e-02,  7.0086e-02,  1.5289e-01,  1.1156e-01,  9.7574e-01,
        -6.2613e-01,  1.2767e-01, -4.3946e-01, -1.3801e-01,  1.2916e-01,
        -1.2124e-02,  4.0093e-01,  4.4163e-01,  2.0606e-01,  3.8869e-01,
         1.6200e-01, -4.5216e-02,  4.4844e-01, -5.0046e-01, -5.1407e-01,
         6.5582e-02,  2.1479e-01,  6.4345e-01, -4.1762e-02,  8.5719e-01,
         8.6206e-01, -2.6951e-02, -2.8461e-01, -1.8812e-01,  2.5312e-01,
         1.2315e-03, -8.4117e-01, -6.1376e-01, -6.5717e-02, -4.6042e-02,
        -2.4405e-03,  1.1480e-01,  7.1823e-02,  7.0447e-03,  1.5133e-02,
         2.4647e-01,  5.7565e-02, -3.1824e-02,  1.6643e-02,  5.1735e-01,
         1.1909e-03, -6.6917e-01,  7.0948e-02, -2.4563e-01, -4.2519e-01,
         3.6524e-01,  1.1463e-01, -1.0976e-01, -1.3186e-01, -3.2995e-01,
         3.2732e-01,  2.7510e-01, -3.3060e-01,  1.4748e-01, -4.5586e-02,
         2.5030e-02, -7.9605e-01,  4.9660e-01, -1.8930e-01, -4.6544e-01,
        -4.7806e-03, -2.9966e-01,  4.6741e-01,  2.5794e-01, -1.8034e-01,
        -1.3379e-01, -8.3788e-02, -1.4426e-01,  4.5113e-01, -1.3760e-01,
         1.7828e-01,  3.4841e-01, -6.4871e-01, -6.6265e-01,  3.1118e-01,
        -1.6078e-01,  4.1702e-02,  9.2368e-02, -1.0922e-03,  3.9919e-01,
        -6.0747e-03, -3.3797e-03, -3.0017e-01, -2.9790e-02,  3.8402e-01,
        -3.3105e-02, -5.7794e-03, -5.1916e-02,  1.1503e-01,  3.3566e-01,
        -2.9883e-01,  2.3668e-01,  1.1334e-02, -2.7307e-01, -2.1446e-02,
         5.3822e-01,  1.0365e-01,  1.8027e-02, -2.6713e-01, -5.3410e-02,
         1.3655e-01, -7.8492e-02, -7.2273e-02, -6.1290e-02,  3.5888e-01,
        -1.2939e-01,  1.6825e-01, -8.3204e-04, -1.5619e-01,  9.2945e-01,
        -2.2071e-01, -6.4073e-02,  3.3851e-01,  4.8310e-01, -2.1324e-01,
        -5.7841e-01, -1.1894e-01,  5.6586e-02,  4.4958e-01,  2.0666e-01,
        -4.2389e-01, -1.1529e-01, -2.1057e-01,  6.1187e-01,  9.5866e-02,
         1.6815e-01,  3.4471e-01,  3.2532e-01, -1.7916e-02,  5.6826e-01,
         1.5608e-01,  4.3487e-02,  8.7703e-01,  2.4791e-01,  9.2718e-02,
         2.6397e-01,  2.0243e-01,  6.6764e-01, -1.7141e-01, -2.6422e-02,
         2.2226e-01,  5.5692e-01, -1.0889e-01,  2.3757e-01, -5.3926e-03,
        -5.4804e-02, -1.5549e-01, -2.7168e-03,  3.6941e-02, -1.8294e-02,
        -1.0627e-01, -4.0721e-01, -7.4082e-01,  4.8937e-02, -4.0754e-01,
        -2.0716e-01, -3.1342e-02,  2.1061e-01, -2.6118e-01, -3.0793e-01,
         1.2309e-01, -3.7757e-01, -1.2043e-02,  4.5154e-01, -1.8111e-01,
        -9.9635e-02,  2.6839e-01,  2.7588e-01,  2.7528e-01,  1.8049e-01,
        -2.5277e-01,  1.6421e-01, -2.7796e-02,  6.6651e-02,  1.2785e-01,
        -1.3338e-01,  2.7490e-01,  9.9449e-02,  7.7394e-02, -7.4721e-02,
        -1.3671e-02,  3.2771e-01,  1.9736e-02,  2.6656e-01, -5.5675e-01,
         3.9457e-01, -9.9514e-02,  2.0779e-02,  1.1948e-01,  3.9379e-02,
         6.0977e-01,  4.1385e-02,  7.0908e-01,  9.7083e-04, -1.7671e-01,
         8.6830e-01, -1.0178e-01, -2.5451e-01,  7.2607e-01, -2.2524e-01,
         1.7009e-01,  3.0318e-01, -2.5535e-02, -2.8519e-01,  7.4481e-01,
        -1.6137e-01,  1.7782e-02,  4.3352e-02, -1.9098e-01, -4.4963e-02,
         2.1432e-01,  1.4275e-01,  5.5266e-01, -6.3516e-01,  2.3370e-01,
         3.1676e-02,  8.7186e-01, -9.1121e-02, -1.8329e-01,  4.6815e-01,
        -1.2057e-01,  4.8097e-01, -7.0653e-01,  4.6713e-03,  4.1877e-01,
        -6.2196e-02,  2.9108e-04, -7.8863e-03,  5.2587e-02,  3.4637e-01,
        -4.2772e-01,  5.3634e-01,  2.8193e-02,  2.4816e-01,  4.7977e-01,
        -2.0887e-01, -1.2205e-01,  1.5709e-03, -4.2056e-01, -1.3009e-03,
        -3.0306e-01,  5.4963e-02, -2.3276e-01,  6.1841e-01, -3.2771e-02,
         6.3384e-01,  3.6256e-02, -2.4193e-01,  1.0148e-02, -6.5338e-01,
         1.5213e-01,  9.0067e-02,  4.0155e-04, -1.8124e-01,  4.1780e-02,
        -3.7421e-02, -1.3815e-01,  9.5144e-01,  3.8597e-01, -4.8607e-02,
         2.0470e-01, -9.4395e-01,  2.5681e-01,  6.9425e-02, -9.6314e-03,
        -2.4465e-01,  6.3043e-01,  2.2519e-02,  5.5401e-04,  4.6864e-01,
        -2.1432e-01,  5.3940e-01,  2.9607e-01,  1.8892e-01, -2.2233e-01,
        -1.5041e-01,  6.5015e-01,  3.2351e-01,  2.9961e-03,  3.2108e-02,
         4.6496e-01, -1.5831e-01, -2.1360e-01,  1.2254e-01, -1.5451e-01,
        -9.9106e-02, -6.4133e-01,  9.8545e-02, -4.8964e-02, -9.1282e-02,
        -5.1380e-01, -3.1101e-01, -1.5879e-01,  6.3941e-01,  1.0695e-01,
        -5.6661e-01, -4.9584e-01,  2.5982e-01, -4.4674e-02,  7.8044e-01,
         1.1890e-01,  5.4891e-01, -8.7752e-03,  1.1075e-01, -1.4531e-01,
        -1.1849e-01, -6.8604e-02,  5.4391e-01, -7.3539e-02,  5.0097e-01,
        -2.7288e-02,  8.6610e-02,  3.8844e-01, -5.5026e-02,  5.7245e-02,
        -4.7057e-03,  1.0283e-01,  7.3576e-01, -3.1376e-01, -1.7154e-01,
         1.2800e-01,  8.4984e-01,  5.2382e-01, -1.6816e-01,  8.0808e-01,
        -4.4904e-01,  6.0777e-01, -6.4261e-01,  2.2741e-01,  1.3904e-02,
         6.8419e-02,  1.1787e-01, -1.2100e-01,  3.1964e-01,  1.3321e-01,
         2.1692e-02, -1.2558e-02, -1.2013e-01,  8.2346e-01, -6.0424e-03,
        -8.0741e-02,  8.8737e-02,  1.0000e+00,  1.2711e-01, -5.0877e-01,
         6.1398e-02, -8.3259e-02, -1.4273e-01,  2.3667e-01,  5.1263e-01,
        -5.8611e-01, -6.0547e-02,  6.6582e-01,  4.7099e-01, -3.1921e-04,
         3.1895e-02,  8.0227e-03, -1.0633e-01,  3.9896e-02,  4.2195e-01,
        -8.1213e-02, -1.1851e-01,  3.1089e-01,  5.8907e-02,  6.1397e-04,
         9.3546e-02,  5.8575e-01,  2.2937e-01, -1.5115e-01, -2.3659e-01,
         5.5872e-02, -2.1307e-01,  1.5340e-01, -1.1432e-02,  2.2660e-01,
         5.7918e-01,  2.6871e-01, -6.8558e-02,  1.5709e-01], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0922,  0.0959,  0.3645,  ...,  0.2347, -0.0171,  0.2686],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0578, -0.0195, -0.0196,  ..., -0.0142, -0.0183,  0.0011],
       device='cuda:0')}
topk_layer inside model after retrieval:  [42, 59, 25, 30, 14, 70, 35, 69, 68, 93, 53, 77, 55, 21, 5]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-0.2630,  0.0083, -0.0191,  0.0142, -0.7412, -0.1396,  0.1026, -0.1063,
        -0.7140, -0.0898,  0.4283,  0.0795,  0.0033, -0.0165,  0.3822, -0.0873,
        -0.2309, -0.0044, -0.3653, -0.7079,  0.1934,  0.3337,  0.0140, -0.0333,
        -0.1300,  0.3305, -0.1130, -0.0865,  0.0273, -0.0352,  0.1947,  0.2420,
         0.0477, -0.8077,  0.2824,  0.7175, -0.8485, -0.2755,  0.4823, -0.0096,
         0.2895,  0.0272, -0.9085, -0.5497,  0.2544, -0.8254, -0.3190, -0.0294,
        -0.0317,  0.2093, -0.4211,  0.4183,  0.2042,  0.4424, -0.0057,  1.0000,
        -0.2801,  0.3765, -0.5709,  0.1538, -0.5018, -0.0155, -0.3676, -0.0315,
        -0.8681, -0.0184, -0.5272, -0.1788,  0.6942,  0.6900,  0.4434,  0.0252,
        -0.0239, -0.6783, -0.0572, -0.6914,  0.0731,  0.4868, -0.9390,  0.1802,
        -0.3681,  0.2990, -1.1148,  0.4157,  0.0550,  0.0094, -0.6950, -0.2249,
        -0.2276, -0.9358, -0.0143, -0.1122, -0.0390,  0.4208, -0.3224, -0.1714],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.6237e-02,  7.0086e-02,  1.5289e-01,  1.1156e-01,  9.7574e-01,
        -6.2613e-01,  1.2767e-01, -4.3946e-01, -1.3801e-01,  1.2916e-01,
        -1.2124e-02,  4.0093e-01,  4.4163e-01,  2.0606e-01,  3.8869e-01,
         1.6200e-01, -4.5216e-02,  4.4844e-01, -5.0046e-01, -5.1407e-01,
         6.5582e-02,  2.1479e-01,  6.4345e-01, -4.1762e-02,  8.5719e-01,
         8.6206e-01, -2.6951e-02, -2.8461e-01, -1.8812e-01,  2.5312e-01,
         1.2315e-03, -8.4117e-01, -6.1376e-01, -6.5717e-02, -4.6042e-02,
        -2.4405e-03,  1.1480e-01,  7.1823e-02,  7.0447e-03,  1.5133e-02,
         2.4647e-01,  5.7565e-02, -3.1824e-02,  1.6643e-02,  5.1735e-01,
         1.1909e-03, -6.6917e-01,  7.0948e-02, -2.4563e-01, -4.2519e-01,
         3.6524e-01,  1.1463e-01, -1.0976e-01, -1.3186e-01, -3.2995e-01,
         3.2732e-01,  2.7510e-01, -3.3060e-01,  1.4748e-01, -4.5586e-02,
         2.5030e-02, -7.9605e-01,  4.9660e-01, -1.8930e-01, -4.6544e-01,
        -4.7806e-03, -2.9966e-01,  4.6741e-01,  2.5794e-01, -1.8034e-01,
        -1.3379e-01, -8.3788e-02, -1.4426e-01,  4.5113e-01, -1.3760e-01,
         1.7828e-01,  3.4841e-01, -6.4871e-01, -6.6265e-01,  3.1118e-01,
        -1.6078e-01,  4.1702e-02,  9.2368e-02, -1.0922e-03,  3.9919e-01,
        -6.0747e-03, -3.3797e-03, -3.0017e-01, -2.9790e-02,  3.8402e-01,
        -3.3105e-02, -5.7794e-03, -5.1916e-02,  1.1503e-01,  3.3566e-01,
        -2.9883e-01,  2.3668e-01,  1.1334e-02, -2.7307e-01, -2.1446e-02,
         5.3822e-01,  1.0365e-01,  1.8027e-02, -2.6713e-01, -5.3410e-02,
         1.3655e-01, -7.8492e-02, -7.2273e-02, -6.1290e-02,  3.5888e-01,
        -1.2939e-01,  1.6825e-01, -8.3204e-04, -1.5619e-01,  9.2945e-01,
        -2.2071e-01, -6.4073e-02,  3.3851e-01,  4.8310e-01, -2.1324e-01,
        -5.7841e-01, -1.1894e-01,  5.6586e-02,  4.4958e-01,  2.0666e-01,
        -4.2389e-01, -1.1529e-01, -2.1057e-01,  6.1187e-01,  9.5866e-02,
         1.6815e-01,  3.4471e-01,  3.2532e-01, -1.7916e-02,  5.6826e-01,
         1.5608e-01,  4.3487e-02,  8.7703e-01,  2.4791e-01,  9.2718e-02,
         2.6397e-01,  2.0243e-01,  6.6764e-01, -1.7141e-01, -2.6422e-02,
         2.2226e-01,  5.5692e-01, -1.0889e-01,  2.3757e-01, -5.3926e-03,
        -5.4804e-02, -1.5549e-01, -2.7168e-03,  3.6941e-02, -1.8294e-02,
        -1.0627e-01, -4.0721e-01, -7.4082e-01,  4.8937e-02, -4.0754e-01,
        -2.0716e-01, -3.1342e-02,  2.1061e-01, -2.6118e-01, -3.0793e-01,
         1.2309e-01, -3.7757e-01, -1.2043e-02,  4.5154e-01, -1.8111e-01,
        -9.9635e-02,  2.6839e-01,  2.7588e-01,  2.7528e-01,  1.8049e-01,
        -2.5277e-01,  1.6421e-01, -2.7796e-02,  6.6651e-02,  1.2785e-01,
        -1.3338e-01,  2.7490e-01,  9.9449e-02,  7.7394e-02, -7.4721e-02,
        -1.3671e-02,  3.2771e-01,  1.9736e-02,  2.6656e-01, -5.5675e-01,
         3.9457e-01, -9.9514e-02,  2.0779e-02,  1.1948e-01,  3.9379e-02,
         6.0977e-01,  4.1385e-02,  7.0908e-01,  9.7083e-04, -1.7671e-01,
         8.6830e-01, -1.0178e-01, -2.5451e-01,  7.2607e-01, -2.2524e-01,
         1.7009e-01,  3.0318e-01, -2.5535e-02, -2.8519e-01,  7.4481e-01,
        -1.6137e-01,  1.7782e-02,  4.3352e-02, -1.9098e-01, -4.4963e-02,
         2.1432e-01,  1.4275e-01,  5.5266e-01, -6.3516e-01,  2.3370e-01,
         3.1676e-02,  8.7186e-01, -9.1121e-02, -1.8329e-01,  4.6815e-01,
        -1.2057e-01,  4.8097e-01, -7.0653e-01,  4.6713e-03,  4.1877e-01,
        -6.2196e-02,  2.9108e-04, -7.8863e-03,  5.2587e-02,  3.4637e-01,
        -4.2772e-01,  5.3634e-01,  2.8193e-02,  2.4816e-01,  4.7977e-01,
        -2.0887e-01, -1.2205e-01,  1.5709e-03, -4.2056e-01, -1.3009e-03,
        -3.0306e-01,  5.4963e-02, -2.3276e-01,  6.1841e-01, -3.2771e-02,
         6.3384e-01,  3.6256e-02, -2.4193e-01,  1.0148e-02, -6.5338e-01,
         1.5213e-01,  9.0067e-02,  4.0155e-04, -1.8124e-01,  4.1780e-02,
        -3.7421e-02, -1.3815e-01,  9.5144e-01,  3.8597e-01, -4.8607e-02,
         2.0470e-01, -9.4395e-01,  2.5681e-01,  6.9425e-02, -9.6314e-03,
        -2.4465e-01,  6.3043e-01,  2.2519e-02,  5.5401e-04,  4.6864e-01,
        -2.1432e-01,  5.3940e-01,  2.9607e-01,  1.8892e-01, -2.2233e-01,
        -1.5041e-01,  6.5015e-01,  3.2351e-01,  2.9961e-03,  3.2108e-02,
         4.6496e-01, -1.5831e-01, -2.1360e-01,  1.2254e-01, -1.5451e-01,
        -9.9106e-02, -6.4133e-01,  9.8545e-02, -4.8964e-02, -9.1282e-02,
        -5.1380e-01, -3.1101e-01, -1.5879e-01,  6.3941e-01,  1.0695e-01,
        -5.6661e-01, -4.9584e-01,  2.5982e-01, -4.4674e-02,  7.8044e-01,
         1.1890e-01,  5.4891e-01, -8.7752e-03,  1.1075e-01, -1.4531e-01,
        -1.1849e-01, -6.8604e-02,  5.4391e-01, -7.3539e-02,  5.0097e-01,
        -2.7288e-02,  8.6610e-02,  3.8844e-01, -5.5026e-02,  5.7245e-02,
        -4.7057e-03,  1.0283e-01,  7.3576e-01, -3.1376e-01, -1.7154e-01,
         1.2800e-01,  8.4984e-01,  5.2382e-01, -1.6816e-01,  8.0808e-01,
        -4.4904e-01,  6.0777e-01, -6.4261e-01,  2.2741e-01,  1.3904e-02,
         6.8419e-02,  1.1787e-01, -1.2100e-01,  3.1964e-01,  1.3321e-01,
         2.1692e-02, -1.2558e-02, -1.2013e-01,  8.2346e-01, -6.0424e-03,
        -8.0741e-02,  8.8737e-02,  1.0000e+00,  1.2711e-01, -5.0877e-01,
         6.1398e-02, -8.3259e-02, -1.4273e-01,  2.3667e-01,  5.1263e-01,
        -5.8611e-01, -6.0547e-02,  6.6582e-01,  4.7099e-01, -3.1921e-04,
         3.1895e-02,  8.0227e-03, -1.0633e-01,  3.9896e-02,  4.2195e-01,
        -8.1213e-02, -1.1851e-01,  3.1089e-01,  5.8907e-02,  6.1397e-04,
         9.3546e-02,  5.8575e-01,  2.2937e-01, -1.5115e-01, -2.3659e-01,
         5.5872e-02, -2.1307e-01,  1.5340e-01, -1.1432e-02,  2.2660e-01,
         5.7918e-01,  2.6871e-01, -6.8558e-02,  1.5709e-01], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0922,  0.0959,  0.3645,  ...,  0.2347, -0.0171,  0.2686],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0578, -0.0195, -0.0196,  ..., -0.0142, -0.0183,  0.0011],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.1.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 3.6237e-02,  7.0086e-02,  1.5289e-01,  1.1156e-01,  9.7574e-01,
        -6.2613e-01,  1.2767e-01, -4.3946e-01, -1.3801e-01,  1.2916e-01,
        -1.2124e-02,  4.0093e-01,  4.4163e-01,  2.0606e-01,  3.8869e-01,
         1.6200e-01, -4.5216e-02,  4.4844e-01, -5.0046e-01, -5.1407e-01,
         6.5582e-02,  2.1479e-01,  6.4345e-01, -4.1762e-02,  8.5719e-01,
         8.6206e-01, -2.6951e-02, -2.8461e-01, -1.8812e-01,  2.5312e-01,
         1.2315e-03, -8.4117e-01, -6.1376e-01, -6.5717e-02, -4.6042e-02,
        -2.4405e-03,  1.1480e-01,  7.1823e-02,  7.0447e-03,  1.5133e-02,
         2.4647e-01,  5.7565e-02, -3.1824e-02,  1.6643e-02,  5.1735e-01,
         1.1909e-03, -6.6917e-01,  7.0948e-02, -2.4563e-01, -4.2519e-01,
         3.6524e-01,  1.1463e-01, -1.0976e-01, -1.3186e-01, -3.2995e-01,
         3.2732e-01,  2.7510e-01, -3.3060e-01,  1.4748e-01, -4.5586e-02,
         2.5030e-02, -7.9605e-01,  4.9660e-01, -1.8930e-01, -4.6544e-01,
        -4.7806e-03, -2.9966e-01,  4.6741e-01,  2.5794e-01, -1.8034e-01,
        -1.3379e-01, -8.3788e-02, -1.4426e-01,  4.5113e-01, -1.3760e-01,
         1.7828e-01,  3.4841e-01, -6.4871e-01, -6.6265e-01,  3.1118e-01,
        -1.6078e-01,  4.1702e-02,  9.2368e-02, -1.0922e-03,  3.9919e-01,
        -6.0747e-03, -3.3797e-03, -3.0017e-01, -2.9790e-02,  3.8402e-01,
        -3.3105e-02, -5.7794e-03, -5.1916e-02,  1.1503e-01,  3.3566e-01,
        -2.9883e-01,  2.3668e-01,  1.1334e-02, -2.7307e-01, -2.1446e-02,
         5.3822e-01,  1.0365e-01,  1.8027e-02, -2.6713e-01, -5.3410e-02,
         1.3655e-01, -7.8492e-02, -7.2273e-02, -6.1290e-02,  3.5888e-01,
        -1.2939e-01,  1.6825e-01, -8.3204e-04, -1.5619e-01,  9.2945e-01,
        -2.2071e-01, -6.4073e-02,  3.3851e-01,  4.8310e-01, -2.1324e-01,
        -5.7841e-01, -1.1894e-01,  5.6586e-02,  4.4958e-01,  2.0666e-01,
        -4.2389e-01, -1.1529e-01, -2.1057e-01,  6.1187e-01,  9.5866e-02,
         1.6815e-01,  3.4471e-01,  3.2532e-01, -1.7916e-02,  5.6826e-01,
         1.5608e-01,  4.3487e-02,  8.7703e-01,  2.4791e-01,  9.2718e-02,
         2.6397e-01,  2.0243e-01,  6.6764e-01, -1.7141e-01, -2.6422e-02,
         2.2226e-01,  5.5692e-01, -1.0889e-01,  2.3757e-01, -5.3926e-03,
        -5.4804e-02, -1.5549e-01, -2.7168e-03,  3.6941e-02, -1.8294e-02,
        -1.0627e-01, -4.0721e-01, -7.4082e-01,  4.8937e-02, -4.0754e-01,
        -2.0716e-01, -3.1342e-02,  2.1061e-01, -2.6118e-01, -3.0793e-01,
         1.2309e-01, -3.7757e-01, -1.2043e-02,  4.5154e-01, -1.8111e-01,
        -9.9635e-02,  2.6839e-01,  2.7588e-01,  2.7528e-01,  1.8049e-01,
        -2.5277e-01,  1.6421e-01, -2.7796e-02,  6.6651e-02,  1.2785e-01,
        -1.3338e-01,  2.7490e-01,  9.9449e-02,  7.7394e-02, -7.4721e-02,
        -1.3671e-02,  3.2771e-01,  1.9736e-02,  2.6656e-01, -5.5675e-01,
         3.9457e-01, -9.9514e-02,  2.0779e-02,  1.1948e-01,  3.9379e-02,
         6.0977e-01,  4.1385e-02,  7.0908e-01,  9.7083e-04, -1.7671e-01,
         8.6830e-01, -1.0178e-01, -2.5451e-01,  7.2607e-01, -2.2524e-01,
         1.7009e-01,  3.0318e-01, -2.5535e-02, -2.8519e-01,  7.4481e-01,
        -1.6137e-01,  1.7782e-02,  4.3352e-02, -1.9098e-01, -4.4963e-02,
         2.1432e-01,  1.4275e-01,  5.5266e-01, -6.3516e-01,  2.3370e-01,
         3.1676e-02,  8.7186e-01, -9.1121e-02, -1.8329e-01,  4.6815e-01,
        -1.2057e-01,  4.8097e-01, -7.0653e-01,  4.6713e-03,  4.1877e-01,
        -6.2196e-02,  2.9108e-04, -7.8863e-03,  5.2587e-02,  3.4637e-01,
        -4.2772e-01,  5.3634e-01,  2.8193e-02,  2.4816e-01,  4.7977e-01,
        -2.0887e-01, -1.2205e-01,  1.5709e-03, -4.2056e-01, -1.3009e-03,
        -3.0306e-01,  5.4963e-02, -2.3276e-01,  6.1841e-01, -3.2771e-02,
         6.3384e-01,  3.6256e-02, -2.4193e-01,  1.0148e-02, -6.5338e-01,
         1.5213e-01,  9.0067e-02,  4.0155e-04, -1.8124e-01,  4.1780e-02,
        -3.7421e-02, -1.3815e-01,  9.5144e-01,  3.8597e-01, -4.8607e-02,
         2.0470e-01, -9.4395e-01,  2.5681e-01,  6.9425e-02, -9.6314e-03,
        -2.4465e-01,  6.3043e-01,  2.2519e-02,  5.5401e-04,  4.6864e-01,
        -2.1432e-01,  5.3940e-01,  2.9607e-01,  1.8892e-01, -2.2233e-01,
        -1.5041e-01,  6.5015e-01,  3.2351e-01,  2.9961e-03,  3.2108e-02,
         4.6496e-01, -1.5831e-01, -2.1360e-01,  1.2254e-01, -1.5451e-01,
        -9.9106e-02, -6.4133e-01,  9.8545e-02, -4.8964e-02, -9.1282e-02,
        -5.1380e-01, -3.1101e-01, -1.5879e-01,  6.3941e-01,  1.0695e-01,
        -5.6661e-01, -4.9584e-01,  2.5982e-01, -4.4674e-02,  7.8044e-01,
         1.1890e-01,  5.4891e-01, -8.7752e-03,  1.1075e-01, -1.4531e-01,
        -1.1849e-01, -6.8604e-02,  5.4391e-01, -7.3539e-02,  5.0097e-01,
        -2.7288e-02,  8.6610e-02,  3.8844e-01, -5.5026e-02,  5.7245e-02,
        -4.7057e-03,  1.0283e-01,  7.3576e-01, -3.1376e-01, -1.7154e-01,
         1.2800e-01,  8.4984e-01,  5.2382e-01, -1.6816e-01,  8.0808e-01,
        -4.4904e-01,  6.0777e-01, -6.4261e-01,  2.2741e-01,  1.3904e-02,
         6.8419e-02,  1.1787e-01, -1.2100e-01,  3.1964e-01,  1.3321e-01,
         2.1692e-02, -1.2558e-02, -1.2013e-01,  8.2346e-01, -6.0424e-03,
        -8.0741e-02,  8.8737e-02,  1.0000e+00,  1.2711e-01, -5.0877e-01,
         6.1398e-02, -8.3259e-02, -1.4273e-01,  2.3667e-01,  5.1263e-01,
        -5.8611e-01, -6.0547e-02,  6.6582e-01,  4.7099e-01, -3.1921e-04,
         3.1895e-02,  8.0227e-03, -1.0633e-01,  3.9896e-02,  4.2195e-01,
        -8.1213e-02, -1.1851e-01,  3.1089e-01,  5.8907e-02,  6.1397e-04,
         9.3546e-02,  5.8575e-01,  2.2937e-01, -1.5115e-01, -2.3659e-01,
         5.5872e-02, -2.1307e-01,  1.5340e-01, -1.1432e-02,  2.2660e-01,
         5.7918e-01,  2.6871e-01, -6.8558e-02,  1.5709e-01], device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-0.2630,  0.0083, -0.0191,  0.0142, -0.7412, -0.1396,  0.1026, -0.1063,
        -0.7140, -0.0898,  0.4283,  0.0795,  0.0033, -0.0165,  0.3822, -0.0873,
        -0.2309, -0.0044, -0.3653, -0.7079,  0.1934,  0.3337,  0.0140, -0.0333,
        -0.1300,  0.3305, -0.1130, -0.0865,  0.0273, -0.0352,  0.1947,  0.2420,
         0.0477, -0.8077,  0.2824,  0.7175, -0.8485, -0.2755,  0.4823, -0.0096,
         0.2895,  0.0272, -0.9085, -0.5497,  0.2544, -0.8254, -0.3190, -0.0294,
        -0.0317,  0.2093, -0.4211,  0.4183,  0.2042,  0.4424, -0.0057,  1.0000,
        -0.2801,  0.3765, -0.5709,  0.1538, -0.5018, -0.0155, -0.3676, -0.0315,
        -0.8681, -0.0184, -0.5272, -0.1788,  0.6942,  0.6900,  0.4434,  0.0252,
        -0.0239, -0.6783, -0.0572, -0.6914,  0.0731,  0.4868, -0.9390,  0.1802,
        -0.3681,  0.2990, -1.1148,  0.4157,  0.0550,  0.0094, -0.6950, -0.2249,
        -0.2276, -0.9358, -0.0143, -0.1122, -0.0390,  0.4208, -0.3224, -0.1714],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.6237e-02,  7.0086e-02,  1.5289e-01,  1.1156e-01,  9.7574e-01,
        -6.2613e-01,  1.2767e-01, -4.3946e-01, -1.3801e-01,  1.2916e-01,
        -1.2124e-02,  4.0093e-01,  4.4163e-01,  2.0606e-01,  3.8869e-01,
         1.6200e-01, -4.5216e-02,  4.4844e-01, -5.0046e-01, -5.1407e-01,
         6.5582e-02,  2.1479e-01,  6.4345e-01, -4.1762e-02,  8.5719e-01,
         8.6206e-01, -2.6951e-02, -2.8461e-01, -1.8812e-01,  2.5312e-01,
         1.2315e-03, -8.4117e-01, -6.1376e-01, -6.5717e-02, -4.6042e-02,
        -2.4405e-03,  1.1480e-01,  7.1823e-02,  7.0447e-03,  1.5133e-02,
         2.4647e-01,  5.7565e-02, -3.1824e-02,  1.6643e-02,  5.1735e-01,
         1.1909e-03, -6.6917e-01,  7.0948e-02, -2.4563e-01, -4.2519e-01,
         3.6524e-01,  1.1463e-01, -1.0976e-01, -1.3186e-01, -3.2995e-01,
         3.2732e-01,  2.7510e-01, -3.3060e-01,  1.4748e-01, -4.5586e-02,
         2.5030e-02, -7.9605e-01,  4.9660e-01, -1.8930e-01, -4.6544e-01,
        -4.7806e-03, -2.9966e-01,  4.6741e-01,  2.5794e-01, -1.8034e-01,
        -1.3379e-01, -8.3788e-02, -1.4426e-01,  4.5113e-01, -1.3760e-01,
         1.7828e-01,  3.4841e-01, -6.4871e-01, -6.6265e-01,  3.1118e-01,
        -1.6078e-01,  4.1702e-02,  9.2368e-02, -1.0922e-03,  3.9919e-01,
        -6.0747e-03, -3.3797e-03, -3.0017e-01, -2.9790e-02,  3.8402e-01,
        -3.3105e-02, -5.7794e-03, -5.1916e-02,  1.1503e-01,  3.3566e-01,
        -2.9883e-01,  2.3668e-01,  1.1334e-02, -2.7307e-01, -2.1446e-02,
         5.3822e-01,  1.0365e-01,  1.8027e-02, -2.6713e-01, -5.3410e-02,
         1.3655e-01, -7.8492e-02, -7.2273e-02, -6.1290e-02,  3.5888e-01,
        -1.2939e-01,  1.6825e-01, -8.3204e-04, -1.5619e-01,  9.2945e-01,
        -2.2071e-01, -6.4073e-02,  3.3851e-01,  4.8310e-01, -2.1324e-01,
        -5.7841e-01, -1.1894e-01,  5.6586e-02,  4.4958e-01,  2.0666e-01,
        -4.2389e-01, -1.1529e-01, -2.1057e-01,  6.1187e-01,  9.5866e-02,
         1.6815e-01,  3.4471e-01,  3.2532e-01, -1.7916e-02,  5.6826e-01,
         1.5608e-01,  4.3487e-02,  8.7703e-01,  2.4791e-01,  9.2718e-02,
         2.6397e-01,  2.0243e-01,  6.6764e-01, -1.7141e-01, -2.6422e-02,
         2.2226e-01,  5.5692e-01, -1.0889e-01,  2.3757e-01, -5.3926e-03,
        -5.4804e-02, -1.5549e-01, -2.7168e-03,  3.6941e-02, -1.8294e-02,
        -1.0627e-01, -4.0721e-01, -7.4082e-01,  4.8937e-02, -4.0754e-01,
        -2.0716e-01, -3.1342e-02,  2.1061e-01, -2.6118e-01, -3.0793e-01,
         1.2309e-01, -3.7757e-01, -1.2043e-02,  4.5154e-01, -1.8111e-01,
        -9.9635e-02,  2.6839e-01,  2.7588e-01,  2.7528e-01,  1.8049e-01,
        -2.5277e-01,  1.6421e-01, -2.7796e-02,  6.6651e-02,  1.2785e-01,
        -1.3338e-01,  2.7490e-01,  9.9449e-02,  7.7394e-02, -7.4721e-02,
        -1.3671e-02,  3.2771e-01,  1.9736e-02,  2.6656e-01, -5.5675e-01,
         3.9457e-01, -9.9514e-02,  2.0779e-02,  1.1948e-01,  3.9379e-02,
         6.0977e-01,  4.1385e-02,  7.0908e-01,  9.7083e-04, -1.7671e-01,
         8.6830e-01, -1.0178e-01, -2.5451e-01,  7.2607e-01, -2.2524e-01,
         1.7009e-01,  3.0318e-01, -2.5535e-02, -2.8519e-01,  7.4481e-01,
        -1.6137e-01,  1.7782e-02,  4.3352e-02, -1.9098e-01, -4.4963e-02,
         2.1432e-01,  1.4275e-01,  5.5266e-01, -6.3516e-01,  2.3370e-01,
         3.1676e-02,  8.7186e-01, -9.1121e-02, -1.8329e-01,  4.6815e-01,
        -1.2057e-01,  4.8097e-01, -7.0653e-01,  4.6713e-03,  4.1877e-01,
        -6.2196e-02,  2.9108e-04, -7.8863e-03,  5.2587e-02,  3.4637e-01,
        -4.2772e-01,  5.3634e-01,  2.8193e-02,  2.4816e-01,  4.7977e-01,
        -2.0887e-01, -1.2205e-01,  1.5709e-03, -4.2056e-01, -1.3009e-03,
        -3.0306e-01,  5.4963e-02, -2.3276e-01,  6.1841e-01, -3.2771e-02,
         6.3384e-01,  3.6256e-02, -2.4193e-01,  1.0148e-02, -6.5338e-01,
         1.5213e-01,  9.0067e-02,  4.0155e-04, -1.8124e-01,  4.1780e-02,
        -3.7421e-02, -1.3815e-01,  9.5144e-01,  3.8597e-01, -4.8607e-02,
         2.0470e-01, -9.4395e-01,  2.5681e-01,  6.9425e-02, -9.6314e-03,
        -2.4465e-01,  6.3043e-01,  2.2519e-02,  5.5401e-04,  4.6864e-01,
        -2.1432e-01,  5.3940e-01,  2.9607e-01,  1.8892e-01, -2.2233e-01,
        -1.5041e-01,  6.5015e-01,  3.2351e-01,  2.9961e-03,  3.2108e-02,
         4.6496e-01, -1.5831e-01, -2.1360e-01,  1.2254e-01, -1.5451e-01,
        -9.9106e-02, -6.4133e-01,  9.8545e-02, -4.8964e-02, -9.1282e-02,
        -5.1380e-01, -3.1101e-01, -1.5879e-01,  6.3941e-01,  1.0695e-01,
        -5.6661e-01, -4.9584e-01,  2.5982e-01, -4.4674e-02,  7.8044e-01,
         1.1890e-01,  5.4891e-01, -8.7752e-03,  1.1075e-01, -1.4531e-01,
        -1.1849e-01, -6.8604e-02,  5.4391e-01, -7.3539e-02,  5.0097e-01,
        -2.7288e-02,  8.6610e-02,  3.8844e-01, -5.5026e-02,  5.7245e-02,
        -4.7057e-03,  1.0283e-01,  7.3576e-01, -3.1376e-01, -1.7154e-01,
         1.2800e-01,  8.4984e-01,  5.2382e-01, -1.6816e-01,  8.0808e-01,
        -4.4904e-01,  6.0777e-01, -6.4261e-01,  2.2741e-01,  1.3904e-02,
         6.8419e-02,  1.1787e-01, -1.2100e-01,  3.1964e-01,  1.3321e-01,
         2.1692e-02, -1.2558e-02, -1.2013e-01,  8.2346e-01, -6.0424e-03,
        -8.0741e-02,  8.8737e-02,  1.0000e+00,  1.2711e-01, -5.0877e-01,
         6.1398e-02, -8.3259e-02, -1.4273e-01,  2.3667e-01,  5.1263e-01,
        -5.8611e-01, -6.0547e-02,  6.6582e-01,  4.7099e-01, -3.1921e-04,
         3.1895e-02,  8.0227e-03, -1.0633e-01,  3.9896e-02,  4.2195e-01,
        -8.1213e-02, -1.1851e-01,  3.1089e-01,  5.8907e-02,  6.1397e-04,
         9.3546e-02,  5.8575e-01,  2.2937e-01, -1.5115e-01, -2.3659e-01,
         5.5872e-02, -2.1307e-01,  1.5340e-01, -1.1432e-02,  2.2660e-01,
         5.7918e-01,  2.6871e-01, -6.8558e-02,  1.5709e-01], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0922,  0.0959,  0.3645,  ...,  0.2347, -0.0171,  0.2686],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0578, -0.0195, -0.0196,  ..., -0.0142, -0.0183,  0.0011],
       device='cuda:0')}
topk_layer inside model after retrieval:  [23, 382, 345, 191, 75, 104, 365, 190, 144, 99, 100, 264, 56, 324, 158, 92, 180, 152, 150, 296, 133, 140, 121, 163, 105, 146, 88, 349, 94, 179, 33, 36, 41, 201, 202, 159, 26, 13, 39, 354, 336, 373, 219, 335, 254, 374, 206, 344, 76, 186, 50, 274, 30, 314, 166, 70, 73, 225, 247]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-0.2630,  0.0083, -0.0191,  0.0142, -0.7412, -0.1396,  0.1026, -0.1063,
        -0.7140, -0.0898,  0.4283,  0.0795,  0.0033, -0.0165,  0.3822, -0.0873,
        -0.2309, -0.0044, -0.3653, -0.7079,  0.1934,  0.3337,  0.0140, -0.0333,
        -0.1300,  0.3305, -0.1130, -0.0865,  0.0273, -0.0352,  0.1947,  0.2420,
         0.0477, -0.8077,  0.2824,  0.7175, -0.8485, -0.2755,  0.4823, -0.0096,
         0.2895,  0.0272, -0.9085, -0.5497,  0.2544, -0.8254, -0.3190, -0.0294,
        -0.0317,  0.2093, -0.4211,  0.4183,  0.2042,  0.4424, -0.0057,  1.0000,
        -0.2801,  0.3765, -0.5709,  0.1538, -0.5018, -0.0155, -0.3676, -0.0315,
        -0.8681, -0.0184, -0.5272, -0.1788,  0.6942,  0.6900,  0.4434,  0.0252,
        -0.0239, -0.6783, -0.0572, -0.6914,  0.0731,  0.4868, -0.9390,  0.1802,
        -0.3681,  0.2990, -1.1148,  0.4157,  0.0550,  0.0094, -0.6950, -0.2249,
        -0.2276, -0.9358, -0.0143, -0.1122, -0.0390,  0.4208, -0.3224, -0.1714],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.6237e-02,  7.0086e-02,  1.5289e-01,  1.1156e-01,  9.7574e-01,
        -6.2613e-01,  1.2767e-01, -4.3946e-01, -1.3801e-01,  1.2916e-01,
        -1.2124e-02,  4.0093e-01,  4.4163e-01,  2.0606e-01,  3.8869e-01,
         1.6200e-01, -4.5216e-02,  4.4844e-01, -5.0046e-01, -5.1407e-01,
         6.5582e-02,  2.1479e-01,  6.4345e-01, -4.1762e-02,  8.5719e-01,
         8.6206e-01, -2.6951e-02, -2.8461e-01, -1.8812e-01,  2.5312e-01,
         1.2315e-03, -8.4117e-01, -6.1376e-01, -6.5717e-02, -4.6042e-02,
        -2.4405e-03,  1.1480e-01,  7.1823e-02,  7.0447e-03,  1.5133e-02,
         2.4647e-01,  5.7565e-02, -3.1824e-02,  1.6643e-02,  5.1735e-01,
         1.1909e-03, -6.6917e-01,  7.0948e-02, -2.4563e-01, -4.2519e-01,
         3.6524e-01,  1.1463e-01, -1.0976e-01, -1.3186e-01, -3.2995e-01,
         3.2732e-01,  2.7510e-01, -3.3060e-01,  1.4748e-01, -4.5586e-02,
         2.5030e-02, -7.9605e-01,  4.9660e-01, -1.8930e-01, -4.6544e-01,
        -4.7806e-03, -2.9966e-01,  4.6741e-01,  2.5794e-01, -1.8034e-01,
        -1.3379e-01, -8.3788e-02, -1.4426e-01,  4.5113e-01, -1.3760e-01,
         1.7828e-01,  3.4841e-01, -6.4871e-01, -6.6265e-01,  3.1118e-01,
        -1.6078e-01,  4.1702e-02,  9.2368e-02, -1.0922e-03,  3.9919e-01,
        -6.0747e-03, -3.3797e-03, -3.0017e-01, -2.9790e-02,  3.8402e-01,
        -3.3105e-02, -5.7794e-03, -5.1916e-02,  1.1503e-01,  3.3566e-01,
        -2.9883e-01,  2.3668e-01,  1.1334e-02, -2.7307e-01, -2.1446e-02,
         5.3822e-01,  1.0365e-01,  1.8027e-02, -2.6713e-01, -5.3410e-02,
         1.3655e-01, -7.8492e-02, -7.2273e-02, -6.1290e-02,  3.5888e-01,
        -1.2939e-01,  1.6825e-01, -8.3204e-04, -1.5619e-01,  9.2945e-01,
        -2.2071e-01, -6.4073e-02,  3.3851e-01,  4.8310e-01, -2.1324e-01,
        -5.7841e-01, -1.1894e-01,  5.6586e-02,  4.4958e-01,  2.0666e-01,
        -4.2389e-01, -1.1529e-01, -2.1057e-01,  6.1187e-01,  9.5866e-02,
         1.6815e-01,  3.4471e-01,  3.2532e-01, -1.7916e-02,  5.6826e-01,
         1.5608e-01,  4.3487e-02,  8.7703e-01,  2.4791e-01,  9.2718e-02,
         2.6397e-01,  2.0243e-01,  6.6764e-01, -1.7141e-01, -2.6422e-02,
         2.2226e-01,  5.5692e-01, -1.0889e-01,  2.3757e-01, -5.3926e-03,
        -5.4804e-02, -1.5549e-01, -2.7168e-03,  3.6941e-02, -1.8294e-02,
        -1.0627e-01, -4.0721e-01, -7.4082e-01,  4.8937e-02, -4.0754e-01,
        -2.0716e-01, -3.1342e-02,  2.1061e-01, -2.6118e-01, -3.0793e-01,
         1.2309e-01, -3.7757e-01, -1.2043e-02,  4.5154e-01, -1.8111e-01,
        -9.9635e-02,  2.6839e-01,  2.7588e-01,  2.7528e-01,  1.8049e-01,
        -2.5277e-01,  1.6421e-01, -2.7796e-02,  6.6651e-02,  1.2785e-01,
        -1.3338e-01,  2.7490e-01,  9.9449e-02,  7.7394e-02, -7.4721e-02,
        -1.3671e-02,  3.2771e-01,  1.9736e-02,  2.6656e-01, -5.5675e-01,
         3.9457e-01, -9.9514e-02,  2.0779e-02,  1.1948e-01,  3.9379e-02,
         6.0977e-01,  4.1385e-02,  7.0908e-01,  9.7083e-04, -1.7671e-01,
         8.6830e-01, -1.0178e-01, -2.5451e-01,  7.2607e-01, -2.2524e-01,
         1.7009e-01,  3.0318e-01, -2.5535e-02, -2.8519e-01,  7.4481e-01,
        -1.6137e-01,  1.7782e-02,  4.3352e-02, -1.9098e-01, -4.4963e-02,
         2.1432e-01,  1.4275e-01,  5.5266e-01, -6.3516e-01,  2.3370e-01,
         3.1676e-02,  8.7186e-01, -9.1121e-02, -1.8329e-01,  4.6815e-01,
        -1.2057e-01,  4.8097e-01, -7.0653e-01,  4.6713e-03,  4.1877e-01,
        -6.2196e-02,  2.9108e-04, -7.8863e-03,  5.2587e-02,  3.4637e-01,
        -4.2772e-01,  5.3634e-01,  2.8193e-02,  2.4816e-01,  4.7977e-01,
        -2.0887e-01, -1.2205e-01,  1.5709e-03, -4.2056e-01, -1.3009e-03,
        -3.0306e-01,  5.4963e-02, -2.3276e-01,  6.1841e-01, -3.2771e-02,
         6.3384e-01,  3.6256e-02, -2.4193e-01,  1.0148e-02, -6.5338e-01,
         1.5213e-01,  9.0067e-02,  4.0155e-04, -1.8124e-01,  4.1780e-02,
        -3.7421e-02, -1.3815e-01,  9.5144e-01,  3.8597e-01, -4.8607e-02,
         2.0470e-01, -9.4395e-01,  2.5681e-01,  6.9425e-02, -9.6314e-03,
        -2.4465e-01,  6.3043e-01,  2.2519e-02,  5.5401e-04,  4.6864e-01,
        -2.1432e-01,  5.3940e-01,  2.9607e-01,  1.8892e-01, -2.2233e-01,
        -1.5041e-01,  6.5015e-01,  3.2351e-01,  2.9961e-03,  3.2108e-02,
         4.6496e-01, -1.5831e-01, -2.1360e-01,  1.2254e-01, -1.5451e-01,
        -9.9106e-02, -6.4133e-01,  9.8545e-02, -4.8964e-02, -9.1282e-02,
        -5.1380e-01, -3.1101e-01, -1.5879e-01,  6.3941e-01,  1.0695e-01,
        -5.6661e-01, -4.9584e-01,  2.5982e-01, -4.4674e-02,  7.8044e-01,
         1.1890e-01,  5.4891e-01, -8.7752e-03,  1.1075e-01, -1.4531e-01,
        -1.1849e-01, -6.8604e-02,  5.4391e-01, -7.3539e-02,  5.0097e-01,
        -2.7288e-02,  8.6610e-02,  3.8844e-01, -5.5026e-02,  5.7245e-02,
        -4.7057e-03,  1.0283e-01,  7.3576e-01, -3.1376e-01, -1.7154e-01,
         1.2800e-01,  8.4984e-01,  5.2382e-01, -1.6816e-01,  8.0808e-01,
        -4.4904e-01,  6.0777e-01, -6.4261e-01,  2.2741e-01,  1.3904e-02,
         6.8419e-02,  1.1787e-01, -1.2100e-01,  3.1964e-01,  1.3321e-01,
         2.1692e-02, -1.2558e-02, -1.2013e-01,  8.2346e-01, -6.0424e-03,
        -8.0741e-02,  8.8737e-02,  1.0000e+00,  1.2711e-01, -5.0877e-01,
         6.1398e-02, -8.3259e-02, -1.4273e-01,  2.3667e-01,  5.1263e-01,
        -5.8611e-01, -6.0547e-02,  6.6582e-01,  4.7099e-01, -3.1921e-04,
         3.1895e-02,  8.0227e-03, -1.0633e-01,  3.9896e-02,  4.2195e-01,
        -8.1213e-02, -1.1851e-01,  3.1089e-01,  5.8907e-02,  6.1397e-04,
         9.3546e-02,  5.8575e-01,  2.2937e-01, -1.5115e-01, -2.3659e-01,
         5.5872e-02, -2.1307e-01,  1.5340e-01, -1.1432e-02,  2.2660e-01,
         5.7918e-01,  2.6871e-01, -6.8558e-02,  1.5709e-01], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0922,  0.0959,  0.3645,  ...,  0.2347, -0.0171,  0.2686],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0578, -0.0195, -0.0196,  ..., -0.0142, -0.0183,  0.0011],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.2.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 0.0922,  0.0959,  0.3645,  ...,  0.2347, -0.0171,  0.2686],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-0.2630,  0.0083, -0.0191,  0.0142, -0.7412, -0.1396,  0.1026, -0.1063,
        -0.7140, -0.0898,  0.4283,  0.0795,  0.0033, -0.0165,  0.3822, -0.0873,
        -0.2309, -0.0044, -0.3653, -0.7079,  0.1934,  0.3337,  0.0140, -0.0333,
        -0.1300,  0.3305, -0.1130, -0.0865,  0.0273, -0.0352,  0.1947,  0.2420,
         0.0477, -0.8077,  0.2824,  0.7175, -0.8485, -0.2755,  0.4823, -0.0096,
         0.2895,  0.0272, -0.9085, -0.5497,  0.2544, -0.8254, -0.3190, -0.0294,
        -0.0317,  0.2093, -0.4211,  0.4183,  0.2042,  0.4424, -0.0057,  1.0000,
        -0.2801,  0.3765, -0.5709,  0.1538, -0.5018, -0.0155, -0.3676, -0.0315,
        -0.8681, -0.0184, -0.5272, -0.1788,  0.6942,  0.6900,  0.4434,  0.0252,
        -0.0239, -0.6783, -0.0572, -0.6914,  0.0731,  0.4868, -0.9390,  0.1802,
        -0.3681,  0.2990, -1.1148,  0.4157,  0.0550,  0.0094, -0.6950, -0.2249,
        -0.2276, -0.9358, -0.0143, -0.1122, -0.0390,  0.4208, -0.3224, -0.1714],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.6237e-02,  7.0086e-02,  1.5289e-01,  1.1156e-01,  9.7574e-01,
        -6.2613e-01,  1.2767e-01, -4.3946e-01, -1.3801e-01,  1.2916e-01,
        -1.2124e-02,  4.0093e-01,  4.4163e-01,  2.0606e-01,  3.8869e-01,
         1.6200e-01, -4.5216e-02,  4.4844e-01, -5.0046e-01, -5.1407e-01,
         6.5582e-02,  2.1479e-01,  6.4345e-01, -4.1762e-02,  8.5719e-01,
         8.6206e-01, -2.6951e-02, -2.8461e-01, -1.8812e-01,  2.5312e-01,
         1.2315e-03, -8.4117e-01, -6.1376e-01, -6.5717e-02, -4.6042e-02,
        -2.4405e-03,  1.1480e-01,  7.1823e-02,  7.0447e-03,  1.5133e-02,
         2.4647e-01,  5.7565e-02, -3.1824e-02,  1.6643e-02,  5.1735e-01,
         1.1909e-03, -6.6917e-01,  7.0948e-02, -2.4563e-01, -4.2519e-01,
         3.6524e-01,  1.1463e-01, -1.0976e-01, -1.3186e-01, -3.2995e-01,
         3.2732e-01,  2.7510e-01, -3.3060e-01,  1.4748e-01, -4.5586e-02,
         2.5030e-02, -7.9605e-01,  4.9660e-01, -1.8930e-01, -4.6544e-01,
        -4.7806e-03, -2.9966e-01,  4.6741e-01,  2.5794e-01, -1.8034e-01,
        -1.3379e-01, -8.3788e-02, -1.4426e-01,  4.5113e-01, -1.3760e-01,
         1.7828e-01,  3.4841e-01, -6.4871e-01, -6.6265e-01,  3.1118e-01,
        -1.6078e-01,  4.1702e-02,  9.2368e-02, -1.0922e-03,  3.9919e-01,
        -6.0747e-03, -3.3797e-03, -3.0017e-01, -2.9790e-02,  3.8402e-01,
        -3.3105e-02, -5.7794e-03, -5.1916e-02,  1.1503e-01,  3.3566e-01,
        -2.9883e-01,  2.3668e-01,  1.1334e-02, -2.7307e-01, -2.1446e-02,
         5.3822e-01,  1.0365e-01,  1.8027e-02, -2.6713e-01, -5.3410e-02,
         1.3655e-01, -7.8492e-02, -7.2273e-02, -6.1290e-02,  3.5888e-01,
        -1.2939e-01,  1.6825e-01, -8.3204e-04, -1.5619e-01,  9.2945e-01,
        -2.2071e-01, -6.4073e-02,  3.3851e-01,  4.8310e-01, -2.1324e-01,
        -5.7841e-01, -1.1894e-01,  5.6586e-02,  4.4958e-01,  2.0666e-01,
        -4.2389e-01, -1.1529e-01, -2.1057e-01,  6.1187e-01,  9.5866e-02,
         1.6815e-01,  3.4471e-01,  3.2532e-01, -1.7916e-02,  5.6826e-01,
         1.5608e-01,  4.3487e-02,  8.7703e-01,  2.4791e-01,  9.2718e-02,
         2.6397e-01,  2.0243e-01,  6.6764e-01, -1.7141e-01, -2.6422e-02,
         2.2226e-01,  5.5692e-01, -1.0889e-01,  2.3757e-01, -5.3926e-03,
        -5.4804e-02, -1.5549e-01, -2.7168e-03,  3.6941e-02, -1.8294e-02,
        -1.0627e-01, -4.0721e-01, -7.4082e-01,  4.8937e-02, -4.0754e-01,
        -2.0716e-01, -3.1342e-02,  2.1061e-01, -2.6118e-01, -3.0793e-01,
         1.2309e-01, -3.7757e-01, -1.2043e-02,  4.5154e-01, -1.8111e-01,
        -9.9635e-02,  2.6839e-01,  2.7588e-01,  2.7528e-01,  1.8049e-01,
        -2.5277e-01,  1.6421e-01, -2.7796e-02,  6.6651e-02,  1.2785e-01,
        -1.3338e-01,  2.7490e-01,  9.9449e-02,  7.7394e-02, -7.4721e-02,
        -1.3671e-02,  3.2771e-01,  1.9736e-02,  2.6656e-01, -5.5675e-01,
         3.9457e-01, -9.9514e-02,  2.0779e-02,  1.1948e-01,  3.9379e-02,
         6.0977e-01,  4.1385e-02,  7.0908e-01,  9.7083e-04, -1.7671e-01,
         8.6830e-01, -1.0178e-01, -2.5451e-01,  7.2607e-01, -2.2524e-01,
         1.7009e-01,  3.0318e-01, -2.5535e-02, -2.8519e-01,  7.4481e-01,
        -1.6137e-01,  1.7782e-02,  4.3352e-02, -1.9098e-01, -4.4963e-02,
         2.1432e-01,  1.4275e-01,  5.5266e-01, -6.3516e-01,  2.3370e-01,
         3.1676e-02,  8.7186e-01, -9.1121e-02, -1.8329e-01,  4.6815e-01,
        -1.2057e-01,  4.8097e-01, -7.0653e-01,  4.6713e-03,  4.1877e-01,
        -6.2196e-02,  2.9108e-04, -7.8863e-03,  5.2587e-02,  3.4637e-01,
        -4.2772e-01,  5.3634e-01,  2.8193e-02,  2.4816e-01,  4.7977e-01,
        -2.0887e-01, -1.2205e-01,  1.5709e-03, -4.2056e-01, -1.3009e-03,
        -3.0306e-01,  5.4963e-02, -2.3276e-01,  6.1841e-01, -3.2771e-02,
         6.3384e-01,  3.6256e-02, -2.4193e-01,  1.0148e-02, -6.5338e-01,
         1.5213e-01,  9.0067e-02,  4.0155e-04, -1.8124e-01,  4.1780e-02,
        -3.7421e-02, -1.3815e-01,  9.5144e-01,  3.8597e-01, -4.8607e-02,
         2.0470e-01, -9.4395e-01,  2.5681e-01,  6.9425e-02, -9.6314e-03,
        -2.4465e-01,  6.3043e-01,  2.2519e-02,  5.5401e-04,  4.6864e-01,
        -2.1432e-01,  5.3940e-01,  2.9607e-01,  1.8892e-01, -2.2233e-01,
        -1.5041e-01,  6.5015e-01,  3.2351e-01,  2.9961e-03,  3.2108e-02,
         4.6496e-01, -1.5831e-01, -2.1360e-01,  1.2254e-01, -1.5451e-01,
        -9.9106e-02, -6.4133e-01,  9.8545e-02, -4.8964e-02, -9.1282e-02,
        -5.1380e-01, -3.1101e-01, -1.5879e-01,  6.3941e-01,  1.0695e-01,
        -5.6661e-01, -4.9584e-01,  2.5982e-01, -4.4674e-02,  7.8044e-01,
         1.1890e-01,  5.4891e-01, -8.7752e-03,  1.1075e-01, -1.4531e-01,
        -1.1849e-01, -6.8604e-02,  5.4391e-01, -7.3539e-02,  5.0097e-01,
        -2.7288e-02,  8.6610e-02,  3.8844e-01, -5.5026e-02,  5.7245e-02,
        -4.7057e-03,  1.0283e-01,  7.3576e-01, -3.1376e-01, -1.7154e-01,
         1.2800e-01,  8.4984e-01,  5.2382e-01, -1.6816e-01,  8.0808e-01,
        -4.4904e-01,  6.0777e-01, -6.4261e-01,  2.2741e-01,  1.3904e-02,
         6.8419e-02,  1.1787e-01, -1.2100e-01,  3.1964e-01,  1.3321e-01,
         2.1692e-02, -1.2558e-02, -1.2013e-01,  8.2346e-01, -6.0424e-03,
        -8.0741e-02,  8.8737e-02,  1.0000e+00,  1.2711e-01, -5.0877e-01,
         6.1398e-02, -8.3259e-02, -1.4273e-01,  2.3667e-01,  5.1263e-01,
        -5.8611e-01, -6.0547e-02,  6.6582e-01,  4.7099e-01, -3.1921e-04,
         3.1895e-02,  8.0227e-03, -1.0633e-01,  3.9896e-02,  4.2195e-01,
        -8.1213e-02, -1.1851e-01,  3.1089e-01,  5.8907e-02,  6.1397e-04,
         9.3546e-02,  5.8575e-01,  2.2937e-01, -1.5115e-01, -2.3659e-01,
         5.5872e-02, -2.1307e-01,  1.5340e-01, -1.1432e-02,  2.2660e-01,
         5.7918e-01,  2.6871e-01, -6.8558e-02,  1.5709e-01], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0922,  0.0959,  0.3645,  ...,  0.2347, -0.0171,  0.2686],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0578, -0.0195, -0.0196,  ..., -0.0142, -0.0183,  0.0011],
       device='cuda:0')}
topk_layer inside model after retrieval:  [445, 875, 1317, 235, 1279, 859, 1307, 119, 187, 662, 784, 1243, 760, 711, 276, 735, 335, 427, 80, 1395, 647, 454, 915, 1086, 356, 247, 1441, 590, 106, 167, 141, 1011, 409, 41, 786, 423, 726, 1464, 914, 817, 1494, 613, 68, 827, 1195, 1184, 835, 801, 519, 1412, 812, 943, 1400, 896, 849, 746, 764, 484, 1126, 1247, 599, 295, 752, 1230, 341, 869, 868, 935, 313, 1532, 1233, 857, 1375, 227, 447, 1297, 1006, 1132, 1271, 782, 22, 731, 425, 472, 1042, 350, 891, 144, 1071, 612, 1474, 1186, 159, 964, 381, 959, 450, 408, 683, 70, 1436, 651, 1113, 1486, 426, 1100, 667, 906, 958, 1012, 456, 1069, 1460, 822, 449, 266, 617, 1269, 121, 1429, 1282, 1530, 1357, 1220, 25, 1320, 461, 474, 917, 953, 937, 243, 971, 441, 635, 644, 614, 854, 1372, 304, 1053, 353, 1032, 79, 289, 305, 1465, 839, 601, 244, 524, 1360, 1471, 1266, 1210, 180, 268, 619, 700, 405, 532, 225, 209, 993, 252, 1358, 440, 229, 28, 207, 545, 346, 326, 421, 501, 847, 1134, 846, 241, 142, 340, 1116, 1046, 214, 640, 1361, 537, 161, 380, 366, 1522, 1211, 1481, 1000, 557, 673, 1454, 627, 430, 1252, 865, 900, 5, 58, 130, 414, 135, 1407, 577, 976, 1500, 942, 294, 981, 78, 1442, 781, 195, 314, 636, 818, 264, 1083, 1110, 204, 1489, 579, 660, 1238, 246, 838]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-0.2630,  0.0083, -0.0191,  0.0142, -0.7412, -0.1396,  0.1026, -0.1063,
        -0.7140, -0.0898,  0.4283,  0.0795,  0.0033, -0.0165,  0.3822, -0.0873,
        -0.2309, -0.0044, -0.3653, -0.7079,  0.1934,  0.3337,  0.0140, -0.0333,
        -0.1300,  0.3305, -0.1130, -0.0865,  0.0273, -0.0352,  0.1947,  0.2420,
         0.0477, -0.8077,  0.2824,  0.7175, -0.8485, -0.2755,  0.4823, -0.0096,
         0.2895,  0.0272, -0.9085, -0.5497,  0.2544, -0.8254, -0.3190, -0.0294,
        -0.0317,  0.2093, -0.4211,  0.4183,  0.2042,  0.4424, -0.0057,  1.0000,
        -0.2801,  0.3765, -0.5709,  0.1538, -0.5018, -0.0155, -0.3676, -0.0315,
        -0.8681, -0.0184, -0.5272, -0.1788,  0.6942,  0.6900,  0.4434,  0.0252,
        -0.0239, -0.6783, -0.0572, -0.6914,  0.0731,  0.4868, -0.9390,  0.1802,
        -0.3681,  0.2990, -1.1148,  0.4157,  0.0550,  0.0094, -0.6950, -0.2249,
        -0.2276, -0.9358, -0.0143, -0.1122, -0.0390,  0.4208, -0.3224, -0.1714],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.6237e-02,  7.0086e-02,  1.5289e-01,  1.1156e-01,  9.7574e-01,
        -6.2613e-01,  1.2767e-01, -4.3946e-01, -1.3801e-01,  1.2916e-01,
        -1.2124e-02,  4.0093e-01,  4.4163e-01,  2.0606e-01,  3.8869e-01,
         1.6200e-01, -4.5216e-02,  4.4844e-01, -5.0046e-01, -5.1407e-01,
         6.5582e-02,  2.1479e-01,  6.4345e-01, -4.1762e-02,  8.5719e-01,
         8.6206e-01, -2.6951e-02, -2.8461e-01, -1.8812e-01,  2.5312e-01,
         1.2315e-03, -8.4117e-01, -6.1376e-01, -6.5717e-02, -4.6042e-02,
        -2.4405e-03,  1.1480e-01,  7.1823e-02,  7.0447e-03,  1.5133e-02,
         2.4647e-01,  5.7565e-02, -3.1824e-02,  1.6643e-02,  5.1735e-01,
         1.1909e-03, -6.6917e-01,  7.0948e-02, -2.4563e-01, -4.2519e-01,
         3.6524e-01,  1.1463e-01, -1.0976e-01, -1.3186e-01, -3.2995e-01,
         3.2732e-01,  2.7510e-01, -3.3060e-01,  1.4748e-01, -4.5586e-02,
         2.5030e-02, -7.9605e-01,  4.9660e-01, -1.8930e-01, -4.6544e-01,
        -4.7806e-03, -2.9966e-01,  4.6741e-01,  2.5794e-01, -1.8034e-01,
        -1.3379e-01, -8.3788e-02, -1.4426e-01,  4.5113e-01, -1.3760e-01,
         1.7828e-01,  3.4841e-01, -6.4871e-01, -6.6265e-01,  3.1118e-01,
        -1.6078e-01,  4.1702e-02,  9.2368e-02, -1.0922e-03,  3.9919e-01,
        -6.0747e-03, -3.3797e-03, -3.0017e-01, -2.9790e-02,  3.8402e-01,
        -3.3105e-02, -5.7794e-03, -5.1916e-02,  1.1503e-01,  3.3566e-01,
        -2.9883e-01,  2.3668e-01,  1.1334e-02, -2.7307e-01, -2.1446e-02,
         5.3822e-01,  1.0365e-01,  1.8027e-02, -2.6713e-01, -5.3410e-02,
         1.3655e-01, -7.8492e-02, -7.2273e-02, -6.1290e-02,  3.5888e-01,
        -1.2939e-01,  1.6825e-01, -8.3204e-04, -1.5619e-01,  9.2945e-01,
        -2.2071e-01, -6.4073e-02,  3.3851e-01,  4.8310e-01, -2.1324e-01,
        -5.7841e-01, -1.1894e-01,  5.6586e-02,  4.4958e-01,  2.0666e-01,
        -4.2389e-01, -1.1529e-01, -2.1057e-01,  6.1187e-01,  9.5866e-02,
         1.6815e-01,  3.4471e-01,  3.2532e-01, -1.7916e-02,  5.6826e-01,
         1.5608e-01,  4.3487e-02,  8.7703e-01,  2.4791e-01,  9.2718e-02,
         2.6397e-01,  2.0243e-01,  6.6764e-01, -1.7141e-01, -2.6422e-02,
         2.2226e-01,  5.5692e-01, -1.0889e-01,  2.3757e-01, -5.3926e-03,
        -5.4804e-02, -1.5549e-01, -2.7168e-03,  3.6941e-02, -1.8294e-02,
        -1.0627e-01, -4.0721e-01, -7.4082e-01,  4.8937e-02, -4.0754e-01,
        -2.0716e-01, -3.1342e-02,  2.1061e-01, -2.6118e-01, -3.0793e-01,
         1.2309e-01, -3.7757e-01, -1.2043e-02,  4.5154e-01, -1.8111e-01,
        -9.9635e-02,  2.6839e-01,  2.7588e-01,  2.7528e-01,  1.8049e-01,
        -2.5277e-01,  1.6421e-01, -2.7796e-02,  6.6651e-02,  1.2785e-01,
        -1.3338e-01,  2.7490e-01,  9.9449e-02,  7.7394e-02, -7.4721e-02,
        -1.3671e-02,  3.2771e-01,  1.9736e-02,  2.6656e-01, -5.5675e-01,
         3.9457e-01, -9.9514e-02,  2.0779e-02,  1.1948e-01,  3.9379e-02,
         6.0977e-01,  4.1385e-02,  7.0908e-01,  9.7083e-04, -1.7671e-01,
         8.6830e-01, -1.0178e-01, -2.5451e-01,  7.2607e-01, -2.2524e-01,
         1.7009e-01,  3.0318e-01, -2.5535e-02, -2.8519e-01,  7.4481e-01,
        -1.6137e-01,  1.7782e-02,  4.3352e-02, -1.9098e-01, -4.4963e-02,
         2.1432e-01,  1.4275e-01,  5.5266e-01, -6.3516e-01,  2.3370e-01,
         3.1676e-02,  8.7186e-01, -9.1121e-02, -1.8329e-01,  4.6815e-01,
        -1.2057e-01,  4.8097e-01, -7.0653e-01,  4.6713e-03,  4.1877e-01,
        -6.2196e-02,  2.9108e-04, -7.8863e-03,  5.2587e-02,  3.4637e-01,
        -4.2772e-01,  5.3634e-01,  2.8193e-02,  2.4816e-01,  4.7977e-01,
        -2.0887e-01, -1.2205e-01,  1.5709e-03, -4.2056e-01, -1.3009e-03,
        -3.0306e-01,  5.4963e-02, -2.3276e-01,  6.1841e-01, -3.2771e-02,
         6.3384e-01,  3.6256e-02, -2.4193e-01,  1.0148e-02, -6.5338e-01,
         1.5213e-01,  9.0067e-02,  4.0155e-04, -1.8124e-01,  4.1780e-02,
        -3.7421e-02, -1.3815e-01,  9.5144e-01,  3.8597e-01, -4.8607e-02,
         2.0470e-01, -9.4395e-01,  2.5681e-01,  6.9425e-02, -9.6314e-03,
        -2.4465e-01,  6.3043e-01,  2.2519e-02,  5.5401e-04,  4.6864e-01,
        -2.1432e-01,  5.3940e-01,  2.9607e-01,  1.8892e-01, -2.2233e-01,
        -1.5041e-01,  6.5015e-01,  3.2351e-01,  2.9961e-03,  3.2108e-02,
         4.6496e-01, -1.5831e-01, -2.1360e-01,  1.2254e-01, -1.5451e-01,
        -9.9106e-02, -6.4133e-01,  9.8545e-02, -4.8964e-02, -9.1282e-02,
        -5.1380e-01, -3.1101e-01, -1.5879e-01,  6.3941e-01,  1.0695e-01,
        -5.6661e-01, -4.9584e-01,  2.5982e-01, -4.4674e-02,  7.8044e-01,
         1.1890e-01,  5.4891e-01, -8.7752e-03,  1.1075e-01, -1.4531e-01,
        -1.1849e-01, -6.8604e-02,  5.4391e-01, -7.3539e-02,  5.0097e-01,
        -2.7288e-02,  8.6610e-02,  3.8844e-01, -5.5026e-02,  5.7245e-02,
        -4.7057e-03,  1.0283e-01,  7.3576e-01, -3.1376e-01, -1.7154e-01,
         1.2800e-01,  8.4984e-01,  5.2382e-01, -1.6816e-01,  8.0808e-01,
        -4.4904e-01,  6.0777e-01, -6.4261e-01,  2.2741e-01,  1.3904e-02,
         6.8419e-02,  1.1787e-01, -1.2100e-01,  3.1964e-01,  1.3321e-01,
         2.1692e-02, -1.2558e-02, -1.2013e-01,  8.2346e-01, -6.0424e-03,
        -8.0741e-02,  8.8737e-02,  1.0000e+00,  1.2711e-01, -5.0877e-01,
         6.1398e-02, -8.3259e-02, -1.4273e-01,  2.3667e-01,  5.1263e-01,
        -5.8611e-01, -6.0547e-02,  6.6582e-01,  4.7099e-01, -3.1921e-04,
         3.1895e-02,  8.0227e-03, -1.0633e-01,  3.9896e-02,  4.2195e-01,
        -8.1213e-02, -1.1851e-01,  3.1089e-01,  5.8907e-02,  6.1397e-04,
         9.3546e-02,  5.8575e-01,  2.2937e-01, -1.5115e-01, -2.3659e-01,
         5.5872e-02, -2.1307e-01,  1.5340e-01, -1.1432e-02,  2.2660e-01,
         5.7918e-01,  2.6871e-01, -6.8558e-02,  1.5709e-01], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0922,  0.0959,  0.3645,  ...,  0.2347, -0.0171,  0.2686],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0578, -0.0195, -0.0196,  ..., -0.0142, -0.0183,  0.0011],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.3.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 0.0578, -0.0195, -0.0196,  ..., -0.0142, -0.0183,  0.0011],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-0.2630,  0.0083, -0.0191,  0.0142, -0.7412, -0.1396,  0.1026, -0.1063,
        -0.7140, -0.0898,  0.4283,  0.0795,  0.0033, -0.0165,  0.3822, -0.0873,
        -0.2309, -0.0044, -0.3653, -0.7079,  0.1934,  0.3337,  0.0140, -0.0333,
        -0.1300,  0.3305, -0.1130, -0.0865,  0.0273, -0.0352,  0.1947,  0.2420,
         0.0477, -0.8077,  0.2824,  0.7175, -0.8485, -0.2755,  0.4823, -0.0096,
         0.2895,  0.0272, -0.9085, -0.5497,  0.2544, -0.8254, -0.3190, -0.0294,
        -0.0317,  0.2093, -0.4211,  0.4183,  0.2042,  0.4424, -0.0057,  1.0000,
        -0.2801,  0.3765, -0.5709,  0.1538, -0.5018, -0.0155, -0.3676, -0.0315,
        -0.8681, -0.0184, -0.5272, -0.1788,  0.6942,  0.6900,  0.4434,  0.0252,
        -0.0239, -0.6783, -0.0572, -0.6914,  0.0731,  0.4868, -0.9390,  0.1802,
        -0.3681,  0.2990, -1.1148,  0.4157,  0.0550,  0.0094, -0.6950, -0.2249,
        -0.2276, -0.9358, -0.0143, -0.1122, -0.0390,  0.4208, -0.3224, -0.1714],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.6237e-02,  7.0086e-02,  1.5289e-01,  1.1156e-01,  9.7574e-01,
        -6.2613e-01,  1.2767e-01, -4.3946e-01, -1.3801e-01,  1.2916e-01,
        -1.2124e-02,  4.0093e-01,  4.4163e-01,  2.0606e-01,  3.8869e-01,
         1.6200e-01, -4.5216e-02,  4.4844e-01, -5.0046e-01, -5.1407e-01,
         6.5582e-02,  2.1479e-01,  6.4345e-01, -4.1762e-02,  8.5719e-01,
         8.6206e-01, -2.6951e-02, -2.8461e-01, -1.8812e-01,  2.5312e-01,
         1.2315e-03, -8.4117e-01, -6.1376e-01, -6.5717e-02, -4.6042e-02,
        -2.4405e-03,  1.1480e-01,  7.1823e-02,  7.0447e-03,  1.5133e-02,
         2.4647e-01,  5.7565e-02, -3.1824e-02,  1.6643e-02,  5.1735e-01,
         1.1909e-03, -6.6917e-01,  7.0948e-02, -2.4563e-01, -4.2519e-01,
         3.6524e-01,  1.1463e-01, -1.0976e-01, -1.3186e-01, -3.2995e-01,
         3.2732e-01,  2.7510e-01, -3.3060e-01,  1.4748e-01, -4.5586e-02,
         2.5030e-02, -7.9605e-01,  4.9660e-01, -1.8930e-01, -4.6544e-01,
        -4.7806e-03, -2.9966e-01,  4.6741e-01,  2.5794e-01, -1.8034e-01,
        -1.3379e-01, -8.3788e-02, -1.4426e-01,  4.5113e-01, -1.3760e-01,
         1.7828e-01,  3.4841e-01, -6.4871e-01, -6.6265e-01,  3.1118e-01,
        -1.6078e-01,  4.1702e-02,  9.2368e-02, -1.0922e-03,  3.9919e-01,
        -6.0747e-03, -3.3797e-03, -3.0017e-01, -2.9790e-02,  3.8402e-01,
        -3.3105e-02, -5.7794e-03, -5.1916e-02,  1.1503e-01,  3.3566e-01,
        -2.9883e-01,  2.3668e-01,  1.1334e-02, -2.7307e-01, -2.1446e-02,
         5.3822e-01,  1.0365e-01,  1.8027e-02, -2.6713e-01, -5.3410e-02,
         1.3655e-01, -7.8492e-02, -7.2273e-02, -6.1290e-02,  3.5888e-01,
        -1.2939e-01,  1.6825e-01, -8.3204e-04, -1.5619e-01,  9.2945e-01,
        -2.2071e-01, -6.4073e-02,  3.3851e-01,  4.8310e-01, -2.1324e-01,
        -5.7841e-01, -1.1894e-01,  5.6586e-02,  4.4958e-01,  2.0666e-01,
        -4.2389e-01, -1.1529e-01, -2.1057e-01,  6.1187e-01,  9.5866e-02,
         1.6815e-01,  3.4471e-01,  3.2532e-01, -1.7916e-02,  5.6826e-01,
         1.5608e-01,  4.3487e-02,  8.7703e-01,  2.4791e-01,  9.2718e-02,
         2.6397e-01,  2.0243e-01,  6.6764e-01, -1.7141e-01, -2.6422e-02,
         2.2226e-01,  5.5692e-01, -1.0889e-01,  2.3757e-01, -5.3926e-03,
        -5.4804e-02, -1.5549e-01, -2.7168e-03,  3.6941e-02, -1.8294e-02,
        -1.0627e-01, -4.0721e-01, -7.4082e-01,  4.8937e-02, -4.0754e-01,
        -2.0716e-01, -3.1342e-02,  2.1061e-01, -2.6118e-01, -3.0793e-01,
         1.2309e-01, -3.7757e-01, -1.2043e-02,  4.5154e-01, -1.8111e-01,
        -9.9635e-02,  2.6839e-01,  2.7588e-01,  2.7528e-01,  1.8049e-01,
        -2.5277e-01,  1.6421e-01, -2.7796e-02,  6.6651e-02,  1.2785e-01,
        -1.3338e-01,  2.7490e-01,  9.9449e-02,  7.7394e-02, -7.4721e-02,
        -1.3671e-02,  3.2771e-01,  1.9736e-02,  2.6656e-01, -5.5675e-01,
         3.9457e-01, -9.9514e-02,  2.0779e-02,  1.1948e-01,  3.9379e-02,
         6.0977e-01,  4.1385e-02,  7.0908e-01,  9.7083e-04, -1.7671e-01,
         8.6830e-01, -1.0178e-01, -2.5451e-01,  7.2607e-01, -2.2524e-01,
         1.7009e-01,  3.0318e-01, -2.5535e-02, -2.8519e-01,  7.4481e-01,
        -1.6137e-01,  1.7782e-02,  4.3352e-02, -1.9098e-01, -4.4963e-02,
         2.1432e-01,  1.4275e-01,  5.5266e-01, -6.3516e-01,  2.3370e-01,
         3.1676e-02,  8.7186e-01, -9.1121e-02, -1.8329e-01,  4.6815e-01,
        -1.2057e-01,  4.8097e-01, -7.0653e-01,  4.6713e-03,  4.1877e-01,
        -6.2196e-02,  2.9108e-04, -7.8863e-03,  5.2587e-02,  3.4637e-01,
        -4.2772e-01,  5.3634e-01,  2.8193e-02,  2.4816e-01,  4.7977e-01,
        -2.0887e-01, -1.2205e-01,  1.5709e-03, -4.2056e-01, -1.3009e-03,
        -3.0306e-01,  5.4963e-02, -2.3276e-01,  6.1841e-01, -3.2771e-02,
         6.3384e-01,  3.6256e-02, -2.4193e-01,  1.0148e-02, -6.5338e-01,
         1.5213e-01,  9.0067e-02,  4.0155e-04, -1.8124e-01,  4.1780e-02,
        -3.7421e-02, -1.3815e-01,  9.5144e-01,  3.8597e-01, -4.8607e-02,
         2.0470e-01, -9.4395e-01,  2.5681e-01,  6.9425e-02, -9.6314e-03,
        -2.4465e-01,  6.3043e-01,  2.2519e-02,  5.5401e-04,  4.6864e-01,
        -2.1432e-01,  5.3940e-01,  2.9607e-01,  1.8892e-01, -2.2233e-01,
        -1.5041e-01,  6.5015e-01,  3.2351e-01,  2.9961e-03,  3.2108e-02,
         4.6496e-01, -1.5831e-01, -2.1360e-01,  1.2254e-01, -1.5451e-01,
        -9.9106e-02, -6.4133e-01,  9.8545e-02, -4.8964e-02, -9.1282e-02,
        -5.1380e-01, -3.1101e-01, -1.5879e-01,  6.3941e-01,  1.0695e-01,
        -5.6661e-01, -4.9584e-01,  2.5982e-01, -4.4674e-02,  7.8044e-01,
         1.1890e-01,  5.4891e-01, -8.7752e-03,  1.1075e-01, -1.4531e-01,
        -1.1849e-01, -6.8604e-02,  5.4391e-01, -7.3539e-02,  5.0097e-01,
        -2.7288e-02,  8.6610e-02,  3.8844e-01, -5.5026e-02,  5.7245e-02,
        -4.7057e-03,  1.0283e-01,  7.3576e-01, -3.1376e-01, -1.7154e-01,
         1.2800e-01,  8.4984e-01,  5.2382e-01, -1.6816e-01,  8.0808e-01,
        -4.4904e-01,  6.0777e-01, -6.4261e-01,  2.2741e-01,  1.3904e-02,
         6.8419e-02,  1.1787e-01, -1.2100e-01,  3.1964e-01,  1.3321e-01,
         2.1692e-02, -1.2558e-02, -1.2013e-01,  8.2346e-01, -6.0424e-03,
        -8.0741e-02,  8.8737e-02,  1.0000e+00,  1.2711e-01, -5.0877e-01,
         6.1398e-02, -8.3259e-02, -1.4273e-01,  2.3667e-01,  5.1263e-01,
        -5.8611e-01, -6.0547e-02,  6.6582e-01,  4.7099e-01, -3.1921e-04,
         3.1895e-02,  8.0227e-03, -1.0633e-01,  3.9896e-02,  4.2195e-01,
        -8.1213e-02, -1.1851e-01,  3.1089e-01,  5.8907e-02,  6.1397e-04,
         9.3546e-02,  5.8575e-01,  2.2937e-01, -1.5115e-01, -2.3659e-01,
         5.5872e-02, -2.1307e-01,  1.5340e-01, -1.1432e-02,  2.2660e-01,
         5.7918e-01,  2.6871e-01, -6.8558e-02,  1.5709e-01], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0922,  0.0959,  0.3645,  ...,  0.2347, -0.0171,  0.2686],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0578, -0.0195, -0.0196,  ..., -0.0142, -0.0183,  0.0011],
       device='cuda:0')}
topk_layer inside model after retrieval:  [4076, 2572, 1544, 944, 1163, 5694, 6119, 5015, 4144, 2067, 4382, 1894, 780, 3980, 3710, 5338, 863, 287, 1439, 2101, 425, 5027, 4431, 2323, 534, 2736, 2070, 3165, 309, 5799, 3559, 2598, 1564, 2460, 3947, 3984, 4394, 4244, 5201, 43, 6088, 1641, 26, 178, 775, 2576, 4299, 2607, 5143, 3827, 3399, 5871, 2835, 2752, 5396, 5890, 4821, 3577, 3783, 4312, 1097, 4876, 5910, 3035, 1312, 3828, 2613, 911, 3383, 3187, 5054, 4537, 2213, 1083, 4651, 4817, 3734, 371, 1845, 1257, 2000, 1949, 3897, 2976, 515, 3986, 6094, 2335, 2244, 5631, 5286, 2491, 3348, 4467, 4992, 1877, 514, 4699, 2559, 1428, 5150, 5277, 3561, 4814, 5003, 750, 5222, 5272, 4979, 1960, 2342, 3181, 1753, 894, 2986, 4614, 1653, 8, 5491, 2138, 4906, 1422, 1950, 5908, 5645, 4791, 2966, 465, 2426, 2315, 1532, 5448, 2185, 4824, 1575, 3964, 1080, 1385, 1873, 2891, 5542, 2471, 985, 2679, 1631, 95, 5846, 2221, 4598, 1730, 2152, 3221, 1685, 294, 632, 2656, 3036, 4339, 61, 5048, 5698, 4097, 3226, 5318, 2261, 4186, 2917, 3429, 1988, 1346, 2089, 2515, 4395, 1749, 493, 5421, 948, 4451, 4899, 1501, 527, 3420, 5014, 3722, 5642, 63, 4227, 2013, 3466, 5349, 3662, 4044, 4908, 2693, 4702, 2950, 3362, 1335, 1438, 1709, 2270, 4110, 4482, 4476, 2069, 4134, 4376, 2026, 4502, 6125, 4163, 1622, 4118, 184, 1938, 825, 3788, 5493, 5580, 3733, 2019, 4884, 1539, 5212, 2787, 2975, 3938, 859, 4034, 4819, 2246, 4269, 2580, 1390, 4606, 2367, 2470, 3537, 4135, 1615, 2083, 1696, 2281, 415, 2585, 5407, 4566, 5839, 6032, 5124, 6021, 451, 5626, 3852, 3698, 4735, 679, 2248, 806, 2533, 1798, 5651, 105, 4181, 4358, 1833, 3718, 2996, 3606, 6084, 4352, 5795, 6070, 2420, 6041, 4000, 361, 1037, 5752, 1131, 5000, 471, 599, 4688, 881, 551, 5466, 3375, 1623, 4081, 4247, 796, 3097, 3571, 1009, 167, 2747, 3950, 2814, 1888, 2447, 1357, 3749, 3775, 706, 1349, 999, 3916, 4323, 2242, 2146, 1897, 1307, 4572, 1055, 686, 2581, 2415, 4341, 2049, 6095, 4120, 2850, 3070, 2095, 1022, 5004, 4301, 3495, 2290, 1138, 1973, 4261, 5753, 1978, 2488, 1115, 4100, 205, 3670, 6072, 4778, 4001, 363, 2001, 3991, 430, 3005, 1368, 4602, 3252, 4980, 257, 2172, 1406, 278, 4662, 4900, 2797, 2075, 4025, 5067, 4455, 827, 5876, 5312, 3145, 1853, 5109, 4962, 3448, 3067, 4222, 3839, 5005, 1519, 132, 5984, 836, 3615, 5589, 2328, 1155, 3818, 3384, 4385, 1168, 3745, 1857, 4007, 4639, 3387, 4143, 4277, 6043, 2608, 1222, 1806, 6100, 4363, 2691, 461, 1909, 5796, 4009, 477, 1220, 1778, 676, 5422, 5023, 2056, 261, 468, 2597, 4218, 4885, 2697, 1158, 730, 4357, 5791, 1104, 2785, 4266, 5811, 2962, 1887, 584, 5376, 3242, 2106, 4004, 5423, 4963, 709, 1513, 4719, 2452, 1252, 4570, 5590, 1028, 4392, 2079, 2769, 14, 516, 822, 5809, 6098, 2555, 3253, 1526, 3641, 3740, 4066, 2262, 4845, 2841, 1361, 1337, 5834, 4292, 5414, 1508, 5669, 2698, 5321, 3717, 5792, 1530, 3084, 3013, 4925, 3959, 4349, 3548, 569, 4080, 3106, 4402, 3009, 1597, 732, 4638, 2865, 2800, 2647, 4739, 248, 1306, 2494, 5477, 667, 5072, 2668, 1130, 4460, 3193, 3470, 3170, 2847, 5363, 2979, 3177, 5103, 1799, 5773, 144, 1831, 1289, 5591, 52, 1680, 3334, 3153, 3173, 5078, 5558, 5658, 1594, 4159, 1822, 5254, 646, 4896, 175, 5582, 3107, 3178, 4457, 2799, 2378, 2863, 943, 40, 6131, 2569, 383, 4954, 3279, 2611, 883, 4475, 3761, 1608, 5070, 4415, 4554, 5035, 6024, 2122, 5961, 4780, 5445, 1336, 117, 3822, 5416, 2817, 4029, 6142, 4306, 2666, 2941, 3750, 993, 5013, 5473, 3435, 5442, 4869, 4445, 5790, 4681, 3981, 4077, 2393, 3658, 774, 6118, 3367, 394, 4072, 1274, 3061, 5069, 6090, 2126, 1677, 5127, 583, 21, 3993, 165, 5504, 4407, 1911, 3027, 5055, 1913, 1000, 2030, 338, 5276, 1402, 152, 1424, 2154, 2355, 2792, 3337, 1036, 5292, 1292, 5507, 3523, 3136, 1226, 3050, 4692, 2282, 5866, 850, 3532, 4350, 5243, 5332, 3323, 2522, 3449, 3034, 508, 5451, 326, 4724, 1296, 4843, 842, 5623, 5643, 292, 5918, 3040, 4807, 426, 1996, 1930, 5283, 767, 5024, 3191, 2418, 211, 5322, 5121, 5065, 909, 526, 2305, 3126, 485, 3054, 2804, 5146, 5381, 5956, 3643, 3474, 1011, 378, 5180, 509, 2860, 2645, 3594, 5308, 2336, 4848, 728, 1181, 4682, 4882, 2317, 4052, 511, 221, 5368, 2675, 3653, 5966, 3294, 2037, 4190, 3480, 3802, 2320, 5671, 2427, 2677, 1095, 4073, 799, 3403, 1936, 5852, 1644, 4704, 5993, 2398, 3873, 45, 3502, 1363, 3998, 2538, 3563, 4637, 1854, 3454, 5929, 2051, 3227, 2345, 5625, 3853, 2694, 4544, 5239, 3361, 976, 1706, 3488, 6133, 1294, 3114, 327, 4640, 1636, 5314, 4551, 2978, 5976, 5188, 660, 5316, 555, 716, 2334, 878, 6044, 3041, 6037, 480, 3860, 1789, 5096, 5169, 1401, 443, 3062, 5256, 1086, 581, 1403, 153, 5776, 5772, 46, 3324, 3680, 4958, 2778, 4924, 6025, 3086, 4142, 1520, 4418, 3347, 3621, 3229, 4027, 2751, 3727, 134, 1453, 873, 2112, 4974, 997, 1903, 5205, 3450, 5629, 194, 1515, 2762, 5130, 3801, 1144, 1096, 5836, 159, 547, 1256, 3069, 3830, 2183, 3464, 3275, 5861, 4203, 2682, 450, 4308, 1183, 2714, 3244, 2989, 2702, 1642, 3175, 5151, 544, 505, 2331, 4653, 3388, 2245, 409, 3657, 1280, 860, 1787, 2173, 3533, 1649, 1194, 4141, 2506, 225, 3108, 4217, 1499, 5568, 3197, 2918, 5806, 4102, 1610, 1459, 3540, 5502, 5936, 1998, 5647, 3736, 5398, 5579, 5419, 4987, 5851, 1042, 1153, 2615, 1904, 2222, 2703, 41, 5190, 2625, 1451, 601, 5635, 5661, 122, 273, 1545, 5922, 888, 1712, 1968, 1112, 3649, 4265, 180, 954, 4151, 1073, 5973, 3936, 3941, 530, 3251, 2859, 2914, 1525, 634, 5401, 5051, 5452, 5409, 4556, 2983, 2005, 5108, 5406, 4298, 3469, 1231, 2885, 446, 776, 3033, 2277, 755, 2824, 1191, 3183, 5393, 3720]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
The device used will be: 
True
cuda:0
The device used will be: 
True
cuda:0
chosen_head:  {'blocks.4.layer.bias': tensor([ 0.0095,  0.0022, -0.0014,  0.0006,  0.0038,  0.0088, -0.0177,  0.0123,
        -0.0043,  0.0127], device='cuda:0'), 'blocks.4.layer.weight': tensor([[ 0.0753,  0.0216,  0.1031,  ..., -0.0119, -0.0026, -0.0461],
        [-0.0021, -0.0158, -0.0219,  ..., -0.1085,  0.1136,  0.0426],
        [-0.0567, -0.0940,  0.1015,  ..., -0.0155, -0.0621,  0.0444],
        ...,
        [-0.0177,  0.0519,  0.0008,  ...,  0.0218,  0.0824, -0.0101],
        [-0.0454,  0.0595, -0.0771,  ...,  0.0664, -0.1257, -0.1442],
        [-0.0302,  0.0074, -0.0728,  ...,  0.0438, -0.0112, -0.0909]],
       device='cuda:0')}

 Model STL10_CIFAR10_CL158728fd-dcb8-497d-8bd4-d5ba2c749836 loaded successfuly with best perf



 ----- Architecture Block BatchNorm2dSK3962(5, 5)1.0reflect, number 0 -----
- BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- AvgPool2d(kernel_size=4, stride=2, padding=1)

 ----- Architecture Block BatchNorm2dSK963842(3, 3)0.6499999761581421reflect, number 1 -----
- BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK38415362(3, 3)0.44999998807907104reflect, number 2 -----
- BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.4)
- AvgPool2d(kernel_size=4, stride=2, padding=1)

 ----- Architecture Block BatchNorm2dSK153661442(3, 3)0.25reflect, number 3 -----
- BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 4 -----
- Flatten(start_dim=1, end_dim=-1)
- Dropout(p=0.5, inplace=False)
- Linear(in_features=221184, out_features=10, bias=True)
model.heads:  [{'blocks.4.layer.bias': tensor([ 0.0016, -0.0015,  0.0004, -0.0005,  0.0020,  0.0031,  0.0054, -0.0016,
         0.0026,  0.0032], device='cuda:0'), 'blocks.4.layer.weight': tensor([[ 0.0400,  0.0110,  0.0159,  ..., -0.0202, -0.0415, -0.0304],
        [ 0.0107, -0.0007,  0.0315,  ..., -0.0158,  0.0113,  0.0158],
        [ 0.0364, -0.0160,  0.0407,  ...,  0.0141,  0.0031,  0.0035],
        ...,
        [ 0.0219,  0.0111,  0.0037,  ...,  0.0494,  0.0133, -0.0125],
        [-0.0186,  0.0274, -0.0178,  ..., -0.0034, -0.0105,  0.0038],
        [-0.0212, -0.0315, -0.0387,  ..., -0.0497,  0.0010, -0.0065]],
       device='cuda:0')}, {'blocks.4.layer.bias': tensor([ 0.0095,  0.0022, -0.0014,  0.0006,  0.0038,  0.0088, -0.0177,  0.0123,
        -0.0043,  0.0127], device='cuda:0'), 'blocks.4.layer.weight': tensor([[ 0.0753,  0.0216,  0.1031,  ..., -0.0119, -0.0026, -0.0461],
        [-0.0021, -0.0158, -0.0219,  ..., -0.1085,  0.1136,  0.0426],
        [-0.0567, -0.0940,  0.1015,  ..., -0.0155, -0.0621,  0.0444],
        ...,
        [-0.0177,  0.0519,  0.0008,  ...,  0.0218,  0.0824, -0.0101],
        [-0.0454,  0.0595, -0.0771,  ...,  0.0664, -0.1257, -0.1442],
        [-0.0302,  0.0074, -0.0728,  ...,  0.0438, -0.0112, -0.0909]],
       device='cuda:0')}]
[Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=1.4)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=221184, out_features=10, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)]
LAYER NAME:  Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
    (activation): Triangle(power=1.4)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=221184, out_features=10, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)
LAYER CHILDREN:  [BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=4, stride=2, padding=1)
  (activation): Triangle(power=1.4)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1)
), BasicBlock(
  (operations): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
  )
  (layer): Linear(in_features=221184, out_features=10, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)]
subsubl NAME:  Sequential(
  (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=4, stride=2, padding=1)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.44999998807907104, bias=False, lr_bias=0.2222, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=4, stride=2, padding=1)
subsubl NAME:  Triangle(power=1.4)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(1536, 6144, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1)
subsubl NAME:  Sequential(
  (0): Flatten(start_dim=1, end_dim=-1)
)
subsubl NAME:  Linear(in_features=221184, out_features=10, bias=True)
subsubl NAME:  Dropout(p=0.5, inplace=False)
CONFIG MODE:  unsupervised
CONFIG MODE:  supervised
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
INDICES:  5000
IMAGE SIZE: torch.Size([64, 3, 96, 96])
The device used will be: 
True
cuda:0
tot_sum:  tensor(271176.8750, device='cuda:0') 0
The device used will be: 
True
cuda:0
tot_sum:  tensor(482798.2812, device='cuda:0') 1
max_key : 1
Accuracy of the network on the 1st dataset: 30.175 %
Test loss on the 1st dataset: 26.732
results:  {'count': 3, 'R0': {'train_loss': 0.03978398069739342, 'train_acc': 98.27600121498108, 'test_loss': 3.0680172443389893, 'test_acc': 66.5625, 'convergence': 23.72207260131836, 'R1': 1, 'dataset_sup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 5000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 4, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': True, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 100, 'print_freq': 10, 'validation': False, 'continual_learning': False}, 'dataset_unsup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 100000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 32, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'unlabeled', 'nb_epoch': 1, 'print_freq': 25, 'validation': False, 'continual_learning': False}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 32, 'nb_epoch': 1, 'print_freq': 25, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 100, 'print_freq': 10, 'lr': 0.001}}}, 'model_config': {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.45-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.45, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c6144-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 6144, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.00032, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 4, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.036828478186799345, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10, 'in_channels': 221184, 'old_channels': 6144, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}, 'R1': {'train_loss': 0.7235910296440125, 'train_acc': 89.78360295295715, 'test_loss': 0.2104601263999939, 'test_acc': 74.73999786376953, 'convergence': 25.553024291992188, 'R1': 15, 'dataset_sup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 96}, 'dataset_unsup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 96}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 10, 'lr': 0.005}, 't1': {'blocks': [4], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}}, 'cl_hyper': {'training_mode': 'consecutive', 'cf_sol': True, 'head_sol': True, 'top_k': 0.15, 'topk_lock': False, 'high_lr': 0.15, 'low_lr': 1.0, 't_criteria': 'activations', 'delta_w_interval': 50.0, 'heads_basis_t': 0.8236999893188477, 'n_tasks': 2}, 'eval_0': {'test_loss': 26.73218536376953, 'test_acc': 30.174999237060547, 'convergence': 24.0014705657959, 'R1': 0, 'dataset_sup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 5000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 4, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': True, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 100, 'print_freq': 10, 'validation': False, 'continual_learning': False}, 'dataset_unsup': {'name': 'STL10', 'noise_std': 0, 'channels': 3, 'width': 96, 'height': 96, 'validation_split': 0.2, 'training_sample': 100000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 32, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'unlabeled', 'nb_epoch': 1, 'print_freq': 25, 'validation': False, 'continual_learning': False}}, 'model_name': 'STL10_CIFAR10_CL158728fd-dcb8-497d-8bd4-d5ba2c749836'}
MULTD_CL_STL10__1_CIFAR10/STL10_CIFAR10_CL158728fd-dcb8-497d-8bd4-d5ba2c749836.json


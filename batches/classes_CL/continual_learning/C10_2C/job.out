--------------- /leonardo/prod/opt/modulefiles/deeplrn/libraries ---------------
cineca-ai/3.0.0  cineca-ai/4.0.0  cineca-ai/4.1.1(default)  
cineca-ai/3.0.1  cineca-ai/4.1.0  cineca-ai/4.3.0           

Key:
(symbolic-version)  
The device used will be: 
True
cuda:0
BLOCKS:  {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3}}, 'b3': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 3, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.25, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10}, 'pool': None, 'activation': None}}
CL:  True
{'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True}
CL:  True
{'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': True}
task 1
SEED:  0
block 0, size : 96 16 16
range = 2.886751345948129
block 1, size : 384 8 8
range = 0.8505172717997146
block 2, size : 1536 4 4
range = 0.4252586358998573
range = 0.11048543456039805
The device used will be: 
True
cuda:0
{}
None
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}

 ----- Architecture Block BatchNorm2dSK3962(5, 5)1.0reflect, number 0 -----
- BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK963842(3, 3)0.6499999761581421reflect, number 1 -----
- BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.4)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK38415362(3, 3)0.25reflect, number 2 -----
- BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 3 -----
- Flatten(start_dim=1, end_dim=-1)
- Dropout(p=0.5, inplace=False)
- Linear(in_features=24576, out_features=2, bias=True)
model.heads:  []
[Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=24576, out_features=2, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)]
LAYER NAME:  Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=24576, out_features=2, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)
LAYER CHILDREN:  [BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=1.4)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
  )
  (layer): Linear(in_features=24576, out_features=2, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)]
subsubl NAME:  Sequential(
  (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=1.4)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): Flatten(start_dim=1, end_dim=-1)
)
subsubl NAME:  Linear(in_features=24576, out_features=2, bias=True)
subsubl NAME:  Dropout(p=0.5, inplace=False)
CONFIG MODE:  unsupervised

 ********** Hebbian Unsupervised learning of blocks [0] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([8, 8, 8, 8, 2, 2, 8, 8, 8, 2, 2, 2, 8, 8, 2, 8, 8, 2, 2, 8])
[2, 8]
TARGETS AFTER CLEANER:  tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1])
2000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 2000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([2, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 8, 2, 8, 2, 8, 8, 8, 2])
[2, 8]
TARGETS AFTER CLEANER:  tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0])
10000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 10000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([2, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 8, 2, 8, 2, 8, 8, 8, 2])
[2, 8]
TARGETS AFTER CLEANER:  tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0])
10000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 10000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: ToTensor()
------------------------
INDICES:  10000
IMAGE SIZE: torch.Size([10, 3, 32, 32])
SAVING FOLDER FOR UNSUP:  C10_2C_CL
DEPTH:  3
WTA IN delta_weight:  tensor([[[-1.6665e-18, -1.4907e-17, -1.8351e-20,  ..., -7.4496e-27,
          -1.7180e-24, -3.6593e-23],
         [-3.8072e-19, -9.3680e-18, -3.9993e-21,  ..., -3.3336e-27,
          -1.5565e-25, -1.5886e-23],
         [-7.9952e-21, -3.1545e-19, -2.6232e-22,  ..., -1.4338e-27,
          -6.2958e-26, -1.2402e-23],
         ...,
         [-8.5188e-22, -3.2605e-22, -1.6717e-23,  ..., -5.6414e-20,
          -1.1596e-20, -7.3742e-20],
         [-3.5636e-20, -1.4413e-20, -2.7238e-19,  ..., -9.6370e-22,
          -3.2661e-22, -6.0507e-21],
         [-2.3707e-22, -1.8813e-21, -2.7276e-21,  ..., -1.3411e-21,
          -9.5384e-22, -1.2543e-20]],

        [[-2.0360e-31, -3.5459e-31, -1.7358e-29,  ..., -6.5847e-37,
          -5.8232e-35, -5.6478e-36],
         [-2.3919e-33, -2.5037e-32, -5.5546e-31,  ..., -7.7292e-38,
          -1.1992e-36, -5.4140e-37],
         [-2.0652e-34, -5.6380e-33, -5.9309e-32,  ..., -1.0664e-37,
          -7.1909e-37, -4.9827e-37],
         ...,
         [-5.5665e-16, -1.5048e-15, -2.0843e-14,  ..., -2.0704e-26,
          -9.3062e-30, -3.7290e-31],
         [-5.1235e-15, -1.4816e-15, -1.6409e-13,  ..., -3.9028e-27,
          -1.1055e-30, -2.2248e-31],
         [-1.9259e-15, -6.7168e-17, -4.8704e-17,  ..., -2.5291e-27,
          -2.2005e-30, -5.7500e-31]],

        [[-3.3220e-33, -7.9145e-33, -7.1453e-32,  ..., -7.2540e-36,
          -2.3284e-35, -2.8901e-36],
         [-1.1210e-33, -5.8875e-33, -1.8208e-32,  ..., -4.0099e-36,
          -2.7463e-36, -1.4368e-36],
         [-1.7248e-33, -1.0745e-32, -8.7536e-33,  ..., -1.0721e-35,
          -6.5478e-36, -3.5103e-36],
         ...,
         [-7.2481e-19, -9.9871e-19, -3.3104e-17,  ..., -9.9069e-27,
          -6.1671e-30, -6.6533e-31],
         [-7.1349e-16, -9.3741e-17, -1.9272e-14,  ..., -7.8926e-28,
          -5.6468e-31, -1.1580e-31],
         [-1.3247e-15, -5.1084e-16, -3.5062e-16,  ..., -1.3292e-27,
          -9.6513e-31, -5.1433e-31]],

        ...,

        [[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         ...,
         [-2.0898e-05, -1.5045e-06, -1.9151e-07,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-3.7485e-05, -1.0360e-03, -6.5880e-04,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-1.2689e-06, -1.6626e-05, -6.2015e-05,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00]],

        [[-1.0099e-26, -3.6550e-26, -2.2451e-23,  ..., -2.3860e-27,
          -7.7343e-28, -1.2665e-28],
         [-5.8897e-28, -2.5560e-27, -5.1175e-25,  ..., -2.1334e-28,
          -2.1353e-29, -1.7967e-29],
         [-2.0812e-28, -8.8378e-28, -4.7548e-26,  ..., -2.0070e-28,
          -1.5849e-29, -1.9629e-29],
         ...,
         [-1.3116e-16, -5.3327e-16, -1.2402e-14,  ..., -1.3874e-19,
          -1.5687e-23, -3.2334e-24],
         [-1.0794e-15, -3.0557e-16, -2.7912e-13,  ..., -6.1440e-21,
          -8.4134e-25, -2.9061e-25],
         [-7.0873e-16, -8.6603e-17, -1.6969e-16,  ..., -1.6396e-20,
          -5.4725e-24, -1.8085e-24]],

        [[-8.6905e-35, -4.5126e-35, -1.5016e-38,  ..., -2.1664e-42,
          -4.3559e-41, -7.5082e-42],
         [-6.3041e-37, -8.5419e-37, -1.8484e-40,  ..., -1.1351e-43,
          -8.7581e-43, -1.2261e-42],
         [-1.0534e-39, -2.6999e-39, -6.3759e-43,  ..., -1.1210e-44,
          -6.7262e-44, -1.8637e-43],
         ...,
         [-2.7127e-14, -3.0569e-14, -1.4297e-14,  ..., -2.0268e-35,
          -5.8938e-36, -3.0603e-36],
         [-1.1374e-13, -4.4992e-13, -6.3122e-13,  ..., -8.2395e-37,
          -3.9639e-37, -7.2780e-37],
         [-2.7505e-16, -4.5989e-15, -2.5471e-15,  ..., -2.1370e-36,
          -2.5408e-36, -2.8636e-36]]], device='cuda:0')
self.avg_deltas_layer:  <class 'NoneType'>
self.top_acts_layer:  <class 'NoneType'>
LAYER_NUM:  0
shape of final_sum:  torch.Size([96, 32, 32])
FINAL SUM LENNNN  96
FINAL_SUM:  [66, 34, 64, 46, 26, 77, 30, 11, 91, 4]
acts len:  1
acts keys:  ['conv0']
acts:  {'conv0': [66, 34, 64, 46, 26, 77, 30, 11, 91, 4, 53, 92, 40, 87, 39, 43, 95, 15, 55, 90, 80, 12, 84, 68, 8, 13, 48, 56, 35, 72]}
final_sum len:  96
delta_weights INFO: 
NUM OF TRACKED COV LAYERS:  1
NUM OF TRACKED WEIGHTS CHANGES PER LAYER:  10
avg_deltas INFO:  <class 'dict'>
avg_deltas keys:  ['blocks.0.layer.weight']
avg_deltas size:  1
num of averages for 0 layer:  torch.Size([96])
################################################
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 1.49e-01	time: 00:00:23	Acc_train 0.00	Acc_test 0.00	convergence: 2.40e+01	R1: 0	Info MB:0.000e+00/SB:0.000e+00/MW:8.960e-03/SW:5.978e-01/MR:4.870e+00/SR:1.768e+00/MeD:1.375e+00/MaD:3.870e+00/MW:0.600/MAW:0.400
|       0 |       1 |       2 |      3 |       4 |       5 |       6 |       7 |       8 |       9 |      10 |      11 |      12 |      13 |      14 |      15 |         16 |      17 |      18 |      19 |      20 |      21 |      22 |     23 |      24 |         25 |      26 |      27 |      28 |      29 |
|---------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+------------+---------+---------+---------+---------+---------+---------+--------+---------+------------+---------+---------+---------+---------|
|   0.107 |   0.163 |   0.154 |   0.16 |   0.134 |   0.108 |   0.159 |   0.196 |   0.145 |   0.199 |   0.178 |   0.148 |   0.189 |   0.146 |   0.155 |   0.204 |   0.000365 |   0.156 |   0.173 |   0.187 |   0.195 |   0.163 |   0.146 |   0.2  |   0.176 |   0.000326 |   0.185 |   0.112 |   0.127 |   0.215 |
|   2.78  |   5.15  |   4.68  |   5.01 |   3.82  |   2.83  |   4.97  |   6.98  |   4.27  |   7.18  |   5.94  |   4.43  |   6.59  |   4.32  |   4.78  |   7.48  |   1        |   4.8   |   5.67  |   6.48  |   6.97  |   5.14  |   4.34  |   7.26 |   5.82  |   1        |   6.37  |   2.95  |   3.52  |   8.19  |
|   0.56  |   0.5   |   0.46  |   0.52 |   0.85  |   1.12  |   0.57  |   0.46  |   0.76  |   0.45  |   1.12  |   0.48  |   0.6   |   0.4   |   0.71  |   0.48  |  14.61     |   0.49  |   0.53  |   0.71  |   0.59  |   0.67  |   0.52  |   0.45 |   0.45  |  15.92     |   0.5   |   0.96  |   0.48  |   0.53  |
| nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan        | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan        | nan     | nan     | nan     | nan     |
| nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan        | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan        | nan     | nan     | nan     | nan     |
| nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan        | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan        | nan     | nan     | nan     | nan     |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_2C_CL/models
CONFIG MODE:  unsupervised

 ********** Hebbian Unsupervised learning of blocks [1] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([8, 8, 8, 8, 2, 2, 8, 8, 8, 2, 2, 2, 8, 8, 2, 8, 8, 2, 2, 8])
[2, 8]
TARGETS AFTER CLEANER:  tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1])
2000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 2000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([2, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 8, 2, 8, 2, 8, 8, 8, 2])
[2, 8]
TARGETS AFTER CLEANER:  tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0])
10000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 10000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([2, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 8, 2, 8, 2, 8, 8, 8, 2])
[2, 8]
TARGETS AFTER CLEANER:  tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0])
10000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 10000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: ToTensor()
------------------------
INDICES:  10000
IMAGE SIZE: torch.Size([10, 3, 32, 32])
SAVING FOLDER FOR UNSUP:  C10_2C_CL
DEPTH:  3
ACTIVATIONS SHAPE:  tensor([[-147.6109, -111.3167, -119.9216, -140.9656, -134.8070, -183.5805,
         -193.2681, -148.7300, -184.1761, -209.5443, -183.2230, -186.1317,
         -173.9512, -193.6617, -211.1500, -205.6062],
        [-114.2683,  -92.7789,  -82.6697, -101.9637, -124.0262, -150.8745,
         -161.2609, -156.8383, -131.7064, -168.7891, -142.3789, -151.4436,
         -140.5738, -190.2836, -203.2707, -168.6599],
        [ -45.8809,  -37.9299,  -59.5380,  -91.0180, -118.5154, -115.6834,
          -97.1006, -175.8634, -164.6092, -185.1016, -153.2218, -128.5664,
          -99.5257, -140.6581, -209.2085, -173.3732],
        [ -25.1347,  -16.9834,  -21.0246, -111.6336, -151.5910, -149.2860,
         -148.3552, -147.4048, -121.3230, -104.4235,  -97.7574,  -76.4402,
         -131.5134, -215.6303, -231.9965, -203.0900],
        [ -70.4999,  -54.8265,  -94.3547, -189.9057, -227.8926, -200.4292,
         -147.7691, -126.6936,  -87.9949,  -34.7336,  -52.3832, -114.5456,
         -144.9315, -157.6764, -131.9276, -142.6781],
        [-101.3286,  -97.4291, -118.9150, -241.9664, -300.9508, -205.6408,
         -171.0550,  -94.3351,  -19.7316,  -10.9063,  -48.0424, -211.1965,
         -178.5630, -125.1730, -117.8750,  -68.0817],
        [ -91.2511,  -31.8335, -102.8673, -224.1208, -256.2903, -116.9528,
          -39.4848,   24.3092,   21.3918,   18.4857,  -63.9253, -169.9178,
         -161.1797,  -44.9140,    4.0703,  -91.1434],
        [ -89.3623, -105.0062, -126.0503, -138.1311, -128.8141,  -37.4778,
           37.6364,   24.1120,  -63.6259, -154.8292, -156.1605, -194.9098,
         -112.4075,   41.9137,   63.1748,   -7.0759],
        [ -21.4560,   11.4438,   -8.9397, -109.7240,    2.1609,  109.1665,
          133.4185,   93.4427,  -15.0379, -103.1468, -195.3661, -169.7926,
          -79.1669,   26.1000,   80.9539,   57.0951],
        [  -5.6479,    4.0639,   23.3434,  -15.8570,   -4.3064,    9.9448,
          -18.4258,  -29.2024,  -79.3324,  -86.0728, -199.0566, -151.2511,
         -139.1528,   -1.5855,   24.1083,   54.8539],
        [   4.5770,   12.2306,   52.1827,  -22.9178,  -83.9183,  -89.0040,
          -65.0837,  -42.3604,  -66.4303,  -69.2840, -140.9765, -101.4327,
          -70.9032,  -12.2325,   32.7631,   43.4013],
        [  -1.8507,  -14.9571,   45.0330,  -16.5187, -130.0895,  -80.0404,
            9.0438,   60.2247,   57.9525,   26.2748,    3.9439,  -64.9605,
          -79.9767,  -10.4083,   11.0960,   10.4035],
        [ -15.2059,  -43.7491,   54.3479,   70.4975,  -51.1796,    1.8393,
           70.9136,  110.5903,   92.0654,  -33.3403,    8.7814,  -58.8494,
          -28.6189,  -31.1445,  -59.4584,  -56.6149],
        [ -61.4924,  -60.6033,   28.7106,  120.0541,   30.2966,   45.8088,
           75.6121,  151.6995,   73.5758,  -25.9138,  -72.4790,  -92.4330,
         -105.1810,  -53.9230,  -83.9796,  -78.5408],
        [  -7.0001,  -25.8380,   47.9380,  100.3752,  132.1918,  137.4135,
          147.3231,   94.4074,   78.8790,   -2.5178,  -14.9922,  -32.5476,
          -44.1818,  -14.5100,  -16.6903,  -41.4311],
        [ -96.7189, -105.1852,  -31.6982,   11.8683,  -35.2001,  -12.1937,
           33.0970,   50.8232,   32.2687,  -46.9168,  -81.0643, -102.2337,
          -88.1120,  -90.1187,  -68.6569,  -51.8037]], device='cuda:0')
self.avg_deltas_layer:  <class 'NoneType'>
self.top_acts_layer:  <class 'NoneType'>
LAYER_NUM:  1
shape of final_sum:  torch.Size([384, 16, 16])
FINAL SUM LENNNN  384
FINAL_SUM:  [59, 247, 93, 204, 116, 61, 19, 175, 180, 147]
acts len:  2
acts keys:  ['conv0', 'conv1']
acts:  {'conv0': [66, 34, 64, 46, 26, 77, 30, 11, 91, 4, 53, 92, 40, 87, 39, 43, 95, 15, 55, 90, 80, 12, 84, 68, 8, 13, 48, 56, 35, 72], 'conv1': [59, 247, 93, 204, 116, 61, 19, 175, 180, 147, 257, 351, 63, 8, 254, 66, 122, 71, 324, 370, 290, 46, 64, 282, 243, 294, 259, 353, 54, 120, 49, 150, 110, 355, 158, 286, 361, 41, 78, 235, 300, 210, 26, 383, 121, 112, 269, 115, 27, 291, 260, 293, 36, 58, 202, 225, 133, 359, 223, 179, 32, 270, 10, 275, 296, 234, 215, 125, 65, 99, 349, 48, 136, 214, 251, 207, 310, 356, 174, 201, 145, 77, 279, 315, 330, 131, 68, 40, 205, 30, 246, 362, 369, 113, 171, 74, 301, 107, 352, 164, 184, 348, 80, 194, 87, 335, 242, 332, 372, 98, 43, 5, 334, 126, 289, 161]}
final_sum len:  384
delta_weights INFO: 
NUM OF TRACKED COV LAYERS:  1
NUM OF TRACKED WEIGHTS CHANGES PER LAYER:  10
avg_deltas INFO:  <class 'dict'>
avg_deltas keys:  ['blocks.0.layer.weight', 'blocks.1.layer.weight']
avg_deltas size:  2
num of averages for 1 layer:  torch.Size([96])
################################################
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 1.49e-01	time: 00:00:39	Acc_train 0.00	Acc_test 0.00	convergence: 2.40e+01	R1: 0	Info MB:0.000e+00/SB:0.000e+00/MW:1.636e-03/SW:3.409e-01/MR:9.843e+00/SR:1.888e+00/MeD:1.497e+00/MaD:6.517e+00/MW:0.567/MAW:0.433
|        0 |        1 |        2 |        3 |        4 |        5 |        6 |        7 |        8 |        9 |       10 |       11 |       12 |       13 |       14 |      15 |      16 |       17 |       18 |       19 |       20 |       21 |       22 |       23 |       24 |       25 |       26 |      27 |       28 |       29 |
|----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------|
|   0.0147 |   0.0138 |   0.0154 |   0.0144 |   0.0118 |   0.0139 |   0.0156 |   0.0128 |   0.0142 |   0.0152 |   0.0135 |   0.0161 |   0.0141 |   0.0165 |   0.0175 |   0.015 |   0.017 |   0.0149 |   0.0109 |   0.0127 |   0.0156 |   0.0159 |   0.0155 |   0.0161 |   0.0142 |   0.0185 |   0.0154 |   0.012 |   0.0147 |   0.0139 |
|   9.62   |   8.6    |  10.46   |   9.35   |   6.6    |   8.74   |  10.71   |   7.57   |   9.09   |  10.28   |   8.3    |  11.36   |   8.95   |  11.84   |  13.29   |   9.94  |  12.62  |   9.88   |   5.72   |   7.47   |  10.79   |  11.13   |  10.61   |  11.35   |   9.09   |  14.66   |  10.45   |   6.73  |   9.62   |   8.76   |
|   0.21   |   0.19   |   0.25   |   0.18   |   1.02   |   0.22   |   0.2    |   0.24   |   0.2    |   0.18   |   0.23   |   0.2    |   0.37   |   0.2    |   0.19   |   0.19  |   0.13  |   0.28   |   0.37   |   0.25   |   0.17   |   0.3    |   0.53   |   0.24   |   0.59   |   0.51   |   0.16   |   0.28  |   0.18   |   0.23   |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_2C_CL/models
CONFIG MODE:  unsupervised

 ********** Hebbian Unsupervised learning of blocks [2] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([8, 8, 8, 8, 2, 2, 8, 8, 8, 2, 2, 2, 8, 8, 2, 8, 8, 2, 2, 8])
[2, 8]
TARGETS AFTER CLEANER:  tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1])
2000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 2000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([2, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 8, 2, 8, 2, 8, 8, 8, 2])
[2, 8]
TARGETS AFTER CLEANER:  tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0])
10000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 10000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([2, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 8, 2, 8, 2, 8, 8, 8, 2])
[2, 8]
TARGETS AFTER CLEANER:  tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0])
10000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 10000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: ToTensor()
------------------------
INDICES:  10000
IMAGE SIZE: torch.Size([10, 3, 32, 32])
SAVING FOLDER FOR UNSUP:  C10_2C_CL
DEPTH:  3
ACTIVATIONS SHAPE:  tensor([[-2.4083e+01, -8.7742e+00, -1.3695e+01, -2.5477e+01, -2.3857e+01,
         -3.8626e+01, -3.8981e+01, -2.0533e+01, -3.2097e+01, -3.9321e+01,
         -2.7880e+01, -3.0552e+01, -2.7912e+01, -3.7056e+01, -4.7021e+01,
         -4.5004e+01],
        [-1.5808e+01, -7.4012e+00, -4.4577e+00, -1.2894e+01, -2.1786e+01,
         -2.6413e+01, -3.0521e+01, -2.8644e+01, -1.5444e+01, -2.5265e+01,
         -1.4795e+01, -2.0456e+01, -1.8753e+01, -3.9251e+01, -4.3390e+01,
         -3.1510e+01],
        [ 1.6886e+01,  1.7499e+01,  6.7392e+00, -9.0163e+00, -2.1132e+01,
         -1.3248e+01, -6.8114e-01, -2.9549e+01, -2.6276e+01, -3.0249e+01,
         -1.8494e+01, -9.6621e+00, -1.0229e+00, -2.0703e+01, -4.6458e+01,
         -3.0964e+01],
        [ 2.4682e+01,  2.2750e+01,  2.0888e+01, -2.1001e+01, -3.6019e+01,
         -2.8098e+01, -1.8995e+01, -1.9004e+01, -1.0982e+01, -2.3180e+00,
          3.5704e+00,  1.1008e+01, -1.4394e+01, -5.1777e+01, -5.6924e+01,
         -4.3191e+01],
        [ 3.0862e+00,  8.2039e+00, -1.0513e+01, -5.2096e+01, -6.3860e+01,
         -4.6917e+01, -1.7449e+01, -1.0832e+01,  3.5903e+00,  2.6519e+01,
          2.3257e+01, -1.9913e+00, -1.9653e+01, -3.0173e+01, -1.9468e+01,
         -2.2977e+01],
        [-9.6150e+00, -7.0586e+00, -1.6614e+01, -6.6231e+01, -8.7187e+01,
         -4.1156e+01, -2.3841e+01,  4.7801e+00,  2.9400e+01,  3.6798e+01,
          2.3572e+01, -4.0946e+01, -3.2146e+01, -1.3082e+01, -1.5187e+01,
          6.0507e+00],
        [-5.1894e+00,  1.7497e+01, -8.5346e+00, -5.7348e+01, -6.7499e+01,
         -5.5767e+00,  2.6670e+01,  5.0801e+01,  4.3544e+01,  4.2437e+01,
          1.1057e+01, -2.7938e+01, -2.4151e+01,  1.9344e+01,  3.3994e+01,
         -8.6563e+00],
        [-4.0135e+00, -1.2149e+01, -1.9408e+01, -2.3831e+01, -1.7714e+01,
          2.3306e+01,  5.5637e+01,  4.3260e+01,  3.8832e+00, -3.1083e+01,
         -2.8148e+01, -3.5573e+01,  3.5983e-01,  5.9491e+01,  6.2795e+01,
          2.5706e+01],
        [ 2.2801e+01,  3.4627e+01,  2.8928e+01, -1.1450e+01,  3.3179e+01,
          8.2809e+01,  9.0026e+01,  7.0496e+01,  2.7146e+01, -1.0602e+01,
         -4.1110e+01, -2.3570e+01,  1.5399e+01,  5.3813e+01,  6.9860e+01,
          5.2865e+01],
        [ 2.5909e+01,  2.8375e+01,  3.7760e+01,  1.9368e+01,  2.7923e+01,
          3.5121e+01,  2.6089e+01,  1.7109e+01, -2.8111e+00, -8.8342e+00,
         -5.0687e+01, -2.2492e+01, -1.0236e+01,  4.0949e+01,  4.4138e+01,
          4.8514e+01],
        [ 2.7120e+01,  2.5155e+01,  4.1186e+01,  1.2552e+01, -1.2883e+01,
         -1.1312e+01, -2.9955e+00,  8.7467e+00, -4.5094e-01,  1.2457e+00,
         -2.6468e+01, -7.9263e+00,  6.6758e+00,  3.1449e+01,  4.2010e+01,
          3.7175e+01],
        [ 2.2037e+01,  1.1898e+01,  3.9993e+01,  1.1666e+01, -3.6303e+01,
         -1.3917e+01,  2.3703e+01,  4.2930e+01,  4.5170e+01,  3.4980e+01,
          3.1941e+01,  5.4740e+00, -4.0222e-02,  2.4621e+01,  2.6228e+01,
          1.6770e+01],
        [ 1.1508e+01, -4.1494e+00,  3.7182e+01,  4.1641e+01, -8.4132e+00,
          1.1400e+01,  4.1945e+01,  6.2464e+01,  5.2324e+01,  5.5006e+00,
          2.5782e+01,  1.3181e+00,  1.1684e+01,  5.4869e+00, -1.2702e+01,
         -1.5393e+01],
        [-1.2086e+01, -1.2217e+01,  2.5281e+01,  5.5841e+01,  1.9740e+01,
          2.3383e+01,  3.3816e+01,  6.8372e+01,  3.7482e+01, -2.7311e+00,
         -1.3841e+01, -1.9345e+01, -2.7994e+01, -1.3735e+01, -2.9251e+01,
         -2.8495e+01],
        [ 5.6010e+00, -4.1934e+00,  2.6404e+01,  4.6221e+01,  5.4482e+01,
          5.7518e+01,  5.7832e+01,  3.8820e+01,  2.8225e+01,  9.1283e-01,
         -9.7618e-01, -5.2699e+00, -1.2396e+01, -1.9334e+00, -3.6828e+00,
         -1.3912e+01],
        [-3.2996e+01, -3.7470e+01, -8.4010e+00,  6.5146e+00, -1.3357e+01,
         -4.6474e+00,  1.1681e+01,  1.9901e+01,  1.0401e+01, -2.1769e+01,
         -3.2437e+01, -3.6271e+01, -3.1268e+01, -3.2836e+01, -2.4851e+01,
         -1.7601e+01]], device='cuda:0')
self.avg_deltas_layer:  <class 'NoneType'>
self.top_acts_layer:  <class 'NoneType'>
LAYER_NUM:  2
shape of final_sum:  torch.Size([1536, 8, 8])
FINAL SUM LENNNN  1536
FINAL_SUM:  [268, 1458, 660, 196, 272, 128, 737, 1323, 1261, 1500]
acts len:  3
acts keys:  ['conv0', 'conv1', 'conv2']
acts:  {'conv0': [66, 34, 64, 46, 26, 77, 30, 11, 91, 4, 53, 92, 40, 87, 39, 43, 95, 15, 55, 90, 80, 12, 84, 68, 8, 13, 48, 56, 35, 72], 'conv1': [59, 247, 93, 204, 116, 61, 19, 175, 180, 147, 257, 351, 63, 8, 254, 66, 122, 71, 324, 370, 290, 46, 64, 282, 243, 294, 259, 353, 54, 120, 49, 150, 110, 355, 158, 286, 361, 41, 78, 235, 300, 210, 26, 383, 121, 112, 269, 115, 27, 291, 260, 293, 36, 58, 202, 225, 133, 359, 223, 179, 32, 270, 10, 275, 296, 234, 215, 125, 65, 99, 349, 48, 136, 214, 251, 207, 310, 356, 174, 201, 145, 77, 279, 315, 330, 131, 68, 40, 205, 30, 246, 362, 369, 113, 171, 74, 301, 107, 352, 164, 184, 348, 80, 194, 87, 335, 242, 332, 372, 98, 43, 5, 334, 126, 289, 161], 'conv2': [268, 1458, 660, 196, 272, 128, 737, 1323, 1261, 1500, 269, 320, 205, 108, 1474, 1371, 1430, 97, 303, 388, 294, 301, 482, 325, 75, 1389, 1149, 1160, 524, 1530, 1195, 730, 1425, 315, 888, 193, 870, 1052, 1412, 637, 627, 451, 998, 252, 1039, 1251, 914, 856, 540, 1066, 941, 350, 1270, 1189, 332, 552, 48, 640, 1490, 1174, 1005, 1483, 766, 539, 995, 1222, 296, 1155, 507, 1527, 693, 111, 68, 927, 1239, 1305, 1501, 175, 994, 65, 1094, 138, 1461, 1383, 855, 1015, 71, 760, 1154, 1053, 519, 876, 553, 431, 1076, 1106, 1151, 1113, 650, 1146, 1464, 1184, 335, 408, 1086, 466, 1135, 323, 98, 919, 747, 49, 1050, 1082, 873, 525, 463, 882, 1213, 1144, 967, 377, 869, 1273, 764, 986, 1284, 1110, 868, 1156, 165, 848, 453, 245, 701, 707, 931, 417, 238, 1060, 676, 662, 956, 99, 503, 996, 130, 939, 727, 436, 392, 985, 1141, 840, 1221, 1030, 617, 1218, 1398, 112, 567, 573, 905, 1196, 1382, 969, 91, 168, 88, 79, 772, 1257, 1470, 4, 763, 804, 1532, 611, 1495, 633, 576, 866, 454, 1016, 1359, 607, 801, 471, 965, 7, 1404, 1471, 1453, 746, 847, 1211, 696, 736, 605, 137, 381, 226, 1435, 703, 634, 830, 971, 1137, 844, 117, 933, 1510, 401, 811, 30, 842, 548, 545, 217, 81, 459, 973, 1409, 54, 1004, 710, 77, 1164, 427, 1394, 468, 314, 1165, 867, 691, 1319, 581, 533, 997, 749, 1299, 326, 674, 1024, 1021, 704, 889, 242, 1378, 212, 1192, 1044, 1157, 1393, 595, 759, 1349, 832, 843, 1008, 318, 1303, 281, 910, 113, 39, 94, 278, 1462, 55, 1434, 430, 1099, 1057, 355, 191, 638, 475, 593, 1441, 959, 689, 1230, 934, 606, 423, 1059, 810, 267, 1476, 903, 369, 84, 601, 188, 76, 817, 887, 1313, 514, 926, 1065, 1456, 647, 1274, 1426, 512, 851, 786, 27, 13, 483, 26, 556, 713, 1013, 415, 745, 1091, 827, 506, 449, 489, 1140, 915, 683, 1201, 1431, 174, 334, 461, 1432, 59, 171, 652, 599, 1185, 232, 529, 1111, 58, 1244, 1328, 992, 1200, 107, 906, 488, 1054, 486, 283, 813, 1001, 654, 705, 946, 536, 1073, 342, 944, 862, 479, 1345, 517, 1167, 160, 699, 1267, 1092, 838, 1459, 960, 1505, 1291, 823, 211, 115, 1324, 1107, 1209, 1183, 1529, 1332, 1405, 43, 1236, 1365, 661, 682, 134, 275, 247, 829, 1231, 177, 189, 724, 474, 1019, 1194, 1219, 1263, 1000, 60, 614, 1132, 295, 132, 715, 1387, 1411, 163, 987, 1519, 814, 1122, 496, 184, 988, 557, 167, 1482, 765, 964, 859, 25, 1009, 426, 1096, 775, 518, 47, 837, 375, 604, 279, 1002, 564, 950, 812, 1514, 446, 467, 119, 1334, 549, 476, 221, 1285, 1258, 1497, 1124, 337, 1136, 118, 257, 1214, 880, 494, 993, 626, 1465]}
final_sum len:  1536
delta_weights INFO: 
NUM OF TRACKED COV LAYERS:  1
NUM OF TRACKED WEIGHTS CHANGES PER LAYER:  10
avg_deltas INFO:  <class 'dict'>
avg_deltas keys:  ['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight']
avg_deltas size:  3
num of averages for 2 layer:  torch.Size([96])
################################################
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 4.51e-02	time: 00:00:56	Acc_train 0.00	Acc_test 0.00	convergence: 2.05e+01	R1: 0	Info MB:0.000e+00/SB:0.000e+00/MW:1.223e-03/SW:3.694e-01/MR:2.149e+01/SR:3.104e+00/MeD:2.459e+00/MaD:2.034e+01/MW:0.449/MAW:0.551
|        0 |        1 |        2 |        3 |        4 |        5 |        6 |        7 |        8 |        9 |      10 |       11 |       12 |       13 |       14 |       15 |       16 |       17 |       18 |       19 |       20 |       21 |       22 |      23 |       24 |       25 |       26 |      27 |       28 |       29 |
|----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------+---------+----------+----------|
|   0.0466 |   0.0463 |   0.0468 |   0.0515 |   0.0455 |   0.0412 |   0.0481 |   0.0398 |   0.0479 |   0.0485 |   0.045 |   0.0457 |   0.0458 |   0.0362 |   0.0449 |   0.0433 |   0.0491 |   0.0488 |   0.0434 |   0.0418 |   0.0486 |   0.0451 |   0.0353 |   0.044 |   0.0454 |   0.0371 |   0.0442 |   0.042 |   0.0423 |   0.0454 |
|  22.73   |  22.48   |  22.89   |  27.49   |  21.7    |  18      |  24.17   |  16.83   |  23.96   |  24.52   |  21.28  |  21.9    |  21.97   |  14.1    |  21.16   |  19.74   |  25.07   |  24.86   |  19.85   |  18.5    |  24.58   |  21.35   |  13.44   |  20.32  |  21.61   |  14.76   |  20.52   |  18.67  |  18.89   |  21.61   |
|   0.03   |   0.04   |   0.07   |   0.02   |   0.03   |   0.05   |   0.07   |   0.06   |   0.06   |   0.02   |   0.06  |   0.07   |   0.17   |   0.08   |   0.22   |   0.04   |   0.03   |   0.05   |   0.1    |   0.04   |   0.04   |   0.05   |   0.51   |   0.04  |   0.05   |   0.08   |   0.05   |   0.05  |   0.05   |   0.03   |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan     | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan     | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan     | nan      | nan      |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_2C_CL/models
CONFIG MODE:  supervised

 ********** Supervised learning of blocks [3] **********
SAVING FOLDER FOR SUP:  C10_2C_CL
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([8, 8, 8, 8, 2, 2, 8, 8, 8, 2, 2, 2, 8, 8, 2, 8, 8, 2, 2, 8])
[2, 8]
TARGETS AFTER CLEANER:  tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1])
2000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 2000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([2, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 8, 2, 8, 2, 8, 8, 8, 2])
[2, 8]
TARGETS AFTER CLEANER:  tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0])
10000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 10000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([2, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 8, 2, 8, 2, 8, 8, 8, 2])
[2, 8]
TARGETS AFTER CLEANER:  tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0])
10000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 10000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: ToTensor()
------------------------
INDICES:  10000
IMAGE SIZE: torch.Size([64, 3, 32, 32])
Epoch: [1/50]	lr: 1.00e-03	time: 00:01:10	Loss_train 0.11127	Acc_train 90.17	/	Loss_test 0.00158	Acc_test 95.05
BLOCKS IN SUP:  [3]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_2C_CL/models
Epoch: [10/50]	lr: 1.00e-03	time: 00:01:14	Loss_train 0.04606	Acc_train 95.50	/	Loss_test 0.00368	Acc_test 96.60
BLOCKS IN SUP:  [3]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_2C_CL/models
Epoch: [20/50]	lr: 2.50e-04	time: 00:01:19	Loss_train 0.02111	Acc_train 97.93	/	Loss_test 0.00271	Acc_test 96.90
BLOCKS IN SUP:  [3]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_2C_CL/models
Epoch: [30/50]	lr: 1.25e-04	time: 00:01:23	Loss_train 0.01298	Acc_train 98.47	/	Loss_test 0.00306	Acc_test 97.05
BLOCKS IN SUP:  [3]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_2C_CL/models
Epoch: [40/50]	lr: 3.13e-05	time: 00:01:28	Loss_train 0.01003	Acc_train 98.74	/	Loss_test 0.00266	Acc_test 96.75
BLOCKS IN SUP:  [3]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_2C_CL/models
Epoch: [50/50]	lr: 7.81e-06	time: 00:01:33	Loss_train 0.00901	Acc_train 98.76	/	Loss_test 0.00271	Acc_test 96.95
new_head:  {'blocks.3.layer.bias': tensor([-0.0069,  0.0053], device='cuda:0'), 'blocks.3.layer.weight': tensor([[-0.0522, -0.0461, -0.0302,  ..., -0.0106,  0.0262,  0.0255],
        [ 0.0549,  0.0405,  0.0358,  ...,  0.0098, -0.0252, -0.0243]],
       device='cuda:0')}
BLOCKS IN SUP:  [3]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_2C_CL/models
RESULT:  {'train_loss': 0.00900841411203146, 'train_acc': 98.76099824905396, 'test_loss': 0.0027053889352828264, 'test_acc': 96.94999694824219, 'convergence': 20.49114418029785, 'R1': 0, 'dataset_sup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [2, 8]}, 'dataset_unsup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [2, 8]}, 'train_config': {'t0': {'blocks': [0], 'mode': 'unsupervised', 'lr': 0.08, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't2': {'blocks': [1], 'mode': 'unsupervised', 'lr': 0.005, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't4': {'blocks': [2], 'mode': 'unsupervised', 'lr': 0.01, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't6': {'blocks': [3], 'mode': 'supervised', 'lr': 0.001, 'nb_epoch': 50, 'batch_size': 64, 'print_freq': 10}}}
IN R1:  {'R1': {'train_loss': 0.00900841411203146, 'train_acc': 98.76099824905396, 'test_loss': 0.0027053889352828264, 'test_acc': 96.94999694824219, 'convergence': 20.49114418029785, 'R1': 0, 'dataset_sup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [2, 8]}, 'dataset_unsup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [2, 8]}, 'train_config': {'t0': {'blocks': [0], 'mode': 'unsupervised', 'lr': 0.08, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't2': {'blocks': [1], 'mode': 'unsupervised', 'lr': 0.005, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't4': {'blocks': [2], 'mode': 'unsupervised', 'lr': 0.01, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't6': {'blocks': [3], 'mode': 'supervised', 'lr': 0.001, 'nb_epoch': 50, 'batch_size': 64, 'print_freq': 10}}}}
task 2
SEED:  0
block 0, size : 96 16 16
range = 2.886751345948129
block 1, size : 384 8 8
range = 0.8505172717997146
block 2, size : 1536 4 4
range = 0.4252586358998573
range = 0.11048543456039805
The device used will be: 
True
cuda:0
PARAMSSSS IN LOAD:  {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 3, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.11048543456039805, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 24576, 'old_channels': 1536, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
{'blocks.0.layer.weight': tensor([-0.2606,  0.0505,  0.0065,  0.0500, -0.4829, -0.7314,  0.1846, -0.0528,
        -0.5379,  0.0973,  0.2685,  0.2378,  0.0055,  0.0389,  0.5440, -0.1001,
        -0.7422,  0.0115, -0.3550, -0.3719,  0.1806,  0.4561, -0.0467,  0.0127,
         0.1501,  0.5667, -0.0418, -0.0647,  0.1056, -0.0224,  0.6200,  0.2558,
        -0.1757, -0.4928,  0.4918,  0.5312, -0.4740, -0.0290,  0.4834,  0.0347,
         0.2610,  0.1206,  0.4482, -0.1765,  0.2571, -0.4176, -0.1331,  0.1997,
        -0.0133,  0.2561, -0.4534,  0.4708,  0.2213,  0.3884, -0.0229,  0.5611,
        -0.2288,  0.4049, -0.2618,  1.0000, -0.0773, -0.0100, -0.2465,  0.0501,
        -0.5407, -0.1922, -0.4086, -0.0290,  0.8514,  0.7919,  0.4537,  0.1745,
        -0.0163, -0.5093, -0.0266, -0.3269,  0.0684,  0.8498, -0.3915,  0.3193,
        -0.2578,  0.5616, -0.5112,  0.3557,  0.0422, -0.1164, -0.4549, -0.1695,
        -0.0658, -0.5466,  0.0194, -0.1355, -0.1833,  0.7314, -0.1808,  0.1999],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 1.1086e-01,  2.4740e-01,  2.1142e-01,  1.8533e-01,  1.0000e+00,
        -2.4486e-01,  1.7224e-01, -1.6643e-01, -4.7170e-02,  1.6415e-01,
        -9.0541e-02,  3.6487e-01,  3.9468e-01,  1.6159e-01,  2.3916e-01,
         2.6098e-01, -5.8345e-02,  3.0289e-01, -3.7336e-01, -1.7680e-01,
         1.4372e-01,  3.9937e-01,  5.1323e-01,  3.2445e-01,  6.4896e-01,
         4.6541e-01,  8.2880e-02, -1.8355e-01, -2.8294e-02,  3.6397e-01,
         8.5858e-02, -3.9802e-01, -3.6412e-01, -4.0852e-03,  4.6038e-02,
         5.0356e-02,  1.7945e-01,  1.6515e-01,  2.3257e-02,  6.5172e-02,
         2.1937e-01,  5.8496e-02,  2.0309e-02,  9.7503e-02,  4.6656e-01,
         1.2086e-01, -2.1108e-01,  9.0696e-02, -1.4872e-01, -1.3139e-01,
         2.7344e-01,  1.7554e-01,  1.1969e-04, -2.1791e-01, -1.1560e-01,
         2.3769e-01,  2.4463e-01, -2.9401e-01,  9.2733e-02, -3.3476e-02,
         2.6119e-02, -8.3464e-01,  3.9190e-01, -5.5683e-01, -1.7015e-01,
         2.8756e-02, -1.7259e-01,  5.1442e-01,  9.1053e-02, -4.2485e-02,
        -6.3815e-02, -1.8732e-02, -9.4582e-03,  3.7752e-01, -5.5547e-02,
         2.1988e-01,  2.8745e-01, -6.1958e-01, -6.7434e-01,  3.7139e-01,
        -2.1952e-02,  1.0092e-01,  1.6814e-01,  6.7174e-02,  3.5194e-01,
         1.3358e-02,  1.2266e-01, -2.2910e-01,  4.2100e-02,  3.5711e-01,
         3.5628e-02,  3.6778e-02,  5.5298e-02,  1.1870e-01,  3.7912e-01,
        -9.8480e-02,  2.5308e-01,  1.2243e-01, -5.1136e-01,  7.4246e-02,
         4.9631e-01,  1.2386e-01,  8.6452e-02, -4.1891e-02,  6.2827e-02,
         1.7809e-01,  4.3996e-02, -1.0686e-01,  1.8727e-02,  1.8474e-01,
        -2.2345e-02,  2.3828e-01, -2.4028e-02, -1.9049e-01,  5.8583e-01,
        -2.1972e-01, -2.0745e-01,  2.3517e-01,  3.5561e-01,  3.2942e-03,
        -4.0840e-01, -2.0369e-01,  1.0059e-01,  3.2348e-01,  1.6676e-01,
        -3.0735e-01,  1.0033e-02, -2.7357e-01,  4.3431e-01,  5.7649e-02,
         6.7436e-02,  2.5514e-01,  1.8184e-01,  1.0036e-01,  5.0188e-01,
         1.0504e-01,  1.2985e-01,  7.5867e-01,  1.6487e-01,  1.4952e-01,
         2.7279e-01,  3.5778e-01,  5.3363e-01, -1.1444e-01,  3.5962e-01,
         2.9919e-01,  4.7214e-01, -9.2768e-02,  1.4541e-01,  3.2617e-02,
         3.5817e-02, -1.8370e-01,  2.3734e-01,  8.5995e-02,  5.7268e-02,
        -2.4864e-01, -6.7736e-01, -9.9681e-01,  7.9677e-02, -3.9666e-01,
        -6.5680e-02,  4.0999e-02,  1.1148e-01, -2.1504e-01, -1.2695e-01,
         1.8918e-01, -2.4432e-01,  5.4377e-02,  2.3343e-01, -1.0238e-01,
        -8.1209e-03,  1.5525e-01,  3.9994e-01,  2.6159e-01,  8.1843e-02,
        -1.2379e-01,  2.2969e-01,  2.9985e-01,  1.3888e-01,  1.3976e-01,
        -2.4191e-01,  1.9996e-01,  1.7690e-01,  1.2561e-01, -2.9998e-02,
         2.2919e-01,  2.5009e-01,  1.0045e-01,  1.0910e-01, -3.4196e-01,
         2.8495e-01, -6.6957e-04,  7.2688e-02,  1.4028e-01,  6.5126e-02,
         5.6045e-01,  7.2012e-02,  7.5561e-01,  9.7227e-02, -1.1213e-01,
         6.9063e-01, -1.0474e-01, -1.9146e-01,  5.4544e-01, -5.0815e-02,
         2.1484e-01,  3.8794e-01, -7.1196e-03, -7.1195e-02,  4.7154e-01,
        -1.7976e-01,  1.1872e-01,  1.3759e-01, -7.9641e-02,  2.5809e-02,
         1.6360e-01,  1.5587e-01,  4.2591e-01, -3.2788e-01,  1.5400e-01,
         1.4775e-01,  5.7269e-01, -2.8863e-01, -2.8713e-02,  3.0842e-01,
        -1.2680e-01,  4.1977e-01, -4.4211e-01,  1.1987e-01,  1.9325e-01,
         2.9040e-02, -4.4702e-02,  8.9200e-02,  4.0339e-02,  3.0002e-01,
        -1.3856e-01,  3.3699e-01,  1.8006e-01,  2.4005e-01,  4.6215e-01,
        -1.1135e-01, -2.3276e-02,  6.1541e-02, -2.4906e-01,  4.4864e-02,
        -1.0878e-01,  2.4200e-01, -1.1002e-01,  3.0220e-01, -1.0063e-01,
         3.9775e-01,  9.9684e-02, -1.5703e-02,  7.8637e-02, -3.4390e-01,
         2.3897e-01,  2.0583e-01,  8.6565e-02, -1.3592e-01,  9.6210e-02,
         8.2008e-02, -2.0528e-02,  7.7394e-01,  2.3314e-01, -6.6420e-03,
         1.5997e-01, -9.3872e-01,  1.6728e-01,  1.9095e-01,  5.4530e-02,
        -1.0135e-01,  4.6801e-01,  1.6055e-01,  7.5962e-02,  4.5915e-01,
        -1.4832e-02,  2.1497e-01,  1.8498e-01,  1.7030e-01, -9.6181e-02,
        -1.4853e-03,  4.8395e-01,  2.1884e-01,  1.0374e-01,  1.2780e-01,
         3.1286e-01, -5.7568e-03, -1.6425e-01,  1.3483e-01, -1.5453e-01,
        -7.8186e-02, -4.2919e-01,  1.2827e-01,  4.6626e-02, -1.4070e-01,
        -2.7186e-01, -2.8594e-01,  8.5205e-03,  4.0137e-01,  1.2164e-01,
        -3.2787e-01, -1.9716e-01,  1.7233e-01, -1.4538e-01,  8.8679e-01,
         1.8249e-01,  2.7961e-01,  1.0475e-01,  1.8294e-01,  3.4992e-02,
        -1.6352e-02, -3.4037e-02,  3.4197e-01,  2.3297e-02,  5.9565e-01,
         4.9512e-02,  1.3883e-01,  5.1175e-01,  7.5808e-03,  7.6817e-02,
         4.7093e-02,  1.0463e-01,  3.8010e-01, -1.3054e-01, -3.4631e-01,
         1.6879e-01,  6.2130e-01,  3.7944e-01, -2.8699e-02,  7.0300e-01,
        -8.7695e-01,  4.2510e-01, -3.3448e-01,  1.6093e-01,  8.9184e-02,
         1.3515e-01,  1.1972e-01,  2.6845e-02,  2.1691e-01,  1.4196e-01,
         1.5040e-01,  1.1860e-01, -3.3149e-02,  6.4861e-01, -2.3293e-01,
        -6.4567e-02,  1.3641e-01,  7.8886e-01,  1.5253e-01, -5.7879e-01,
         1.3735e-01, -1.7906e-02, -5.3863e-02,  2.2321e-01,  3.5803e-01,
        -4.2313e-01, -5.4497e-02,  5.4049e-01,  2.6532e-01, -1.1695e-01,
         1.0930e-01,  1.3069e-01,  2.7457e-02,  8.6839e-02,  3.2047e-01,
        -1.9655e-01,  8.6167e-03,  2.7562e-01,  1.1702e-01, -8.2972e-02,
         8.9696e-02,  5.0252e-01,  2.3068e-01, -1.6575e-01, -9.5619e-02,
         1.3927e-01, -6.0768e-02,  1.5948e-01, -5.7752e-03,  1.6758e-01,
         4.0897e-01,  1.9953e-01,  4.4397e-02,  9.0457e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0963, -0.0130,  0.2054,  ...,  0.4212, -0.2074,  0.4419],
       device='cuda:0')}
None
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
The device used will be: 
True
cuda:0
The device used will be: 
True
cuda:0
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([1, 1, 5, 5, 5, 5, 5, 1, 5, 5, 1, 1, 1, 5, 5, 1, 1, 1, 1, 5])
[1, 5]
TARGETS AFTER CLEANER:  tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1])
2000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 2000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([1, 1, 5, 1, 5, 1, 1, 1, 5, 5, 1, 1, 1, 1, 5, 1, 1, 5, 5, 1])
[1, 5]
TARGETS AFTER CLEANER:  tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0])
10000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 10000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([1, 1, 5, 1, 5, 1, 1, 1, 5, 5, 1, 1, 1, 1, 5, 1, 1, 5, 5, 1])
[1, 5]
TARGETS AFTER CLEANER:  tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0])
10000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 10000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: ToTensor()
------------------------
INDICES:  10000
IMAGE SIZE: torch.Size([64, 3, 32, 32])
test_acc:  tensor(0.1340)
chosen_head:  {'blocks.3.layer.bias': tensor([-0.0069,  0.0053], device='cuda:0'), 'blocks.3.layer.weight': tensor([[-0.0522, -0.0461, -0.0302,  ..., -0.0106,  0.0262,  0.0255],
        [ 0.0549,  0.0405,  0.0358,  ...,  0.0098, -0.0252, -0.0243]],
       device='cuda:0')}

 Model C10_2C_CL loaded successfuly with best perf



 ----- Architecture Block BatchNorm2dSK3962(5, 5)1.0reflect, number 0 -----
- BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK963842(3, 3)0.6499999761581421reflect, number 1 -----
- BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.4)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK38415362(3, 3)0.25reflect, number 2 -----
- BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 3 -----
- Flatten(start_dim=1, end_dim=-1)
- Dropout(p=0.5, inplace=False)
- Linear(in_features=24576, out_features=2, bias=True)
model.heads:  [{'blocks.3.layer.bias': tensor([-0.0069,  0.0053], device='cuda:0'), 'blocks.3.layer.weight': tensor([[-0.0522, -0.0461, -0.0302,  ..., -0.0106,  0.0262,  0.0255],
        [ 0.0549,  0.0405,  0.0358,  ...,  0.0098, -0.0252, -0.0243]],
       device='cuda:0')}]
[Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=24576, out_features=2, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)]
LAYER NAME:  Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=24576, out_features=2, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)
LAYER CHILDREN:  [BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=1.4)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
  )
  (layer): Linear(in_features=24576, out_features=2, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)]
subsubl NAME:  Sequential(
  (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=1.4)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): Flatten(start_dim=1, end_dim=-1)
)
subsubl NAME:  Linear(in_features=24576, out_features=2, bias=True)
subsubl NAME:  Dropout(p=0.5, inplace=False)
CONFIG MODE:  unsupervised

 ********** Hebbian Unsupervised learning of blocks [0] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([1, 1, 5, 5, 5, 5, 5, 1, 5, 5, 1, 1, 1, 5, 5, 1, 1, 1, 1, 5])
[1, 5]
TARGETS AFTER CLEANER:  tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1])
2000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 2000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([1, 1, 5, 1, 5, 1, 1, 1, 5, 5, 1, 1, 1, 1, 5, 1, 1, 5, 5, 1])
[1, 5]
TARGETS AFTER CLEANER:  tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0])
10000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 10000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([1, 1, 5, 1, 5, 1, 1, 1, 5, 5, 1, 1, 1, 1, 5, 1, 1, 5, 5, 1])
[1, 5]
TARGETS AFTER CLEANER:  tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0])
10000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 10000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: ToTensor()
------------------------
INDICES:  10000
IMAGE SIZE: torch.Size([10, 3, 32, 32])
SAVING FOLDER FOR UNSUP:  C10_2C_CL
DEPTH:  3
ACTIVATIONS SHAPE:  tensor([[-1266.2107, -2055.8262, -2048.4536, -2347.4060, -2448.4666, -2272.0952,
         -2121.7148, -2081.8052, -1979.3174, -1860.9055, -1662.0231, -1717.3813,
         -1785.6973, -1608.7961, -1467.7534,  -872.7908],
        [ -681.1580, -1416.6493, -1319.0415, -1891.8735, -2053.4253, -2175.4880,
         -2220.2231, -2182.4199, -2131.6467, -1851.4956, -1782.7880, -1903.8055,
         -1930.2002, -1558.2550, -1231.7412,  -560.1807],
        [ -387.4550, -1258.2727, -1172.1167, -1605.4058, -1661.0295, -1504.6182,
         -1580.9902, -1856.3552, -2021.7743, -1813.9182, -1878.0874, -1892.9485,
         -2014.0433, -1677.9592, -1223.3228,  -620.8954],
        [ -232.2264, -1141.5459,  -985.0170, -1539.5344, -1535.9479, -1190.4646,
         -1070.3174, -1055.1736, -1245.7815, -1689.5969, -1818.0149, -1809.9227,
         -2016.9377, -1790.8201, -1120.8154,  -739.1327],
        [  -17.9829, -1068.6648,  -716.8049,  -971.5148,  -646.3825,  -327.2279,
          -160.6902,   -83.1235,  -453.7715,  -941.8304, -1349.1506, -1403.7830,
         -1410.4668, -1243.1963,  -865.1556,  -521.6400],
        [   98.8012, -1128.6677,  -703.6259,  -782.8115,  -165.7197,   435.8796,
           784.8551,   542.0389,   305.4051,  -367.9562,  -932.0326, -1164.2085,
         -1228.8414,  -842.5928,  -654.1138,  -254.0504],
        [  213.4045,  -939.2935,  -317.6172,  -269.3818,   397.6328,   889.0721,
          1106.0077,   734.7063,   516.7740,    19.4976,  -572.9084,  -692.2161,
          -583.9127,  -669.0995,  -821.6339,  -306.9262],
        [  733.7848,  -520.0397,    79.3365,    79.6022,   599.5392,  1122.9656,
          1360.5759,  1102.2157,   617.1684,   159.0864,  -213.3143,    60.8783,
            89.6253,  -511.5678, -1010.2529,  -357.7139],
        [ 1197.6689,  -104.7122,   728.3251,   746.9031,  1136.5752,  1499.3323,
          1549.3611,  1171.5166,   876.1538,   514.4779,   259.2342,    99.6853,
           347.5338,  -363.5142,  -863.2699,  -231.1531],
        [ 1607.2222,   348.3969,  1386.8196,  1348.9614,  1631.0712,  1360.9344,
          1537.9932,  1229.7430,   848.2992,   701.8331,   517.2208,   105.0760,
           102.0732,  -690.2877,  -815.6710,  -286.5810],
        [ 1826.8866,   759.3286,  1600.1494,  1337.3038,  1406.6726,  1182.0686,
          1402.0181,  1157.5781,   687.5959,   540.9822,   212.6527,    90.7928,
           -43.0938,  -789.8929,  -843.2047,  -281.9636],
        [ 2021.0464,  1044.5439,  1860.5546,  1666.5833,  1417.1704,  1097.2729,
          1157.2073,  1207.6101,  1176.9480,   860.2919,   319.0980,   347.4573,
            17.4278,  -524.3002,  -484.6743,    72.2932],
        [ 2177.8477,  1356.3782,  2145.6130,  1670.7750,  1361.9594,  1221.6890,
          1181.3843,  1474.0115,  1452.4752,  1260.1213,   630.4816,   532.5825,
           332.7473,  -129.5904,  -255.8130,   549.9933],
        [ 1867.3660,   854.2000,  2019.5593,  1685.3655,  1435.6656,  1370.1195,
          1333.1078,  1583.9873,  1706.1011,  1304.5878,   777.7843,   591.2932,
           617.3492,   120.2034,    34.6521,   709.2778],
        [ 1667.0294,   561.7030,  1814.4810,  1623.1721,  1503.0887,  1033.6111,
          1058.9021,  1464.1586,  1501.5177,  1244.6780,   655.5187,   578.8912,
           428.1436,   -80.8137,   147.9868,   483.4330],
        [  510.8629,  -730.4922,   311.2966,   128.7492,     7.0029,  -520.3292,
          -554.0315,  -190.1431,   -20.1650,  -218.1238,  -640.7657,  -563.2781,
          -786.2045, -1079.6689,  -655.1610,  -325.7944]], device='cuda:0')
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.top_acts_layer:  <class 'list'>
topk_mask  tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1.,
        0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        1., 1., 1., 0., 0., 1.])
threshold_mask  tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0],
       dtype=torch.uint8)
final_mask tensor([1., 1., 1., 0., 2., 1., 0., 1., 2., 0., 0., 1., 1., 1., 0., 1., 1., 0.,
        1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1.,
        1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 2., 0., 1., 1., 1., 0., 0., 1.,
        1., 2., 2., 0., 0., 0., 0., 1., 1., 0., 1., 1., 2., 0., 1., 0., 0., 0.,
        1., 1., 0., 1., 0., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 2., 1., 0., 0., 1.])
topk_mask  tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1.,
        0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        1., 1., 1., 0., 0., 1.])
not_topk_mask  tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0],
       dtype=torch.uint8)
lower_lr_mask  tensor([-0.0000, -0.0000, -0.0000, -0.0000, -0.7000, -0.0000, -0.0000, -0.0000,
        -0.7000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,
        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,
        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,
        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,
        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.7000, -0.0000,
        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.7000,
        -0.7000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,
        -0.0000, -0.0000, -0.7000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,
        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,
        -0.7000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,
        -0.0000, -0.0000, -0.0000, -0.7000, -0.0000, -0.0000, -0.0000, -0.0000])
higher_lr_mask  tensor([0.3000, 0.3000, 0.3000, 0.3000, 0.0000, 0.3000, 0.3000, 0.3000, 0.0000,
        0.3000, 0.3000, 0.0000, 0.0000, 0.0000, 0.3000, 0.0000, 0.3000, 0.3000,
        0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.0000,
        0.3000, 0.3000, 0.3000, 0.0000, 0.3000, 0.3000, 0.3000, 0.0000, 0.0000,
        0.3000, 0.3000, 0.3000, 0.0000, 0.0000, 0.3000, 0.3000, 0.0000, 0.3000,
        0.3000, 0.0000, 0.3000, 0.0000, 0.3000, 0.3000, 0.3000, 0.3000, 0.0000,
        0.3000, 0.0000, 0.0000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000,
        0.3000, 0.0000, 0.3000, 0.0000, 0.3000, 0.0000, 0.3000, 0.3000, 0.3000,
        0.0000, 0.3000, 0.3000, 0.3000, 0.3000, 0.0000, 0.3000, 0.3000, 0.0000,
        0.3000, 0.3000, 0.3000, 0.0000, 0.3000, 0.3000, 0.0000, 0.3000, 0.3000,
        0.0000, 0.0000, 0.0000, 0.3000, 0.3000, 0.0000])
lr_mask  tensor([[[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[0.3000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]]])
torch.Size([96, 3, 5, 5])
self.lr tensor([[[0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066]],

        [[0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066]],

        [[0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066]]], device='cuda:0')
LAYER_NUM:  0
shape of final_sum:  torch.Size([96, 32, 32])
FINAL SUM LENNNN  96
FINAL_SUM:  [64, 66, 43, 25, 29, 67, 26, 46, 75, 84]
acts len:  3
acts keys:  ['conv0', 'conv1', 'conv2']
acts:  {'conv0': [64, 66, 43, 25, 29, 67, 26, 46, 75, 84, 87, 5, 41, 91, 4, 85, 27, 72, 11, 92, 15, 19, 57, 50, 82, 55, 30, 77, 61, 17], 'conv1': [59, 247, 93, 204, 116, 61, 19, 175, 180, 147, 257, 351, 63, 8, 254, 66, 122, 71, 324, 370, 290, 46, 64, 282, 243, 294, 259, 353, 54, 120, 49, 150, 110, 355, 158, 286, 361, 41, 78, 235, 300, 210, 26, 383, 121, 112, 269, 115, 27, 291, 260, 293, 36, 58, 202, 225, 133, 359, 223, 179, 32, 270, 10, 275, 296, 234, 215, 125, 65, 99, 349, 48, 136, 214, 251, 207, 310, 356, 174, 201, 145, 77, 279, 315, 330, 131, 68, 40, 205, 30, 246, 362, 369, 113, 171, 74, 301, 107, 352, 164, 184, 348, 80, 194, 87, 335, 242, 332, 372, 98, 43, 5, 334, 126, 289, 161], 'conv2': [268, 1458, 660, 196, 272, 128, 737, 1323, 1261, 1500, 269, 320, 205, 108, 1474, 1371, 1430, 97, 303, 388, 294, 301, 482, 325, 75, 1389, 1149, 1160, 524, 1530, 1195, 730, 1425, 315, 888, 193, 870, 1052, 1412, 637, 627, 451, 998, 252, 1039, 1251, 914, 856, 540, 1066, 941, 350, 1270, 1189, 332, 552, 48, 640, 1490, 1174, 1005, 1483, 766, 539, 995, 1222, 296, 1155, 507, 1527, 693, 111, 68, 927, 1239, 1305, 1501, 175, 994, 65, 1094, 138, 1461, 1383, 855, 1015, 71, 760, 1154, 1053, 519, 876, 553, 431, 1076, 1106, 1151, 1113, 650, 1146, 1464, 1184, 335, 408, 1086, 466, 1135, 323, 98, 919, 747, 49, 1050, 1082, 873, 525, 463, 882, 1213, 1144, 967, 377, 869, 1273, 764, 986, 1284, 1110, 868, 1156, 165, 848, 453, 245, 701, 707, 931, 417, 238, 1060, 676, 662, 956, 99, 503, 996, 130, 939, 727, 436, 392, 985, 1141, 840, 1221, 1030, 617, 1218, 1398, 112, 567, 573, 905, 1196, 1382, 969, 91, 168, 88, 79, 772, 1257, 1470, 4, 763, 804, 1532, 611, 1495, 633, 576, 866, 454, 1016, 1359, 607, 801, 471, 965, 7, 1404, 1471, 1453, 746, 847, 1211, 696, 736, 605, 137, 381, 226, 1435, 703, 634, 830, 971, 1137, 844, 117, 933, 1510, 401, 811, 30, 842, 548, 545, 217, 81, 459, 973, 1409, 54, 1004, 710, 77, 1164, 427, 1394, 468, 314, 1165, 867, 691, 1319, 581, 533, 997, 749, 1299, 326, 674, 1024, 1021, 704, 889, 242, 1378, 212, 1192, 1044, 1157, 1393, 595, 759, 1349, 832, 843, 1008, 318, 1303, 281, 910, 113, 39, 94, 278, 1462, 55, 1434, 430, 1099, 1057, 355, 191, 638, 475, 593, 1441, 959, 689, 1230, 934, 606, 423, 1059, 810, 267, 1476, 903, 369, 84, 601, 188, 76, 817, 887, 1313, 514, 926, 1065, 1456, 647, 1274, 1426, 512, 851, 786, 27, 13, 483, 26, 556, 713, 1013, 415, 745, 1091, 827, 506, 449, 489, 1140, 915, 683, 1201, 1431, 174, 334, 461, 1432, 59, 171, 652, 599, 1185, 232, 529, 1111, 58, 1244, 1328, 992, 1200, 107, 906, 488, 1054, 486, 283, 813, 1001, 654, 705, 946, 536, 1073, 342, 944, 862, 479, 1345, 517, 1167, 160, 699, 1267, 1092, 838, 1459, 960, 1505, 1291, 823, 211, 115, 1324, 1107, 1209, 1183, 1529, 1332, 1405, 43, 1236, 1365, 661, 682, 134, 275, 247, 829, 1231, 177, 189, 724, 474, 1019, 1194, 1219, 1263, 1000, 60, 614, 1132, 295, 132, 715, 1387, 1411, 163, 987, 1519, 814, 1122, 496, 184, 988, 557, 167, 1482, 765, 964, 859, 25, 1009, 426, 1096, 775, 518, 47, 837, 375, 604, 279, 1002, 564, 950, 812, 1514, 446, 467, 119, 1334, 549, 476, 221, 1285, 1258, 1497, 1124, 337, 1136, 118, 257, 1214, 880, 494, 993, 626, 1465]}
final_sum len:  96
delta_weights INFO: 
NUM OF TRACKED COV LAYERS:  1
NUM OF TRACKED WEIGHTS CHANGES PER LAYER:  10
avg_deltas INFO:  <class 'dict'>
avg_deltas keys:  ['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight']
avg_deltas size:  3
num of averages for 0 layer:  torch.Size([96])
################################################
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 1.37e-01	time: 00:00:17	Acc_train 0.00	Acc_test 0.00	convergence: 2.40e+01	R1: 0	Info MB:0.000e+00/SB:0.000e+00/MW:9.713e-03/SW:5.518e-01/MR:4.441e+00/SR:1.775e+00/MeD:1.320e+00/MaD:4.176e+00/MW:0.588/MAW:0.412
|         0 |       1 |       2 |       3 |       4 |        5 |       6 |       7 |       8 |      9 |      10 |      11 |      12 |      13 |      14 |      15 |        16 |      17 |      18 |      19 |      20 |      21 |      22 |      23 |      24 |         25 |     26 |        27 |     28 |     29 |
|-----------+---------+---------+---------+---------+----------+---------+---------+---------+--------+---------+---------+---------+---------+---------+---------+-----------+---------+---------+---------+---------+---------+---------+---------+---------+------------+--------+-----------+--------+--------|
|   0.00134 |   0.159 |   0.143 |   0.148 |   0.143 |   0.0864 |   0.141 |   0.163 |   0.134 |   0.17 |   0.162 |   0.145 |   0.144 |   0.147 |   0.134 |   0.177 |   0.00031 |   0.186 |   0.165 |   0.169 |   0.185 |   0.148 |   0.161 |   0.189 |   0.207 |   0.000351 |   0.14 |   0.00371 |   0.15 |   0.16 |
|   1       |   4.93  |   4.21  |   4.41  |   4.18  |   2.17   |   4.1   |   5.16  |   3.79  |   5.51 |   5.13  |   4.28  |   4.23  |   4.35  |   3.79  |   5.89  |   1       |   6.42  |   5.26  |   5.45  |   6.34  |   4.41  |   5.04  |   6.6   |   7.66  |   1        |   4.08 |   1       |   4.51 |   5    |
|   1.02    |   0.61  |   0.59  |   0.57  |   0.59  |   0.51   |   0.38  |   0.53  |   0.68  |   0.54 |   0.57  |   0.56  |   0.45  |   0.62  |   0.45  |   0.54  |  19.38    |   0.59  |   0.46  |   0.47  |   0.57  |   0.43  |   0.5   |   0.55  |   0.58  |  24.81     |   0.36 |   0.92    |   0.53 |   0.46 |
| nan       | nan     | nan     | nan     | nan     | nan      | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan       | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan        | nan    | nan       | nan    | nan    |
| nan       | nan     | nan     | nan     | nan     | nan      | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan       | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan        | nan    | nan       | nan    | nan    |
| nan       | nan     | nan     | nan     | nan     | nan      | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan       | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan        | nan    | nan       | nan    | nan    |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_2C_CL/models
CONFIG MODE:  unsupervised

 ********** Hebbian Unsupervised learning of blocks [1] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([1, 1, 5, 5, 5, 5, 5, 1, 5, 5, 1, 1, 1, 5, 5, 1, 1, 1, 1, 5])
[1, 5]
TARGETS AFTER CLEANER:  tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1])
2000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 2000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([1, 1, 5, 1, 5, 1, 1, 1, 5, 5, 1, 1, 1, 1, 5, 1, 1, 5, 5, 1])
[1, 5]
TARGETS AFTER CLEANER:  tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0])
10000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 10000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([1, 1, 5, 1, 5, 1, 1, 1, 5, 5, 1, 1, 1, 1, 5, 1, 1, 5, 5, 1])
[1, 5]
TARGETS AFTER CLEANER:  tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0])
10000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 10000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: ToTensor()
------------------------
INDICES:  10000
IMAGE SIZE: torch.Size([10, 3, 32, 32])
SAVING FOLDER FOR UNSUP:  C10_2C_CL
DEPTH:  3
ACTIVATIONS SHAPE:  tensor([[ 3.3186e+01,  7.1401e+00,  2.4177e+01,  2.7733e+01, -1.6222e+01,
         -4.6280e+01, -5.1313e+01, -3.7251e+01, -1.0939e+01, -2.4549e+01,
         -2.3999e+01, -3.1680e+01, -4.3082e+01, -4.5783e+01, -2.5517e+01,
         -1.1385e+01],
        [ 2.8241e+01,  6.8505e+00,  4.1941e+01,  4.0377e+01,  6.8850e+00,
         -4.6469e+01, -5.3562e+01, -2.8836e+01, -2.9393e+01, -3.4018e+01,
         -4.6621e+01, -5.3713e+01, -6.5741e+01, -5.8639e+01, -2.1338e+01,
         -1.3937e+01],
        [ 3.8119e+00, -1.2480e+01,  1.1344e+01,  2.0996e+01, -1.8063e+01,
         -6.0115e+01, -5.4500e+01, -4.1478e+01, -2.9207e+01, -2.4947e+01,
         -2.8641e+01, -3.3211e+01, -5.5986e+01, -4.8142e+01, -2.4746e+01,
         -1.5994e+01],
        [ 3.0359e+00, -2.2870e+01, -1.0371e+01,  1.1571e+00, -3.1590e+01,
         -5.2410e+01, -3.4963e+01, -2.8598e+01, -1.3525e+01,  1.1918e+01,
          1.3379e+01, -1.5722e+01, -4.0729e+01, -1.8849e+01, -1.5032e+01,
         -1.6666e+01],
        [ 1.4538e+01, -1.8826e+01, -1.2615e+00, -1.3465e+01, -3.3423e+01,
         -4.3251e+01, -2.9387e+01, -4.9694e+01, -1.2249e+01,  1.9647e+01,
          4.1390e+01,  1.6277e+01,  1.3021e-01, -2.8553e+01, -1.4301e+01,
         -1.0173e+01],
        [ 1.6360e+01,  7.5726e+00, -2.3855e+00, -3.7874e+01, -1.9450e+01,
         -2.5574e+01, -4.4991e+01, -5.1666e+01, -2.6856e+01,  2.1844e+01,
          3.5496e+01,  1.8306e+01,  2.2674e+01, -1.6328e+01, -3.0044e+01,
          1.6787e+00],
        [ 1.0111e+01,  4.1889e+00, -1.1991e+01, -1.9599e+01, -6.0055e+00,
         -1.9934e+01, -4.6131e+01, -5.8699e+01, -3.8396e+01,  7.3312e+00,
          2.5464e+01,  1.4361e+01,  6.7421e+00, -2.2737e+01, -5.2542e+01,
         -7.7385e+00],
        [ 8.8661e+00,  1.6764e+00,  2.2449e+01, -2.6032e+00, -1.1623e+01,
         -1.0133e+01, -3.3554e+01, -5.5578e+01, -2.3240e+01, -6.8488e+00,
          5.0033e+00,  3.5030e+00,  2.2677e+00, -1.5833e+01, -1.2821e+01,
          1.3539e+01],
        [ 1.6077e+01,  1.1282e+01,  3.9943e+01,  1.6634e+01, -6.4063e+00,
         -1.3545e+01, -2.7366e+01, -4.7388e+01, -4.4725e+01, -6.5003e+00,
          9.4417e+00,  1.0960e+01, -5.7043e+00,  1.3538e+01, -6.3887e+00,
          2.5965e+01],
        [ 1.1275e+01,  9.9036e+00,  1.9943e+01, -9.8828e+00, -2.0923e+01,
          2.1059e+00, -3.5874e+00, -3.0276e+01, -2.5710e+01,  1.2532e+01,
          2.3187e+01,  1.5301e-01, -2.9305e+01, -2.1291e+01, -2.4865e+01,
          2.8126e+01],
        [ 3.5385e+01,  2.3686e+01,  2.1468e+01, -8.7038e+00, -1.4526e+01,
          6.5026e+00,  5.9328e+00, -1.2225e+01, -7.2803e+00,  1.4326e+01,
          3.5366e+01, -8.1854e-01, -1.7453e+01, -6.7815e+00, -1.2397e+01,
          4.4602e+01],
        [ 3.7515e+01,  4.4589e+01,  5.1405e+01,  8.6620e+00, -4.9276e-03,
          2.7009e+00,  5.9286e+00, -1.7450e+01, -3.5528e+00,  1.2816e+01,
          3.2227e+01,  1.4175e+01,  5.8754e+00,  1.9653e+01,  1.3573e+01,
          3.9476e+01],
        [ 3.2330e+01,  5.0197e+01,  6.0940e+01,  3.1246e+01, -8.7880e+00,
         -1.2028e+01, -9.8599e+00, -2.8973e+01, -1.0390e+01, -5.0964e-01,
          2.0105e+01,  2.2685e+01,  1.2449e+01,  1.8625e+01,  1.3794e+01,
          2.9686e+01],
        [-7.1622e+00,  2.1064e+01,  4.4858e+01,  1.3522e+01, -2.4984e+01,
         -4.8069e+01, -3.1217e+01, -4.1469e+01, -7.1034e+00,  8.6648e-01,
          1.2663e+01,  1.7059e+01,  1.1361e+01,  2.2091e+00, -5.7182e+00,
          2.3209e+01],
        [-2.4107e+01,  9.2293e+00,  3.4520e+01,  2.2438e+01,  1.7245e+01,
         -2.2576e+01, -1.9120e+01, -8.8856e+00,  2.1303e+01,  3.0616e+01,
          2.7875e+01,  3.7964e+01,  4.2997e+01,  1.4174e+01, -1.8243e+01,
          2.7300e+00],
        [-3.5578e+01, -1.3441e+01,  1.3633e+01,  1.8224e+00, -2.1654e+01,
         -5.0869e+01, -4.0660e+01, -2.9869e+01, -2.1417e+00,  3.9620e+00,
          5.5424e+00,  2.3376e+01,  2.2445e+01, -4.6082e+00, -2.4024e+01,
         -3.0944e+01]], device='cuda:0')
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.top_acts_layer:  <class 'list'>
topk_mask  tensor([0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1.,
        0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1.,
        1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1.])
threshold_mask  tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0],
       dtype=torch.uint8)
final_mask tensor([0., 0., 0., 0., 0., 2., 0., 1., 2., 0., 2., 0., 0., 0., 0., 0., 1., 0.,
        1., 2., 0., 0., 0., 0., 0., 1., 1., 2., 1., 0., 1., 1., 2., 1., 0., 1.,
        1., 0., 0., 0., 1., 1., 0., 2., 0., 0., 2., 0., 2., 2., 0., 1., 0., 1.,
        2., 0., 0., 1., 1., 2., 0., 2., 0., 2., 2., 1., 2., 0., 1., 1., 1., 2.,
        0., 0., 2., 0., 0., 2., 2., 0., 1., 0., 0., 0., 0., 1., 0., 2., 0., 0.,
        1., 0., 0., 1., 0., 1., 0., 0., 2., 1., 0., 1., 1., 1., 0., 0., 0., 2.,
        0., 0., 2., 0., 2., 2., 0., 2., 2., 0., 0., 1., 2., 2., 2., 0., 0., 2.,
        1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 2., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 2.,
        1., 1., 2., 0., 1., 0., 0., 1., 1., 2., 0., 0., 1., 2., 0., 0., 0., 1.,
        2., 0., 0., 0., 2., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 2., 2., 0., 2., 1., 0., 2., 1., 0., 2., 0., 0., 1., 2., 1.,
        0., 0., 1., 0., 0., 0., 1., 2., 0., 2., 0., 1., 0., 0., 1., 1., 0., 1.,
        1., 2., 0., 0., 0., 0., 1., 1., 1., 2., 0., 1., 1., 2., 0., 1., 0., 1.,
        1., 0., 2., 0., 1., 1., 1., 1., 2., 1., 0., 1., 1., 0., 1., 0., 0., 1.,
        2., 0., 0., 1., 0., 2., 0., 0., 0., 2., 1., 0., 1., 0., 0., 0., 2., 1.,
        0., 2., 2., 2., 0., 1., 2., 1., 2., 0., 0., 1., 2., 2., 0., 1., 0., 0.,
        0., 0., 0., 0., 2., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        2., 0., 0., 0., 1., 0., 2., 0., 2., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        1., 0., 1., 1., 0., 0., 1., 2., 0., 2., 2., 1., 0., 2., 2., 0., 0., 2.,
        0., 1., 1., 0., 0., 1., 1., 0., 0., 2., 1., 0., 1., 1., 1., 0., 1., 0.,
        1., 0., 0., 0., 0., 1.])
topk_mask  tensor([0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1.,
        0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1.,
        1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1.])
not_topk_mask  tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],
       dtype=torch.uint8)
lower_lr_mask  tensor([-0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.7000, -0.0000, -0.0000,
        -0.7000, -0.0000, -0.7000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,
        -0.0000, -0.0000, -0.0000, -0.7000, -0.0000, -0.0000, -0.0000, -0.0000,
        -0.0000, -0.0000, -0.0000, -0.7000, -0.0000, -0.0000, -0.0000, -0.0000,
        -0.7000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,
        -0.0000, -0.0000, -0.0000, -0.7000, -0.0000, -0.0000, -0.7000, -0.0000,
        -0.7000, -0.7000, -0.0000, -0.0000, -0.0000, -0.0000, -0.7000, -0.0000,
        -0.0000, -0.0000, -0.0000, -0.7000, -0.0000, -0.7000, -0.0000, -0.7000,
        -0.7000, -0.0000, -0.7000, -0.0000, -0.0000, -0.0000, -0.0000, -0.7000,
        -0.0000, -0.0000, -0.7000, -0.0000, -0.0000, -0.7000, -0.7000, -0.0000,
        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.7000,
        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,
        -0.0000, -0.0000, -0.7000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,
        -0.0000, -0.0000, -0.0000, -0.7000, -0.0000, -0.0000, -0.7000, -0.0000,
        -0.7000, -0.7000, -0.0000, -0.7000, -0.7000, -0.0000, -0.0000, -0.0000,
        -0.7000, -0.7000, -0.7000, -0.0000, -0.0000, -0.7000, -0.0000, -0.0000,
        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,
        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,
        -0.0000, -0.0000, -0.0000, -0.7000, -0.0000, -0.0000, -0.0000, -0.0000,
        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,
        -0.0000, -0.7000, -0.0000, -0.0000, -0.7000, -0.0000, -0.0000, -0.0000,
        -0.0000, -0.0000, -0.0000, -0.7000, -0.0000, -0.0000, -0.0000, -0.7000,
        -0.0000, -0.0000, -0.0000, -0.0000, -0.7000, -0.0000, -0.0000, -0.0000,
        -0.7000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,
        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,
        -0.0000, -0.7000, -0.7000, -0.0000, -0.7000, -0.0000, -0.0000, -0.7000,
        -0.0000, -0.0000, -0.7000, -0.0000, -0.0000, -0.0000, -0.7000, -0.0000,
        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.7000,
        -0.0000, -0.7000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,
        -0.0000, -0.0000, -0.0000, -0.7000, -0.0000, -0.0000, -0.0000, -0.0000,
        -0.0000, -0.0000, -0.0000, -0.7000, -0.0000, -0.0000, -0.0000, -0.7000,
        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.7000, -0.0000,
        -0.0000, -0.0000, -0.0000, -0.0000, -0.7000, -0.0000, -0.0000, -0.0000,
        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.7000, -0.0000,
        -0.0000, -0.0000, -0.0000, -0.7000, -0.0000, -0.0000, -0.0000, -0.7000,
        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.7000, -0.0000,
        -0.0000, -0.7000, -0.7000, -0.7000, -0.0000, -0.0000, -0.7000, -0.0000,
        -0.7000, -0.0000, -0.0000, -0.0000, -0.7000, -0.7000, -0.0000, -0.0000,
        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.7000, -0.0000,
        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,
        -0.0000, -0.0000, -0.0000, -0.0000, -0.7000, -0.0000, -0.0000, -0.0000,
        -0.0000, -0.0000, -0.7000, -0.0000, -0.7000, -0.0000, -0.0000, -0.0000,
        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,
        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.7000, -0.0000, -0.7000,
        -0.7000, -0.0000, -0.0000, -0.7000, -0.7000, -0.0000, -0.0000, -0.7000,
        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,
        -0.0000, -0.7000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,
        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000])
higher_lr_mask  tensor([0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.0000, 0.3000, 0.3000, 0.0000,
        0.3000, 0.0000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000,
        0.3000, 0.0000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.0000,
        0.0000, 0.3000, 0.3000, 0.0000, 0.3000, 0.0000, 0.3000, 0.3000, 0.3000,
        0.0000, 0.3000, 0.3000, 0.3000, 0.0000, 0.0000, 0.3000, 0.0000, 0.3000,
        0.3000, 0.0000, 0.3000, 0.0000, 0.0000, 0.3000, 0.3000, 0.3000, 0.3000,
        0.0000, 0.3000, 0.3000, 0.3000, 0.0000, 0.0000, 0.3000, 0.0000, 0.3000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.3000, 0.0000, 0.3000, 0.3000, 0.0000,
        0.3000, 0.3000, 0.0000, 0.3000, 0.3000, 0.0000, 0.0000, 0.3000, 0.0000,
        0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.0000, 0.3000, 0.3000,
        0.3000, 0.3000, 0.3000, 0.0000, 0.3000, 0.3000, 0.3000, 0.3000, 0.0000,
        0.0000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.0000,
        0.3000, 0.3000, 0.0000, 0.3000, 0.0000, 0.0000, 0.3000, 0.0000, 0.0000,
        0.3000, 0.3000, 0.3000, 0.0000, 0.0000, 0.0000, 0.3000, 0.3000, 0.0000,
        0.0000, 0.3000, 0.3000, 0.3000, 0.3000, 0.0000, 0.3000, 0.0000, 0.3000,
        0.3000, 0.0000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000,
        0.3000, 0.0000, 0.3000, 0.0000, 0.3000, 0.3000, 0.0000, 0.3000, 0.3000,
        0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.0000, 0.3000, 0.3000, 0.0000,
        0.3000, 0.3000, 0.0000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000,
        0.0000, 0.3000, 0.3000, 0.0000, 0.0000, 0.3000, 0.3000, 0.3000, 0.0000,
        0.0000, 0.3000, 0.3000, 0.3000, 0.0000, 0.3000, 0.3000, 0.3000, 0.3000,
        0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.0000, 0.3000, 0.3000, 0.3000,
        0.3000, 0.3000, 0.3000, 0.0000, 0.0000, 0.3000, 0.0000, 0.0000, 0.3000,
        0.0000, 0.3000, 0.3000, 0.0000, 0.3000, 0.3000, 0.3000, 0.0000, 0.0000,
        0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.0000, 0.3000,
        0.0000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000,
        0.0000, 0.0000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.0000,
        0.0000, 0.3000, 0.3000, 0.0000, 0.0000, 0.3000, 0.3000, 0.3000, 0.0000,
        0.3000, 0.3000, 0.0000, 0.3000, 0.3000, 0.0000, 0.3000, 0.0000, 0.0000,
        0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.0000,
        0.0000, 0.3000, 0.3000, 0.3000, 0.3000, 0.0000, 0.3000, 0.3000, 0.3000,
        0.0000, 0.3000, 0.3000, 0.0000, 0.3000, 0.3000, 0.3000, 0.0000, 0.3000,
        0.3000, 0.0000, 0.0000, 0.0000, 0.3000, 0.0000, 0.0000, 0.3000, 0.0000,
        0.3000, 0.3000, 0.3000, 0.0000, 0.0000, 0.3000, 0.3000, 0.3000, 0.3000,
        0.3000, 0.3000, 0.3000, 0.3000, 0.0000, 0.3000, 0.3000, 0.3000, 0.3000,
        0.0000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000,
        0.0000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.0000, 0.3000, 0.0000,
        0.3000, 0.0000, 0.0000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000,
        0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.0000, 0.0000, 0.3000,
        0.0000, 0.0000, 0.0000, 0.3000, 0.0000, 0.0000, 0.3000, 0.3000, 0.0000,
        0.3000, 0.0000, 0.0000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000,
        0.0000, 0.0000, 0.3000, 0.0000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000,
        0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.0000])
lr_mask  tensor([[[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[0.3000]]],


        [[[1.0000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[0.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[0.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[0.3000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[0.3000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[1.0000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.0000]]]])
torch.Size([384, 96, 3, 3])
self.lr tensor([[[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]]], device='cuda:0')
LAYER_NUM:  1
shape of final_sum:  torch.Size([384, 16, 16])
FINAL SUM LENNNN  384
FINAL_SUM:  [359, 254, 291, 78, 77, 332, 355, 210, 98, 61]
acts len:  3
acts keys:  ['conv0', 'conv1', 'conv2']
acts:  {'conv0': [64, 66, 43, 25, 29, 67, 26, 46, 75, 84, 87, 5, 41, 91, 4, 85, 27, 72, 11, 92, 15, 19, 57, 50, 82, 55, 30, 77, 61, 17], 'conv1': [359, 254, 291, 78, 77, 332, 355, 210, 98, 61, 64, 112, 49, 19, 266, 260, 157, 225, 32, 27, 63, 300, 330, 92, 324, 72, 227, 204, 338, 127, 349, 245, 243, 99, 184, 95, 113, 54, 151, 301, 66, 46, 57, 280, 115, 159, 191, 119, 121, 259, 5, 294, 31, 249, 116, 303, 180, 218, 125, 365, 18, 53, 7, 8, 189, 296, 242, 198, 269, 279, 222, 290, 104, 147, 213, 352, 345, 199, 164, 201, 175, 369, 376, 284, 366, 374, 120, 382, 169, 235, 103, 143, 107, 244, 166, 69, 214, 333, 52, 270, 346, 174, 158, 13, 156, 161, 155, 258, 208, 220, 34, 126, 139, 223, 47, 319], 'conv2': [268, 1458, 660, 196, 272, 128, 737, 1323, 1261, 1500, 269, 320, 205, 108, 1474, 1371, 1430, 97, 303, 388, 294, 301, 482, 325, 75, 1389, 1149, 1160, 524, 1530, 1195, 730, 1425, 315, 888, 193, 870, 1052, 1412, 637, 627, 451, 998, 252, 1039, 1251, 914, 856, 540, 1066, 941, 350, 1270, 1189, 332, 552, 48, 640, 1490, 1174, 1005, 1483, 766, 539, 995, 1222, 296, 1155, 507, 1527, 693, 111, 68, 927, 1239, 1305, 1501, 175, 994, 65, 1094, 138, 1461, 1383, 855, 1015, 71, 760, 1154, 1053, 519, 876, 553, 431, 1076, 1106, 1151, 1113, 650, 1146, 1464, 1184, 335, 408, 1086, 466, 1135, 323, 98, 919, 747, 49, 1050, 1082, 873, 525, 463, 882, 1213, 1144, 967, 377, 869, 1273, 764, 986, 1284, 1110, 868, 1156, 165, 848, 453, 245, 701, 707, 931, 417, 238, 1060, 676, 662, 956, 99, 503, 996, 130, 939, 727, 436, 392, 985, 1141, 840, 1221, 1030, 617, 1218, 1398, 112, 567, 573, 905, 1196, 1382, 969, 91, 168, 88, 79, 772, 1257, 1470, 4, 763, 804, 1532, 611, 1495, 633, 576, 866, 454, 1016, 1359, 607, 801, 471, 965, 7, 1404, 1471, 1453, 746, 847, 1211, 696, 736, 605, 137, 381, 226, 1435, 703, 634, 830, 971, 1137, 844, 117, 933, 1510, 401, 811, 30, 842, 548, 545, 217, 81, 459, 973, 1409, 54, 1004, 710, 77, 1164, 427, 1394, 468, 314, 1165, 867, 691, 1319, 581, 533, 997, 749, 1299, 326, 674, 1024, 1021, 704, 889, 242, 1378, 212, 1192, 1044, 1157, 1393, 595, 759, 1349, 832, 843, 1008, 318, 1303, 281, 910, 113, 39, 94, 278, 1462, 55, 1434, 430, 1099, 1057, 355, 191, 638, 475, 593, 1441, 959, 689, 1230, 934, 606, 423, 1059, 810, 267, 1476, 903, 369, 84, 601, 188, 76, 817, 887, 1313, 514, 926, 1065, 1456, 647, 1274, 1426, 512, 851, 786, 27, 13, 483, 26, 556, 713, 1013, 415, 745, 1091, 827, 506, 449, 489, 1140, 915, 683, 1201, 1431, 174, 334, 461, 1432, 59, 171, 652, 599, 1185, 232, 529, 1111, 58, 1244, 1328, 992, 1200, 107, 906, 488, 1054, 486, 283, 813, 1001, 654, 705, 946, 536, 1073, 342, 944, 862, 479, 1345, 517, 1167, 160, 699, 1267, 1092, 838, 1459, 960, 1505, 1291, 823, 211, 115, 1324, 1107, 1209, 1183, 1529, 1332, 1405, 43, 1236, 1365, 661, 682, 134, 275, 247, 829, 1231, 177, 189, 724, 474, 1019, 1194, 1219, 1263, 1000, 60, 614, 1132, 295, 132, 715, 1387, 1411, 163, 987, 1519, 814, 1122, 496, 184, 988, 557, 167, 1482, 765, 964, 859, 25, 1009, 426, 1096, 775, 518, 47, 837, 375, 604, 279, 1002, 564, 950, 812, 1514, 446, 467, 119, 1334, 549, 476, 221, 1285, 1258, 1497, 1124, 337, 1136, 118, 257, 1214, 880, 494, 993, 626, 1465]}
final_sum len:  384
delta_weights INFO: 
NUM OF TRACKED COV LAYERS:  1
NUM OF TRACKED WEIGHTS CHANGES PER LAYER:  10
avg_deltas INFO:  <class 'dict'>
avg_deltas keys:  ['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight']
avg_deltas size:  3
num of averages for 1 layer:  torch.Size([96])
################################################
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 1.37e-01	time: 00:00:40	Acc_train 0.00	Acc_test 0.00	convergence: 2.40e+01	R1: 0	Info MB:0.000e+00/SB:0.000e+00/MW:5.386e-03/SW:2.008e-01/MR:5.753e+00/SR:1.331e+00/MeD:1.021e+00/MaD:4.753e+00/MW:0.542/MAW:0.458
|        0 |        1 |        2 |         3 |         4 |        5 |        6 |        7 |       8 |        9 |        10 |       11 |       12 |       13 |       14 |       15 |       16 |       17 |        18 |       19 |       20 |       21 |      22 |       23 |       24 |       25 |       26 |        27 |       28 |        29 |
|----------+----------+----------+-----------+-----------+----------+----------+----------+---------+----------+-----------+----------+----------+----------+----------+----------+----------+----------+-----------+----------+----------+----------+---------+----------+----------+----------+----------+-----------+----------+-----------|
|   0.0101 |   0.0102 |   0.0104 |   0.00753 |   0.00444 |   0.0115 |   0.0102 |   0.0101 |   0.011 |   0.0107 |   0.00963 |   0.0115 |   0.0107 |   0.0122 |   0.0115 |   0.0104 |   0.0121 |   0.0126 |   0.00577 |   0.0106 |   0.0128 |   0.0105 |   0.012 |   0.0129 |   0.0107 |   0.0133 |   0.0133 |   0.00894 |   0.0118 |   0.00646 |
|   5.12   |   5.16   |   5.29   |   3.27    |   1.79    |   6.25   |   5.13   |   5.1    |   5.8   |   5.56   |   4.71    |   6.27   |   5.55   |   6.91   |   6.25   |   5.36   |   6.82   |   7.4    |   2.33    |   5.49   |   7.56   |   5.39   |   6.76  |   7.71   |   5.54   |   8.11   |   8.06   |   4.19    |   6.57   |   2.67    |
|   0.17   |   0.15   |   0.22   |   0.21    |   0.44    |   0.36   |   0.14   |   0.19   |   0.2   |   0.17   |   0.32    |   0.22   |   0.31   |   0.22   |   0.18   |   0.23   |   0.12   |   0.21   |   0.26    |   0.34   |   0.16   |   0.29   |   0.29  |   0.15   |   0.3    |   0.23   |   0.16   |   0.36    |   0.17   |   0.23    |
| nan      | nan      | nan      | nan       | nan       | nan      | nan      | nan      | nan     | nan      | nan       | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan       | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan       | nan      | nan       |
| nan      | nan      | nan      | nan       | nan       | nan      | nan      | nan      | nan     | nan      | nan       | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan       | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan       | nan      | nan       |
| nan      | nan      | nan      | nan       | nan       | nan      | nan      | nan      | nan     | nan      | nan       | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan       | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan       | nan      | nan       |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_2C_CL/models
CONFIG MODE:  unsupervised

 ********** Hebbian Unsupervised learning of blocks [2] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([1, 1, 5, 5, 5, 5, 5, 1, 5, 5, 1, 1, 1, 5, 5, 1, 1, 1, 1, 5])
[1, 5]
TARGETS AFTER CLEANER:  tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1])
2000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 2000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([1, 1, 5, 1, 5, 1, 1, 1, 5, 5, 1, 1, 1, 1, 5, 1, 1, 5, 5, 1])
[1, 5]
TARGETS AFTER CLEANER:  tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0])
10000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 10000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([1, 1, 5, 1, 5, 1, 1, 1, 5, 5, 1, 1, 1, 1, 5, 1, 1, 5, 5, 1])
[1, 5]
TARGETS AFTER CLEANER:  tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0])
10000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 10000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: ToTensor()
------------------------
INDICES:  10000
IMAGE SIZE: torch.Size([10, 3, 32, 32])
SAVING FOLDER FOR UNSUP:  C10_2C_CL
DEPTH:  3
ACTIVATIONS SHAPE:  tensor([[ 20.2469,   1.4090,  14.9038,  20.6874, -10.8122, -31.0042, -32.9134,
         -18.9703,   0.4252,  -9.2009, -11.0412, -14.4959, -25.4894, -26.2495,
         -18.6530, -13.2107],
        [ 18.0229,   0.9213,  26.8911,  30.7553,   4.8910, -28.4480, -32.3579,
         -13.8842, -11.8040, -15.7843, -24.5863, -29.7533, -39.3652, -31.9507,
         -14.1559, -10.7419],
        [  1.8668, -12.0364,   6.5115,  19.5908,  -7.7894, -32.1943, -28.3554,
         -17.0266,  -4.7689,  -1.5099,  -8.2413, -11.5891, -25.9629, -22.2018,
         -15.4523,  -9.8584],
        [  2.2325, -15.8458,  -2.0612,  10.0532,  -8.4931, -22.9722, -15.0286,
          -7.6813,   5.9770,  21.6262,  21.2347,   2.0419, -13.8691,  -5.5145,
         -10.6700,  -9.7927],
        [ 10.7288,  -9.3713,   6.5664,   1.3046,  -8.1281, -16.0713,  -8.1531,
         -23.5931,   1.9246,  23.3627,  37.5623,  22.9907,  12.5289, -11.5190,
          -8.3031,  -4.4324],
        [ 15.3169,  14.2758,  10.8512, -13.8709,  -2.2273,  -7.1724, -23.6629,
         -29.7690, -12.4804,  22.4041,  30.4018,  19.0231,  20.0303,  -6.9998,
         -17.3147,   6.5269],
        [ 16.0850,  12.2244,   6.7428,  -0.8029,   2.2405,  -7.7558, -26.9578,
         -38.2216, -23.3437,  10.5846,  22.6573,  14.4027,   9.4139, -11.1197,
         -27.9186,   4.6095],
        [ 19.6610,  14.2906,  29.0490,   8.6260,  -3.5935,  -2.1708, -18.3625,
         -34.8751, -13.8364,   2.2919,  12.2980,   9.4131,   5.9861,  -7.0437,
           0.0697,  21.5119],
        [ 23.5689,  21.3532,  41.7204,  22.6225,   1.1875,  -3.1046, -13.4371,
         -29.4594, -24.0912,   6.6317,  18.4641,  16.3387,  -0.2509,  12.7878,
           3.2801,  25.8462],
        [ 20.2931,  18.2235,  30.4074,   7.0746,  -2.6801,  11.5841,   4.6635,
         -16.0514,  -8.4054,  22.9723,  31.7638,  11.4145, -13.8132,  -8.0718,
          -8.4052,  22.3652],
        [ 31.3950,  22.7729,  30.1675,   5.7781,  -1.4484,  13.8877,  12.1474,
          -2.1953,   2.1330,  21.9811,  39.2382,  12.4848,  -5.5354,   1.2618,
          -2.5039,  28.2757],
        [ 29.4794,  34.4893,  49.0669,  18.1610,   8.6963,  12.6613,  15.9856,
          -3.5976,   2.4146,  15.4596,  31.8641,  18.1647,   9.5791,  17.2083,
          11.5017,  25.7418],
        [ 24.6254,  34.3213,  51.9548,  28.6418,  -2.2106,  -3.3998,   2.4316,
         -14.1052,  -3.7456,   4.6447,  20.8120,  20.4126,  10.5436,  14.9239,
           6.2687,  17.0116],
        [ -3.1757,  10.3321,  35.3743,  13.4615, -13.4622, -29.8296, -17.7195,
         -26.0676,  -3.3573,   3.4232,  11.4639,  12.3970,   8.1160,   0.8570,
         -10.1230,   7.6726],
        [-19.3829,  -2.8041,  19.3775,  11.6328,   5.0216, -19.6759, -18.8683,
         -11.7423,  12.9222,  18.5682,  15.3602,  18.9521,  22.4627,   3.0938,
         -23.4606, -10.9498],
        [-29.2126, -19.2472,   3.9489,  -2.9810, -20.7900, -42.0710, -34.2540,
         -25.2789,  -2.8814,   2.7954,   0.2015,   9.9855,   9.1091, -10.7366,
         -30.3775, -34.6546]], device='cuda:0')
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.top_acts_layer:  <class 'list'>
topk_mask  tensor([0., 0., 0.,  ..., 0., 0., 0.])
threshold_mask  tensor([1, 1, 0,  ..., 0, 1, 0], dtype=torch.uint8)
final_mask tensor([1., 1., 0.,  ..., 0., 1., 0.])
topk_mask  tensor([0., 0., 0.,  ..., 0., 0., 0.])
not_topk_mask  tensor([1, 1, 1,  ..., 1, 1, 1], dtype=torch.uint8)
lower_lr_mask  tensor([-0., -0., -0.,  ..., -0., -0., -0.])
higher_lr_mask  tensor([0.3000, 0.3000, 0.3000,  ..., 0.3000, 0.3000, 0.3000])
lr_mask  tensor([[[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]],


        ...,


        [[[1.3000]]],


        [[[1.3000]]],


        [[[1.3000]]]])
torch.Size([1536, 384, 3, 3])
self.lr tensor([[[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]],

        [[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]],

        [[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]],

        ...,

        [[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]],

        [[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]],

        [[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]]], device='cuda:0')
LAYER_NUM:  2
shape of final_sum:  torch.Size([1536, 8, 8])
FINAL SUM LENNNN  1536
FINAL_SUM:  [660, 1458, 1174, 205, 1082, 870, 350, 1261, 108, 1323]
acts len:  3
acts keys:  ['conv0', 'conv1', 'conv2']
acts:  {'conv0': [64, 66, 43, 25, 29, 67, 26, 46, 75, 84, 87, 5, 41, 91, 4, 85, 27, 72, 11, 92, 15, 19, 57, 50, 82, 55, 30, 77, 61, 17], 'conv1': [359, 254, 291, 78, 77, 332, 355, 210, 98, 61, 64, 112, 49, 19, 266, 260, 157, 225, 32, 27, 63, 300, 330, 92, 324, 72, 227, 204, 338, 127, 349, 245, 243, 99, 184, 95, 113, 54, 151, 301, 66, 46, 57, 280, 115, 159, 191, 119, 121, 259, 5, 294, 31, 249, 116, 303, 180, 218, 125, 365, 18, 53, 7, 8, 189, 296, 242, 198, 269, 279, 222, 290, 104, 147, 213, 352, 345, 199, 164, 201, 175, 369, 376, 284, 366, 374, 120, 382, 169, 235, 103, 143, 107, 244, 166, 69, 214, 333, 52, 270, 346, 174, 158, 13, 156, 161, 155, 258, 208, 220, 34, 126, 139, 223, 47, 319], 'conv2': [660, 1458, 1174, 205, 1082, 870, 350, 1261, 108, 1323, 837, 1412, 1382, 269, 294, 128, 97, 268, 1490, 1113, 1195, 1005, 919, 1387, 1030, 1149, 674, 998, 525, 985, 760, 1383, 456, 605, 191, 1532, 973, 1284, 519, 537, 627, 964, 553, 576, 332, 133, 1483, 637, 1213, 905, 1086, 315, 1039, 1222, 1319, 726, 903, 196, 127, 1371, 1270, 1265, 1500, 1065, 1053, 1505, 1239, 1218, 710, 301, 503, 564, 540, 617, 272, 507, 914, 451, 727, 1016, 996, 987, 1135, 1050, 1154, 496, 843, 877, 138, 454, 193, 75, 650, 1110, 119, 1151, 238, 282, 171, 422, 117, 1434, 539, 1155, 27, 53, 759, 274, 786, 142, 1471, 840, 81, 1225, 325, 557, 603, 611, 832, 1310, 278, 1358, 1370, 1305, 1435, 866, 705, 1411, 1389, 1019, 1484, 1141, 599, 1221, 257, 68, 1205, 1359, 874, 1132, 689, 377, 1257, 927, 1121, 817, 221, 489, 335, 888, 1352, 1066, 48, 847, 303, 1409, 226, 1530, 1060, 501, 1425, 252, 474, 844, 197, 1197, 1211, 468, 924, 65, 1285, 889, 634, 1146, 415, 430, 1299, 1076, 1507, 941, 1251, 408, 463, 648, 1464, 1321, 765, 830, 1021, 583, 1078, 371, 971, 931, 1253, 588, 1111, 989, 995, 347, 47, 293, 334, 1263, 1140, 256, 479, 595, 1008, 956, 284, 245, 511, 930, 442, 1160, 302, 965, 1220, 150, 229, 1416, 289, 777, 1041, 737, 35, 766, 1357, 1102, 567, 1157, 554, 763, 868, 1096, 1312, 963, 747, 1298, 1361, 55, 1313, 545, 259, 179, 388, 952, 676, 87, 704, 934, 1033, 168, 94, 15, 401, 1004, 1028, 1194, 693, 607, 1097, 775, 862, 1400, 1430, 1470, 1147, 1001, 88, 1145, 394, 444, 543, 1044, 536, 3, 812, 263, 1106, 1189, 286, 593, 1378, 381, 157, 1510, 490, 112, 296, 1236, 1099, 1394, 513, 281, 1230, 730, 882, 928, 772, 1112, 1506, 1148, 459, 800, 61, 626, 810, 994, 1495, 174, 1327, 1492, 1501, 876, 1509, 873, 814, 556, 320, 30, 369, 707, 590, 1200, 79, 1165, 1288, 1073, 910, 1354, 1324, 1015, 77, 137, 1068, 898, 1476, 212, 63, 672, 880, 950, 1214, 629, 1315, 1167, 558, 666, 654, 1024, 533, 160, 130, 261, 49, 1351, 887, 93, 701, 691, 577, 1059, 54, 295, 1307, 461, 182, 959, 1012, 804, 1414, 807, 233, 1474, 180, 71, 863, 306, 1393, 330, 685, 232, 217, 802, 1052, 376, 1363, 427, 1192, 1122, 1094, 25, 1303, 258, 652, 620, 379, 1529, 1343, 466, 429, 323, 829, 884, 1249, 418, 1184, 752, 601, 1240, 433, 247, 486, 1413, 449, 1136, 1345, 1139, 574, 18, 1223, 1291, 527, 827, 858, 1533, 792, 1356, 855, 1232, 988, 1405, 640, 151, 939, 355, 722, 614, 7, 1254, 378, 314, 144, 610, 609, 579, 484, 287, 1504, 184, 167, 431, 1061, 1404, 1446, 728, 744, 453, 645, 264]}
final_sum len:  1536
delta_weights INFO: 
NUM OF TRACKED COV LAYERS:  1
NUM OF TRACKED WEIGHTS CHANGES PER LAYER:  10
avg_deltas INFO:  <class 'dict'>
avg_deltas keys:  ['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight']
avg_deltas size:  3
num of averages for 2 layer:  torch.Size([96])
################################################
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 4.24e-02	time: 00:01:23	Acc_train 0.00	Acc_test 0.00	convergence: 1.81e+01	R1: 0	Info MB:0.000e+00/SB:0.000e+00/MW:3.058e-03/SW:3.271e-01/MR:1.906e+01/SR:2.507e+00/MeD:1.998e+00/MaD:1.805e+01/MW:0.427/MAW:0.573
|       0 |       1 |        2 |        3 |        4 |        5 |        6 |        7 |        8 |        9 |      10 |       11 |      12 |      13 |       14 |      15 |       16 |       17 |       18 |       19 |       20 |       21 |       22 |       23 |       24 |      25 |       26 |       27 |       28 |       29 |
|---------+---------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+---------+---------+----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------|
|   0.044 |   0.043 |   0.0412 |   0.0449 |   0.0384 |   0.0433 |   0.0453 |   0.0387 |   0.0376 |   0.0465 |   0.043 |   0.0424 |   0.041 |   0.036 |   0.0426 |   0.042 |   0.0467 |   0.0439 |   0.0399 |   0.0425 |   0.0425 |   0.0431 |   0.0408 |   0.0437 |   0.0431 |   0.034 |   0.0429 |   0.0445 |   0.0359 |   0.0409 |
|  20.38  |  19.48  |  17.94   |  21.13   |  15.71   |  19.72   |  21.56   |  15.96   |  15.11   |  22.65   |  19.52  |  18.99   |  17.83  |  13.94  |  19.11   |  18.68  |  22.81   |  20.31   |  16.89   |  19.08   |  19.1    |  19.55   |  17.62   |  20.13   |  19.58   |  12.57  |  19.39   |  20.83   |  13.87   |  17.72   |
|   0.04  |   0.04  |   0.07   |   0.1    |   0.05   |   0.02   |   0.05   |   0.03   |   0.06   |   0.06   |   0.08  |   0.08   |   0.17  |   0.02  |   0.13   |   0.06  |   0.06   |   0.09   |   0.08   |   0.03   |   0.11   |   0.06   |   0.03   |   0.02   |   0.05   |   0.06  |   0.08   |   0.03   |   0.06   |   0.04   |
| nan     | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan     | nan     | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      |
| nan     | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan     | nan     | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      |
| nan     | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan     | nan     | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_2C_CL/models
CONFIG MODE:  supervised

 ********** Supervised learning of blocks [3] **********
SAVING FOLDER FOR SUP:  C10_2C_CL
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([1, 1, 5, 5, 5, 5, 5, 1, 5, 5, 1, 1, 1, 5, 5, 1, 1, 1, 1, 5])
[1, 5]
TARGETS AFTER CLEANER:  tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1])
2000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 2000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([1, 1, 5, 1, 5, 1, 1, 1, 5, 5, 1, 1, 1, 1, 5, 1, 1, 5, 5, 1])
[1, 5]
TARGETS AFTER CLEANER:  tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0])
10000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 10000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([1, 1, 5, 1, 5, 1, 1, 1, 5, 5, 1, 1, 1, 1, 5, 1, 1, 5, 5, 1])
[1, 5]
TARGETS AFTER CLEANER:  tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0])
10000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 10000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: ToTensor()
------------------------
INDICES:  10000
IMAGE SIZE: torch.Size([64, 3, 32, 32])
Epoch: [1/50]	lr: 1.00e-03	time: 00:01:37	Loss_train 0.23862	Acc_train 90.42	/	Loss_test 0.00293	Acc_test 96.95
BLOCKS IN SUP:  [3]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_2C_CL/models
Epoch: [10/50]	lr: 1.00e-03	time: 00:01:41	Loss_train 0.03739	Acc_train 97.59	/	Loss_test 0.00308	Acc_test 97.75
BLOCKS IN SUP:  [3]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_2C_CL/models
Epoch: [20/50]	lr: 2.50e-04	time: 00:01:46	Loss_train 0.00912	Acc_train 99.14	/	Loss_test 0.00255	Acc_test 98.15
BLOCKS IN SUP:  [3]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_2C_CL/models
Epoch: [30/50]	lr: 1.25e-04	time: 00:01:50	Loss_train 0.00462	Acc_train 99.47	/	Loss_test 0.00223	Acc_test 98.30
BLOCKS IN SUP:  [3]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_2C_CL/models
Epoch: [40/50]	lr: 3.13e-05	time: 00:01:55	Loss_train 0.00347	Acc_train 99.59	/	Loss_test 0.00202	Acc_test 98.50
BLOCKS IN SUP:  [3]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_2C_CL/models
Epoch: [50/50]	lr: 7.81e-06	time: 00:01:59	Loss_train 0.00330	Acc_train 99.63	/	Loss_test 0.00203	Acc_test 98.45
new_head:  {'blocks.3.layer.bias': tensor([-0.0153,  0.0137], device='cuda:0'), 'blocks.3.layer.weight': tensor([[-0.0373, -0.0843,  0.0169,  ..., -0.0013, -0.0046, -0.0038],
        [ 0.0399,  0.0787, -0.0113,  ...,  0.0005,  0.0057,  0.0050]],
       device='cuda:0')}
BLOCKS IN SUP:  [3]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_2C_CL/models
RESULT:  {'train_loss': 0.00329587166197598, 'train_acc': 99.63300228118896, 'test_loss': 0.0020254196133464575, 'test_acc': 98.44999694824219, 'convergence': 18.063566207885742, 'R1': 0, 'dataset_sup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [1, 5]}, 'dataset_unsup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [1, 5]}, 'train_config': {'t0': {'blocks': [0], 'mode': 'unsupervised', 'lr': 0.08, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't2': {'blocks': [1], 'mode': 'unsupervised', 'lr': 0.005, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't4': {'blocks': [2], 'mode': 'unsupervised', 'lr': 0.01, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't6': {'blocks': [3], 'mode': 'supervised', 'lr': 0.001, 'nb_epoch': 50, 'batch_size': 64, 'print_freq': 10}}}
IN R2:  {'R1': {'train_loss': 0.00900841411203146, 'train_acc': 98.76099824905396, 'test_loss': 0.0027053889352828264, 'test_acc': 96.94999694824219, 'convergence': 20.49114418029785, 'R1': 0, 'dataset_sup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [2, 8]}, 'dataset_unsup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [2, 8]}, 'train_config': {'t0': {'blocks': [0], 'mode': 'unsupervised', 'lr': 0.08, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't2': {'blocks': [1], 'mode': 'unsupervised', 'lr': 0.005, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't4': {'blocks': [2], 'mode': 'unsupervised', 'lr': 0.01, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't6': {'blocks': [3], 'mode': 'supervised', 'lr': 0.001, 'nb_epoch': 50, 'batch_size': 64, 'print_freq': 10}}}, 'R2': {'train_loss': 0.00329587166197598, 'train_acc': 99.63300228118896, 'test_loss': 0.0020254196133464575, 'test_acc': 98.44999694824219, 'convergence': 18.063566207885742, 'R1': 0, 'dataset_sup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [1, 5]}, 'dataset_unsup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [1, 5]}, 'train_config': {'t0': {'blocks': [0], 'mode': 'unsupervised', 'lr': 0.08, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't2': {'blocks': [1], 'mode': 'unsupervised', 'lr': 0.005, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't4': {'blocks': [2], 'mode': 'unsupervised', 'lr': 0.01, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't6': {'blocks': [3], 'mode': 'supervised', 'lr': 0.001, 'nb_epoch': 50, 'batch_size': 64, 'print_freq': 10}}}}
SEED:  0
block 0, size : 96 16 16
range = 2.886751345948129
block 1, size : 384 8 8
range = 0.8505172717997146
block 2, size : 1536 4 4
range = 0.4252586358998573
range = 0.11048543456039805
The device used will be: 
True
cuda:0
PARAMSSSS IN LOAD:  {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 3, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.11048543456039805, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 24576, 'old_channels': 1536, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
{'blocks.0.layer.weight': tensor([-0.0893,  0.0682, -0.1893,  0.1486, -0.0137,  0.0543,  0.3593,  0.1832,
         0.0673, -0.0404, -0.0788,  0.2304,  0.2177, -0.2858,  0.1145,  0.0203,
        -0.0023,  0.0635,  0.0999,  0.0525, -0.1413,  0.2679,  0.0533,  0.1906,
         0.0362,  0.0010,  0.0772, -0.3105,  0.1396,  0.1159,  0.2772,  0.3068,
        -0.0156,  0.1994,  0.2580,  0.1819,  0.0994,  0.1958,  0.2141, -0.1811,
         0.2537, -0.1083,  0.2447,  0.1205,  0.1461,  0.1716,  0.1726,  0.1869,
        -0.0817,  0.0239,  0.1838,  0.0616,  0.1569, -0.0765,  0.0508,  1.0000,
        -0.1961,  0.0037, -0.6914, -0.0560,  0.0438,  0.0027,  0.0610,  0.2151,
        -0.2399,  0.0618,  0.3117,  0.1505, -0.0335,  0.1963,  0.2664,  0.2914,
         0.2306, -0.0163,  0.1076,  0.1763,  0.1224, -0.5319,  0.1898,  0.1759,
        -0.0779, -0.1286,  0.1476, -0.0133,  0.1663,  0.1462,  0.3758,  0.0601,
         0.2125,  0.0904,  0.0438, -0.1008,  0.0298, -0.1049, -0.2394,  0.1098],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 1.4733e-01,  1.0162e-01,  4.2738e-01,  2.5037e-01,  4.6185e-01,
        -1.5373e-01,  2.4198e-01, -2.7186e-01, -1.5374e-01,  2.3019e-01,
        -9.2731e-02,  6.0045e-01,  5.2573e-01,  4.7052e-01,  4.5994e-01,
         2.4350e-01,  1.4212e-01,  5.4695e-01, -1.3889e-01, -1.5452e-01,
         4.8952e-02,  6.6858e-01,  7.7026e-01,  4.1097e-01,  6.8508e-01,
         8.6194e-01, -1.0557e-01, -1.6597e-01, -9.8983e-02,  2.9587e-01,
        -5.9714e-02, -3.1888e-01, -9.5671e-02, -9.7752e-02,  1.5260e-02,
        -9.5692e-03,  2.1085e-01,  1.6453e-01,  1.1146e-02,  2.9293e-01,
         1.6865e-01,  4.2144e-02,  6.1984e-02,  8.2158e-02,  4.4240e-01,
         7.5465e-02, -1.8956e-01,  1.3693e-01, -1.4009e-01, -2.4716e-01,
         3.7758e-01,  2.2655e-01, -3.0521e-02, -3.2785e-01, -1.9544e-01,
         4.5158e-01,  5.6652e-01, -1.9637e-01, -8.5378e-03, -1.8847e-01,
         8.1254e-02, -1.6703e-01,  5.0520e-01, -1.3390e-01, -1.3824e-01,
        -1.1752e-01, -8.6997e-02,  7.7740e-01,  1.0125e-01, -4.5119e-02,
        -5.0477e-02, -4.1890e-02, -1.1806e-01,  5.8330e-01, -1.3127e-01,
         2.7516e-01,  5.8627e-01, -1.5449e-01,  1.8534e-02,  8.9013e-02,
        -1.1324e-01,  2.2854e-01,  1.8536e-01,  4.1567e-02,  3.0803e-01,
        -1.3588e-01,  4.1583e-01, -2.3205e-01, -1.0783e-01,  6.8299e-01,
         3.4281e-01,  1.7168e-01,  4.0762e-02,  4.7027e-02,  6.1001e-01,
        -2.0408e-01,  4.0793e-01, -3.9889e-03, -1.7807e-01, -8.2258e-02,
         9.3637e-01,  5.9195e-02,  2.4007e-01, -1.1866e-01,  5.9280e-04,
         2.0896e-01, -9.9343e-03, -1.7924e-01,  1.9156e-03,  3.6385e-01,
        -1.7154e-01,  4.2827e-01, -1.7743e-01, -1.0359e-01,  7.1029e-01,
        -1.8920e-01, -1.0663e-01,  4.1719e-01,  7.4293e-01, -1.1136e-01,
        -9.0100e-02, -1.1649e-01,  7.6573e-03,  6.4556e-01,  5.3522e-02,
        -1.7160e-01, -1.7329e-01, -3.1031e-01,  7.3531e-01,  8.3312e-02,
         1.9530e-01,  3.0300e-01,  3.3367e-01, -1.0280e-01,  5.9207e-01,
         3.3468e-01,  7.4495e-03,  6.0891e-01,  3.3220e-01,  1.6701e-01,
         3.8095e-01,  7.0347e-01,  6.1189e-01, -1.2629e-01,  6.7094e-01,
         4.5267e-01,  5.6833e-01, -1.8524e-01,  4.8003e-01,  1.4261e-01,
        -5.6129e-02, -2.0540e-01,  2.5030e-01,  2.3580e-01,  1.9064e-01,
        -3.4632e-01, -3.7953e-01, -4.4306e-01,  4.8178e-02, -2.8269e-01,
        -1.4758e-01, -6.6420e-02,  5.2436e-02, -2.6462e-01, -7.6983e-02,
         8.1739e-02, -2.6234e-01,  6.4033e-02,  4.9374e-01, -3.8936e-01,
        -9.8231e-02,  2.4162e-01,  7.3278e-01,  4.7644e-01,  1.9871e-01,
        -8.9520e-02,  2.1764e-01,  4.9830e-01,  4.3556e-02,  1.5118e-01,
        -1.0921e-01,  2.1478e-01,  1.3159e-01,  2.3801e-01, -1.1422e-01,
         5.1119e-01,  2.9972e-01,  1.5925e-01,  2.1106e-01, -4.0300e-01,
         8.1680e-01,  1.1654e-01,  7.8541e-02,  9.5309e-02,  3.7291e-02,
        -5.2335e-02,  1.5406e-01,  9.5806e-01,  8.3361e-02, -9.1684e-02,
         1.0000e+00, -1.8807e-01, -1.7371e-01,  6.9004e-01, -1.1843e-01,
         3.0083e-01,  1.9011e-01, -1.2930e-01, -1.4431e-01,  5.6890e-01,
        -1.2423e-01,  1.0202e-01,  1.6674e-01,  3.9491e-02, -9.0908e-02,
         2.7723e-01,  2.0637e-01,  5.6430e-01, -3.5699e-01,  3.3278e-01,
         1.3920e-01,  7.0478e-01, -2.6168e-01, -1.5416e-01,  1.0124e-03,
        -1.8242e-01,  7.1016e-01, -2.3460e-01,  4.7341e-02,  4.9245e-01,
         1.0063e-01, -9.5912e-02,  1.9076e-01,  2.5571e-01,  4.2617e-01,
        -1.3283e-01,  6.0654e-01,  3.4062e-01,  3.4997e-01,  7.9148e-01,
        -1.0381e-01, -6.6912e-02, -1.2900e-01, -1.0789e-01, -5.1259e-02,
        -2.8808e-01,  3.7571e-01, -1.3918e-01,  6.9211e-01,  3.9961e-02,
         5.1155e-01,  4.7163e-02, -4.3341e-02,  3.2515e-01, -1.5979e-01,
         2.6140e-01,  3.4007e-01,  6.3509e-02, -7.4100e-02,  1.0967e-01,
        -4.7687e-02,  7.6437e-03,  8.1383e-01,  6.1915e-01,  2.2179e-02,
         2.0902e-01, -2.3783e-01,  4.4999e-01,  4.7832e-01, -5.3083e-02,
        -8.7116e-02,  3.9448e-01,  2.1762e-01,  3.6371e-02,  4.9589e-01,
        -1.7652e-01,  5.5311e-01,  3.9307e-01,  2.7284e-01, -1.3902e-01,
        -2.1844e-02,  6.0708e-01,  3.9961e-01, -4.4799e-02,  2.8802e-02,
         2.6856e-01, -1.6687e-01, -2.8755e-01,  1.8761e-01, -1.5907e-01,
        -1.5248e-01, -1.4058e-01,  2.1971e-01, -1.3771e-02, -2.6301e-02,
        -1.1168e-01, -1.5733e-01, -1.5248e-01,  6.5571e-01,  1.7933e-01,
        -9.5615e-02, -1.5790e-01,  3.3843e-01, -7.7776e-02,  8.0030e-01,
         3.8429e-01,  6.2248e-01,  6.1959e-02,  2.3457e-01, -5.2168e-02,
        -9.5021e-02,  9.0604e-03,  6.3872e-01, -6.6291e-03,  6.4973e-01,
        -1.0501e-01,  3.8253e-01,  6.0388e-01, -1.5826e-01,  4.4355e-02,
         9.1834e-02,  4.3935e-01,  4.9539e-01, -1.6805e-01, -1.2071e-01,
         1.2531e-01,  7.3395e-01,  7.5771e-01, -7.3078e-03,  8.2798e-01,
        -1.3779e-01,  4.9269e-01, -1.5853e-01,  2.1624e-01, -9.5771e-03,
         1.2777e-01,  1.2578e-01,  4.6119e-02,  3.9701e-01,  2.8027e-01,
         2.4933e-01, -2.6439e-02, -2.6121e-02,  7.0138e-01, -1.8673e-01,
        -2.0976e-01,  1.8518e-01,  6.6481e-02,  1.4781e-01, -1.1805e-01,
         1.0573e-01, -9.5799e-02, -9.0316e-02,  2.1525e-01,  8.0956e-01,
        -1.4364e-01, -1.1305e-01,  7.9322e-01,  5.4191e-01, -1.4946e-01,
         9.5025e-02, -1.7736e-02, -1.2246e-01,  1.2820e-01,  4.4952e-01,
        -3.3472e-01, -1.6918e-02,  2.6003e-01,  2.1410e-01, -1.9386e-01,
         2.5722e-02,  6.4208e-01,  1.6027e-01,  2.1245e-02, -1.3629e-01,
         1.2057e-01,  3.7786e-02,  2.3774e-01,  1.9846e-01,  3.9491e-01,
         5.4355e-01,  5.0752e-01, -1.5875e-02,  4.8568e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([0.1741, 0.0493, 0.5084,  ..., 0.2412, 0.2171, 0.4346], device='cuda:0')}
None
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
The device used will be: 
True
cuda:0
The device used will be: 
True
cuda:0
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([8, 8, 8, 8, 2, 2, 8, 8, 8, 2, 2, 2, 8, 8, 2, 8, 8, 2, 2, 8])
[2, 8]
TARGETS AFTER CLEANER:  tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1])
2000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 2000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([2, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 8, 2, 8, 2, 8, 8, 8, 2])
[2, 8]
TARGETS AFTER CLEANER:  tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0])
10000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 10000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([2, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 8, 2, 8, 2, 8, 8, 8, 2])
[2, 8]
TARGETS AFTER CLEANER:  tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0])
10000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 10000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: ToTensor()
------------------------
INDICES:  10000
IMAGE SIZE: torch.Size([64, 3, 32, 32])
test_acc:  tensor(0.9530)
chosen_acc:  tensor(0.9530)
test_acc:  tensor(0.1595)
chosen_head:  {'blocks.3.layer.bias': tensor([-0.0069,  0.0053], device='cuda:0'), 'blocks.3.layer.weight': tensor([[-0.0522, -0.0461, -0.0302,  ..., -0.0106,  0.0262,  0.0255],
        [ 0.0549,  0.0405,  0.0358,  ...,  0.0098, -0.0252, -0.0243]],
       device='cuda:0')}

 Model C10_2C_CL loaded successfuly with best perf



 ----- Architecture Block BatchNorm2dSK3962(5, 5)1.0reflect, number 0 -----
- BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK963842(3, 3)0.6499999761581421reflect, number 1 -----
- BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.4)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK38415362(3, 3)0.25reflect, number 2 -----
- BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 3 -----
- Flatten(start_dim=1, end_dim=-1)
- Dropout(p=0.5, inplace=False)
- Linear(in_features=24576, out_features=2, bias=True)
model.heads:  [{'blocks.3.layer.bias': tensor([-0.0069,  0.0053], device='cuda:0'), 'blocks.3.layer.weight': tensor([[-0.0522, -0.0461, -0.0302,  ..., -0.0106,  0.0262,  0.0255],
        [ 0.0549,  0.0405,  0.0358,  ...,  0.0098, -0.0252, -0.0243]],
       device='cuda:0')}, {'blocks.3.layer.bias': tensor([-0.0153,  0.0137], device='cuda:0'), 'blocks.3.layer.weight': tensor([[-0.0373, -0.0843,  0.0169,  ..., -0.0013, -0.0046, -0.0038],
        [ 0.0399,  0.0787, -0.0113,  ...,  0.0005,  0.0057,  0.0050]],
       device='cuda:0')}]
[Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=24576, out_features=2, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)]
LAYER NAME:  Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=24576, out_features=2, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)
LAYER CHILDREN:  [BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=1.4)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
  )
  (layer): Linear(in_features=24576, out_features=2, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)]
subsubl NAME:  Sequential(
  (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=1.4)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): Flatten(start_dim=1, end_dim=-1)
)
subsubl NAME:  Linear(in_features=24576, out_features=2, bias=True)
subsubl NAME:  Dropout(p=0.5, inplace=False)
CONFIG MODE:  unsupervised
CONFIG MODE:  unsupervised
CONFIG MODE:  unsupervised
CONFIG MODE:  supervised
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([8, 8, 8, 8, 2, 2, 8, 8, 8, 2, 2, 2, 8, 8, 2, 8, 8, 2, 2, 8])
[2, 8]
TARGETS AFTER CLEANER:  tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1])
2000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 2000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([2, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 8, 2, 8, 2, 8, 8, 8, 2])
[2, 8]
TARGETS AFTER CLEANER:  tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0])
10000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 10000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([2, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 8, 2, 8, 2, 8, 8, 8, 2])
[2, 8]
TARGETS AFTER CLEANER:  tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0])
10000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 10000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: ToTensor()
------------------------
INDICES:  10000
IMAGE SIZE: torch.Size([64, 3, 32, 32])
Accuracy of the network on the 1st dataset: 95.300 %
Test loss on the 1st dataset: 0.004


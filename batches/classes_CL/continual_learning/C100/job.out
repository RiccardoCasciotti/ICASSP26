--------------- /leonardo/prod/opt/modulefiles/deeplrn/libraries ---------------
cineca-ai/3.0.0  cineca-ai/4.0.0  cineca-ai/4.1.1(default)  
cineca-ai/3.0.1  cineca-ai/4.1.0  cineca-ai/4.3.0           

Key:
(symbolic-version)  
The device used will be: 
True
cuda:0
BLOCKS:  {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.25, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10}, 'pool': None, 'activation': None}}
[[93, 32], [92, 35], [16, 41], [2, 56], [51, 79]]
{'training_mode': 'consecutive', 'cf_sol': True, 'head_sol': True, 'top_k': 0.15, 'topk_lock': False, 'high_lr': 0.15, 'low_lr': 1.0, 't_criteria': 'activations', 'delta_w_interval': 50.0, 'heads_basis_t': 0.9, 'classes_per_task': 2, 'n_tasks': 5, 'selected_classes': [[93, 32], [92, 35], [16, 41], [2, 56], [51, 79]], 'evaluated_tasks': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}
CL:  True
{'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 100, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True}
CL:  True
{'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 100, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 50, 'validation': False, 'continual_learning': True}
task 1
[93, 32]
SEED:  0
block 0, size : 96 16 16
96
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.25, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10}, 'pool': None, 'activation': None}}
range = 2.886751345948129
block 1, size : 384 8 8
384
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.25, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10}, 'pool': None, 'activation': None}}
range = 0.8505172717997146
block 2, size : 1536 4 4
1536
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.25, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10}, 'pool': None, 'activation': None}}
range = 0.4252586358998573
block 3, size : 2560 2 2
2560
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.25, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10}, 'pool': None, 'activation': None}}
range = 0.21262931794992865
block 4, size : 2560 1 1
2560
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.25, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10}, 'pool': None, 'activation': None}}
range = 0.1647019614671031
NOWWWW
1 1
2
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.25, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560}, 'pool': None, 'activation': None}}
blocks['b%s' % 5] 2560 2560
range = 0.3423265984407288
train_layer_order: consecutive {'t0': {'blocks': [0, 1, 2, 3, 4], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 50, 'lr': 0.005}, 't1': {'blocks': [5], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}
The device used will be: 
True
cuda:0

 Model C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c not found
avg_deltas:  {}
topk_layer:  None
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}



 ----- Architecture Block BatchNorm2dSK3962(5, 5)1.0reflect, number 0 -----
- BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK963842(3, 3)0.6499999761581421reflect, number 1 -----
- BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.4)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK38415362(3, 3)0.25reflect, number 2 -----
- BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block BatchNorm2dSK153625602(3, 3)0.25reflect, number 3 -----
- BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block BatchNorm2dSK256025602(3, 3)0.25reflect, number 4 -----
- BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 5 -----
- Flatten(start_dim=1, end_dim=-1)
- Dropout(p=0.5, inplace=False)
- Linear(in_features=2560, out_features=2, bias=True)
model.heads:  []
[Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (5): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=2560, out_features=2, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)]
LAYER NAME:  Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (5): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=2560, out_features=2, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)
LAYER CHILDREN:  [BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=1.4)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
  )
  (layer): Linear(in_features=2560, out_features=2, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)]
subsubl NAME:  Sequential(
  (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=1.4)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): Flatten(start_dim=1, end_dim=-1)
)
subsubl NAME:  Linear(in_features=2560, out_features=2, bias=True)
subsubl NAME:  Dropout(p=0.5, inplace=False)
CONFIG MODE:  unsupervised

 ********** Hebbian Unsupervised learning of blocks [0, 1, 2, 3, 4] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([49, 33, 72, 51, 71, 92, 15, 14, 23,  0, 71, 75, 81, 69, 40, 43, 92, 97,
        70, 53], device='cuda:0')
TARGETS AFTER SUB:  tensor([93, 32, 32, 32, 32, 93, 32, 93, 32, 93, 32, 93, 93, 93, 32, 32, 93, 32,
        32, 93])
[32, 93]
TARGETS AFTER CLEANER:  tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1])
200
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 200
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               None
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([19, 29,  0, 11,  1, 86, 90, 28, 23, 31, 39, 96, 82, 17, 71, 39,  8, 97,
        80, 71], device='cuda:0')
TARGETS AFTER SUB:  tensor([93, 93, 32, 32, 32, 32, 32, 32, 32, 32, 93, 32, 93, 93, 93, 32, 32, 32,
        93, 32])
[32, 93]
TARGETS AFTER CLEANER:  tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0])
1000
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 1000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               None
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([19, 29,  0, 11,  1, 86, 90, 28, 23, 31, 39, 96, 82, 17, 71, 39,  8, 97,
        80, 71], device='cuda:0')
TARGETS AFTER SUB:  tensor([93, 93, 32, 32, 32, 32, 32, 32, 32, 32, 93, 32, 93, 93, 93, 32, 32, 32,
        93, 32])
[32, 93]
TARGETS AFTER CLEANER:  tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0])
1000
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 1000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
------------------------
INDICES:  1000
IMAGE SIZE: torch.Size([10, 3, 32, 32])
SAVING FOLDER FOR UNSUP:  C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c
prev_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight', 'blocks.4.layer.weight'])
activations_sum.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight', 'blocks.4.layer.weight'])
DEPTH:  5
WTA IN delta_weight:  tensor([[[-3.6506e-18, -6.4738e-22, -6.7331e-26,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-1.6613e-16, -9.0138e-21, -1.1113e-25,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-1.6134e-15, -1.0593e-17, -2.0740e-26,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         ...,
         [-8.3984e-22, -6.9171e-17, -1.8362e-18,  ..., -2.6664e-39,
          -1.7885e-41, -3.8225e-40],
         [-1.2803e-21, -2.8689e-17, -1.9457e-17,  ..., -8.5437e-41,
          -7.2111e-42, -9.0482e-42],
         [-4.9964e-22, -5.3510e-18, -2.7924e-18,  ..., -2.6513e-41,
          -6.0956e-43, -5.9582e-39]],

        [[-3.9832e-14, -2.9432e-16, -9.3467e-18,  ..., -3.0688e-43,
          -6.8029e-41, -1.9436e-42],
         [-6.9989e-13, -2.3823e-17, -1.5971e-17,  ..., -2.6765e-43,
          -3.5678e-41, -7.7632e-43],
         [-8.3714e-11, -2.6326e-12, -3.8659e-15,  ..., -1.3593e-43,
          -3.9026e-42, -1.7128e-41],
         ...,
         [-1.6998e-23, -6.0491e-20, -2.6041e-24,  ..., -9.3484e-33,
          -6.9948e-34, -1.0852e-33],
         [-9.1599e-22, -2.8261e-19, -1.7392e-22,  ..., -6.8636e-32,
          -3.7794e-33, -4.3284e-34],
         [-3.2815e-22, -1.3935e-19, -7.6304e-23,  ..., -2.7705e-32,
          -5.4531e-35, -1.2336e-32]],

        [[-1.6987e-15, -1.3348e-14, -5.2621e-15,  ..., -7.0205e-43,
          -5.0769e-42, -7.4689e-43],
         [-3.2235e-12, -5.9260e-15, -7.1369e-18,  ..., -4.2599e-43,
          -3.3309e-42, -1.6129e-42],
         [-6.9292e-11, -3.5790e-10, -2.0700e-14,  ..., -9.6269e-43,
          -3.7022e-42, -4.6391e-41],
         ...,
         [-9.0021e-24, -4.8030e-23, -1.2424e-24,  ..., -2.6113e-31,
          -1.5330e-34, -7.1683e-35],
         [-2.8304e-22, -5.9075e-22, -6.2943e-24,  ..., -4.2929e-32,
          -3.4719e-33, -4.7936e-35],
         [-6.5808e-22, -2.3897e-21, -3.6304e-24,  ..., -9.3744e-32,
          -1.7989e-34, -1.3215e-33]],

        ...,

        [[-2.7324e-08, -3.8193e-09, -2.1451e-09,  ..., -4.1232e-16,
          -5.2596e-11, -1.1333e-11],
         [-1.4910e-05, -8.0173e-08, -6.3713e-11,  ..., -1.0036e-13,
          -3.5862e-13, -8.7637e-12],
         [-7.6475e-08, -2.2074e-08, -2.3588e-11,  ..., -2.5970e-12,
          -1.4522e-12, -2.4895e-10],
         ...,
         [-1.3719e-33, -7.4348e-32, -1.8569e-35,  ..., -5.2168e-09,
          -4.5521e-10, -1.0803e-08],
         [-2.3019e-32, -1.3994e-30, -7.5081e-34,  ..., -1.0724e-10,
          -2.3993e-10, -1.7365e-08],
         [-9.6085e-32, -1.6798e-29, -2.7896e-33,  ..., -4.5639e-10,
          -1.5032e-09, -9.9181e-07]],

        [[-2.4233e-12, -5.4699e-15, -1.2672e-16,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-4.1611e-12, -8.0076e-14, -3.9421e-17,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-4.9747e-09, -5.6396e-10, -5.3493e-14,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         ...,
         [-9.0181e-11, -3.7372e-11, -9.4153e-11,  ..., -1.4260e-39,
          -2.9175e-42, -8.2032e-42],
         [-6.3514e-11, -8.3314e-11, -9.0197e-11,  ..., -3.1915e-41,
          -6.2960e-42, -6.3058e-44],
         [-7.4598e-11, -4.2758e-11, -1.5655e-10,  ..., -5.5636e-40,
          -1.0103e-42, -1.0283e-41]],

        [[-4.8594e-12, -4.3720e-12, -1.6940e-13,  ..., -9.6043e-37,
          -7.5617e-34, -4.0956e-34],
         [-9.8894e-12, -2.7543e-12, -6.0267e-15,  ..., -1.4945e-35,
          -7.2859e-35, -6.6785e-34],
         [-1.0295e-12, -6.7734e-11, -1.1225e-12,  ..., -2.1073e-34,
          -2.1258e-35, -1.6023e-33],
         ...,
         [-1.2904e-24, -3.0748e-24, -2.1683e-26,  ..., -1.5311e-24,
          -4.6867e-27, -5.3435e-26],
         [-1.2679e-24, -2.3482e-23, -3.3176e-25,  ..., -1.6880e-25,
          -8.8044e-27, -1.1810e-26],
         [-5.8572e-25, -1.0179e-23, -4.8843e-25,  ..., -1.2818e-25,
          -9.3258e-28, -3.0582e-25]]], device='cuda:0')
self.avg_deltas_layer:  <class 'NoneType'>
self.topk_layer:  <class 'NoneType'>
self.avg_deltas_layer:  <class 'NoneType'>
self.topk_layer:  <class 'NoneType'>
self.avg_deltas_layer:  <class 'NoneType'>
self.topk_layer:  <class 'NoneType'>
self.avg_deltas_layer:  <class 'NoneType'>
self.topk_layer:  <class 'NoneType'>
self.avg_deltas_layer:  <class 'NoneType'>
self.topk_layer:  <class 'NoneType'>
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight', 'blocks.4.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight', 'blocks.4.layer.weight'])
activations_sum[k] 96
activations_sum[k]:  [77, 68, 25, 59, 93, 5, 30, 14, 35, 70]
topk_kernels len:  1
topk_kernels keys:  ['conv0']
topk_kernels:  {'conv0': [77, 68, 25, 59, 93, 5, 30, 14, 35, 70, 4, 45, 69, 82, 86]}
activations_sum[k] len:  96
activations_sum[k] 384
activations_sum[k]:  [365, 50, 382, 75, 180, 264, 56, 345, 104, 352]
topk_kernels len:  2
topk_kernels keys:  ['conv0', 'conv1']
topk_kernels:  {'conv0': [77, 68, 25, 59, 93, 5, 30, 14, 35, 70, 4, 45, 69, 82, 86], 'conv1': [365, 50, 382, 75, 180, 264, 56, 345, 104, 352, 206, 100, 190, 324, 354, 99, 73, 140, 336, 355, 121, 191, 233, 144, 243, 163, 193, 150, 4, 26, 373, 266, 118, 92, 357, 117, 94, 90, 158, 68, 179, 202, 368, 76, 36, 170, 304, 146, 41, 35, 33, 138, 6, 105, 262, 197, 293, 317, 17]}
activations_sum[k] len:  384
activations_sum[k] 1536
activations_sum[k]:  [307, 937, 662, 141, 1307, 1006, 869, 683, 1441, 1238]
topk_kernels len:  3
topk_kernels keys:  ['conv0', 'conv1', 'conv2']
topk_kernels:  {'conv0': [77, 68, 25, 59, 93, 5, 30, 14, 35, 70, 4, 45, 69, 82, 86], 'conv1': [365, 50, 382, 75, 180, 264, 56, 345, 104, 352, 206, 100, 190, 324, 354, 99, 73, 140, 336, 355, 121, 191, 233, 144, 243, 163, 193, 150, 4, 26, 373, 266, 118, 92, 357, 117, 94, 90, 158, 68, 179, 202, 368, 76, 36, 170, 304, 146, 41, 35, 33, 138, 6, 105, 262, 197, 293, 317, 17], 'conv2': [307, 937, 662, 141, 1307, 1006, 869, 683, 1441, 1238, 348, 1086, 1278, 37, 1404, 427, 889, 311, 341, 1405, 796, 312, 1500, 794, 1486, 1370, 490, 304, 449, 835, 1297, 1100, 28, 942, 731, 537, 710, 380, 612, 451, 1159, 735, 1139, 1046, 142, 1385, 998, 176, 41, 295, 958, 346, 313, 1436, 1034, 167, 1360, 906, 529, 441, 214, 1306, 1395, 79, 1450, 544, 1412, 660, 542, 1000, 1372, 700, 405, 613, 61, 14, 1243, 711, 195, 673, 636, 1082, 107, 1317, 635, 590, 551, 997, 209, 207, 1474, 987, 1226, 865, 706, 1172, 421, 733, 70, 1143, 94, 959, 896, 547, 229, 423, 11, 838, 923, 856, 407, 851, 1345, 914, 1359, 622, 259, 1464, 1400, 460, 268, 730, 1532, 1266, 454, 634, 806, 450, 577, 35, 12, 1027, 1449, 1534, 1466, 872, 534, 752, 1489, 1101, 1230, 1134, 199, 434, 1185, 932, 915, 123, 766, 1429, 250, 1033, 155, 335, 1030, 276, 457, 953, 174, 799, 828, 1452, 891, 626, 496, 1149, 831, 456, 510, 194, 68, 810, 761, 981, 1242, 1291, 1294, 1535, 75, 499, 935, 1272, 501, 912, 1347, 921, 353, 859, 640, 1032, 1507, 1171, 373, 100, 287, 281, 1442, 944, 1083, 406, 356, 549, 187, 883, 1265, 919, 619, 109, 783, 870, 760, 927, 868, 904, 644, 1249, 420, 1113, 1455, 1211, 1183, 78, 95, 13, 1167, 261, 839, 817, 1274, 252, 994]}
activations_sum[k] len:  1536
activations_sum[k] 2560
activations_sum[k]:  [1422, 43, 1677, 114, 1319, 2000, 471, 2051, 165, 909]
topk_kernels len:  4
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3']
topk_kernels:  {'conv0': [77, 68, 25, 59, 93, 5, 30, 14, 35, 70, 4, 45, 69, 82, 86], 'conv1': [365, 50, 382, 75, 180, 264, 56, 345, 104, 352, 206, 100, 190, 324, 354, 99, 73, 140, 336, 355, 121, 191, 233, 144, 243, 163, 193, 150, 4, 26, 373, 266, 118, 92, 357, 117, 94, 90, 158, 68, 179, 202, 368, 76, 36, 170, 304, 146, 41, 35, 33, 138, 6, 105, 262, 197, 293, 317, 17], 'conv2': [307, 937, 662, 141, 1307, 1006, 869, 683, 1441, 1238, 348, 1086, 1278, 37, 1404, 427, 889, 311, 341, 1405, 796, 312, 1500, 794, 1486, 1370, 490, 304, 449, 835, 1297, 1100, 28, 942, 731, 537, 710, 380, 612, 451, 1159, 735, 1139, 1046, 142, 1385, 998, 176, 41, 295, 958, 346, 313, 1436, 1034, 167, 1360, 906, 529, 441, 214, 1306, 1395, 79, 1450, 544, 1412, 660, 542, 1000, 1372, 700, 405, 613, 61, 14, 1243, 711, 195, 673, 636, 1082, 107, 1317, 635, 590, 551, 997, 209, 207, 1474, 987, 1226, 865, 706, 1172, 421, 733, 70, 1143, 94, 959, 896, 547, 229, 423, 11, 838, 923, 856, 407, 851, 1345, 914, 1359, 622, 259, 1464, 1400, 460, 268, 730, 1532, 1266, 454, 634, 806, 450, 577, 35, 12, 1027, 1449, 1534, 1466, 872, 534, 752, 1489, 1101, 1230, 1134, 199, 434, 1185, 932, 915, 123, 766, 1429, 250, 1033, 155, 335, 1030, 276, 457, 953, 174, 799, 828, 1452, 891, 626, 496, 1149, 831, 456, 510, 194, 68, 810, 761, 981, 1242, 1291, 1294, 1535, 75, 499, 935, 1272, 501, 912, 1347, 921, 353, 859, 640, 1032, 1507, 1171, 373, 100, 287, 281, 1442, 944, 1083, 406, 356, 549, 187, 883, 1265, 919, 619, 109, 783, 870, 760, 927, 868, 904, 644, 1249, 420, 1113, 1455, 1211, 1183, 78, 95, 13, 1167, 261, 839, 817, 1274, 252, 994], 'conv3': [1422, 43, 1677, 114, 1319, 2000, 471, 2051, 165, 909, 1874, 1660, 934, 1549, 1608, 2302, 2494, 1312, 2352, 1548, 589, 2069, 1083, 499, 1520, 1988, 1873, 1187, 1110, 195, 2438, 1168, 167, 2138, 1090, 882, 512, 505, 398, 1938, 1827, 648, 1127, 1776, 468, 423, 1331, 1109, 2448, 784, 1495, 1554, 2081, 907, 1558, 2533, 2281, 781, 1616, 694, 278, 1063, 980, 2247, 249, 1245, 305, 1857, 867, 443, 2556, 1709, 359, 178, 1164, 419, 841, 2095, 1049, 95, 28, 137, 2181, 1403, 1359, 377, 231, 136, 2001, 1447, 1744, 2315, 1443, 2067, 2101, 1723, 1990, 210, 1866, 1926, 1303, 1887, 2515, 646, 356, 2391, 129, 667, 2037, 1038, 2049, 2152, 1783, 2460, 103, 1641, 878, 757, 272, 1073, 2221, 1417, 565, 2007, 1088, 192, 1506, 2085, 1930, 1435, 2170, 251, 2342, 327, 452, 2241, 1045, 1789, 2244, 2185, 1468, 148, 932, 147, 2008, 2248, 303, 1774, 2500, 1651, 507, 923, 339, 792, 1522, 1879, 2300, 2374, 1280, 2154, 334, 564, 2334, 2151, 1177, 68, 1854, 2435, 1865, 1955, 2131, 533, 611, 446, 1689, 939, 2317, 540, 689, 922, 1843, 450, 1036, 261, 1798, 1260, 2278, 1801, 1497, 372, 1452, 1324, 1614, 1612, 347, 529, 621, 1446, 2471, 2146, 2249, 2456, 892, 514, 555, 2290, 2429, 1043, 1720, 633, 1363, 894, 634, 2021, 477, 1591, 2106, 2193, 1232, 739, 1269, 185, 674, 2102, 2511, 1758, 828, 1845, 1824, 2547, 1105, 780, 750, 350, 1395, 21, 526, 2549, 233, 2479, 1188, 33, 297, 2100, 1205, 1211, 1346, 709, 881, 54, 861, 1029, 1283, 819, 534, 299, 1670, 1698, 652, 245, 46, 2119, 1982, 2378, 556, 1183, 1242, 1527, 940, 1054, 2279, 497, 2497, 196, 1344, 949, 2018, 2414, 1009, 2215, 1394, 1958, 1226, 2475, 42, 989, 1872, 1089, 863, 1685, 2309, 454, 2242, 1256, 2243, 2161, 1425, 361, 1559, 1567, 2112, 187, 697, 1112, 1037, 1784, 1898, 1378, 1348, 193, 241, 627, 2401, 2145, 481, 2283, 1333, 1894, 1146, 2125, 1326, 1411, 1057, 1924, 2338, 1438, 679, 653, 518, 436, 1998, 1968, 994, 1252, 1978, 2030, 465, 328, 494, 1599, 2450, 660, 664, 197, 1578, 1735, 506, 2465, 2276, 225, 1141, 202, 2316, 106, 389, 80, 1584, 1085, 2331, 2072, 1362, 378, 1208, 2427, 1385, 1755, 1684, 2090, 2261, 2122, 1066, 544, 1266, 1349, 1664, 2183, 2293, 1131, 945, 2282, 122, 982, 1413, 1258, 1995]}
activations_sum[k] len:  2560
activations_sum[k] 2560
activations_sum[k]:  [1575, 1644, 315, 2388, 4, 1323, 1622, 1859, 2228, 2029]
topk_kernels len:  5
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3', 'conv4']
topk_kernels:  {'conv0': [77, 68, 25, 59, 93, 5, 30, 14, 35, 70, 4, 45, 69, 82, 86], 'conv1': [365, 50, 382, 75, 180, 264, 56, 345, 104, 352, 206, 100, 190, 324, 354, 99, 73, 140, 336, 355, 121, 191, 233, 144, 243, 163, 193, 150, 4, 26, 373, 266, 118, 92, 357, 117, 94, 90, 158, 68, 179, 202, 368, 76, 36, 170, 304, 146, 41, 35, 33, 138, 6, 105, 262, 197, 293, 317, 17], 'conv2': [307, 937, 662, 141, 1307, 1006, 869, 683, 1441, 1238, 348, 1086, 1278, 37, 1404, 427, 889, 311, 341, 1405, 796, 312, 1500, 794, 1486, 1370, 490, 304, 449, 835, 1297, 1100, 28, 942, 731, 537, 710, 380, 612, 451, 1159, 735, 1139, 1046, 142, 1385, 998, 176, 41, 295, 958, 346, 313, 1436, 1034, 167, 1360, 906, 529, 441, 214, 1306, 1395, 79, 1450, 544, 1412, 660, 542, 1000, 1372, 700, 405, 613, 61, 14, 1243, 711, 195, 673, 636, 1082, 107, 1317, 635, 590, 551, 997, 209, 207, 1474, 987, 1226, 865, 706, 1172, 421, 733, 70, 1143, 94, 959, 896, 547, 229, 423, 11, 838, 923, 856, 407, 851, 1345, 914, 1359, 622, 259, 1464, 1400, 460, 268, 730, 1532, 1266, 454, 634, 806, 450, 577, 35, 12, 1027, 1449, 1534, 1466, 872, 534, 752, 1489, 1101, 1230, 1134, 199, 434, 1185, 932, 915, 123, 766, 1429, 250, 1033, 155, 335, 1030, 276, 457, 953, 174, 799, 828, 1452, 891, 626, 496, 1149, 831, 456, 510, 194, 68, 810, 761, 981, 1242, 1291, 1294, 1535, 75, 499, 935, 1272, 501, 912, 1347, 921, 353, 859, 640, 1032, 1507, 1171, 373, 100, 287, 281, 1442, 944, 1083, 406, 356, 549, 187, 883, 1265, 919, 619, 109, 783, 870, 760, 927, 868, 904, 644, 1249, 420, 1113, 1455, 1211, 1183, 78, 95, 13, 1167, 261, 839, 817, 1274, 252, 994], 'conv3': [1422, 43, 1677, 114, 1319, 2000, 471, 2051, 165, 909, 1874, 1660, 934, 1549, 1608, 2302, 2494, 1312, 2352, 1548, 589, 2069, 1083, 499, 1520, 1988, 1873, 1187, 1110, 195, 2438, 1168, 167, 2138, 1090, 882, 512, 505, 398, 1938, 1827, 648, 1127, 1776, 468, 423, 1331, 1109, 2448, 784, 1495, 1554, 2081, 907, 1558, 2533, 2281, 781, 1616, 694, 278, 1063, 980, 2247, 249, 1245, 305, 1857, 867, 443, 2556, 1709, 359, 178, 1164, 419, 841, 2095, 1049, 95, 28, 137, 2181, 1403, 1359, 377, 231, 136, 2001, 1447, 1744, 2315, 1443, 2067, 2101, 1723, 1990, 210, 1866, 1926, 1303, 1887, 2515, 646, 356, 2391, 129, 667, 2037, 1038, 2049, 2152, 1783, 2460, 103, 1641, 878, 757, 272, 1073, 2221, 1417, 565, 2007, 1088, 192, 1506, 2085, 1930, 1435, 2170, 251, 2342, 327, 452, 2241, 1045, 1789, 2244, 2185, 1468, 148, 932, 147, 2008, 2248, 303, 1774, 2500, 1651, 507, 923, 339, 792, 1522, 1879, 2300, 2374, 1280, 2154, 334, 564, 2334, 2151, 1177, 68, 1854, 2435, 1865, 1955, 2131, 533, 611, 446, 1689, 939, 2317, 540, 689, 922, 1843, 450, 1036, 261, 1798, 1260, 2278, 1801, 1497, 372, 1452, 1324, 1614, 1612, 347, 529, 621, 1446, 2471, 2146, 2249, 2456, 892, 514, 555, 2290, 2429, 1043, 1720, 633, 1363, 894, 634, 2021, 477, 1591, 2106, 2193, 1232, 739, 1269, 185, 674, 2102, 2511, 1758, 828, 1845, 1824, 2547, 1105, 780, 750, 350, 1395, 21, 526, 2549, 233, 2479, 1188, 33, 297, 2100, 1205, 1211, 1346, 709, 881, 54, 861, 1029, 1283, 819, 534, 299, 1670, 1698, 652, 245, 46, 2119, 1982, 2378, 556, 1183, 1242, 1527, 940, 1054, 2279, 497, 2497, 196, 1344, 949, 2018, 2414, 1009, 2215, 1394, 1958, 1226, 2475, 42, 989, 1872, 1089, 863, 1685, 2309, 454, 2242, 1256, 2243, 2161, 1425, 361, 1559, 1567, 2112, 187, 697, 1112, 1037, 1784, 1898, 1378, 1348, 193, 241, 627, 2401, 2145, 481, 2283, 1333, 1894, 1146, 2125, 1326, 1411, 1057, 1924, 2338, 1438, 679, 653, 518, 436, 1998, 1968, 994, 1252, 1978, 2030, 465, 328, 494, 1599, 2450, 660, 664, 197, 1578, 1735, 506, 2465, 2276, 225, 1141, 202, 2316, 106, 389, 80, 1584, 1085, 2331, 2072, 1362, 378, 1208, 2427, 1385, 1755, 1684, 2090, 2261, 2122, 1066, 544, 1266, 1349, 1664, 2183, 2293, 1131, 945, 2282, 122, 982, 1413, 1258, 1995], 'conv4': [1575, 1644, 315, 2388, 4, 1323, 1622, 1859, 2228, 2029, 263, 770, 1131, 1289, 2356, 544, 1187, 2123, 1193, 188, 2237, 1610, 1746, 1064, 264, 1908, 806, 1444, 2413, 891, 1580, 412, 1288, 1993, 1706, 2207, 1402, 1757, 1185, 1109, 493, 502, 2285, 2353, 2475, 778, 1143, 2537, 2008, 615, 67, 1524, 2142, 1795, 1366, 2070, 2041, 448, 2064, 1654, 1167, 514, 549, 1975, 309, 1732, 1019, 1957, 1266, 1629, 1481, 1855, 893, 34, 1067, 1326, 231, 2346, 919, 1819, 366, 1667, 2268, 1082, 1620, 132, 6, 1521, 929, 1428, 1867, 1969, 1631, 621, 2463, 1585, 774, 277, 551, 2254, 783, 1659, 1684, 2131, 1813, 603, 792, 556, 288, 1604, 1041, 2209, 2292, 1499, 2510, 1641, 1749, 805, 2262, 1682, 1967, 2517, 1668, 1857, 593, 1676, 2524, 969, 2038, 802, 489, 670, 435, 1063, 1988, 2286, 1062, 829, 2225, 230, 1285, 687, 437, 2183, 568, 673, 2158, 2518, 99, 2396, 851, 1084, 1152, 895, 711, 397, 1833, 1922, 8, 2015, 2461, 1603, 272, 292, 1066, 1244, 2143, 2523, 21, 586, 1214, 1439, 648, 703, 1734, 1727, 401, 433, 2014, 1874, 2385, 718, 294, 26, 675, 2218, 704, 1490, 1468, 1267, 931, 1191, 1251, 2439, 331, 1212, 592, 558, 59, 1756, 139, 1674, 2335, 2528, 2539, 1538, 2210, 2383, 1529, 389, 2267, 1787, 2163, 1657, 1726, 2338, 527, 843, 91, 1254, 2121, 386, 1498, 2359, 1877, 865, 507, 1739, 1190, 2022, 1134, 7, 936, 1434, 1412, 1828, 150, 2152, 2224, 1974, 862, 1870, 1672, 1035, 725, 1043, 1539, 297, 339, 1070, 1261, 2336, 2078, 1764, 889, 1541, 2300, 1898, 1801, 599, 1221, 1208, 381, 1286, 1996, 1399, 671, 1218, 1618, 1257, 1688, 1375, 1534, 1730, 674, 233, 1429, 1454, 2249, 2415, 536, 1778, 2046, 987, 1448, 553, 2543, 473, 1296, 1130, 1204, 641, 2133, 1883, 1535, 70, 1355, 1354, 2174, 1452, 1991, 2053, 1656, 997, 508, 2227, 1832, 2089, 1445, 612, 1652, 182, 1338, 1115, 1471, 1483, 2557, 175, 761, 1685, 1313, 2211, 2496, 510, 455, 1772, 2472, 1472, 1998, 1596, 894, 1308, 1104, 1018, 859, 734, 1624, 1034, 1536, 945, 1514, 1613, 1915, 33, 343, 1518, 2203, 1540, 1865, 65, 1276, 1201, 172, 656, 460, 2555, 1560, 2259, 1731, 2404, 405, 135, 203, 1766, 149, 225, 1928, 2195, 1612, 457, 253, 208, 518, 487, 258, 1885, 1357, 1567, 2213, 1990, 1784, 1485, 2149, 2257, 1422]}
activations_sum[k] len:  2560
['conv0', 'conv1', 'conv2', 'conv3', 'conv4']
delta_weights INFO: 
NUM OF TRACKED COV LAYERS:  5
NUM OF TRACKED WEIGHTS CHANGES PER LAYER:  2
avg_deltas INFO:  <class 'dict'>
avg_deltas keys:  ['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight', 'blocks.4.layer.weight']
################################################
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 4.84e-02	time: 00:00:15	Acc_train 0.00	Acc_test 0.00	convergence: 2.37e+01	R1: 0	Info MB:0.000e+00/SB:0.000e+00/MW:1.019e-06/SW:1.627e-01/MR:2.466e+01/SR:1.355e+00/MeD:7.433e-01/MaD:2.086e+01/MW:0.643/MAW:0.357
|        0 |        1 |        2 |        3 |        4 |        5 |        6 |        7 |        8 |        9 |       10 |       11 |       12 |       13 |       14 |       15 |       16 |       17 |       18 |       19 |       20 |       21 |       22 |       23 |       24 |       25 |       26 |       27 |       28 |       29 |
|----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------|
|   0.0497 |   0.0498 |   0.0496 |   0.0491 |   0.0487 |   0.0475 |   0.0482 |   0.0478 |   0.0473 |   0.0476 |   0.0489 |   0.0491 |   0.0491 |   0.0501 |   0.0492 |   0.0474 |   0.0485 |   0.0486 |   0.0496 |   0.0491 |   0.0494 |   0.0501 |   0.0483 |   0.0493 |   0.0491 |   0.0483 |   0.0491 |   0.0489 |   0.0495 |   0.0492 |
|  25.67   |  25.81   |  25.56   |  25.1    |  24.76   |  23.58   |  24.19   |  23.85   |  23.39   |  23.7    |  24.93   |  25.1    |  25.13   |  26.07   |  25.21   |  23.46   |  24.5    |  24.61   |  25.56   |  25.07   |  25.44   |  26.05   |  24.32   |  25.27   |  25.08   |  24.36   |  25.13   |  24.95   |  25.48   |  25.21   |
|   0.02   |   0.08   |   0.02   |   0      |   0.03   |   0.07   |   0.04   |   0.13   |   0.07   |   0.09   |   0.01   |   0.02   |   0.01   |   0.03   |   0.1    |   0.05   |   0.02   |   0.01   |   0.01   |   0      |   0.01   |   0.04   |   0.02   |   0.02   |   0      |   0.02   |   0.01   |   0.03   |   0.02   |   0      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:-4.432e-05/SW:2.086e-01/MR:2.449e+01/SR:1.272e+00/MeD:8.125e-01/MaD:1.515e+01/MW:0.566/MAW:0.434
|        0 |        1 |        2 |        3 |        4 |        5 |        6 |        7 |        8 |        9 |       10 |      11 |       12 |       13 |       14 |       15 |       16 |       17 |       18 |       19 |       20 |       21 |       22 |       23 |       24 |       25 |       26 |       27 |       28 |       29 |
|----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------|
|   0.0482 |   0.0498 |   0.0486 |   0.0498 |   0.0493 |   0.0476 |   0.0496 |   0.0485 |   0.0402 |   0.0489 |   0.0479 |   0.048 |   0.0491 |   0.0488 |   0.0494 |   0.0492 |   0.0498 |   0.0494 |   0.0482 |   0.0491 |   0.0495 |   0.0472 |   0.0489 |   0.0491 |   0.0492 |   0.0462 |   0.0492 |   0.0489 |   0.0487 |   0.0483 |
|  24.26   |  25.8    |  24.63   |  25.79   |  25.32   |  23.67   |  25.64   |  24.48   |  17.17   |  24.87   |  23.99   |  24.06  |  25.1    |  24.86   |  25.39   |  25.18   |  25.76   |  25.36   |  24.23   |  25.14   |  25.5    |  23.29   |  24.91   |  25.1    |  25.16   |  22.3    |  25.21   |  24.88   |  24.73   |  24.31   |
|   0.05   |   0.05   |   0.04   |   0.04   |   0      |   0.04   |   0.02   |   0.01   |   0.23   |   0      |   0.04   |   0.06  |   0      |   0.02   |   0.01   |   0.02   |   0.04   |   0.01   |   0.01   |   0.01   |   0.01   |   0.07   |   0.01   |   0.01   |   0      |   0.08   |   0.06   |   0.01   |   0.03   |   0.04   |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:1.146e-04/SW:4.166e-01/MR:2.445e+01/SR:1.386e+00/MeD:9.320e-01/MaD:9.048e+00/MW:0.496/MAW:0.504
|       0 |        1 |        2 |        3 |        4 |        5 |        6 |        7 |        8 |       9 |       10 |       11 |       12 |       13 |       14 |       15 |       16 |       17 |       18 |       19 |       20 |       21 |       22 |       23 |       24 |       25 |       26 |       27 |       28 |       29 |
|---------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------|
|   0.049 |   0.0494 |   0.0492 |   0.0467 |   0.0479 |   0.0489 |   0.0495 |   0.0459 |   0.0483 |   0.049 |   0.0495 |   0.0485 |   0.0491 |   0.0449 |   0.0492 |   0.0491 |   0.0472 |   0.0496 |   0.0484 |   0.0492 |   0.0467 |   0.0491 |   0.0482 |   0.0496 |   0.0473 |   0.0491 |   0.0492 |   0.0478 |   0.0483 |   0.0485 |
|  25.06  |  25.38   |  25.2    |  22.82   |  23.99   |  24.96   |  25.51   |  22.05   |  24.36   |  25.04  |  25.49   |  24.54   |  25.09   |  21.2    |  25.16   |  25.14   |  23.32   |  25.64   |  24.47   |  25.23   |  22.8    |  25.13   |  24.24   |  25.56   |  23.37   |  25.07   |  25.22   |  23.86   |  24.3    |  24.56   |
|   0     |   0.01   |   0.02   |   0.14   |   0.04   |   0.03   |   0      |   0.11   |   0.05   |   0     |   0.01   |   0.1    |   0.06   |   0.19   |   0.17   |   0.05   |   0.27   |   0.01   |   0.05   |   0      |   0.21   |   0.03   |   0.17   |   0.03   |   0.08   |   0.01   |   0.01   |   0.07   |   0.12   |   0.01   |
| nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |
| nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |
| nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:-3.225e-03/SW:8.205e-01/MR:2.406e+01/SR:1.629e+00/MeD:1.107e+00/MaD:7.987e+00/MW:0.792/MAW:0.208
|        0 |        1 |        2 |        3 |       4 |        5 |        6 |       7 |        8 |       9 |       10 |       11 |       12 |       13 |       14 |       15 |       16 |       17 |       18 |       19 |       20 |       21 |       22 |       23 |       24 |       25 |       26 |       27 |       28 |       29 |
|----------+----------+----------+----------+---------+----------+----------+---------+----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------|
|   0.0246 |   0.0245 |   0.0242 |   0.0241 |   0.021 |   0.0246 |   0.0244 |   0.025 |   0.0244 |   0.024 |   0.0248 |   0.0241 |   0.0248 |   0.0246 |   0.0242 |   0.0242 |   0.0241 |   0.0236 |   0.0242 |   0.0245 |   0.0238 |   0.0248 |   0.0245 |   0.0245 |   0.0242 |   0.0239 |   0.0239 |   0.0241 |   0.0241 |   0.0251 |
|  25.19   |  25.11   |  24.49   |  24.32   |  18.72  |  25.18   |  24.85   |  26.1   |  24.87   |  24.12  |  25.61   |  24.25   |  25.55   |  25.13   |  24.47   |  24.5    |  24.16   |  23.2    |  24.48   |  25.08   |  23.67   |  25.68   |  24.92   |  25.07   |  24.42   |  23.94   |  23.86   |  24.3    |  24.23   |  26.12   |
|   0.08   |   0.01   |   0      |   0.01   |   2.3   |   0.25   |   0.02   |   0.06  |   0.01   |   0.12  |   0      |   0.04   |   0.03   |   0.03   |   0.01   |   0.04   |   0.02   |   0.53   |   0.07   |   0.15   |   0.08   |   0      |   0.11   |   0.15   |   0.06   |   0.09   |   0.46   |   0.06   |   0.17   |   0.02   |
| nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan     | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan     | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan     | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:-8.042e-03/SW:1.357e+00/MR:1.144e+01/SR:2.693e+00/MeD:2.132e+00/MaD:8.129e+00/MW:0.577/MAW:0.423
|       0 |       1 |       2 |       3 |       4 |       5 |       6 |       7 |       8 |       9 |      10 |      11 |      12 |      13 |      14 |      15 |     16 |      17 |      18 |      19 |      20 |      21 |      22 |     23 |      24 |     25 |      26 |      27 |      28 |     29 |
|---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------+---------+---------+---------+---------+---------+---------+--------+---------+--------+---------+---------+---------+--------|
|   0.242 |   0.321 |   0.299 |   0.298 |   0.219 |   0.208 |   0.255 |   0.333 |   0.234 |   0.345 |   0.239 |   0.246 |   0.273 |   0.291 |   0.241 |   0.297 |   0.19 |   0.258 |   0.265 |   0.244 |   0.282 |   0.246 |   0.278 |   0.32 |   0.296 |   0.21 |   0.286 |   0.264 |   0.263 |   0.31 |
|  10.11  |  17.12  |  14.98  |  14.85  |   8.53  |   7.73  |  11.19  |  18.31  |   9.55  |  19.57  |   9.94  |  10.43  |  12.63  |  14.22  |  10.1   |  14.76  |   6.67 |  11.37  |  11.99  |  10.33  |  13.44  |  10.47  |  13.07  |  17.05 |  14.68  |   7.91 |  13.79  |  11.88  |  11.84  |  16.03 |
|   0.75  |   0.2   |   0.15  |   0.27  |   1.5   |   3.07  |   1.09  |   0.25  |   1.2   |   0.14  |   1.6   |   0.67  |   0.64  |   0.3   |   1.68  |   0.24  |   3.11 |   0.64  |   0.69  |   1.13  |   0.6   |   1.03  |   0.54  |   0.3  |   0.42  |   2.26 |   0.5   |   0.72  |   0.4   |   0.21 |
| nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan    | nan     | nan     | nan     | nan    |
| nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan    | nan     | nan     | nan     | nan    |
| nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan    | nan     | nan     | nan     | nan    |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [77, 68, 25, 59, 93, 5, 30, 14, 35, 70, 4, 45, 69, 82, 86], 'conv1': [365, 50, 382, 75, 180, 264, 56, 345, 104, 352, 206, 100, 190, 324, 354, 99, 73, 140, 336, 355, 121, 191, 233, 144, 243, 163, 193, 150, 4, 26, 373, 266, 118, 92, 357, 117, 94, 90, 158, 68, 179, 202, 368, 76, 36, 170, 304, 146, 41, 35, 33, 138, 6, 105, 262, 197, 293, 317, 17], 'conv2': [307, 937, 662, 141, 1307, 1006, 869, 683, 1441, 1238, 348, 1086, 1278, 37, 1404, 427, 889, 311, 341, 1405, 796, 312, 1500, 794, 1486, 1370, 490, 304, 449, 835, 1297, 1100, 28, 942, 731, 537, 710, 380, 612, 451, 1159, 735, 1139, 1046, 142, 1385, 998, 176, 41, 295, 958, 346, 313, 1436, 1034, 167, 1360, 906, 529, 441, 214, 1306, 1395, 79, 1450, 544, 1412, 660, 542, 1000, 1372, 700, 405, 613, 61, 14, 1243, 711, 195, 673, 636, 1082, 107, 1317, 635, 590, 551, 997, 209, 207, 1474, 987, 1226, 865, 706, 1172, 421, 733, 70, 1143, 94, 959, 896, 547, 229, 423, 11, 838, 923, 856, 407, 851, 1345, 914, 1359, 622, 259, 1464, 1400, 460, 268, 730, 1532, 1266, 454, 634, 806, 450, 577, 35, 12, 1027, 1449, 1534, 1466, 872, 534, 752, 1489, 1101, 1230, 1134, 199, 434, 1185, 932, 915, 123, 766, 1429, 250, 1033, 155, 335, 1030, 276, 457, 953, 174, 799, 828, 1452, 891, 626, 496, 1149, 831, 456, 510, 194, 68, 810, 761, 981, 1242, 1291, 1294, 1535, 75, 499, 935, 1272, 501, 912, 1347, 921, 353, 859, 640, 1032, 1507, 1171, 373, 100, 287, 281, 1442, 944, 1083, 406, 356, 549, 187, 883, 1265, 919, 619, 109, 783, 870, 760, 927, 868, 904, 644, 1249, 420, 1113, 1455, 1211, 1183, 78, 95, 13, 1167, 261, 839, 817, 1274, 252, 994], 'conv3': [1422, 43, 1677, 114, 1319, 2000, 471, 2051, 165, 909, 1874, 1660, 934, 1549, 1608, 2302, 2494, 1312, 2352, 1548, 589, 2069, 1083, 499, 1520, 1988, 1873, 1187, 1110, 195, 2438, 1168, 167, 2138, 1090, 882, 512, 505, 398, 1938, 1827, 648, 1127, 1776, 468, 423, 1331, 1109, 2448, 784, 1495, 1554, 2081, 907, 1558, 2533, 2281, 781, 1616, 694, 278, 1063, 980, 2247, 249, 1245, 305, 1857, 867, 443, 2556, 1709, 359, 178, 1164, 419, 841, 2095, 1049, 95, 28, 137, 2181, 1403, 1359, 377, 231, 136, 2001, 1447, 1744, 2315, 1443, 2067, 2101, 1723, 1990, 210, 1866, 1926, 1303, 1887, 2515, 646, 356, 2391, 129, 667, 2037, 1038, 2049, 2152, 1783, 2460, 103, 1641, 878, 757, 272, 1073, 2221, 1417, 565, 2007, 1088, 192, 1506, 2085, 1930, 1435, 2170, 251, 2342, 327, 452, 2241, 1045, 1789, 2244, 2185, 1468, 148, 932, 147, 2008, 2248, 303, 1774, 2500, 1651, 507, 923, 339, 792, 1522, 1879, 2300, 2374, 1280, 2154, 334, 564, 2334, 2151, 1177, 68, 1854, 2435, 1865, 1955, 2131, 533, 611, 446, 1689, 939, 2317, 540, 689, 922, 1843, 450, 1036, 261, 1798, 1260, 2278, 1801, 1497, 372, 1452, 1324, 1614, 1612, 347, 529, 621, 1446, 2471, 2146, 2249, 2456, 892, 514, 555, 2290, 2429, 1043, 1720, 633, 1363, 894, 634, 2021, 477, 1591, 2106, 2193, 1232, 739, 1269, 185, 674, 2102, 2511, 1758, 828, 1845, 1824, 2547, 1105, 780, 750, 350, 1395, 21, 526, 2549, 233, 2479, 1188, 33, 297, 2100, 1205, 1211, 1346, 709, 881, 54, 861, 1029, 1283, 819, 534, 299, 1670, 1698, 652, 245, 46, 2119, 1982, 2378, 556, 1183, 1242, 1527, 940, 1054, 2279, 497, 2497, 196, 1344, 949, 2018, 2414, 1009, 2215, 1394, 1958, 1226, 2475, 42, 989, 1872, 1089, 863, 1685, 2309, 454, 2242, 1256, 2243, 2161, 1425, 361, 1559, 1567, 2112, 187, 697, 1112, 1037, 1784, 1898, 1378, 1348, 193, 241, 627, 2401, 2145, 481, 2283, 1333, 1894, 1146, 2125, 1326, 1411, 1057, 1924, 2338, 1438, 679, 653, 518, 436, 1998, 1968, 994, 1252, 1978, 2030, 465, 328, 494, 1599, 2450, 660, 664, 197, 1578, 1735, 506, 2465, 2276, 225, 1141, 202, 2316, 106, 389, 80, 1584, 1085, 2331, 2072, 1362, 378, 1208, 2427, 1385, 1755, 1684, 2090, 2261, 2122, 1066, 544, 1266, 1349, 1664, 2183, 2293, 1131, 945, 2282, 122, 982, 1413, 1258, 1995], 'conv4': [1575, 1644, 315, 2388, 4, 1323, 1622, 1859, 2228, 2029, 263, 770, 1131, 1289, 2356, 544, 1187, 2123, 1193, 188, 2237, 1610, 1746, 1064, 264, 1908, 806, 1444, 2413, 891, 1580, 412, 1288, 1993, 1706, 2207, 1402, 1757, 1185, 1109, 493, 502, 2285, 2353, 2475, 778, 1143, 2537, 2008, 615, 67, 1524, 2142, 1795, 1366, 2070, 2041, 448, 2064, 1654, 1167, 514, 549, 1975, 309, 1732, 1019, 1957, 1266, 1629, 1481, 1855, 893, 34, 1067, 1326, 231, 2346, 919, 1819, 366, 1667, 2268, 1082, 1620, 132, 6, 1521, 929, 1428, 1867, 1969, 1631, 621, 2463, 1585, 774, 277, 551, 2254, 783, 1659, 1684, 2131, 1813, 603, 792, 556, 288, 1604, 1041, 2209, 2292, 1499, 2510, 1641, 1749, 805, 2262, 1682, 1967, 2517, 1668, 1857, 593, 1676, 2524, 969, 2038, 802, 489, 670, 435, 1063, 1988, 2286, 1062, 829, 2225, 230, 1285, 687, 437, 2183, 568, 673, 2158, 2518, 99, 2396, 851, 1084, 1152, 895, 711, 397, 1833, 1922, 8, 2015, 2461, 1603, 272, 292, 1066, 1244, 2143, 2523, 21, 586, 1214, 1439, 648, 703, 1734, 1727, 401, 433, 2014, 1874, 2385, 718, 294, 26, 675, 2218, 704, 1490, 1468, 1267, 931, 1191, 1251, 2439, 331, 1212, 592, 558, 59, 1756, 139, 1674, 2335, 2528, 2539, 1538, 2210, 2383, 1529, 389, 2267, 1787, 2163, 1657, 1726, 2338, 527, 843, 91, 1254, 2121, 386, 1498, 2359, 1877, 865, 507, 1739, 1190, 2022, 1134, 7, 936, 1434, 1412, 1828, 150, 2152, 2224, 1974, 862, 1870, 1672, 1035, 725, 1043, 1539, 297, 339, 1070, 1261, 2336, 2078, 1764, 889, 1541, 2300, 1898, 1801, 599, 1221, 1208, 381, 1286, 1996, 1399, 671, 1218, 1618, 1257, 1688, 1375, 1534, 1730, 674, 233, 1429, 1454, 2249, 2415, 536, 1778, 2046, 987, 1448, 553, 2543, 473, 1296, 1130, 1204, 641, 2133, 1883, 1535, 70, 1355, 1354, 2174, 1452, 1991, 2053, 1656, 997, 508, 2227, 1832, 2089, 1445, 612, 1652, 182, 1338, 1115, 1471, 1483, 2557, 175, 761, 1685, 1313, 2211, 2496, 510, 455, 1772, 2472, 1472, 1998, 1596, 894, 1308, 1104, 1018, 859, 734, 1624, 1034, 1536, 945, 1514, 1613, 1915, 33, 343, 1518, 2203, 1540, 1865, 65, 1276, 1201, 172, 656, 460, 2555, 1560, 2259, 1731, 2404, 405, 135, 203, 1766, 149, 225, 1928, 2195, 1612, 457, 253, 208, 518, 487, 258, 1885, 1357, 1567, 2213, 1990, 1784, 1485, 2149, 2257, 1422]}
SAVED model.heads_thresh 0.9
CONFIG MODE:  supervised

 ********** Supervised learning of blocks [5] **********
SAVING FOLDER FOR SUP:  C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([49, 33, 72, 51, 71, 92, 15, 14, 23,  0, 71, 75, 81, 69, 40, 43, 92, 97,
        70, 53], device='cuda:0')
TARGETS AFTER SUB:  tensor([93, 32, 32, 32, 32, 93, 32, 93, 32, 93, 32, 93, 93, 93, 32, 32, 93, 32,
        32, 93])
[32, 93]
TARGETS AFTER CLEANER:  tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1])
200
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 200
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               None
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([19, 29,  0, 11,  1, 86, 90, 28, 23, 31, 39, 96, 82, 17, 71, 39,  8, 97,
        80, 71], device='cuda:0')
TARGETS AFTER SUB:  tensor([93, 93, 32, 32, 32, 32, 32, 32, 32, 32, 93, 32, 93, 93, 93, 32, 32, 32,
        93, 32])
[32, 93]
TARGETS AFTER CLEANER:  tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0])
1000
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 1000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               None
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([19, 29,  0, 11,  1, 86, 90, 28, 23, 31, 39, 96, 82, 17, 71, 39,  8, 97,
        80, 71], device='cuda:0')
TARGETS AFTER SUB:  tensor([93, 93, 32, 32, 32, 32, 32, 32, 32, 32, 93, 32, 93, 93, 93, 32, 32, 32,
        93, 32])
[32, 93]
TARGETS AFTER CLEANER:  tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0])
1000
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 1000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
------------------------
INDICES:  1000
IMAGE SIZE: torch.Size([64, 3, 32, 32])
num_blocks:  6
Epoch: [1/50]	lr: 1.00e-03	time: 00:00:23	Loss_train 0.12029	Acc_train 62.70	/	Loss_test 0.02323	Acc_test 67.50
BLOCKS IN SUP:  [5]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [77, 68, 25, 59, 93, 5, 30, 14, 35, 70, 4, 45, 69, 82, 86], 'conv1': [365, 50, 382, 75, 180, 264, 56, 345, 104, 352, 206, 100, 190, 324, 354, 99, 73, 140, 336, 355, 121, 191, 233, 144, 243, 163, 193, 150, 4, 26, 373, 266, 118, 92, 357, 117, 94, 90, 158, 68, 179, 202, 368, 76, 36, 170, 304, 146, 41, 35, 33, 138, 6, 105, 262, 197, 293, 317, 17], 'conv2': [307, 937, 662, 141, 1307, 1006, 869, 683, 1441, 1238, 348, 1086, 1278, 37, 1404, 427, 889, 311, 341, 1405, 796, 312, 1500, 794, 1486, 1370, 490, 304, 449, 835, 1297, 1100, 28, 942, 731, 537, 710, 380, 612, 451, 1159, 735, 1139, 1046, 142, 1385, 998, 176, 41, 295, 958, 346, 313, 1436, 1034, 167, 1360, 906, 529, 441, 214, 1306, 1395, 79, 1450, 544, 1412, 660, 542, 1000, 1372, 700, 405, 613, 61, 14, 1243, 711, 195, 673, 636, 1082, 107, 1317, 635, 590, 551, 997, 209, 207, 1474, 987, 1226, 865, 706, 1172, 421, 733, 70, 1143, 94, 959, 896, 547, 229, 423, 11, 838, 923, 856, 407, 851, 1345, 914, 1359, 622, 259, 1464, 1400, 460, 268, 730, 1532, 1266, 454, 634, 806, 450, 577, 35, 12, 1027, 1449, 1534, 1466, 872, 534, 752, 1489, 1101, 1230, 1134, 199, 434, 1185, 932, 915, 123, 766, 1429, 250, 1033, 155, 335, 1030, 276, 457, 953, 174, 799, 828, 1452, 891, 626, 496, 1149, 831, 456, 510, 194, 68, 810, 761, 981, 1242, 1291, 1294, 1535, 75, 499, 935, 1272, 501, 912, 1347, 921, 353, 859, 640, 1032, 1507, 1171, 373, 100, 287, 281, 1442, 944, 1083, 406, 356, 549, 187, 883, 1265, 919, 619, 109, 783, 870, 760, 927, 868, 904, 644, 1249, 420, 1113, 1455, 1211, 1183, 78, 95, 13, 1167, 261, 839, 817, 1274, 252, 994], 'conv3': [1422, 43, 1677, 114, 1319, 2000, 471, 2051, 165, 909, 1874, 1660, 934, 1549, 1608, 2302, 2494, 1312, 2352, 1548, 589, 2069, 1083, 499, 1520, 1988, 1873, 1187, 1110, 195, 2438, 1168, 167, 2138, 1090, 882, 512, 505, 398, 1938, 1827, 648, 1127, 1776, 468, 423, 1331, 1109, 2448, 784, 1495, 1554, 2081, 907, 1558, 2533, 2281, 781, 1616, 694, 278, 1063, 980, 2247, 249, 1245, 305, 1857, 867, 443, 2556, 1709, 359, 178, 1164, 419, 841, 2095, 1049, 95, 28, 137, 2181, 1403, 1359, 377, 231, 136, 2001, 1447, 1744, 2315, 1443, 2067, 2101, 1723, 1990, 210, 1866, 1926, 1303, 1887, 2515, 646, 356, 2391, 129, 667, 2037, 1038, 2049, 2152, 1783, 2460, 103, 1641, 878, 757, 272, 1073, 2221, 1417, 565, 2007, 1088, 192, 1506, 2085, 1930, 1435, 2170, 251, 2342, 327, 452, 2241, 1045, 1789, 2244, 2185, 1468, 148, 932, 147, 2008, 2248, 303, 1774, 2500, 1651, 507, 923, 339, 792, 1522, 1879, 2300, 2374, 1280, 2154, 334, 564, 2334, 2151, 1177, 68, 1854, 2435, 1865, 1955, 2131, 533, 611, 446, 1689, 939, 2317, 540, 689, 922, 1843, 450, 1036, 261, 1798, 1260, 2278, 1801, 1497, 372, 1452, 1324, 1614, 1612, 347, 529, 621, 1446, 2471, 2146, 2249, 2456, 892, 514, 555, 2290, 2429, 1043, 1720, 633, 1363, 894, 634, 2021, 477, 1591, 2106, 2193, 1232, 739, 1269, 185, 674, 2102, 2511, 1758, 828, 1845, 1824, 2547, 1105, 780, 750, 350, 1395, 21, 526, 2549, 233, 2479, 1188, 33, 297, 2100, 1205, 1211, 1346, 709, 881, 54, 861, 1029, 1283, 819, 534, 299, 1670, 1698, 652, 245, 46, 2119, 1982, 2378, 556, 1183, 1242, 1527, 940, 1054, 2279, 497, 2497, 196, 1344, 949, 2018, 2414, 1009, 2215, 1394, 1958, 1226, 2475, 42, 989, 1872, 1089, 863, 1685, 2309, 454, 2242, 1256, 2243, 2161, 1425, 361, 1559, 1567, 2112, 187, 697, 1112, 1037, 1784, 1898, 1378, 1348, 193, 241, 627, 2401, 2145, 481, 2283, 1333, 1894, 1146, 2125, 1326, 1411, 1057, 1924, 2338, 1438, 679, 653, 518, 436, 1998, 1968, 994, 1252, 1978, 2030, 465, 328, 494, 1599, 2450, 660, 664, 197, 1578, 1735, 506, 2465, 2276, 225, 1141, 202, 2316, 106, 389, 80, 1584, 1085, 2331, 2072, 1362, 378, 1208, 2427, 1385, 1755, 1684, 2090, 2261, 2122, 1066, 544, 1266, 1349, 1664, 2183, 2293, 1131, 945, 2282, 122, 982, 1413, 1258, 1995], 'conv4': [1575, 1644, 315, 2388, 4, 1323, 1622, 1859, 2228, 2029, 263, 770, 1131, 1289, 2356, 544, 1187, 2123, 1193, 188, 2237, 1610, 1746, 1064, 264, 1908, 806, 1444, 2413, 891, 1580, 412, 1288, 1993, 1706, 2207, 1402, 1757, 1185, 1109, 493, 502, 2285, 2353, 2475, 778, 1143, 2537, 2008, 615, 67, 1524, 2142, 1795, 1366, 2070, 2041, 448, 2064, 1654, 1167, 514, 549, 1975, 309, 1732, 1019, 1957, 1266, 1629, 1481, 1855, 893, 34, 1067, 1326, 231, 2346, 919, 1819, 366, 1667, 2268, 1082, 1620, 132, 6, 1521, 929, 1428, 1867, 1969, 1631, 621, 2463, 1585, 774, 277, 551, 2254, 783, 1659, 1684, 2131, 1813, 603, 792, 556, 288, 1604, 1041, 2209, 2292, 1499, 2510, 1641, 1749, 805, 2262, 1682, 1967, 2517, 1668, 1857, 593, 1676, 2524, 969, 2038, 802, 489, 670, 435, 1063, 1988, 2286, 1062, 829, 2225, 230, 1285, 687, 437, 2183, 568, 673, 2158, 2518, 99, 2396, 851, 1084, 1152, 895, 711, 397, 1833, 1922, 8, 2015, 2461, 1603, 272, 292, 1066, 1244, 2143, 2523, 21, 586, 1214, 1439, 648, 703, 1734, 1727, 401, 433, 2014, 1874, 2385, 718, 294, 26, 675, 2218, 704, 1490, 1468, 1267, 931, 1191, 1251, 2439, 331, 1212, 592, 558, 59, 1756, 139, 1674, 2335, 2528, 2539, 1538, 2210, 2383, 1529, 389, 2267, 1787, 2163, 1657, 1726, 2338, 527, 843, 91, 1254, 2121, 386, 1498, 2359, 1877, 865, 507, 1739, 1190, 2022, 1134, 7, 936, 1434, 1412, 1828, 150, 2152, 2224, 1974, 862, 1870, 1672, 1035, 725, 1043, 1539, 297, 339, 1070, 1261, 2336, 2078, 1764, 889, 1541, 2300, 1898, 1801, 599, 1221, 1208, 381, 1286, 1996, 1399, 671, 1218, 1618, 1257, 1688, 1375, 1534, 1730, 674, 233, 1429, 1454, 2249, 2415, 536, 1778, 2046, 987, 1448, 553, 2543, 473, 1296, 1130, 1204, 641, 2133, 1883, 1535, 70, 1355, 1354, 2174, 1452, 1991, 2053, 1656, 997, 508, 2227, 1832, 2089, 1445, 612, 1652, 182, 1338, 1115, 1471, 1483, 2557, 175, 761, 1685, 1313, 2211, 2496, 510, 455, 1772, 2472, 1472, 1998, 1596, 894, 1308, 1104, 1018, 859, 734, 1624, 1034, 1536, 945, 1514, 1613, 1915, 33, 343, 1518, 2203, 1540, 1865, 65, 1276, 1201, 172, 656, 460, 2555, 1560, 2259, 1731, 2404, 405, 135, 203, 1766, 149, 225, 1928, 2195, 1612, 457, 253, 208, 518, 487, 258, 1885, 1357, 1567, 2213, 1990, 1784, 1485, 2149, 2257, 1422]}
SAVED model.heads_thresh 0.9
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
Epoch: [10/50]	lr: 1.00e-03	time: 00:00:24	Loss_train 0.04279	Acc_train 75.24	/	Loss_test 0.00703	Acc_test 81.00
BLOCKS IN SUP:  [5]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [77, 68, 25, 59, 93, 5, 30, 14, 35, 70, 4, 45, 69, 82, 86], 'conv1': [365, 50, 382, 75, 180, 264, 56, 345, 104, 352, 206, 100, 190, 324, 354, 99, 73, 140, 336, 355, 121, 191, 233, 144, 243, 163, 193, 150, 4, 26, 373, 266, 118, 92, 357, 117, 94, 90, 158, 68, 179, 202, 368, 76, 36, 170, 304, 146, 41, 35, 33, 138, 6, 105, 262, 197, 293, 317, 17], 'conv2': [307, 937, 662, 141, 1307, 1006, 869, 683, 1441, 1238, 348, 1086, 1278, 37, 1404, 427, 889, 311, 341, 1405, 796, 312, 1500, 794, 1486, 1370, 490, 304, 449, 835, 1297, 1100, 28, 942, 731, 537, 710, 380, 612, 451, 1159, 735, 1139, 1046, 142, 1385, 998, 176, 41, 295, 958, 346, 313, 1436, 1034, 167, 1360, 906, 529, 441, 214, 1306, 1395, 79, 1450, 544, 1412, 660, 542, 1000, 1372, 700, 405, 613, 61, 14, 1243, 711, 195, 673, 636, 1082, 107, 1317, 635, 590, 551, 997, 209, 207, 1474, 987, 1226, 865, 706, 1172, 421, 733, 70, 1143, 94, 959, 896, 547, 229, 423, 11, 838, 923, 856, 407, 851, 1345, 914, 1359, 622, 259, 1464, 1400, 460, 268, 730, 1532, 1266, 454, 634, 806, 450, 577, 35, 12, 1027, 1449, 1534, 1466, 872, 534, 752, 1489, 1101, 1230, 1134, 199, 434, 1185, 932, 915, 123, 766, 1429, 250, 1033, 155, 335, 1030, 276, 457, 953, 174, 799, 828, 1452, 891, 626, 496, 1149, 831, 456, 510, 194, 68, 810, 761, 981, 1242, 1291, 1294, 1535, 75, 499, 935, 1272, 501, 912, 1347, 921, 353, 859, 640, 1032, 1507, 1171, 373, 100, 287, 281, 1442, 944, 1083, 406, 356, 549, 187, 883, 1265, 919, 619, 109, 783, 870, 760, 927, 868, 904, 644, 1249, 420, 1113, 1455, 1211, 1183, 78, 95, 13, 1167, 261, 839, 817, 1274, 252, 994], 'conv3': [1422, 43, 1677, 114, 1319, 2000, 471, 2051, 165, 909, 1874, 1660, 934, 1549, 1608, 2302, 2494, 1312, 2352, 1548, 589, 2069, 1083, 499, 1520, 1988, 1873, 1187, 1110, 195, 2438, 1168, 167, 2138, 1090, 882, 512, 505, 398, 1938, 1827, 648, 1127, 1776, 468, 423, 1331, 1109, 2448, 784, 1495, 1554, 2081, 907, 1558, 2533, 2281, 781, 1616, 694, 278, 1063, 980, 2247, 249, 1245, 305, 1857, 867, 443, 2556, 1709, 359, 178, 1164, 419, 841, 2095, 1049, 95, 28, 137, 2181, 1403, 1359, 377, 231, 136, 2001, 1447, 1744, 2315, 1443, 2067, 2101, 1723, 1990, 210, 1866, 1926, 1303, 1887, 2515, 646, 356, 2391, 129, 667, 2037, 1038, 2049, 2152, 1783, 2460, 103, 1641, 878, 757, 272, 1073, 2221, 1417, 565, 2007, 1088, 192, 1506, 2085, 1930, 1435, 2170, 251, 2342, 327, 452, 2241, 1045, 1789, 2244, 2185, 1468, 148, 932, 147, 2008, 2248, 303, 1774, 2500, 1651, 507, 923, 339, 792, 1522, 1879, 2300, 2374, 1280, 2154, 334, 564, 2334, 2151, 1177, 68, 1854, 2435, 1865, 1955, 2131, 533, 611, 446, 1689, 939, 2317, 540, 689, 922, 1843, 450, 1036, 261, 1798, 1260, 2278, 1801, 1497, 372, 1452, 1324, 1614, 1612, 347, 529, 621, 1446, 2471, 2146, 2249, 2456, 892, 514, 555, 2290, 2429, 1043, 1720, 633, 1363, 894, 634, 2021, 477, 1591, 2106, 2193, 1232, 739, 1269, 185, 674, 2102, 2511, 1758, 828, 1845, 1824, 2547, 1105, 780, 750, 350, 1395, 21, 526, 2549, 233, 2479, 1188, 33, 297, 2100, 1205, 1211, 1346, 709, 881, 54, 861, 1029, 1283, 819, 534, 299, 1670, 1698, 652, 245, 46, 2119, 1982, 2378, 556, 1183, 1242, 1527, 940, 1054, 2279, 497, 2497, 196, 1344, 949, 2018, 2414, 1009, 2215, 1394, 1958, 1226, 2475, 42, 989, 1872, 1089, 863, 1685, 2309, 454, 2242, 1256, 2243, 2161, 1425, 361, 1559, 1567, 2112, 187, 697, 1112, 1037, 1784, 1898, 1378, 1348, 193, 241, 627, 2401, 2145, 481, 2283, 1333, 1894, 1146, 2125, 1326, 1411, 1057, 1924, 2338, 1438, 679, 653, 518, 436, 1998, 1968, 994, 1252, 1978, 2030, 465, 328, 494, 1599, 2450, 660, 664, 197, 1578, 1735, 506, 2465, 2276, 225, 1141, 202, 2316, 106, 389, 80, 1584, 1085, 2331, 2072, 1362, 378, 1208, 2427, 1385, 1755, 1684, 2090, 2261, 2122, 1066, 544, 1266, 1349, 1664, 2183, 2293, 1131, 945, 2282, 122, 982, 1413, 1258, 1995], 'conv4': [1575, 1644, 315, 2388, 4, 1323, 1622, 1859, 2228, 2029, 263, 770, 1131, 1289, 2356, 544, 1187, 2123, 1193, 188, 2237, 1610, 1746, 1064, 264, 1908, 806, 1444, 2413, 891, 1580, 412, 1288, 1993, 1706, 2207, 1402, 1757, 1185, 1109, 493, 502, 2285, 2353, 2475, 778, 1143, 2537, 2008, 615, 67, 1524, 2142, 1795, 1366, 2070, 2041, 448, 2064, 1654, 1167, 514, 549, 1975, 309, 1732, 1019, 1957, 1266, 1629, 1481, 1855, 893, 34, 1067, 1326, 231, 2346, 919, 1819, 366, 1667, 2268, 1082, 1620, 132, 6, 1521, 929, 1428, 1867, 1969, 1631, 621, 2463, 1585, 774, 277, 551, 2254, 783, 1659, 1684, 2131, 1813, 603, 792, 556, 288, 1604, 1041, 2209, 2292, 1499, 2510, 1641, 1749, 805, 2262, 1682, 1967, 2517, 1668, 1857, 593, 1676, 2524, 969, 2038, 802, 489, 670, 435, 1063, 1988, 2286, 1062, 829, 2225, 230, 1285, 687, 437, 2183, 568, 673, 2158, 2518, 99, 2396, 851, 1084, 1152, 895, 711, 397, 1833, 1922, 8, 2015, 2461, 1603, 272, 292, 1066, 1244, 2143, 2523, 21, 586, 1214, 1439, 648, 703, 1734, 1727, 401, 433, 2014, 1874, 2385, 718, 294, 26, 675, 2218, 704, 1490, 1468, 1267, 931, 1191, 1251, 2439, 331, 1212, 592, 558, 59, 1756, 139, 1674, 2335, 2528, 2539, 1538, 2210, 2383, 1529, 389, 2267, 1787, 2163, 1657, 1726, 2338, 527, 843, 91, 1254, 2121, 386, 1498, 2359, 1877, 865, 507, 1739, 1190, 2022, 1134, 7, 936, 1434, 1412, 1828, 150, 2152, 2224, 1974, 862, 1870, 1672, 1035, 725, 1043, 1539, 297, 339, 1070, 1261, 2336, 2078, 1764, 889, 1541, 2300, 1898, 1801, 599, 1221, 1208, 381, 1286, 1996, 1399, 671, 1218, 1618, 1257, 1688, 1375, 1534, 1730, 674, 233, 1429, 1454, 2249, 2415, 536, 1778, 2046, 987, 1448, 553, 2543, 473, 1296, 1130, 1204, 641, 2133, 1883, 1535, 70, 1355, 1354, 2174, 1452, 1991, 2053, 1656, 997, 508, 2227, 1832, 2089, 1445, 612, 1652, 182, 1338, 1115, 1471, 1483, 2557, 175, 761, 1685, 1313, 2211, 2496, 510, 455, 1772, 2472, 1472, 1998, 1596, 894, 1308, 1104, 1018, 859, 734, 1624, 1034, 1536, 945, 1514, 1613, 1915, 33, 343, 1518, 2203, 1540, 1865, 65, 1276, 1201, 172, 656, 460, 2555, 1560, 2259, 1731, 2404, 405, 135, 203, 1766, 149, 225, 1928, 2195, 1612, 457, 253, 208, 518, 487, 258, 1885, 1357, 1567, 2213, 1990, 1784, 1485, 2149, 2257, 1422]}
SAVED model.heads_thresh 0.9
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
Epoch: [20/50]	lr: 2.50e-04	time: 00:00:26	Loss_train 0.02425	Acc_train 81.80	/	Loss_test 0.00641	Acc_test 81.00
BLOCKS IN SUP:  [5]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [77, 68, 25, 59, 93, 5, 30, 14, 35, 70, 4, 45, 69, 82, 86], 'conv1': [365, 50, 382, 75, 180, 264, 56, 345, 104, 352, 206, 100, 190, 324, 354, 99, 73, 140, 336, 355, 121, 191, 233, 144, 243, 163, 193, 150, 4, 26, 373, 266, 118, 92, 357, 117, 94, 90, 158, 68, 179, 202, 368, 76, 36, 170, 304, 146, 41, 35, 33, 138, 6, 105, 262, 197, 293, 317, 17], 'conv2': [307, 937, 662, 141, 1307, 1006, 869, 683, 1441, 1238, 348, 1086, 1278, 37, 1404, 427, 889, 311, 341, 1405, 796, 312, 1500, 794, 1486, 1370, 490, 304, 449, 835, 1297, 1100, 28, 942, 731, 537, 710, 380, 612, 451, 1159, 735, 1139, 1046, 142, 1385, 998, 176, 41, 295, 958, 346, 313, 1436, 1034, 167, 1360, 906, 529, 441, 214, 1306, 1395, 79, 1450, 544, 1412, 660, 542, 1000, 1372, 700, 405, 613, 61, 14, 1243, 711, 195, 673, 636, 1082, 107, 1317, 635, 590, 551, 997, 209, 207, 1474, 987, 1226, 865, 706, 1172, 421, 733, 70, 1143, 94, 959, 896, 547, 229, 423, 11, 838, 923, 856, 407, 851, 1345, 914, 1359, 622, 259, 1464, 1400, 460, 268, 730, 1532, 1266, 454, 634, 806, 450, 577, 35, 12, 1027, 1449, 1534, 1466, 872, 534, 752, 1489, 1101, 1230, 1134, 199, 434, 1185, 932, 915, 123, 766, 1429, 250, 1033, 155, 335, 1030, 276, 457, 953, 174, 799, 828, 1452, 891, 626, 496, 1149, 831, 456, 510, 194, 68, 810, 761, 981, 1242, 1291, 1294, 1535, 75, 499, 935, 1272, 501, 912, 1347, 921, 353, 859, 640, 1032, 1507, 1171, 373, 100, 287, 281, 1442, 944, 1083, 406, 356, 549, 187, 883, 1265, 919, 619, 109, 783, 870, 760, 927, 868, 904, 644, 1249, 420, 1113, 1455, 1211, 1183, 78, 95, 13, 1167, 261, 839, 817, 1274, 252, 994], 'conv3': [1422, 43, 1677, 114, 1319, 2000, 471, 2051, 165, 909, 1874, 1660, 934, 1549, 1608, 2302, 2494, 1312, 2352, 1548, 589, 2069, 1083, 499, 1520, 1988, 1873, 1187, 1110, 195, 2438, 1168, 167, 2138, 1090, 882, 512, 505, 398, 1938, 1827, 648, 1127, 1776, 468, 423, 1331, 1109, 2448, 784, 1495, 1554, 2081, 907, 1558, 2533, 2281, 781, 1616, 694, 278, 1063, 980, 2247, 249, 1245, 305, 1857, 867, 443, 2556, 1709, 359, 178, 1164, 419, 841, 2095, 1049, 95, 28, 137, 2181, 1403, 1359, 377, 231, 136, 2001, 1447, 1744, 2315, 1443, 2067, 2101, 1723, 1990, 210, 1866, 1926, 1303, 1887, 2515, 646, 356, 2391, 129, 667, 2037, 1038, 2049, 2152, 1783, 2460, 103, 1641, 878, 757, 272, 1073, 2221, 1417, 565, 2007, 1088, 192, 1506, 2085, 1930, 1435, 2170, 251, 2342, 327, 452, 2241, 1045, 1789, 2244, 2185, 1468, 148, 932, 147, 2008, 2248, 303, 1774, 2500, 1651, 507, 923, 339, 792, 1522, 1879, 2300, 2374, 1280, 2154, 334, 564, 2334, 2151, 1177, 68, 1854, 2435, 1865, 1955, 2131, 533, 611, 446, 1689, 939, 2317, 540, 689, 922, 1843, 450, 1036, 261, 1798, 1260, 2278, 1801, 1497, 372, 1452, 1324, 1614, 1612, 347, 529, 621, 1446, 2471, 2146, 2249, 2456, 892, 514, 555, 2290, 2429, 1043, 1720, 633, 1363, 894, 634, 2021, 477, 1591, 2106, 2193, 1232, 739, 1269, 185, 674, 2102, 2511, 1758, 828, 1845, 1824, 2547, 1105, 780, 750, 350, 1395, 21, 526, 2549, 233, 2479, 1188, 33, 297, 2100, 1205, 1211, 1346, 709, 881, 54, 861, 1029, 1283, 819, 534, 299, 1670, 1698, 652, 245, 46, 2119, 1982, 2378, 556, 1183, 1242, 1527, 940, 1054, 2279, 497, 2497, 196, 1344, 949, 2018, 2414, 1009, 2215, 1394, 1958, 1226, 2475, 42, 989, 1872, 1089, 863, 1685, 2309, 454, 2242, 1256, 2243, 2161, 1425, 361, 1559, 1567, 2112, 187, 697, 1112, 1037, 1784, 1898, 1378, 1348, 193, 241, 627, 2401, 2145, 481, 2283, 1333, 1894, 1146, 2125, 1326, 1411, 1057, 1924, 2338, 1438, 679, 653, 518, 436, 1998, 1968, 994, 1252, 1978, 2030, 465, 328, 494, 1599, 2450, 660, 664, 197, 1578, 1735, 506, 2465, 2276, 225, 1141, 202, 2316, 106, 389, 80, 1584, 1085, 2331, 2072, 1362, 378, 1208, 2427, 1385, 1755, 1684, 2090, 2261, 2122, 1066, 544, 1266, 1349, 1664, 2183, 2293, 1131, 945, 2282, 122, 982, 1413, 1258, 1995], 'conv4': [1575, 1644, 315, 2388, 4, 1323, 1622, 1859, 2228, 2029, 263, 770, 1131, 1289, 2356, 544, 1187, 2123, 1193, 188, 2237, 1610, 1746, 1064, 264, 1908, 806, 1444, 2413, 891, 1580, 412, 1288, 1993, 1706, 2207, 1402, 1757, 1185, 1109, 493, 502, 2285, 2353, 2475, 778, 1143, 2537, 2008, 615, 67, 1524, 2142, 1795, 1366, 2070, 2041, 448, 2064, 1654, 1167, 514, 549, 1975, 309, 1732, 1019, 1957, 1266, 1629, 1481, 1855, 893, 34, 1067, 1326, 231, 2346, 919, 1819, 366, 1667, 2268, 1082, 1620, 132, 6, 1521, 929, 1428, 1867, 1969, 1631, 621, 2463, 1585, 774, 277, 551, 2254, 783, 1659, 1684, 2131, 1813, 603, 792, 556, 288, 1604, 1041, 2209, 2292, 1499, 2510, 1641, 1749, 805, 2262, 1682, 1967, 2517, 1668, 1857, 593, 1676, 2524, 969, 2038, 802, 489, 670, 435, 1063, 1988, 2286, 1062, 829, 2225, 230, 1285, 687, 437, 2183, 568, 673, 2158, 2518, 99, 2396, 851, 1084, 1152, 895, 711, 397, 1833, 1922, 8, 2015, 2461, 1603, 272, 292, 1066, 1244, 2143, 2523, 21, 586, 1214, 1439, 648, 703, 1734, 1727, 401, 433, 2014, 1874, 2385, 718, 294, 26, 675, 2218, 704, 1490, 1468, 1267, 931, 1191, 1251, 2439, 331, 1212, 592, 558, 59, 1756, 139, 1674, 2335, 2528, 2539, 1538, 2210, 2383, 1529, 389, 2267, 1787, 2163, 1657, 1726, 2338, 527, 843, 91, 1254, 2121, 386, 1498, 2359, 1877, 865, 507, 1739, 1190, 2022, 1134, 7, 936, 1434, 1412, 1828, 150, 2152, 2224, 1974, 862, 1870, 1672, 1035, 725, 1043, 1539, 297, 339, 1070, 1261, 2336, 2078, 1764, 889, 1541, 2300, 1898, 1801, 599, 1221, 1208, 381, 1286, 1996, 1399, 671, 1218, 1618, 1257, 1688, 1375, 1534, 1730, 674, 233, 1429, 1454, 2249, 2415, 536, 1778, 2046, 987, 1448, 553, 2543, 473, 1296, 1130, 1204, 641, 2133, 1883, 1535, 70, 1355, 1354, 2174, 1452, 1991, 2053, 1656, 997, 508, 2227, 1832, 2089, 1445, 612, 1652, 182, 1338, 1115, 1471, 1483, 2557, 175, 761, 1685, 1313, 2211, 2496, 510, 455, 1772, 2472, 1472, 1998, 1596, 894, 1308, 1104, 1018, 859, 734, 1624, 1034, 1536, 945, 1514, 1613, 1915, 33, 343, 1518, 2203, 1540, 1865, 65, 1276, 1201, 172, 656, 460, 2555, 1560, 2259, 1731, 2404, 405, 135, 203, 1766, 149, 225, 1928, 2195, 1612, 457, 253, 208, 518, 487, 258, 1885, 1357, 1567, 2213, 1990, 1784, 1485, 2149, 2257, 1422]}
SAVED model.heads_thresh 0.9
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
Epoch: [30/50]	lr: 1.25e-04	time: 00:00:28	Loss_train 0.01839	Acc_train 83.50	/	Loss_test 0.00539	Acc_test 79.50
BLOCKS IN SUP:  [5]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [77, 68, 25, 59, 93, 5, 30, 14, 35, 70, 4, 45, 69, 82, 86], 'conv1': [365, 50, 382, 75, 180, 264, 56, 345, 104, 352, 206, 100, 190, 324, 354, 99, 73, 140, 336, 355, 121, 191, 233, 144, 243, 163, 193, 150, 4, 26, 373, 266, 118, 92, 357, 117, 94, 90, 158, 68, 179, 202, 368, 76, 36, 170, 304, 146, 41, 35, 33, 138, 6, 105, 262, 197, 293, 317, 17], 'conv2': [307, 937, 662, 141, 1307, 1006, 869, 683, 1441, 1238, 348, 1086, 1278, 37, 1404, 427, 889, 311, 341, 1405, 796, 312, 1500, 794, 1486, 1370, 490, 304, 449, 835, 1297, 1100, 28, 942, 731, 537, 710, 380, 612, 451, 1159, 735, 1139, 1046, 142, 1385, 998, 176, 41, 295, 958, 346, 313, 1436, 1034, 167, 1360, 906, 529, 441, 214, 1306, 1395, 79, 1450, 544, 1412, 660, 542, 1000, 1372, 700, 405, 613, 61, 14, 1243, 711, 195, 673, 636, 1082, 107, 1317, 635, 590, 551, 997, 209, 207, 1474, 987, 1226, 865, 706, 1172, 421, 733, 70, 1143, 94, 959, 896, 547, 229, 423, 11, 838, 923, 856, 407, 851, 1345, 914, 1359, 622, 259, 1464, 1400, 460, 268, 730, 1532, 1266, 454, 634, 806, 450, 577, 35, 12, 1027, 1449, 1534, 1466, 872, 534, 752, 1489, 1101, 1230, 1134, 199, 434, 1185, 932, 915, 123, 766, 1429, 250, 1033, 155, 335, 1030, 276, 457, 953, 174, 799, 828, 1452, 891, 626, 496, 1149, 831, 456, 510, 194, 68, 810, 761, 981, 1242, 1291, 1294, 1535, 75, 499, 935, 1272, 501, 912, 1347, 921, 353, 859, 640, 1032, 1507, 1171, 373, 100, 287, 281, 1442, 944, 1083, 406, 356, 549, 187, 883, 1265, 919, 619, 109, 783, 870, 760, 927, 868, 904, 644, 1249, 420, 1113, 1455, 1211, 1183, 78, 95, 13, 1167, 261, 839, 817, 1274, 252, 994], 'conv3': [1422, 43, 1677, 114, 1319, 2000, 471, 2051, 165, 909, 1874, 1660, 934, 1549, 1608, 2302, 2494, 1312, 2352, 1548, 589, 2069, 1083, 499, 1520, 1988, 1873, 1187, 1110, 195, 2438, 1168, 167, 2138, 1090, 882, 512, 505, 398, 1938, 1827, 648, 1127, 1776, 468, 423, 1331, 1109, 2448, 784, 1495, 1554, 2081, 907, 1558, 2533, 2281, 781, 1616, 694, 278, 1063, 980, 2247, 249, 1245, 305, 1857, 867, 443, 2556, 1709, 359, 178, 1164, 419, 841, 2095, 1049, 95, 28, 137, 2181, 1403, 1359, 377, 231, 136, 2001, 1447, 1744, 2315, 1443, 2067, 2101, 1723, 1990, 210, 1866, 1926, 1303, 1887, 2515, 646, 356, 2391, 129, 667, 2037, 1038, 2049, 2152, 1783, 2460, 103, 1641, 878, 757, 272, 1073, 2221, 1417, 565, 2007, 1088, 192, 1506, 2085, 1930, 1435, 2170, 251, 2342, 327, 452, 2241, 1045, 1789, 2244, 2185, 1468, 148, 932, 147, 2008, 2248, 303, 1774, 2500, 1651, 507, 923, 339, 792, 1522, 1879, 2300, 2374, 1280, 2154, 334, 564, 2334, 2151, 1177, 68, 1854, 2435, 1865, 1955, 2131, 533, 611, 446, 1689, 939, 2317, 540, 689, 922, 1843, 450, 1036, 261, 1798, 1260, 2278, 1801, 1497, 372, 1452, 1324, 1614, 1612, 347, 529, 621, 1446, 2471, 2146, 2249, 2456, 892, 514, 555, 2290, 2429, 1043, 1720, 633, 1363, 894, 634, 2021, 477, 1591, 2106, 2193, 1232, 739, 1269, 185, 674, 2102, 2511, 1758, 828, 1845, 1824, 2547, 1105, 780, 750, 350, 1395, 21, 526, 2549, 233, 2479, 1188, 33, 297, 2100, 1205, 1211, 1346, 709, 881, 54, 861, 1029, 1283, 819, 534, 299, 1670, 1698, 652, 245, 46, 2119, 1982, 2378, 556, 1183, 1242, 1527, 940, 1054, 2279, 497, 2497, 196, 1344, 949, 2018, 2414, 1009, 2215, 1394, 1958, 1226, 2475, 42, 989, 1872, 1089, 863, 1685, 2309, 454, 2242, 1256, 2243, 2161, 1425, 361, 1559, 1567, 2112, 187, 697, 1112, 1037, 1784, 1898, 1378, 1348, 193, 241, 627, 2401, 2145, 481, 2283, 1333, 1894, 1146, 2125, 1326, 1411, 1057, 1924, 2338, 1438, 679, 653, 518, 436, 1998, 1968, 994, 1252, 1978, 2030, 465, 328, 494, 1599, 2450, 660, 664, 197, 1578, 1735, 506, 2465, 2276, 225, 1141, 202, 2316, 106, 389, 80, 1584, 1085, 2331, 2072, 1362, 378, 1208, 2427, 1385, 1755, 1684, 2090, 2261, 2122, 1066, 544, 1266, 1349, 1664, 2183, 2293, 1131, 945, 2282, 122, 982, 1413, 1258, 1995], 'conv4': [1575, 1644, 315, 2388, 4, 1323, 1622, 1859, 2228, 2029, 263, 770, 1131, 1289, 2356, 544, 1187, 2123, 1193, 188, 2237, 1610, 1746, 1064, 264, 1908, 806, 1444, 2413, 891, 1580, 412, 1288, 1993, 1706, 2207, 1402, 1757, 1185, 1109, 493, 502, 2285, 2353, 2475, 778, 1143, 2537, 2008, 615, 67, 1524, 2142, 1795, 1366, 2070, 2041, 448, 2064, 1654, 1167, 514, 549, 1975, 309, 1732, 1019, 1957, 1266, 1629, 1481, 1855, 893, 34, 1067, 1326, 231, 2346, 919, 1819, 366, 1667, 2268, 1082, 1620, 132, 6, 1521, 929, 1428, 1867, 1969, 1631, 621, 2463, 1585, 774, 277, 551, 2254, 783, 1659, 1684, 2131, 1813, 603, 792, 556, 288, 1604, 1041, 2209, 2292, 1499, 2510, 1641, 1749, 805, 2262, 1682, 1967, 2517, 1668, 1857, 593, 1676, 2524, 969, 2038, 802, 489, 670, 435, 1063, 1988, 2286, 1062, 829, 2225, 230, 1285, 687, 437, 2183, 568, 673, 2158, 2518, 99, 2396, 851, 1084, 1152, 895, 711, 397, 1833, 1922, 8, 2015, 2461, 1603, 272, 292, 1066, 1244, 2143, 2523, 21, 586, 1214, 1439, 648, 703, 1734, 1727, 401, 433, 2014, 1874, 2385, 718, 294, 26, 675, 2218, 704, 1490, 1468, 1267, 931, 1191, 1251, 2439, 331, 1212, 592, 558, 59, 1756, 139, 1674, 2335, 2528, 2539, 1538, 2210, 2383, 1529, 389, 2267, 1787, 2163, 1657, 1726, 2338, 527, 843, 91, 1254, 2121, 386, 1498, 2359, 1877, 865, 507, 1739, 1190, 2022, 1134, 7, 936, 1434, 1412, 1828, 150, 2152, 2224, 1974, 862, 1870, 1672, 1035, 725, 1043, 1539, 297, 339, 1070, 1261, 2336, 2078, 1764, 889, 1541, 2300, 1898, 1801, 599, 1221, 1208, 381, 1286, 1996, 1399, 671, 1218, 1618, 1257, 1688, 1375, 1534, 1730, 674, 233, 1429, 1454, 2249, 2415, 536, 1778, 2046, 987, 1448, 553, 2543, 473, 1296, 1130, 1204, 641, 2133, 1883, 1535, 70, 1355, 1354, 2174, 1452, 1991, 2053, 1656, 997, 508, 2227, 1832, 2089, 1445, 612, 1652, 182, 1338, 1115, 1471, 1483, 2557, 175, 761, 1685, 1313, 2211, 2496, 510, 455, 1772, 2472, 1472, 1998, 1596, 894, 1308, 1104, 1018, 859, 734, 1624, 1034, 1536, 945, 1514, 1613, 1915, 33, 343, 1518, 2203, 1540, 1865, 65, 1276, 1201, 172, 656, 460, 2555, 1560, 2259, 1731, 2404, 405, 135, 203, 1766, 149, 225, 1928, 2195, 1612, 457, 253, 208, 518, 487, 258, 1885, 1357, 1567, 2213, 1990, 1784, 1485, 2149, 2257, 1422]}
SAVED model.heads_thresh 0.9
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
Epoch: [40/50]	lr: 3.13e-05	time: 00:00:29	Loss_train 0.01493	Acc_train 84.73	/	Loss_test 0.00511	Acc_test 82.00
BLOCKS IN SUP:  [5]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [77, 68, 25, 59, 93, 5, 30, 14, 35, 70, 4, 45, 69, 82, 86], 'conv1': [365, 50, 382, 75, 180, 264, 56, 345, 104, 352, 206, 100, 190, 324, 354, 99, 73, 140, 336, 355, 121, 191, 233, 144, 243, 163, 193, 150, 4, 26, 373, 266, 118, 92, 357, 117, 94, 90, 158, 68, 179, 202, 368, 76, 36, 170, 304, 146, 41, 35, 33, 138, 6, 105, 262, 197, 293, 317, 17], 'conv2': [307, 937, 662, 141, 1307, 1006, 869, 683, 1441, 1238, 348, 1086, 1278, 37, 1404, 427, 889, 311, 341, 1405, 796, 312, 1500, 794, 1486, 1370, 490, 304, 449, 835, 1297, 1100, 28, 942, 731, 537, 710, 380, 612, 451, 1159, 735, 1139, 1046, 142, 1385, 998, 176, 41, 295, 958, 346, 313, 1436, 1034, 167, 1360, 906, 529, 441, 214, 1306, 1395, 79, 1450, 544, 1412, 660, 542, 1000, 1372, 700, 405, 613, 61, 14, 1243, 711, 195, 673, 636, 1082, 107, 1317, 635, 590, 551, 997, 209, 207, 1474, 987, 1226, 865, 706, 1172, 421, 733, 70, 1143, 94, 959, 896, 547, 229, 423, 11, 838, 923, 856, 407, 851, 1345, 914, 1359, 622, 259, 1464, 1400, 460, 268, 730, 1532, 1266, 454, 634, 806, 450, 577, 35, 12, 1027, 1449, 1534, 1466, 872, 534, 752, 1489, 1101, 1230, 1134, 199, 434, 1185, 932, 915, 123, 766, 1429, 250, 1033, 155, 335, 1030, 276, 457, 953, 174, 799, 828, 1452, 891, 626, 496, 1149, 831, 456, 510, 194, 68, 810, 761, 981, 1242, 1291, 1294, 1535, 75, 499, 935, 1272, 501, 912, 1347, 921, 353, 859, 640, 1032, 1507, 1171, 373, 100, 287, 281, 1442, 944, 1083, 406, 356, 549, 187, 883, 1265, 919, 619, 109, 783, 870, 760, 927, 868, 904, 644, 1249, 420, 1113, 1455, 1211, 1183, 78, 95, 13, 1167, 261, 839, 817, 1274, 252, 994], 'conv3': [1422, 43, 1677, 114, 1319, 2000, 471, 2051, 165, 909, 1874, 1660, 934, 1549, 1608, 2302, 2494, 1312, 2352, 1548, 589, 2069, 1083, 499, 1520, 1988, 1873, 1187, 1110, 195, 2438, 1168, 167, 2138, 1090, 882, 512, 505, 398, 1938, 1827, 648, 1127, 1776, 468, 423, 1331, 1109, 2448, 784, 1495, 1554, 2081, 907, 1558, 2533, 2281, 781, 1616, 694, 278, 1063, 980, 2247, 249, 1245, 305, 1857, 867, 443, 2556, 1709, 359, 178, 1164, 419, 841, 2095, 1049, 95, 28, 137, 2181, 1403, 1359, 377, 231, 136, 2001, 1447, 1744, 2315, 1443, 2067, 2101, 1723, 1990, 210, 1866, 1926, 1303, 1887, 2515, 646, 356, 2391, 129, 667, 2037, 1038, 2049, 2152, 1783, 2460, 103, 1641, 878, 757, 272, 1073, 2221, 1417, 565, 2007, 1088, 192, 1506, 2085, 1930, 1435, 2170, 251, 2342, 327, 452, 2241, 1045, 1789, 2244, 2185, 1468, 148, 932, 147, 2008, 2248, 303, 1774, 2500, 1651, 507, 923, 339, 792, 1522, 1879, 2300, 2374, 1280, 2154, 334, 564, 2334, 2151, 1177, 68, 1854, 2435, 1865, 1955, 2131, 533, 611, 446, 1689, 939, 2317, 540, 689, 922, 1843, 450, 1036, 261, 1798, 1260, 2278, 1801, 1497, 372, 1452, 1324, 1614, 1612, 347, 529, 621, 1446, 2471, 2146, 2249, 2456, 892, 514, 555, 2290, 2429, 1043, 1720, 633, 1363, 894, 634, 2021, 477, 1591, 2106, 2193, 1232, 739, 1269, 185, 674, 2102, 2511, 1758, 828, 1845, 1824, 2547, 1105, 780, 750, 350, 1395, 21, 526, 2549, 233, 2479, 1188, 33, 297, 2100, 1205, 1211, 1346, 709, 881, 54, 861, 1029, 1283, 819, 534, 299, 1670, 1698, 652, 245, 46, 2119, 1982, 2378, 556, 1183, 1242, 1527, 940, 1054, 2279, 497, 2497, 196, 1344, 949, 2018, 2414, 1009, 2215, 1394, 1958, 1226, 2475, 42, 989, 1872, 1089, 863, 1685, 2309, 454, 2242, 1256, 2243, 2161, 1425, 361, 1559, 1567, 2112, 187, 697, 1112, 1037, 1784, 1898, 1378, 1348, 193, 241, 627, 2401, 2145, 481, 2283, 1333, 1894, 1146, 2125, 1326, 1411, 1057, 1924, 2338, 1438, 679, 653, 518, 436, 1998, 1968, 994, 1252, 1978, 2030, 465, 328, 494, 1599, 2450, 660, 664, 197, 1578, 1735, 506, 2465, 2276, 225, 1141, 202, 2316, 106, 389, 80, 1584, 1085, 2331, 2072, 1362, 378, 1208, 2427, 1385, 1755, 1684, 2090, 2261, 2122, 1066, 544, 1266, 1349, 1664, 2183, 2293, 1131, 945, 2282, 122, 982, 1413, 1258, 1995], 'conv4': [1575, 1644, 315, 2388, 4, 1323, 1622, 1859, 2228, 2029, 263, 770, 1131, 1289, 2356, 544, 1187, 2123, 1193, 188, 2237, 1610, 1746, 1064, 264, 1908, 806, 1444, 2413, 891, 1580, 412, 1288, 1993, 1706, 2207, 1402, 1757, 1185, 1109, 493, 502, 2285, 2353, 2475, 778, 1143, 2537, 2008, 615, 67, 1524, 2142, 1795, 1366, 2070, 2041, 448, 2064, 1654, 1167, 514, 549, 1975, 309, 1732, 1019, 1957, 1266, 1629, 1481, 1855, 893, 34, 1067, 1326, 231, 2346, 919, 1819, 366, 1667, 2268, 1082, 1620, 132, 6, 1521, 929, 1428, 1867, 1969, 1631, 621, 2463, 1585, 774, 277, 551, 2254, 783, 1659, 1684, 2131, 1813, 603, 792, 556, 288, 1604, 1041, 2209, 2292, 1499, 2510, 1641, 1749, 805, 2262, 1682, 1967, 2517, 1668, 1857, 593, 1676, 2524, 969, 2038, 802, 489, 670, 435, 1063, 1988, 2286, 1062, 829, 2225, 230, 1285, 687, 437, 2183, 568, 673, 2158, 2518, 99, 2396, 851, 1084, 1152, 895, 711, 397, 1833, 1922, 8, 2015, 2461, 1603, 272, 292, 1066, 1244, 2143, 2523, 21, 586, 1214, 1439, 648, 703, 1734, 1727, 401, 433, 2014, 1874, 2385, 718, 294, 26, 675, 2218, 704, 1490, 1468, 1267, 931, 1191, 1251, 2439, 331, 1212, 592, 558, 59, 1756, 139, 1674, 2335, 2528, 2539, 1538, 2210, 2383, 1529, 389, 2267, 1787, 2163, 1657, 1726, 2338, 527, 843, 91, 1254, 2121, 386, 1498, 2359, 1877, 865, 507, 1739, 1190, 2022, 1134, 7, 936, 1434, 1412, 1828, 150, 2152, 2224, 1974, 862, 1870, 1672, 1035, 725, 1043, 1539, 297, 339, 1070, 1261, 2336, 2078, 1764, 889, 1541, 2300, 1898, 1801, 599, 1221, 1208, 381, 1286, 1996, 1399, 671, 1218, 1618, 1257, 1688, 1375, 1534, 1730, 674, 233, 1429, 1454, 2249, 2415, 536, 1778, 2046, 987, 1448, 553, 2543, 473, 1296, 1130, 1204, 641, 2133, 1883, 1535, 70, 1355, 1354, 2174, 1452, 1991, 2053, 1656, 997, 508, 2227, 1832, 2089, 1445, 612, 1652, 182, 1338, 1115, 1471, 1483, 2557, 175, 761, 1685, 1313, 2211, 2496, 510, 455, 1772, 2472, 1472, 1998, 1596, 894, 1308, 1104, 1018, 859, 734, 1624, 1034, 1536, 945, 1514, 1613, 1915, 33, 343, 1518, 2203, 1540, 1865, 65, 1276, 1201, 172, 656, 460, 2555, 1560, 2259, 1731, 2404, 405, 135, 203, 1766, 149, 225, 1928, 2195, 1612, 457, 253, 208, 518, 487, 258, 1885, 1357, 1567, 2213, 1990, 1784, 1485, 2149, 2257, 1422]}
SAVED model.heads_thresh 0.9
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
Epoch: [50/50]	lr: 7.81e-06	time: 00:00:31	Loss_train 0.01446	Acc_train 84.65	/	Loss_test 0.00506	Acc_test 82.00
new_head:  {'blocks.5.layer.bias': tensor([ 1.5791e-05, -1.6506e-02], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0120,  0.0150,  0.0043,  ...,  0.0072,  0.0111, -0.0045],
        [ 0.0163,  0.0075,  0.0001,  ..., -0.0045,  0.0101,  0.0122]],
       device='cuda:0')}
BLOCKS IN SUP:  [5]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [77, 68, 25, 59, 93, 5, 30, 14, 35, 70, 4, 45, 69, 82, 86], 'conv1': [365, 50, 382, 75, 180, 264, 56, 345, 104, 352, 206, 100, 190, 324, 354, 99, 73, 140, 336, 355, 121, 191, 233, 144, 243, 163, 193, 150, 4, 26, 373, 266, 118, 92, 357, 117, 94, 90, 158, 68, 179, 202, 368, 76, 36, 170, 304, 146, 41, 35, 33, 138, 6, 105, 262, 197, 293, 317, 17], 'conv2': [307, 937, 662, 141, 1307, 1006, 869, 683, 1441, 1238, 348, 1086, 1278, 37, 1404, 427, 889, 311, 341, 1405, 796, 312, 1500, 794, 1486, 1370, 490, 304, 449, 835, 1297, 1100, 28, 942, 731, 537, 710, 380, 612, 451, 1159, 735, 1139, 1046, 142, 1385, 998, 176, 41, 295, 958, 346, 313, 1436, 1034, 167, 1360, 906, 529, 441, 214, 1306, 1395, 79, 1450, 544, 1412, 660, 542, 1000, 1372, 700, 405, 613, 61, 14, 1243, 711, 195, 673, 636, 1082, 107, 1317, 635, 590, 551, 997, 209, 207, 1474, 987, 1226, 865, 706, 1172, 421, 733, 70, 1143, 94, 959, 896, 547, 229, 423, 11, 838, 923, 856, 407, 851, 1345, 914, 1359, 622, 259, 1464, 1400, 460, 268, 730, 1532, 1266, 454, 634, 806, 450, 577, 35, 12, 1027, 1449, 1534, 1466, 872, 534, 752, 1489, 1101, 1230, 1134, 199, 434, 1185, 932, 915, 123, 766, 1429, 250, 1033, 155, 335, 1030, 276, 457, 953, 174, 799, 828, 1452, 891, 626, 496, 1149, 831, 456, 510, 194, 68, 810, 761, 981, 1242, 1291, 1294, 1535, 75, 499, 935, 1272, 501, 912, 1347, 921, 353, 859, 640, 1032, 1507, 1171, 373, 100, 287, 281, 1442, 944, 1083, 406, 356, 549, 187, 883, 1265, 919, 619, 109, 783, 870, 760, 927, 868, 904, 644, 1249, 420, 1113, 1455, 1211, 1183, 78, 95, 13, 1167, 261, 839, 817, 1274, 252, 994], 'conv3': [1422, 43, 1677, 114, 1319, 2000, 471, 2051, 165, 909, 1874, 1660, 934, 1549, 1608, 2302, 2494, 1312, 2352, 1548, 589, 2069, 1083, 499, 1520, 1988, 1873, 1187, 1110, 195, 2438, 1168, 167, 2138, 1090, 882, 512, 505, 398, 1938, 1827, 648, 1127, 1776, 468, 423, 1331, 1109, 2448, 784, 1495, 1554, 2081, 907, 1558, 2533, 2281, 781, 1616, 694, 278, 1063, 980, 2247, 249, 1245, 305, 1857, 867, 443, 2556, 1709, 359, 178, 1164, 419, 841, 2095, 1049, 95, 28, 137, 2181, 1403, 1359, 377, 231, 136, 2001, 1447, 1744, 2315, 1443, 2067, 2101, 1723, 1990, 210, 1866, 1926, 1303, 1887, 2515, 646, 356, 2391, 129, 667, 2037, 1038, 2049, 2152, 1783, 2460, 103, 1641, 878, 757, 272, 1073, 2221, 1417, 565, 2007, 1088, 192, 1506, 2085, 1930, 1435, 2170, 251, 2342, 327, 452, 2241, 1045, 1789, 2244, 2185, 1468, 148, 932, 147, 2008, 2248, 303, 1774, 2500, 1651, 507, 923, 339, 792, 1522, 1879, 2300, 2374, 1280, 2154, 334, 564, 2334, 2151, 1177, 68, 1854, 2435, 1865, 1955, 2131, 533, 611, 446, 1689, 939, 2317, 540, 689, 922, 1843, 450, 1036, 261, 1798, 1260, 2278, 1801, 1497, 372, 1452, 1324, 1614, 1612, 347, 529, 621, 1446, 2471, 2146, 2249, 2456, 892, 514, 555, 2290, 2429, 1043, 1720, 633, 1363, 894, 634, 2021, 477, 1591, 2106, 2193, 1232, 739, 1269, 185, 674, 2102, 2511, 1758, 828, 1845, 1824, 2547, 1105, 780, 750, 350, 1395, 21, 526, 2549, 233, 2479, 1188, 33, 297, 2100, 1205, 1211, 1346, 709, 881, 54, 861, 1029, 1283, 819, 534, 299, 1670, 1698, 652, 245, 46, 2119, 1982, 2378, 556, 1183, 1242, 1527, 940, 1054, 2279, 497, 2497, 196, 1344, 949, 2018, 2414, 1009, 2215, 1394, 1958, 1226, 2475, 42, 989, 1872, 1089, 863, 1685, 2309, 454, 2242, 1256, 2243, 2161, 1425, 361, 1559, 1567, 2112, 187, 697, 1112, 1037, 1784, 1898, 1378, 1348, 193, 241, 627, 2401, 2145, 481, 2283, 1333, 1894, 1146, 2125, 1326, 1411, 1057, 1924, 2338, 1438, 679, 653, 518, 436, 1998, 1968, 994, 1252, 1978, 2030, 465, 328, 494, 1599, 2450, 660, 664, 197, 1578, 1735, 506, 2465, 2276, 225, 1141, 202, 2316, 106, 389, 80, 1584, 1085, 2331, 2072, 1362, 378, 1208, 2427, 1385, 1755, 1684, 2090, 2261, 2122, 1066, 544, 1266, 1349, 1664, 2183, 2293, 1131, 945, 2282, 122, 982, 1413, 1258, 1995], 'conv4': [1575, 1644, 315, 2388, 4, 1323, 1622, 1859, 2228, 2029, 263, 770, 1131, 1289, 2356, 544, 1187, 2123, 1193, 188, 2237, 1610, 1746, 1064, 264, 1908, 806, 1444, 2413, 891, 1580, 412, 1288, 1993, 1706, 2207, 1402, 1757, 1185, 1109, 493, 502, 2285, 2353, 2475, 778, 1143, 2537, 2008, 615, 67, 1524, 2142, 1795, 1366, 2070, 2041, 448, 2064, 1654, 1167, 514, 549, 1975, 309, 1732, 1019, 1957, 1266, 1629, 1481, 1855, 893, 34, 1067, 1326, 231, 2346, 919, 1819, 366, 1667, 2268, 1082, 1620, 132, 6, 1521, 929, 1428, 1867, 1969, 1631, 621, 2463, 1585, 774, 277, 551, 2254, 783, 1659, 1684, 2131, 1813, 603, 792, 556, 288, 1604, 1041, 2209, 2292, 1499, 2510, 1641, 1749, 805, 2262, 1682, 1967, 2517, 1668, 1857, 593, 1676, 2524, 969, 2038, 802, 489, 670, 435, 1063, 1988, 2286, 1062, 829, 2225, 230, 1285, 687, 437, 2183, 568, 673, 2158, 2518, 99, 2396, 851, 1084, 1152, 895, 711, 397, 1833, 1922, 8, 2015, 2461, 1603, 272, 292, 1066, 1244, 2143, 2523, 21, 586, 1214, 1439, 648, 703, 1734, 1727, 401, 433, 2014, 1874, 2385, 718, 294, 26, 675, 2218, 704, 1490, 1468, 1267, 931, 1191, 1251, 2439, 331, 1212, 592, 558, 59, 1756, 139, 1674, 2335, 2528, 2539, 1538, 2210, 2383, 1529, 389, 2267, 1787, 2163, 1657, 1726, 2338, 527, 843, 91, 1254, 2121, 386, 1498, 2359, 1877, 865, 507, 1739, 1190, 2022, 1134, 7, 936, 1434, 1412, 1828, 150, 2152, 2224, 1974, 862, 1870, 1672, 1035, 725, 1043, 1539, 297, 339, 1070, 1261, 2336, 2078, 1764, 889, 1541, 2300, 1898, 1801, 599, 1221, 1208, 381, 1286, 1996, 1399, 671, 1218, 1618, 1257, 1688, 1375, 1534, 1730, 674, 233, 1429, 1454, 2249, 2415, 536, 1778, 2046, 987, 1448, 553, 2543, 473, 1296, 1130, 1204, 641, 2133, 1883, 1535, 70, 1355, 1354, 2174, 1452, 1991, 2053, 1656, 997, 508, 2227, 1832, 2089, 1445, 612, 1652, 182, 1338, 1115, 1471, 1483, 2557, 175, 761, 1685, 1313, 2211, 2496, 510, 455, 1772, 2472, 1472, 1998, 1596, 894, 1308, 1104, 1018, 859, 734, 1624, 1034, 1536, 945, 1514, 1613, 1915, 33, 343, 1518, 2203, 1540, 1865, 65, 1276, 1201, 172, 656, 460, 2555, 1560, 2259, 1731, 2404, 405, 135, 203, 1766, 149, 225, 1928, 2195, 1612, 457, 253, 208, 518, 487, 258, 1885, 1357, 1567, 2213, 1990, 1784, 1485, 2149, 2257, 1422]}
SAVED model.heads_thresh 0.9
RESULT:  {'train_loss': 0.014464263804256916, 'train_acc': 84.64999794960022, 'test_loss': 0.005055953748524189, 'test_acc': 82.0, 'convergence': 23.65986442565918, 'R1': 0, 'dataset_sup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [32, 93]}, 'dataset_unsup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 50, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [32, 93]}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3, 4], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 50, 'lr': 0.005}, 't1': {'blocks': [5], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}}
IN R0:  {'count': 0, 'R0': {'train_loss': 0.014464263804256916, 'train_acc': 84.64999794960022, 'test_loss': 0.005055953748524189, 'test_acc': 82.0, 'convergence': 23.65986442565918, 'R1': 0, 'dataset_sup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [32, 93]}, 'dataset_unsup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 50, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [32, 93]}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3, 4], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 50, 'lr': 0.005}, 't1': {'blocks': [5], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}}}
################################## TASK 1 ############################################
[92, 35]
SEED:  0
block 0, size : 96 16 16
96
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 2.886751345948129
block 1, size : 384 8 8
384
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.8505172717997146
block 2, size : 1536 4 4
1536
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.4252586358998573
block 3, size : 2560 2 2
2560
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.21262931794992865
block 4, size : 2560 1 1
2560
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.1647019614671031
NOWWWW
1 1
2
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
blocks['b%s' % 5] 2560 2560
range = 0.3423265984407288
train_layer_order: consecutive {'t0': {'blocks': [0, 1, 2, 3, 4], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 50, 'lr': 0.005}, 't1': {'blocks': [5], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}
The device used will be: 
True
cuda:0
PARAMSSSS IN LOAD:  {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
avg_deltas:  {'blocks.0.layer.weight': tensor([-1.4532e-01,  8.7761e-04,  5.7996e-05,  1.9021e-03, -3.7310e-01,
        -6.7027e-01,  9.7028e-02, -1.7941e-03, -4.0346e-01,  7.2447e-04,
         1.6538e-01,  1.0592e-01, -7.4874e-04,  3.5001e-03,  3.0900e-01,
        -4.0094e-03, -8.4942e-01,  1.0030e-03, -1.8284e-01, -1.9295e-01,
         4.5407e-02,  2.2242e-01, -1.5329e-02,  2.6259e-04,  3.2590e-02,
         4.7867e-01, -8.1054e-03, -2.1419e-02,  2.1114e-02, -1.4097e-03,
         3.9547e-01,  6.7029e-02, -7.0707e-03, -2.9653e-01,  3.1926e-01,
         2.9254e-01, -2.8872e-01, -1.9610e-02,  2.3829e-01,  6.3319e-03,
         1.2408e-01,  7.7772e-02,  1.1616e-01, -7.2632e-02,  4.4684e-02,
        -2.0923e-01, -5.5327e-03,  1.7412e-02, -8.6930e-03,  1.4219e-01,
        -3.1874e-01,  3.0855e-01,  1.1764e-01,  8.1276e-02, -2.7714e-03,
         3.9494e-01, -4.3484e-02,  1.8097e-01, -1.7396e-01,  1.0000e+00,
        -1.1112e-02, -5.0324e-03, -1.2712e-01,  7.4909e-03, -4.0420e-01,
        -6.3497e-02, -2.3460e-01, -5.4490e-03,  7.7608e-01,  7.3809e-01,
         1.8613e-01,  3.7175e-02, -6.1988e-03, -4.1727e-01, -1.9685e-02,
        -1.4673e-01,  3.0621e-02,  8.1144e-01, -1.4049e-01,  1.8670e-01,
        -1.4354e-01,  4.0347e-01, -3.4343e-01,  1.5191e-01,  1.3946e-02,
        -2.1194e-02, -2.7387e-01, -4.4525e-02, -3.5677e-02, -3.6691e-01,
         4.3799e-03, -7.8093e-02, -9.8441e-02,  5.9217e-01, -3.9111e-02,
         2.0596e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 1.6497e-03,  1.9875e-04,  4.3818e-04,  2.2631e-04,  9.2604e-01,
        -3.6322e-02,  1.4620e-03, -3.1491e-03, -2.0400e-04,  4.8712e-03,
        -7.3878e-05,  3.3166e-03,  2.5829e-03,  1.5562e-03, -1.4293e-04,
         2.2982e-03, -5.5295e-04,  4.8175e-02, -1.3645e-02, -1.6232e-02,
         1.1711e-03,  2.5179e-04,  9.7178e-03,  5.2320e-03,  3.9179e-03,
         9.3389e-03, -5.1052e-03, -3.9680e-03, -6.8696e-03,  1.2207e-03,
         1.0053e-03, -1.7635e-02, -1.7809e-02, -2.2217e-04, -5.8817e-03,
        -1.4746e-04,  7.9396e-02,  1.1548e-02,  8.5405e-05,  1.3117e-04,
         1.7224e-03,  1.2198e-02,  6.7023e-04,  5.8088e-05,  1.4750e-03,
         5.5404e-09, -6.1092e-02,  6.7793e-03, -8.7042e-03, -2.1595e-02,
         5.3426e-02, -1.3242e-05, -4.2702e-04,  1.2886e-04, -6.4118e-03,
         5.9787e-02,  8.0946e-02, -2.3455e-04,  1.0323e-02, -2.4025e-04,
         1.2172e-04, -9.7062e-02,  4.3293e-04, -1.1938e-02, -2.4034e-02,
        -1.6284e-05, -8.0959e-02,  2.6137e-04,  5.5975e-02, -2.1299e-03,
        -6.3298e-02, -2.1935e-03, -4.5828e-02,  3.1943e-01, -5.8868e-04,
         6.9664e-02,  2.8211e-01, -3.6960e-02, -4.1849e-02,  7.2758e-02,
        -2.0004e-03,  1.2947e-04,  3.4534e-04, -5.3514e-05,  2.8851e-02,
         1.8986e-05,  4.3380e-05, -1.6957e-01, -1.8001e-03,  8.1271e-06,
        -1.9054e-03,  1.8689e-04, -1.6949e-03,  5.8045e-05,  8.3057e-03,
        -9.1768e-03,  6.3129e-03,  7.7744e-04, -1.8647e-02,  2.0871e-03,
         2.8462e-01,  2.2765e-04,  1.6052e-04, -1.6849e-02, -9.9119e-03,
         8.0280e-03, -1.0734e-03, -4.7587e-03, -6.5447e-03,  6.2858e-02,
        -1.0420e-02, -4.2226e-05,  2.3345e-05, -2.5564e-02,  4.2997e-02,
        -1.9941e-03, -3.0231e-03,  8.8025e-02,  5.5176e-02, -5.8188e-03,
        -1.1201e-01, -3.4873e-01,  2.0603e-04,  1.3139e-02,  3.3899e-02,
        -1.8469e-01, -1.5970e-03, -4.6929e-02,  1.3180e-03,  1.0804e-04,
         7.1643e-05,  2.9450e-03,  1.0574e-03,  2.2648e-06,  9.5943e-03,
         1.7341e-03,  5.1794e-06, -2.7626e-04,  1.4144e-02,  6.8763e-04,
         2.9027e-02, -9.6313e-05,  6.2271e-03, -2.6152e-04,  4.8838e-02,
         1.3392e-04,  3.7421e-02, -1.0279e-03,  1.9578e-03, -2.4686e-06,
        -3.1391e-02, -2.7439e-03,  2.6205e-04,  4.4403e-04,  5.8927e-05,
        -2.0026e-04, -3.3617e-02, -2.0904e-01,  3.7527e-03, -6.1212e-02,
        -7.5932e-05,  3.4279e-04,  1.0103e-01, -7.2149e-02, -6.7910e-03,
         3.1017e-03, -4.9871e-03, -2.4827e-05,  5.7320e-03, -9.8641e-03,
        -1.0900e-02,  2.3263e-03,  3.6481e-03,  4.4674e-03,  1.8615e-02,
        -8.9003e-03,  7.2513e-03, -8.9819e-06,  1.4622e-03,  9.9692e-03,
        -6.1444e-01,  3.5636e-03,  4.6653e-03, -1.3793e-05, -6.2969e-04,
         3.4120e-05,  1.1020e-03, -3.0469e-05,  4.1266e-03, -3.6619e-02,
         3.8302e-01, -3.0779e-03,  1.7754e-04,  4.4800e-02,  3.6007e-04,
         6.4968e-01,  5.2967e-05,  1.0000e+00, -1.5822e-04,  3.0491e-05,
         2.4169e-02, -3.4941e-04, -1.9727e-02,  9.8392e-02, -4.4161e-03,
         1.1326e-03,  7.1525e-02, -1.4324e-05,  3.2677e-05,  5.8709e-03,
        -1.9198e-03,  1.7674e-03,  4.1423e-05, -3.1888e-02, -3.7341e-06,
         6.1054e-04,  4.8628e-04,  7.5724e-03, -4.3960e-02,  8.3225e-03,
         1.3550e-04,  1.1450e-02, -1.8919e-03, -2.6429e-04,  9.9447e-03,
        -1.3199e-02,  8.2843e-04, -3.4749e-02,  9.8738e-04,  1.5413e-03,
        -3.8671e-05, -2.3892e-05,  9.4244e-04,  6.9910e-04,  2.9414e-03,
        -2.7643e-03,  3.2199e-03,  4.3235e-05,  6.0167e-03,  2.1491e-03,
        -2.2352e-02, -1.3359e-02,  1.7401e-07, -2.3843e-01, -7.2115e-05,
        -3.0431e-03, -8.7497e-05, -4.1749e-02,  3.2445e-02, -1.0811e-04,
         1.9581e-02,  8.2049e-05, -1.3406e-03,  3.0483e-04, -2.3033e-02,
         2.2119e-03,  2.9305e-05,  3.3734e-04, -7.1218e-05,  6.1239e-05,
        -3.7863e-04, -4.0345e-05,  1.2659e-01,  7.5645e-04, -4.2602e-02,
         1.4669e-03, -9.1165e-01, -2.5074e-05,  1.2828e-05,  1.6369e-05,
        -9.4926e-03,  6.3717e-02, -3.8056e-05,  3.3859e-06,  7.6375e-02,
        -2.5977e-04,  1.9502e-03,  5.1995e-03,  1.3606e-02, -2.9814e-05,
        -1.2566e-03,  1.0382e-02,  8.9141e-03, -1.0568e-07,  2.4576e-04,
         1.1888e-02, -5.9848e-04, -7.8868e-03,  2.4651e-03, -5.5964e-03,
        -2.7051e-04, -1.6336e-01,  3.1006e-03, -3.7294e-04, -1.3225e-03,
        -4.1570e-02, -2.9464e-02, -1.0904e-03,  1.1279e-02,  2.5964e-03,
        -1.0496e-01, -4.4629e-02,  4.6441e-04, -1.4105e-03,  1.0779e-01,
         2.7615e-04,  4.9523e-02,  5.8190e-05,  1.2694e-04, -7.1184e-04,
        -1.0205e-03, -6.0117e-05,  7.1328e-03, -1.9950e-03,  3.5335e-03,
         1.7623e-05,  2.5985e-04,  1.6081e-03, -3.1294e-04, -1.7198e-06,
        -6.9620e-07,  1.9185e-04,  6.4374e-03, -3.3447e-03, -2.5361e-01,
         1.2631e-02,  9.3823e-03,  8.2233e-03, -4.7936e-02,  2.4008e-01,
        -1.3753e-01,  1.7838e-03, -1.1939e-01,  1.5563e-03, -2.1886e-05,
         2.6196e-03,  1.4795e-02, -3.5906e-05,  8.7130e-04,  3.6218e-04,
         2.7082e-04,  1.6384e-04, -2.2462e-04,  2.3106e-02, -6.3837e-03,
        -2.1288e-01,  6.5514e-04,  1.9160e-01,  2.7915e-03, -2.2252e-01,
         2.7847e-04, -1.4735e-04, -9.4040e-02,  1.4203e-03,  2.4017e-01,
        -1.9699e-01, -4.4020e-04,  8.0017e-02,  1.9920e-02, -3.3929e-04,
         6.6552e-05,  1.6553e-05, -2.8286e-03,  2.2965e-03,  6.0745e-03,
        -5.1032e-01, -2.2038e-05,  8.0501e-03,  3.5752e-03, -1.4031e-04,
         3.3737e-04,  2.2669e-02,  2.8834e-03, -8.0148e-02, -9.3541e-03,
         6.0478e-04, -9.2107e-04,  2.8046e-04,  4.2740e-04,  7.4344e-03,
         6.2248e-04,  5.2942e-03, -6.2590e-02,  3.1001e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0008,  0.0005, -0.0125,  ...,  0.0090,  0.0078,  0.0085],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.0956,  0.0150, -0.0350,  ..., -0.0298, -0.0049,  0.0246],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([-0.0106,  0.0003,  0.0154,  ...,  0.0085,  0.0011,  0.0535],
       device='cuda:0')}
topk_layer:  None
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-1.4532e-01,  8.7761e-04,  5.7996e-05,  1.9021e-03, -3.7310e-01,
        -6.7027e-01,  9.7028e-02, -1.7941e-03, -4.0346e-01,  7.2447e-04,
         1.6538e-01,  1.0592e-01, -7.4874e-04,  3.5001e-03,  3.0900e-01,
        -4.0094e-03, -8.4942e-01,  1.0030e-03, -1.8284e-01, -1.9295e-01,
         4.5407e-02,  2.2242e-01, -1.5329e-02,  2.6259e-04,  3.2590e-02,
         4.7867e-01, -8.1054e-03, -2.1419e-02,  2.1114e-02, -1.4097e-03,
         3.9547e-01,  6.7029e-02, -7.0707e-03, -2.9653e-01,  3.1926e-01,
         2.9254e-01, -2.8872e-01, -1.9610e-02,  2.3829e-01,  6.3319e-03,
         1.2408e-01,  7.7772e-02,  1.1616e-01, -7.2632e-02,  4.4684e-02,
        -2.0923e-01, -5.5327e-03,  1.7412e-02, -8.6930e-03,  1.4219e-01,
        -3.1874e-01,  3.0855e-01,  1.1764e-01,  8.1276e-02, -2.7714e-03,
         3.9494e-01, -4.3484e-02,  1.8097e-01, -1.7396e-01,  1.0000e+00,
        -1.1112e-02, -5.0324e-03, -1.2712e-01,  7.4909e-03, -4.0420e-01,
        -6.3497e-02, -2.3460e-01, -5.4490e-03,  7.7608e-01,  7.3809e-01,
         1.8613e-01,  3.7175e-02, -6.1988e-03, -4.1727e-01, -1.9685e-02,
        -1.4673e-01,  3.0621e-02,  8.1144e-01, -1.4049e-01,  1.8670e-01,
        -1.4354e-01,  4.0347e-01, -3.4343e-01,  1.5191e-01,  1.3946e-02,
        -2.1194e-02, -2.7387e-01, -4.4525e-02, -3.5677e-02, -3.6691e-01,
         4.3799e-03, -7.8093e-02, -9.8441e-02,  5.9217e-01, -3.9111e-02,
         2.0596e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 1.6497e-03,  1.9875e-04,  4.3818e-04,  2.2631e-04,  9.2604e-01,
        -3.6322e-02,  1.4620e-03, -3.1491e-03, -2.0400e-04,  4.8712e-03,
        -7.3878e-05,  3.3166e-03,  2.5829e-03,  1.5562e-03, -1.4293e-04,
         2.2982e-03, -5.5295e-04,  4.8175e-02, -1.3645e-02, -1.6232e-02,
         1.1711e-03,  2.5179e-04,  9.7178e-03,  5.2320e-03,  3.9179e-03,
         9.3389e-03, -5.1052e-03, -3.9680e-03, -6.8696e-03,  1.2207e-03,
         1.0053e-03, -1.7635e-02, -1.7809e-02, -2.2217e-04, -5.8817e-03,
        -1.4746e-04,  7.9396e-02,  1.1548e-02,  8.5405e-05,  1.3117e-04,
         1.7224e-03,  1.2198e-02,  6.7023e-04,  5.8088e-05,  1.4750e-03,
         5.5404e-09, -6.1092e-02,  6.7793e-03, -8.7042e-03, -2.1595e-02,
         5.3426e-02, -1.3242e-05, -4.2702e-04,  1.2886e-04, -6.4118e-03,
         5.9787e-02,  8.0946e-02, -2.3455e-04,  1.0323e-02, -2.4025e-04,
         1.2172e-04, -9.7062e-02,  4.3293e-04, -1.1938e-02, -2.4034e-02,
        -1.6284e-05, -8.0959e-02,  2.6137e-04,  5.5975e-02, -2.1299e-03,
        -6.3298e-02, -2.1935e-03, -4.5828e-02,  3.1943e-01, -5.8868e-04,
         6.9664e-02,  2.8211e-01, -3.6960e-02, -4.1849e-02,  7.2758e-02,
        -2.0004e-03,  1.2947e-04,  3.4534e-04, -5.3514e-05,  2.8851e-02,
         1.8986e-05,  4.3380e-05, -1.6957e-01, -1.8001e-03,  8.1271e-06,
        -1.9054e-03,  1.8689e-04, -1.6949e-03,  5.8045e-05,  8.3057e-03,
        -9.1768e-03,  6.3129e-03,  7.7744e-04, -1.8647e-02,  2.0871e-03,
         2.8462e-01,  2.2765e-04,  1.6052e-04, -1.6849e-02, -9.9119e-03,
         8.0280e-03, -1.0734e-03, -4.7587e-03, -6.5447e-03,  6.2858e-02,
        -1.0420e-02, -4.2226e-05,  2.3345e-05, -2.5564e-02,  4.2997e-02,
        -1.9941e-03, -3.0231e-03,  8.8025e-02,  5.5176e-02, -5.8188e-03,
        -1.1201e-01, -3.4873e-01,  2.0603e-04,  1.3139e-02,  3.3899e-02,
        -1.8469e-01, -1.5970e-03, -4.6929e-02,  1.3180e-03,  1.0804e-04,
         7.1643e-05,  2.9450e-03,  1.0574e-03,  2.2648e-06,  9.5943e-03,
         1.7341e-03,  5.1794e-06, -2.7626e-04,  1.4144e-02,  6.8763e-04,
         2.9027e-02, -9.6313e-05,  6.2271e-03, -2.6152e-04,  4.8838e-02,
         1.3392e-04,  3.7421e-02, -1.0279e-03,  1.9578e-03, -2.4686e-06,
        -3.1391e-02, -2.7439e-03,  2.6205e-04,  4.4403e-04,  5.8927e-05,
        -2.0026e-04, -3.3617e-02, -2.0904e-01,  3.7527e-03, -6.1212e-02,
        -7.5932e-05,  3.4279e-04,  1.0103e-01, -7.2149e-02, -6.7910e-03,
         3.1017e-03, -4.9871e-03, -2.4827e-05,  5.7320e-03, -9.8641e-03,
        -1.0900e-02,  2.3263e-03,  3.6481e-03,  4.4674e-03,  1.8615e-02,
        -8.9003e-03,  7.2513e-03, -8.9819e-06,  1.4622e-03,  9.9692e-03,
        -6.1444e-01,  3.5636e-03,  4.6653e-03, -1.3793e-05, -6.2969e-04,
         3.4120e-05,  1.1020e-03, -3.0469e-05,  4.1266e-03, -3.6619e-02,
         3.8302e-01, -3.0779e-03,  1.7754e-04,  4.4800e-02,  3.6007e-04,
         6.4968e-01,  5.2967e-05,  1.0000e+00, -1.5822e-04,  3.0491e-05,
         2.4169e-02, -3.4941e-04, -1.9727e-02,  9.8392e-02, -4.4161e-03,
         1.1326e-03,  7.1525e-02, -1.4324e-05,  3.2677e-05,  5.8709e-03,
        -1.9198e-03,  1.7674e-03,  4.1423e-05, -3.1888e-02, -3.7341e-06,
         6.1054e-04,  4.8628e-04,  7.5724e-03, -4.3960e-02,  8.3225e-03,
         1.3550e-04,  1.1450e-02, -1.8919e-03, -2.6429e-04,  9.9447e-03,
        -1.3199e-02,  8.2843e-04, -3.4749e-02,  9.8738e-04,  1.5413e-03,
        -3.8671e-05, -2.3892e-05,  9.4244e-04,  6.9910e-04,  2.9414e-03,
        -2.7643e-03,  3.2199e-03,  4.3235e-05,  6.0167e-03,  2.1491e-03,
        -2.2352e-02, -1.3359e-02,  1.7401e-07, -2.3843e-01, -7.2115e-05,
        -3.0431e-03, -8.7497e-05, -4.1749e-02,  3.2445e-02, -1.0811e-04,
         1.9581e-02,  8.2049e-05, -1.3406e-03,  3.0483e-04, -2.3033e-02,
         2.2119e-03,  2.9305e-05,  3.3734e-04, -7.1218e-05,  6.1239e-05,
        -3.7863e-04, -4.0345e-05,  1.2659e-01,  7.5645e-04, -4.2602e-02,
         1.4669e-03, -9.1165e-01, -2.5074e-05,  1.2828e-05,  1.6369e-05,
        -9.4926e-03,  6.3717e-02, -3.8056e-05,  3.3859e-06,  7.6375e-02,
        -2.5977e-04,  1.9502e-03,  5.1995e-03,  1.3606e-02, -2.9814e-05,
        -1.2566e-03,  1.0382e-02,  8.9141e-03, -1.0568e-07,  2.4576e-04,
         1.1888e-02, -5.9848e-04, -7.8868e-03,  2.4651e-03, -5.5964e-03,
        -2.7051e-04, -1.6336e-01,  3.1006e-03, -3.7294e-04, -1.3225e-03,
        -4.1570e-02, -2.9464e-02, -1.0904e-03,  1.1279e-02,  2.5964e-03,
        -1.0496e-01, -4.4629e-02,  4.6441e-04, -1.4105e-03,  1.0779e-01,
         2.7615e-04,  4.9523e-02,  5.8190e-05,  1.2694e-04, -7.1184e-04,
        -1.0205e-03, -6.0117e-05,  7.1328e-03, -1.9950e-03,  3.5335e-03,
         1.7623e-05,  2.5985e-04,  1.6081e-03, -3.1294e-04, -1.7198e-06,
        -6.9620e-07,  1.9185e-04,  6.4374e-03, -3.3447e-03, -2.5361e-01,
         1.2631e-02,  9.3823e-03,  8.2233e-03, -4.7936e-02,  2.4008e-01,
        -1.3753e-01,  1.7838e-03, -1.1939e-01,  1.5563e-03, -2.1886e-05,
         2.6196e-03,  1.4795e-02, -3.5906e-05,  8.7130e-04,  3.6218e-04,
         2.7082e-04,  1.6384e-04, -2.2462e-04,  2.3106e-02, -6.3837e-03,
        -2.1288e-01,  6.5514e-04,  1.9160e-01,  2.7915e-03, -2.2252e-01,
         2.7847e-04, -1.4735e-04, -9.4040e-02,  1.4203e-03,  2.4017e-01,
        -1.9699e-01, -4.4020e-04,  8.0017e-02,  1.9920e-02, -3.3929e-04,
         6.6552e-05,  1.6553e-05, -2.8286e-03,  2.2965e-03,  6.0745e-03,
        -5.1032e-01, -2.2038e-05,  8.0501e-03,  3.5752e-03, -1.4031e-04,
         3.3737e-04,  2.2669e-02,  2.8834e-03, -8.0148e-02, -9.3541e-03,
         6.0478e-04, -9.2107e-04,  2.8046e-04,  4.2740e-04,  7.4344e-03,
         6.2248e-04,  5.2942e-03, -6.2590e-02,  3.1001e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0008,  0.0005, -0.0125,  ...,  0.0090,  0.0078,  0.0085],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.0956,  0.0150, -0.0350,  ..., -0.0298, -0.0049,  0.0246],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([-0.0106,  0.0003,  0.0154,  ...,  0.0085,  0.0011,  0.0535],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.0.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([-1.4532e-01,  8.7761e-04,  5.7996e-05,  1.9021e-03, -3.7310e-01,
        -6.7027e-01,  9.7028e-02, -1.7941e-03, -4.0346e-01,  7.2447e-04,
         1.6538e-01,  1.0592e-01, -7.4874e-04,  3.5001e-03,  3.0900e-01,
        -4.0094e-03, -8.4942e-01,  1.0030e-03, -1.8284e-01, -1.9295e-01,
         4.5407e-02,  2.2242e-01, -1.5329e-02,  2.6259e-04,  3.2590e-02,
         4.7867e-01, -8.1054e-03, -2.1419e-02,  2.1114e-02, -1.4097e-03,
         3.9547e-01,  6.7029e-02, -7.0707e-03, -2.9653e-01,  3.1926e-01,
         2.9254e-01, -2.8872e-01, -1.9610e-02,  2.3829e-01,  6.3319e-03,
         1.2408e-01,  7.7772e-02,  1.1616e-01, -7.2632e-02,  4.4684e-02,
        -2.0923e-01, -5.5327e-03,  1.7412e-02, -8.6930e-03,  1.4219e-01,
        -3.1874e-01,  3.0855e-01,  1.1764e-01,  8.1276e-02, -2.7714e-03,
         3.9494e-01, -4.3484e-02,  1.8097e-01, -1.7396e-01,  1.0000e+00,
        -1.1112e-02, -5.0324e-03, -1.2712e-01,  7.4909e-03, -4.0420e-01,
        -6.3497e-02, -2.3460e-01, -5.4490e-03,  7.7608e-01,  7.3809e-01,
         1.8613e-01,  3.7175e-02, -6.1988e-03, -4.1727e-01, -1.9685e-02,
        -1.4673e-01,  3.0621e-02,  8.1144e-01, -1.4049e-01,  1.8670e-01,
        -1.4354e-01,  4.0347e-01, -3.4343e-01,  1.5191e-01,  1.3946e-02,
        -2.1194e-02, -2.7387e-01, -4.4525e-02, -3.5677e-02, -3.6691e-01,
         4.3799e-03, -7.8093e-02, -9.8441e-02,  5.9217e-01, -3.9111e-02,
         2.0596e-02], device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-1.4532e-01,  8.7761e-04,  5.7996e-05,  1.9021e-03, -3.7310e-01,
        -6.7027e-01,  9.7028e-02, -1.7941e-03, -4.0346e-01,  7.2447e-04,
         1.6538e-01,  1.0592e-01, -7.4874e-04,  3.5001e-03,  3.0900e-01,
        -4.0094e-03, -8.4942e-01,  1.0030e-03, -1.8284e-01, -1.9295e-01,
         4.5407e-02,  2.2242e-01, -1.5329e-02,  2.6259e-04,  3.2590e-02,
         4.7867e-01, -8.1054e-03, -2.1419e-02,  2.1114e-02, -1.4097e-03,
         3.9547e-01,  6.7029e-02, -7.0707e-03, -2.9653e-01,  3.1926e-01,
         2.9254e-01, -2.8872e-01, -1.9610e-02,  2.3829e-01,  6.3319e-03,
         1.2408e-01,  7.7772e-02,  1.1616e-01, -7.2632e-02,  4.4684e-02,
        -2.0923e-01, -5.5327e-03,  1.7412e-02, -8.6930e-03,  1.4219e-01,
        -3.1874e-01,  3.0855e-01,  1.1764e-01,  8.1276e-02, -2.7714e-03,
         3.9494e-01, -4.3484e-02,  1.8097e-01, -1.7396e-01,  1.0000e+00,
        -1.1112e-02, -5.0324e-03, -1.2712e-01,  7.4909e-03, -4.0420e-01,
        -6.3497e-02, -2.3460e-01, -5.4490e-03,  7.7608e-01,  7.3809e-01,
         1.8613e-01,  3.7175e-02, -6.1988e-03, -4.1727e-01, -1.9685e-02,
        -1.4673e-01,  3.0621e-02,  8.1144e-01, -1.4049e-01,  1.8670e-01,
        -1.4354e-01,  4.0347e-01, -3.4343e-01,  1.5191e-01,  1.3946e-02,
        -2.1194e-02, -2.7387e-01, -4.4525e-02, -3.5677e-02, -3.6691e-01,
         4.3799e-03, -7.8093e-02, -9.8441e-02,  5.9217e-01, -3.9111e-02,
         2.0596e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 1.6497e-03,  1.9875e-04,  4.3818e-04,  2.2631e-04,  9.2604e-01,
        -3.6322e-02,  1.4620e-03, -3.1491e-03, -2.0400e-04,  4.8712e-03,
        -7.3878e-05,  3.3166e-03,  2.5829e-03,  1.5562e-03, -1.4293e-04,
         2.2982e-03, -5.5295e-04,  4.8175e-02, -1.3645e-02, -1.6232e-02,
         1.1711e-03,  2.5179e-04,  9.7178e-03,  5.2320e-03,  3.9179e-03,
         9.3389e-03, -5.1052e-03, -3.9680e-03, -6.8696e-03,  1.2207e-03,
         1.0053e-03, -1.7635e-02, -1.7809e-02, -2.2217e-04, -5.8817e-03,
        -1.4746e-04,  7.9396e-02,  1.1548e-02,  8.5405e-05,  1.3117e-04,
         1.7224e-03,  1.2198e-02,  6.7023e-04,  5.8088e-05,  1.4750e-03,
         5.5404e-09, -6.1092e-02,  6.7793e-03, -8.7042e-03, -2.1595e-02,
         5.3426e-02, -1.3242e-05, -4.2702e-04,  1.2886e-04, -6.4118e-03,
         5.9787e-02,  8.0946e-02, -2.3455e-04,  1.0323e-02, -2.4025e-04,
         1.2172e-04, -9.7062e-02,  4.3293e-04, -1.1938e-02, -2.4034e-02,
        -1.6284e-05, -8.0959e-02,  2.6137e-04,  5.5975e-02, -2.1299e-03,
        -6.3298e-02, -2.1935e-03, -4.5828e-02,  3.1943e-01, -5.8868e-04,
         6.9664e-02,  2.8211e-01, -3.6960e-02, -4.1849e-02,  7.2758e-02,
        -2.0004e-03,  1.2947e-04,  3.4534e-04, -5.3514e-05,  2.8851e-02,
         1.8986e-05,  4.3380e-05, -1.6957e-01, -1.8001e-03,  8.1271e-06,
        -1.9054e-03,  1.8689e-04, -1.6949e-03,  5.8045e-05,  8.3057e-03,
        -9.1768e-03,  6.3129e-03,  7.7744e-04, -1.8647e-02,  2.0871e-03,
         2.8462e-01,  2.2765e-04,  1.6052e-04, -1.6849e-02, -9.9119e-03,
         8.0280e-03, -1.0734e-03, -4.7587e-03, -6.5447e-03,  6.2858e-02,
        -1.0420e-02, -4.2226e-05,  2.3345e-05, -2.5564e-02,  4.2997e-02,
        -1.9941e-03, -3.0231e-03,  8.8025e-02,  5.5176e-02, -5.8188e-03,
        -1.1201e-01, -3.4873e-01,  2.0603e-04,  1.3139e-02,  3.3899e-02,
        -1.8469e-01, -1.5970e-03, -4.6929e-02,  1.3180e-03,  1.0804e-04,
         7.1643e-05,  2.9450e-03,  1.0574e-03,  2.2648e-06,  9.5943e-03,
         1.7341e-03,  5.1794e-06, -2.7626e-04,  1.4144e-02,  6.8763e-04,
         2.9027e-02, -9.6313e-05,  6.2271e-03, -2.6152e-04,  4.8838e-02,
         1.3392e-04,  3.7421e-02, -1.0279e-03,  1.9578e-03, -2.4686e-06,
        -3.1391e-02, -2.7439e-03,  2.6205e-04,  4.4403e-04,  5.8927e-05,
        -2.0026e-04, -3.3617e-02, -2.0904e-01,  3.7527e-03, -6.1212e-02,
        -7.5932e-05,  3.4279e-04,  1.0103e-01, -7.2149e-02, -6.7910e-03,
         3.1017e-03, -4.9871e-03, -2.4827e-05,  5.7320e-03, -9.8641e-03,
        -1.0900e-02,  2.3263e-03,  3.6481e-03,  4.4674e-03,  1.8615e-02,
        -8.9003e-03,  7.2513e-03, -8.9819e-06,  1.4622e-03,  9.9692e-03,
        -6.1444e-01,  3.5636e-03,  4.6653e-03, -1.3793e-05, -6.2969e-04,
         3.4120e-05,  1.1020e-03, -3.0469e-05,  4.1266e-03, -3.6619e-02,
         3.8302e-01, -3.0779e-03,  1.7754e-04,  4.4800e-02,  3.6007e-04,
         6.4968e-01,  5.2967e-05,  1.0000e+00, -1.5822e-04,  3.0491e-05,
         2.4169e-02, -3.4941e-04, -1.9727e-02,  9.8392e-02, -4.4161e-03,
         1.1326e-03,  7.1525e-02, -1.4324e-05,  3.2677e-05,  5.8709e-03,
        -1.9198e-03,  1.7674e-03,  4.1423e-05, -3.1888e-02, -3.7341e-06,
         6.1054e-04,  4.8628e-04,  7.5724e-03, -4.3960e-02,  8.3225e-03,
         1.3550e-04,  1.1450e-02, -1.8919e-03, -2.6429e-04,  9.9447e-03,
        -1.3199e-02,  8.2843e-04, -3.4749e-02,  9.8738e-04,  1.5413e-03,
        -3.8671e-05, -2.3892e-05,  9.4244e-04,  6.9910e-04,  2.9414e-03,
        -2.7643e-03,  3.2199e-03,  4.3235e-05,  6.0167e-03,  2.1491e-03,
        -2.2352e-02, -1.3359e-02,  1.7401e-07, -2.3843e-01, -7.2115e-05,
        -3.0431e-03, -8.7497e-05, -4.1749e-02,  3.2445e-02, -1.0811e-04,
         1.9581e-02,  8.2049e-05, -1.3406e-03,  3.0483e-04, -2.3033e-02,
         2.2119e-03,  2.9305e-05,  3.3734e-04, -7.1218e-05,  6.1239e-05,
        -3.7863e-04, -4.0345e-05,  1.2659e-01,  7.5645e-04, -4.2602e-02,
         1.4669e-03, -9.1165e-01, -2.5074e-05,  1.2828e-05,  1.6369e-05,
        -9.4926e-03,  6.3717e-02, -3.8056e-05,  3.3859e-06,  7.6375e-02,
        -2.5977e-04,  1.9502e-03,  5.1995e-03,  1.3606e-02, -2.9814e-05,
        -1.2566e-03,  1.0382e-02,  8.9141e-03, -1.0568e-07,  2.4576e-04,
         1.1888e-02, -5.9848e-04, -7.8868e-03,  2.4651e-03, -5.5964e-03,
        -2.7051e-04, -1.6336e-01,  3.1006e-03, -3.7294e-04, -1.3225e-03,
        -4.1570e-02, -2.9464e-02, -1.0904e-03,  1.1279e-02,  2.5964e-03,
        -1.0496e-01, -4.4629e-02,  4.6441e-04, -1.4105e-03,  1.0779e-01,
         2.7615e-04,  4.9523e-02,  5.8190e-05,  1.2694e-04, -7.1184e-04,
        -1.0205e-03, -6.0117e-05,  7.1328e-03, -1.9950e-03,  3.5335e-03,
         1.7623e-05,  2.5985e-04,  1.6081e-03, -3.1294e-04, -1.7198e-06,
        -6.9620e-07,  1.9185e-04,  6.4374e-03, -3.3447e-03, -2.5361e-01,
         1.2631e-02,  9.3823e-03,  8.2233e-03, -4.7936e-02,  2.4008e-01,
        -1.3753e-01,  1.7838e-03, -1.1939e-01,  1.5563e-03, -2.1886e-05,
         2.6196e-03,  1.4795e-02, -3.5906e-05,  8.7130e-04,  3.6218e-04,
         2.7082e-04,  1.6384e-04, -2.2462e-04,  2.3106e-02, -6.3837e-03,
        -2.1288e-01,  6.5514e-04,  1.9160e-01,  2.7915e-03, -2.2252e-01,
         2.7847e-04, -1.4735e-04, -9.4040e-02,  1.4203e-03,  2.4017e-01,
        -1.9699e-01, -4.4020e-04,  8.0017e-02,  1.9920e-02, -3.3929e-04,
         6.6552e-05,  1.6553e-05, -2.8286e-03,  2.2965e-03,  6.0745e-03,
        -5.1032e-01, -2.2038e-05,  8.0501e-03,  3.5752e-03, -1.4031e-04,
         3.3737e-04,  2.2669e-02,  2.8834e-03, -8.0148e-02, -9.3541e-03,
         6.0478e-04, -9.2107e-04,  2.8046e-04,  4.2740e-04,  7.4344e-03,
         6.2248e-04,  5.2942e-03, -6.2590e-02,  3.1001e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0008,  0.0005, -0.0125,  ...,  0.0090,  0.0078,  0.0085],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.0956,  0.0150, -0.0350,  ..., -0.0298, -0.0049,  0.0246],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([-0.0106,  0.0003,  0.0154,  ...,  0.0085,  0.0011,  0.0535],
       device='cuda:0')}
topk_layer inside model after retrieval:  [77, 68, 25, 59, 93, 5, 30, 14, 35, 70, 4, 45, 69, 82, 86]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-1.4532e-01,  8.7761e-04,  5.7996e-05,  1.9021e-03, -3.7310e-01,
        -6.7027e-01,  9.7028e-02, -1.7941e-03, -4.0346e-01,  7.2447e-04,
         1.6538e-01,  1.0592e-01, -7.4874e-04,  3.5001e-03,  3.0900e-01,
        -4.0094e-03, -8.4942e-01,  1.0030e-03, -1.8284e-01, -1.9295e-01,
         4.5407e-02,  2.2242e-01, -1.5329e-02,  2.6259e-04,  3.2590e-02,
         4.7867e-01, -8.1054e-03, -2.1419e-02,  2.1114e-02, -1.4097e-03,
         3.9547e-01,  6.7029e-02, -7.0707e-03, -2.9653e-01,  3.1926e-01,
         2.9254e-01, -2.8872e-01, -1.9610e-02,  2.3829e-01,  6.3319e-03,
         1.2408e-01,  7.7772e-02,  1.1616e-01, -7.2632e-02,  4.4684e-02,
        -2.0923e-01, -5.5327e-03,  1.7412e-02, -8.6930e-03,  1.4219e-01,
        -3.1874e-01,  3.0855e-01,  1.1764e-01,  8.1276e-02, -2.7714e-03,
         3.9494e-01, -4.3484e-02,  1.8097e-01, -1.7396e-01,  1.0000e+00,
        -1.1112e-02, -5.0324e-03, -1.2712e-01,  7.4909e-03, -4.0420e-01,
        -6.3497e-02, -2.3460e-01, -5.4490e-03,  7.7608e-01,  7.3809e-01,
         1.8613e-01,  3.7175e-02, -6.1988e-03, -4.1727e-01, -1.9685e-02,
        -1.4673e-01,  3.0621e-02,  8.1144e-01, -1.4049e-01,  1.8670e-01,
        -1.4354e-01,  4.0347e-01, -3.4343e-01,  1.5191e-01,  1.3946e-02,
        -2.1194e-02, -2.7387e-01, -4.4525e-02, -3.5677e-02, -3.6691e-01,
         4.3799e-03, -7.8093e-02, -9.8441e-02,  5.9217e-01, -3.9111e-02,
         2.0596e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 1.6497e-03,  1.9875e-04,  4.3818e-04,  2.2631e-04,  9.2604e-01,
        -3.6322e-02,  1.4620e-03, -3.1491e-03, -2.0400e-04,  4.8712e-03,
        -7.3878e-05,  3.3166e-03,  2.5829e-03,  1.5562e-03, -1.4293e-04,
         2.2982e-03, -5.5295e-04,  4.8175e-02, -1.3645e-02, -1.6232e-02,
         1.1711e-03,  2.5179e-04,  9.7178e-03,  5.2320e-03,  3.9179e-03,
         9.3389e-03, -5.1052e-03, -3.9680e-03, -6.8696e-03,  1.2207e-03,
         1.0053e-03, -1.7635e-02, -1.7809e-02, -2.2217e-04, -5.8817e-03,
        -1.4746e-04,  7.9396e-02,  1.1548e-02,  8.5405e-05,  1.3117e-04,
         1.7224e-03,  1.2198e-02,  6.7023e-04,  5.8088e-05,  1.4750e-03,
         5.5404e-09, -6.1092e-02,  6.7793e-03, -8.7042e-03, -2.1595e-02,
         5.3426e-02, -1.3242e-05, -4.2702e-04,  1.2886e-04, -6.4118e-03,
         5.9787e-02,  8.0946e-02, -2.3455e-04,  1.0323e-02, -2.4025e-04,
         1.2172e-04, -9.7062e-02,  4.3293e-04, -1.1938e-02, -2.4034e-02,
        -1.6284e-05, -8.0959e-02,  2.6137e-04,  5.5975e-02, -2.1299e-03,
        -6.3298e-02, -2.1935e-03, -4.5828e-02,  3.1943e-01, -5.8868e-04,
         6.9664e-02,  2.8211e-01, -3.6960e-02, -4.1849e-02,  7.2758e-02,
        -2.0004e-03,  1.2947e-04,  3.4534e-04, -5.3514e-05,  2.8851e-02,
         1.8986e-05,  4.3380e-05, -1.6957e-01, -1.8001e-03,  8.1271e-06,
        -1.9054e-03,  1.8689e-04, -1.6949e-03,  5.8045e-05,  8.3057e-03,
        -9.1768e-03,  6.3129e-03,  7.7744e-04, -1.8647e-02,  2.0871e-03,
         2.8462e-01,  2.2765e-04,  1.6052e-04, -1.6849e-02, -9.9119e-03,
         8.0280e-03, -1.0734e-03, -4.7587e-03, -6.5447e-03,  6.2858e-02,
        -1.0420e-02, -4.2226e-05,  2.3345e-05, -2.5564e-02,  4.2997e-02,
        -1.9941e-03, -3.0231e-03,  8.8025e-02,  5.5176e-02, -5.8188e-03,
        -1.1201e-01, -3.4873e-01,  2.0603e-04,  1.3139e-02,  3.3899e-02,
        -1.8469e-01, -1.5970e-03, -4.6929e-02,  1.3180e-03,  1.0804e-04,
         7.1643e-05,  2.9450e-03,  1.0574e-03,  2.2648e-06,  9.5943e-03,
         1.7341e-03,  5.1794e-06, -2.7626e-04,  1.4144e-02,  6.8763e-04,
         2.9027e-02, -9.6313e-05,  6.2271e-03, -2.6152e-04,  4.8838e-02,
         1.3392e-04,  3.7421e-02, -1.0279e-03,  1.9578e-03, -2.4686e-06,
        -3.1391e-02, -2.7439e-03,  2.6205e-04,  4.4403e-04,  5.8927e-05,
        -2.0026e-04, -3.3617e-02, -2.0904e-01,  3.7527e-03, -6.1212e-02,
        -7.5932e-05,  3.4279e-04,  1.0103e-01, -7.2149e-02, -6.7910e-03,
         3.1017e-03, -4.9871e-03, -2.4827e-05,  5.7320e-03, -9.8641e-03,
        -1.0900e-02,  2.3263e-03,  3.6481e-03,  4.4674e-03,  1.8615e-02,
        -8.9003e-03,  7.2513e-03, -8.9819e-06,  1.4622e-03,  9.9692e-03,
        -6.1444e-01,  3.5636e-03,  4.6653e-03, -1.3793e-05, -6.2969e-04,
         3.4120e-05,  1.1020e-03, -3.0469e-05,  4.1266e-03, -3.6619e-02,
         3.8302e-01, -3.0779e-03,  1.7754e-04,  4.4800e-02,  3.6007e-04,
         6.4968e-01,  5.2967e-05,  1.0000e+00, -1.5822e-04,  3.0491e-05,
         2.4169e-02, -3.4941e-04, -1.9727e-02,  9.8392e-02, -4.4161e-03,
         1.1326e-03,  7.1525e-02, -1.4324e-05,  3.2677e-05,  5.8709e-03,
        -1.9198e-03,  1.7674e-03,  4.1423e-05, -3.1888e-02, -3.7341e-06,
         6.1054e-04,  4.8628e-04,  7.5724e-03, -4.3960e-02,  8.3225e-03,
         1.3550e-04,  1.1450e-02, -1.8919e-03, -2.6429e-04,  9.9447e-03,
        -1.3199e-02,  8.2843e-04, -3.4749e-02,  9.8738e-04,  1.5413e-03,
        -3.8671e-05, -2.3892e-05,  9.4244e-04,  6.9910e-04,  2.9414e-03,
        -2.7643e-03,  3.2199e-03,  4.3235e-05,  6.0167e-03,  2.1491e-03,
        -2.2352e-02, -1.3359e-02,  1.7401e-07, -2.3843e-01, -7.2115e-05,
        -3.0431e-03, -8.7497e-05, -4.1749e-02,  3.2445e-02, -1.0811e-04,
         1.9581e-02,  8.2049e-05, -1.3406e-03,  3.0483e-04, -2.3033e-02,
         2.2119e-03,  2.9305e-05,  3.3734e-04, -7.1218e-05,  6.1239e-05,
        -3.7863e-04, -4.0345e-05,  1.2659e-01,  7.5645e-04, -4.2602e-02,
         1.4669e-03, -9.1165e-01, -2.5074e-05,  1.2828e-05,  1.6369e-05,
        -9.4926e-03,  6.3717e-02, -3.8056e-05,  3.3859e-06,  7.6375e-02,
        -2.5977e-04,  1.9502e-03,  5.1995e-03,  1.3606e-02, -2.9814e-05,
        -1.2566e-03,  1.0382e-02,  8.9141e-03, -1.0568e-07,  2.4576e-04,
         1.1888e-02, -5.9848e-04, -7.8868e-03,  2.4651e-03, -5.5964e-03,
        -2.7051e-04, -1.6336e-01,  3.1006e-03, -3.7294e-04, -1.3225e-03,
        -4.1570e-02, -2.9464e-02, -1.0904e-03,  1.1279e-02,  2.5964e-03,
        -1.0496e-01, -4.4629e-02,  4.6441e-04, -1.4105e-03,  1.0779e-01,
         2.7615e-04,  4.9523e-02,  5.8190e-05,  1.2694e-04, -7.1184e-04,
        -1.0205e-03, -6.0117e-05,  7.1328e-03, -1.9950e-03,  3.5335e-03,
         1.7623e-05,  2.5985e-04,  1.6081e-03, -3.1294e-04, -1.7198e-06,
        -6.9620e-07,  1.9185e-04,  6.4374e-03, -3.3447e-03, -2.5361e-01,
         1.2631e-02,  9.3823e-03,  8.2233e-03, -4.7936e-02,  2.4008e-01,
        -1.3753e-01,  1.7838e-03, -1.1939e-01,  1.5563e-03, -2.1886e-05,
         2.6196e-03,  1.4795e-02, -3.5906e-05,  8.7130e-04,  3.6218e-04,
         2.7082e-04,  1.6384e-04, -2.2462e-04,  2.3106e-02, -6.3837e-03,
        -2.1288e-01,  6.5514e-04,  1.9160e-01,  2.7915e-03, -2.2252e-01,
         2.7847e-04, -1.4735e-04, -9.4040e-02,  1.4203e-03,  2.4017e-01,
        -1.9699e-01, -4.4020e-04,  8.0017e-02,  1.9920e-02, -3.3929e-04,
         6.6552e-05,  1.6553e-05, -2.8286e-03,  2.2965e-03,  6.0745e-03,
        -5.1032e-01, -2.2038e-05,  8.0501e-03,  3.5752e-03, -1.4031e-04,
         3.3737e-04,  2.2669e-02,  2.8834e-03, -8.0148e-02, -9.3541e-03,
         6.0478e-04, -9.2107e-04,  2.8046e-04,  4.2740e-04,  7.4344e-03,
         6.2248e-04,  5.2942e-03, -6.2590e-02,  3.1001e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0008,  0.0005, -0.0125,  ...,  0.0090,  0.0078,  0.0085],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.0956,  0.0150, -0.0350,  ..., -0.0298, -0.0049,  0.0246],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([-0.0106,  0.0003,  0.0154,  ...,  0.0085,  0.0011,  0.0535],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.1.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 1.6497e-03,  1.9875e-04,  4.3818e-04,  2.2631e-04,  9.2604e-01,
        -3.6322e-02,  1.4620e-03, -3.1491e-03, -2.0400e-04,  4.8712e-03,
        -7.3878e-05,  3.3166e-03,  2.5829e-03,  1.5562e-03, -1.4293e-04,
         2.2982e-03, -5.5295e-04,  4.8175e-02, -1.3645e-02, -1.6232e-02,
         1.1711e-03,  2.5179e-04,  9.7178e-03,  5.2320e-03,  3.9179e-03,
         9.3389e-03, -5.1052e-03, -3.9680e-03, -6.8696e-03,  1.2207e-03,
         1.0053e-03, -1.7635e-02, -1.7809e-02, -2.2217e-04, -5.8817e-03,
        -1.4746e-04,  7.9396e-02,  1.1548e-02,  8.5405e-05,  1.3117e-04,
         1.7224e-03,  1.2198e-02,  6.7023e-04,  5.8088e-05,  1.4750e-03,
         5.5404e-09, -6.1092e-02,  6.7793e-03, -8.7042e-03, -2.1595e-02,
         5.3426e-02, -1.3242e-05, -4.2702e-04,  1.2886e-04, -6.4118e-03,
         5.9787e-02,  8.0946e-02, -2.3455e-04,  1.0323e-02, -2.4025e-04,
         1.2172e-04, -9.7062e-02,  4.3293e-04, -1.1938e-02, -2.4034e-02,
        -1.6284e-05, -8.0959e-02,  2.6137e-04,  5.5975e-02, -2.1299e-03,
        -6.3298e-02, -2.1935e-03, -4.5828e-02,  3.1943e-01, -5.8868e-04,
         6.9664e-02,  2.8211e-01, -3.6960e-02, -4.1849e-02,  7.2758e-02,
        -2.0004e-03,  1.2947e-04,  3.4534e-04, -5.3514e-05,  2.8851e-02,
         1.8986e-05,  4.3380e-05, -1.6957e-01, -1.8001e-03,  8.1271e-06,
        -1.9054e-03,  1.8689e-04, -1.6949e-03,  5.8045e-05,  8.3057e-03,
        -9.1768e-03,  6.3129e-03,  7.7744e-04, -1.8647e-02,  2.0871e-03,
         2.8462e-01,  2.2765e-04,  1.6052e-04, -1.6849e-02, -9.9119e-03,
         8.0280e-03, -1.0734e-03, -4.7587e-03, -6.5447e-03,  6.2858e-02,
        -1.0420e-02, -4.2226e-05,  2.3345e-05, -2.5564e-02,  4.2997e-02,
        -1.9941e-03, -3.0231e-03,  8.8025e-02,  5.5176e-02, -5.8188e-03,
        -1.1201e-01, -3.4873e-01,  2.0603e-04,  1.3139e-02,  3.3899e-02,
        -1.8469e-01, -1.5970e-03, -4.6929e-02,  1.3180e-03,  1.0804e-04,
         7.1643e-05,  2.9450e-03,  1.0574e-03,  2.2648e-06,  9.5943e-03,
         1.7341e-03,  5.1794e-06, -2.7626e-04,  1.4144e-02,  6.8763e-04,
         2.9027e-02, -9.6313e-05,  6.2271e-03, -2.6152e-04,  4.8838e-02,
         1.3392e-04,  3.7421e-02, -1.0279e-03,  1.9578e-03, -2.4686e-06,
        -3.1391e-02, -2.7439e-03,  2.6205e-04,  4.4403e-04,  5.8927e-05,
        -2.0026e-04, -3.3617e-02, -2.0904e-01,  3.7527e-03, -6.1212e-02,
        -7.5932e-05,  3.4279e-04,  1.0103e-01, -7.2149e-02, -6.7910e-03,
         3.1017e-03, -4.9871e-03, -2.4827e-05,  5.7320e-03, -9.8641e-03,
        -1.0900e-02,  2.3263e-03,  3.6481e-03,  4.4674e-03,  1.8615e-02,
        -8.9003e-03,  7.2513e-03, -8.9819e-06,  1.4622e-03,  9.9692e-03,
        -6.1444e-01,  3.5636e-03,  4.6653e-03, -1.3793e-05, -6.2969e-04,
         3.4120e-05,  1.1020e-03, -3.0469e-05,  4.1266e-03, -3.6619e-02,
         3.8302e-01, -3.0779e-03,  1.7754e-04,  4.4800e-02,  3.6007e-04,
         6.4968e-01,  5.2967e-05,  1.0000e+00, -1.5822e-04,  3.0491e-05,
         2.4169e-02, -3.4941e-04, -1.9727e-02,  9.8392e-02, -4.4161e-03,
         1.1326e-03,  7.1525e-02, -1.4324e-05,  3.2677e-05,  5.8709e-03,
        -1.9198e-03,  1.7674e-03,  4.1423e-05, -3.1888e-02, -3.7341e-06,
         6.1054e-04,  4.8628e-04,  7.5724e-03, -4.3960e-02,  8.3225e-03,
         1.3550e-04,  1.1450e-02, -1.8919e-03, -2.6429e-04,  9.9447e-03,
        -1.3199e-02,  8.2843e-04, -3.4749e-02,  9.8738e-04,  1.5413e-03,
        -3.8671e-05, -2.3892e-05,  9.4244e-04,  6.9910e-04,  2.9414e-03,
        -2.7643e-03,  3.2199e-03,  4.3235e-05,  6.0167e-03,  2.1491e-03,
        -2.2352e-02, -1.3359e-02,  1.7401e-07, -2.3843e-01, -7.2115e-05,
        -3.0431e-03, -8.7497e-05, -4.1749e-02,  3.2445e-02, -1.0811e-04,
         1.9581e-02,  8.2049e-05, -1.3406e-03,  3.0483e-04, -2.3033e-02,
         2.2119e-03,  2.9305e-05,  3.3734e-04, -7.1218e-05,  6.1239e-05,
        -3.7863e-04, -4.0345e-05,  1.2659e-01,  7.5645e-04, -4.2602e-02,
         1.4669e-03, -9.1165e-01, -2.5074e-05,  1.2828e-05,  1.6369e-05,
        -9.4926e-03,  6.3717e-02, -3.8056e-05,  3.3859e-06,  7.6375e-02,
        -2.5977e-04,  1.9502e-03,  5.1995e-03,  1.3606e-02, -2.9814e-05,
        -1.2566e-03,  1.0382e-02,  8.9141e-03, -1.0568e-07,  2.4576e-04,
         1.1888e-02, -5.9848e-04, -7.8868e-03,  2.4651e-03, -5.5964e-03,
        -2.7051e-04, -1.6336e-01,  3.1006e-03, -3.7294e-04, -1.3225e-03,
        -4.1570e-02, -2.9464e-02, -1.0904e-03,  1.1279e-02,  2.5964e-03,
        -1.0496e-01, -4.4629e-02,  4.6441e-04, -1.4105e-03,  1.0779e-01,
         2.7615e-04,  4.9523e-02,  5.8190e-05,  1.2694e-04, -7.1184e-04,
        -1.0205e-03, -6.0117e-05,  7.1328e-03, -1.9950e-03,  3.5335e-03,
         1.7623e-05,  2.5985e-04,  1.6081e-03, -3.1294e-04, -1.7198e-06,
        -6.9620e-07,  1.9185e-04,  6.4374e-03, -3.3447e-03, -2.5361e-01,
         1.2631e-02,  9.3823e-03,  8.2233e-03, -4.7936e-02,  2.4008e-01,
        -1.3753e-01,  1.7838e-03, -1.1939e-01,  1.5563e-03, -2.1886e-05,
         2.6196e-03,  1.4795e-02, -3.5906e-05,  8.7130e-04,  3.6218e-04,
         2.7082e-04,  1.6384e-04, -2.2462e-04,  2.3106e-02, -6.3837e-03,
        -2.1288e-01,  6.5514e-04,  1.9160e-01,  2.7915e-03, -2.2252e-01,
         2.7847e-04, -1.4735e-04, -9.4040e-02,  1.4203e-03,  2.4017e-01,
        -1.9699e-01, -4.4020e-04,  8.0017e-02,  1.9920e-02, -3.3929e-04,
         6.6552e-05,  1.6553e-05, -2.8286e-03,  2.2965e-03,  6.0745e-03,
        -5.1032e-01, -2.2038e-05,  8.0501e-03,  3.5752e-03, -1.4031e-04,
         3.3737e-04,  2.2669e-02,  2.8834e-03, -8.0148e-02, -9.3541e-03,
         6.0478e-04, -9.2107e-04,  2.8046e-04,  4.2740e-04,  7.4344e-03,
         6.2248e-04,  5.2942e-03, -6.2590e-02,  3.1001e-02], device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-1.4532e-01,  8.7761e-04,  5.7996e-05,  1.9021e-03, -3.7310e-01,
        -6.7027e-01,  9.7028e-02, -1.7941e-03, -4.0346e-01,  7.2447e-04,
         1.6538e-01,  1.0592e-01, -7.4874e-04,  3.5001e-03,  3.0900e-01,
        -4.0094e-03, -8.4942e-01,  1.0030e-03, -1.8284e-01, -1.9295e-01,
         4.5407e-02,  2.2242e-01, -1.5329e-02,  2.6259e-04,  3.2590e-02,
         4.7867e-01, -8.1054e-03, -2.1419e-02,  2.1114e-02, -1.4097e-03,
         3.9547e-01,  6.7029e-02, -7.0707e-03, -2.9653e-01,  3.1926e-01,
         2.9254e-01, -2.8872e-01, -1.9610e-02,  2.3829e-01,  6.3319e-03,
         1.2408e-01,  7.7772e-02,  1.1616e-01, -7.2632e-02,  4.4684e-02,
        -2.0923e-01, -5.5327e-03,  1.7412e-02, -8.6930e-03,  1.4219e-01,
        -3.1874e-01,  3.0855e-01,  1.1764e-01,  8.1276e-02, -2.7714e-03,
         3.9494e-01, -4.3484e-02,  1.8097e-01, -1.7396e-01,  1.0000e+00,
        -1.1112e-02, -5.0324e-03, -1.2712e-01,  7.4909e-03, -4.0420e-01,
        -6.3497e-02, -2.3460e-01, -5.4490e-03,  7.7608e-01,  7.3809e-01,
         1.8613e-01,  3.7175e-02, -6.1988e-03, -4.1727e-01, -1.9685e-02,
        -1.4673e-01,  3.0621e-02,  8.1144e-01, -1.4049e-01,  1.8670e-01,
        -1.4354e-01,  4.0347e-01, -3.4343e-01,  1.5191e-01,  1.3946e-02,
        -2.1194e-02, -2.7387e-01, -4.4525e-02, -3.5677e-02, -3.6691e-01,
         4.3799e-03, -7.8093e-02, -9.8441e-02,  5.9217e-01, -3.9111e-02,
         2.0596e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 1.6497e-03,  1.9875e-04,  4.3818e-04,  2.2631e-04,  9.2604e-01,
        -3.6322e-02,  1.4620e-03, -3.1491e-03, -2.0400e-04,  4.8712e-03,
        -7.3878e-05,  3.3166e-03,  2.5829e-03,  1.5562e-03, -1.4293e-04,
         2.2982e-03, -5.5295e-04,  4.8175e-02, -1.3645e-02, -1.6232e-02,
         1.1711e-03,  2.5179e-04,  9.7178e-03,  5.2320e-03,  3.9179e-03,
         9.3389e-03, -5.1052e-03, -3.9680e-03, -6.8696e-03,  1.2207e-03,
         1.0053e-03, -1.7635e-02, -1.7809e-02, -2.2217e-04, -5.8817e-03,
        -1.4746e-04,  7.9396e-02,  1.1548e-02,  8.5405e-05,  1.3117e-04,
         1.7224e-03,  1.2198e-02,  6.7023e-04,  5.8088e-05,  1.4750e-03,
         5.5404e-09, -6.1092e-02,  6.7793e-03, -8.7042e-03, -2.1595e-02,
         5.3426e-02, -1.3242e-05, -4.2702e-04,  1.2886e-04, -6.4118e-03,
         5.9787e-02,  8.0946e-02, -2.3455e-04,  1.0323e-02, -2.4025e-04,
         1.2172e-04, -9.7062e-02,  4.3293e-04, -1.1938e-02, -2.4034e-02,
        -1.6284e-05, -8.0959e-02,  2.6137e-04,  5.5975e-02, -2.1299e-03,
        -6.3298e-02, -2.1935e-03, -4.5828e-02,  3.1943e-01, -5.8868e-04,
         6.9664e-02,  2.8211e-01, -3.6960e-02, -4.1849e-02,  7.2758e-02,
        -2.0004e-03,  1.2947e-04,  3.4534e-04, -5.3514e-05,  2.8851e-02,
         1.8986e-05,  4.3380e-05, -1.6957e-01, -1.8001e-03,  8.1271e-06,
        -1.9054e-03,  1.8689e-04, -1.6949e-03,  5.8045e-05,  8.3057e-03,
        -9.1768e-03,  6.3129e-03,  7.7744e-04, -1.8647e-02,  2.0871e-03,
         2.8462e-01,  2.2765e-04,  1.6052e-04, -1.6849e-02, -9.9119e-03,
         8.0280e-03, -1.0734e-03, -4.7587e-03, -6.5447e-03,  6.2858e-02,
        -1.0420e-02, -4.2226e-05,  2.3345e-05, -2.5564e-02,  4.2997e-02,
        -1.9941e-03, -3.0231e-03,  8.8025e-02,  5.5176e-02, -5.8188e-03,
        -1.1201e-01, -3.4873e-01,  2.0603e-04,  1.3139e-02,  3.3899e-02,
        -1.8469e-01, -1.5970e-03, -4.6929e-02,  1.3180e-03,  1.0804e-04,
         7.1643e-05,  2.9450e-03,  1.0574e-03,  2.2648e-06,  9.5943e-03,
         1.7341e-03,  5.1794e-06, -2.7626e-04,  1.4144e-02,  6.8763e-04,
         2.9027e-02, -9.6313e-05,  6.2271e-03, -2.6152e-04,  4.8838e-02,
         1.3392e-04,  3.7421e-02, -1.0279e-03,  1.9578e-03, -2.4686e-06,
        -3.1391e-02, -2.7439e-03,  2.6205e-04,  4.4403e-04,  5.8927e-05,
        -2.0026e-04, -3.3617e-02, -2.0904e-01,  3.7527e-03, -6.1212e-02,
        -7.5932e-05,  3.4279e-04,  1.0103e-01, -7.2149e-02, -6.7910e-03,
         3.1017e-03, -4.9871e-03, -2.4827e-05,  5.7320e-03, -9.8641e-03,
        -1.0900e-02,  2.3263e-03,  3.6481e-03,  4.4674e-03,  1.8615e-02,
        -8.9003e-03,  7.2513e-03, -8.9819e-06,  1.4622e-03,  9.9692e-03,
        -6.1444e-01,  3.5636e-03,  4.6653e-03, -1.3793e-05, -6.2969e-04,
         3.4120e-05,  1.1020e-03, -3.0469e-05,  4.1266e-03, -3.6619e-02,
         3.8302e-01, -3.0779e-03,  1.7754e-04,  4.4800e-02,  3.6007e-04,
         6.4968e-01,  5.2967e-05,  1.0000e+00, -1.5822e-04,  3.0491e-05,
         2.4169e-02, -3.4941e-04, -1.9727e-02,  9.8392e-02, -4.4161e-03,
         1.1326e-03,  7.1525e-02, -1.4324e-05,  3.2677e-05,  5.8709e-03,
        -1.9198e-03,  1.7674e-03,  4.1423e-05, -3.1888e-02, -3.7341e-06,
         6.1054e-04,  4.8628e-04,  7.5724e-03, -4.3960e-02,  8.3225e-03,
         1.3550e-04,  1.1450e-02, -1.8919e-03, -2.6429e-04,  9.9447e-03,
        -1.3199e-02,  8.2843e-04, -3.4749e-02,  9.8738e-04,  1.5413e-03,
        -3.8671e-05, -2.3892e-05,  9.4244e-04,  6.9910e-04,  2.9414e-03,
        -2.7643e-03,  3.2199e-03,  4.3235e-05,  6.0167e-03,  2.1491e-03,
        -2.2352e-02, -1.3359e-02,  1.7401e-07, -2.3843e-01, -7.2115e-05,
        -3.0431e-03, -8.7497e-05, -4.1749e-02,  3.2445e-02, -1.0811e-04,
         1.9581e-02,  8.2049e-05, -1.3406e-03,  3.0483e-04, -2.3033e-02,
         2.2119e-03,  2.9305e-05,  3.3734e-04, -7.1218e-05,  6.1239e-05,
        -3.7863e-04, -4.0345e-05,  1.2659e-01,  7.5645e-04, -4.2602e-02,
         1.4669e-03, -9.1165e-01, -2.5074e-05,  1.2828e-05,  1.6369e-05,
        -9.4926e-03,  6.3717e-02, -3.8056e-05,  3.3859e-06,  7.6375e-02,
        -2.5977e-04,  1.9502e-03,  5.1995e-03,  1.3606e-02, -2.9814e-05,
        -1.2566e-03,  1.0382e-02,  8.9141e-03, -1.0568e-07,  2.4576e-04,
         1.1888e-02, -5.9848e-04, -7.8868e-03,  2.4651e-03, -5.5964e-03,
        -2.7051e-04, -1.6336e-01,  3.1006e-03, -3.7294e-04, -1.3225e-03,
        -4.1570e-02, -2.9464e-02, -1.0904e-03,  1.1279e-02,  2.5964e-03,
        -1.0496e-01, -4.4629e-02,  4.6441e-04, -1.4105e-03,  1.0779e-01,
         2.7615e-04,  4.9523e-02,  5.8190e-05,  1.2694e-04, -7.1184e-04,
        -1.0205e-03, -6.0117e-05,  7.1328e-03, -1.9950e-03,  3.5335e-03,
         1.7623e-05,  2.5985e-04,  1.6081e-03, -3.1294e-04, -1.7198e-06,
        -6.9620e-07,  1.9185e-04,  6.4374e-03, -3.3447e-03, -2.5361e-01,
         1.2631e-02,  9.3823e-03,  8.2233e-03, -4.7936e-02,  2.4008e-01,
        -1.3753e-01,  1.7838e-03, -1.1939e-01,  1.5563e-03, -2.1886e-05,
         2.6196e-03,  1.4795e-02, -3.5906e-05,  8.7130e-04,  3.6218e-04,
         2.7082e-04,  1.6384e-04, -2.2462e-04,  2.3106e-02, -6.3837e-03,
        -2.1288e-01,  6.5514e-04,  1.9160e-01,  2.7915e-03, -2.2252e-01,
         2.7847e-04, -1.4735e-04, -9.4040e-02,  1.4203e-03,  2.4017e-01,
        -1.9699e-01, -4.4020e-04,  8.0017e-02,  1.9920e-02, -3.3929e-04,
         6.6552e-05,  1.6553e-05, -2.8286e-03,  2.2965e-03,  6.0745e-03,
        -5.1032e-01, -2.2038e-05,  8.0501e-03,  3.5752e-03, -1.4031e-04,
         3.3737e-04,  2.2669e-02,  2.8834e-03, -8.0148e-02, -9.3541e-03,
         6.0478e-04, -9.2107e-04,  2.8046e-04,  4.2740e-04,  7.4344e-03,
         6.2248e-04,  5.2942e-03, -6.2590e-02,  3.1001e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0008,  0.0005, -0.0125,  ...,  0.0090,  0.0078,  0.0085],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.0956,  0.0150, -0.0350,  ..., -0.0298, -0.0049,  0.0246],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([-0.0106,  0.0003,  0.0154,  ...,  0.0085,  0.0011,  0.0535],
       device='cuda:0')}
topk_layer inside model after retrieval:  [365, 50, 382, 75, 180, 264, 56, 345, 104, 352, 206, 100, 190, 324, 354, 99, 73, 140, 336, 355, 121, 191, 233, 144, 243, 163, 193, 150, 4, 26, 373, 266, 118, 92, 357, 117, 94, 90, 158, 68, 179, 202, 368, 76, 36, 170, 304, 146, 41, 35, 33, 138, 6, 105, 262, 197, 293, 317, 17]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-1.4532e-01,  8.7761e-04,  5.7996e-05,  1.9021e-03, -3.7310e-01,
        -6.7027e-01,  9.7028e-02, -1.7941e-03, -4.0346e-01,  7.2447e-04,
         1.6538e-01,  1.0592e-01, -7.4874e-04,  3.5001e-03,  3.0900e-01,
        -4.0094e-03, -8.4942e-01,  1.0030e-03, -1.8284e-01, -1.9295e-01,
         4.5407e-02,  2.2242e-01, -1.5329e-02,  2.6259e-04,  3.2590e-02,
         4.7867e-01, -8.1054e-03, -2.1419e-02,  2.1114e-02, -1.4097e-03,
         3.9547e-01,  6.7029e-02, -7.0707e-03, -2.9653e-01,  3.1926e-01,
         2.9254e-01, -2.8872e-01, -1.9610e-02,  2.3829e-01,  6.3319e-03,
         1.2408e-01,  7.7772e-02,  1.1616e-01, -7.2632e-02,  4.4684e-02,
        -2.0923e-01, -5.5327e-03,  1.7412e-02, -8.6930e-03,  1.4219e-01,
        -3.1874e-01,  3.0855e-01,  1.1764e-01,  8.1276e-02, -2.7714e-03,
         3.9494e-01, -4.3484e-02,  1.8097e-01, -1.7396e-01,  1.0000e+00,
        -1.1112e-02, -5.0324e-03, -1.2712e-01,  7.4909e-03, -4.0420e-01,
        -6.3497e-02, -2.3460e-01, -5.4490e-03,  7.7608e-01,  7.3809e-01,
         1.8613e-01,  3.7175e-02, -6.1988e-03, -4.1727e-01, -1.9685e-02,
        -1.4673e-01,  3.0621e-02,  8.1144e-01, -1.4049e-01,  1.8670e-01,
        -1.4354e-01,  4.0347e-01, -3.4343e-01,  1.5191e-01,  1.3946e-02,
        -2.1194e-02, -2.7387e-01, -4.4525e-02, -3.5677e-02, -3.6691e-01,
         4.3799e-03, -7.8093e-02, -9.8441e-02,  5.9217e-01, -3.9111e-02,
         2.0596e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 1.6497e-03,  1.9875e-04,  4.3818e-04,  2.2631e-04,  9.2604e-01,
        -3.6322e-02,  1.4620e-03, -3.1491e-03, -2.0400e-04,  4.8712e-03,
        -7.3878e-05,  3.3166e-03,  2.5829e-03,  1.5562e-03, -1.4293e-04,
         2.2982e-03, -5.5295e-04,  4.8175e-02, -1.3645e-02, -1.6232e-02,
         1.1711e-03,  2.5179e-04,  9.7178e-03,  5.2320e-03,  3.9179e-03,
         9.3389e-03, -5.1052e-03, -3.9680e-03, -6.8696e-03,  1.2207e-03,
         1.0053e-03, -1.7635e-02, -1.7809e-02, -2.2217e-04, -5.8817e-03,
        -1.4746e-04,  7.9396e-02,  1.1548e-02,  8.5405e-05,  1.3117e-04,
         1.7224e-03,  1.2198e-02,  6.7023e-04,  5.8088e-05,  1.4750e-03,
         5.5404e-09, -6.1092e-02,  6.7793e-03, -8.7042e-03, -2.1595e-02,
         5.3426e-02, -1.3242e-05, -4.2702e-04,  1.2886e-04, -6.4118e-03,
         5.9787e-02,  8.0946e-02, -2.3455e-04,  1.0323e-02, -2.4025e-04,
         1.2172e-04, -9.7062e-02,  4.3293e-04, -1.1938e-02, -2.4034e-02,
        -1.6284e-05, -8.0959e-02,  2.6137e-04,  5.5975e-02, -2.1299e-03,
        -6.3298e-02, -2.1935e-03, -4.5828e-02,  3.1943e-01, -5.8868e-04,
         6.9664e-02,  2.8211e-01, -3.6960e-02, -4.1849e-02,  7.2758e-02,
        -2.0004e-03,  1.2947e-04,  3.4534e-04, -5.3514e-05,  2.8851e-02,
         1.8986e-05,  4.3380e-05, -1.6957e-01, -1.8001e-03,  8.1271e-06,
        -1.9054e-03,  1.8689e-04, -1.6949e-03,  5.8045e-05,  8.3057e-03,
        -9.1768e-03,  6.3129e-03,  7.7744e-04, -1.8647e-02,  2.0871e-03,
         2.8462e-01,  2.2765e-04,  1.6052e-04, -1.6849e-02, -9.9119e-03,
         8.0280e-03, -1.0734e-03, -4.7587e-03, -6.5447e-03,  6.2858e-02,
        -1.0420e-02, -4.2226e-05,  2.3345e-05, -2.5564e-02,  4.2997e-02,
        -1.9941e-03, -3.0231e-03,  8.8025e-02,  5.5176e-02, -5.8188e-03,
        -1.1201e-01, -3.4873e-01,  2.0603e-04,  1.3139e-02,  3.3899e-02,
        -1.8469e-01, -1.5970e-03, -4.6929e-02,  1.3180e-03,  1.0804e-04,
         7.1643e-05,  2.9450e-03,  1.0574e-03,  2.2648e-06,  9.5943e-03,
         1.7341e-03,  5.1794e-06, -2.7626e-04,  1.4144e-02,  6.8763e-04,
         2.9027e-02, -9.6313e-05,  6.2271e-03, -2.6152e-04,  4.8838e-02,
         1.3392e-04,  3.7421e-02, -1.0279e-03,  1.9578e-03, -2.4686e-06,
        -3.1391e-02, -2.7439e-03,  2.6205e-04,  4.4403e-04,  5.8927e-05,
        -2.0026e-04, -3.3617e-02, -2.0904e-01,  3.7527e-03, -6.1212e-02,
        -7.5932e-05,  3.4279e-04,  1.0103e-01, -7.2149e-02, -6.7910e-03,
         3.1017e-03, -4.9871e-03, -2.4827e-05,  5.7320e-03, -9.8641e-03,
        -1.0900e-02,  2.3263e-03,  3.6481e-03,  4.4674e-03,  1.8615e-02,
        -8.9003e-03,  7.2513e-03, -8.9819e-06,  1.4622e-03,  9.9692e-03,
        -6.1444e-01,  3.5636e-03,  4.6653e-03, -1.3793e-05, -6.2969e-04,
         3.4120e-05,  1.1020e-03, -3.0469e-05,  4.1266e-03, -3.6619e-02,
         3.8302e-01, -3.0779e-03,  1.7754e-04,  4.4800e-02,  3.6007e-04,
         6.4968e-01,  5.2967e-05,  1.0000e+00, -1.5822e-04,  3.0491e-05,
         2.4169e-02, -3.4941e-04, -1.9727e-02,  9.8392e-02, -4.4161e-03,
         1.1326e-03,  7.1525e-02, -1.4324e-05,  3.2677e-05,  5.8709e-03,
        -1.9198e-03,  1.7674e-03,  4.1423e-05, -3.1888e-02, -3.7341e-06,
         6.1054e-04,  4.8628e-04,  7.5724e-03, -4.3960e-02,  8.3225e-03,
         1.3550e-04,  1.1450e-02, -1.8919e-03, -2.6429e-04,  9.9447e-03,
        -1.3199e-02,  8.2843e-04, -3.4749e-02,  9.8738e-04,  1.5413e-03,
        -3.8671e-05, -2.3892e-05,  9.4244e-04,  6.9910e-04,  2.9414e-03,
        -2.7643e-03,  3.2199e-03,  4.3235e-05,  6.0167e-03,  2.1491e-03,
        -2.2352e-02, -1.3359e-02,  1.7401e-07, -2.3843e-01, -7.2115e-05,
        -3.0431e-03, -8.7497e-05, -4.1749e-02,  3.2445e-02, -1.0811e-04,
         1.9581e-02,  8.2049e-05, -1.3406e-03,  3.0483e-04, -2.3033e-02,
         2.2119e-03,  2.9305e-05,  3.3734e-04, -7.1218e-05,  6.1239e-05,
        -3.7863e-04, -4.0345e-05,  1.2659e-01,  7.5645e-04, -4.2602e-02,
         1.4669e-03, -9.1165e-01, -2.5074e-05,  1.2828e-05,  1.6369e-05,
        -9.4926e-03,  6.3717e-02, -3.8056e-05,  3.3859e-06,  7.6375e-02,
        -2.5977e-04,  1.9502e-03,  5.1995e-03,  1.3606e-02, -2.9814e-05,
        -1.2566e-03,  1.0382e-02,  8.9141e-03, -1.0568e-07,  2.4576e-04,
         1.1888e-02, -5.9848e-04, -7.8868e-03,  2.4651e-03, -5.5964e-03,
        -2.7051e-04, -1.6336e-01,  3.1006e-03, -3.7294e-04, -1.3225e-03,
        -4.1570e-02, -2.9464e-02, -1.0904e-03,  1.1279e-02,  2.5964e-03,
        -1.0496e-01, -4.4629e-02,  4.6441e-04, -1.4105e-03,  1.0779e-01,
         2.7615e-04,  4.9523e-02,  5.8190e-05,  1.2694e-04, -7.1184e-04,
        -1.0205e-03, -6.0117e-05,  7.1328e-03, -1.9950e-03,  3.5335e-03,
         1.7623e-05,  2.5985e-04,  1.6081e-03, -3.1294e-04, -1.7198e-06,
        -6.9620e-07,  1.9185e-04,  6.4374e-03, -3.3447e-03, -2.5361e-01,
         1.2631e-02,  9.3823e-03,  8.2233e-03, -4.7936e-02,  2.4008e-01,
        -1.3753e-01,  1.7838e-03, -1.1939e-01,  1.5563e-03, -2.1886e-05,
         2.6196e-03,  1.4795e-02, -3.5906e-05,  8.7130e-04,  3.6218e-04,
         2.7082e-04,  1.6384e-04, -2.2462e-04,  2.3106e-02, -6.3837e-03,
        -2.1288e-01,  6.5514e-04,  1.9160e-01,  2.7915e-03, -2.2252e-01,
         2.7847e-04, -1.4735e-04, -9.4040e-02,  1.4203e-03,  2.4017e-01,
        -1.9699e-01, -4.4020e-04,  8.0017e-02,  1.9920e-02, -3.3929e-04,
         6.6552e-05,  1.6553e-05, -2.8286e-03,  2.2965e-03,  6.0745e-03,
        -5.1032e-01, -2.2038e-05,  8.0501e-03,  3.5752e-03, -1.4031e-04,
         3.3737e-04,  2.2669e-02,  2.8834e-03, -8.0148e-02, -9.3541e-03,
         6.0478e-04, -9.2107e-04,  2.8046e-04,  4.2740e-04,  7.4344e-03,
         6.2248e-04,  5.2942e-03, -6.2590e-02,  3.1001e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0008,  0.0005, -0.0125,  ...,  0.0090,  0.0078,  0.0085],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.0956,  0.0150, -0.0350,  ..., -0.0298, -0.0049,  0.0246],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([-0.0106,  0.0003,  0.0154,  ...,  0.0085,  0.0011,  0.0535],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.2.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 0.0008,  0.0005, -0.0125,  ...,  0.0090,  0.0078,  0.0085],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-1.4532e-01,  8.7761e-04,  5.7996e-05,  1.9021e-03, -3.7310e-01,
        -6.7027e-01,  9.7028e-02, -1.7941e-03, -4.0346e-01,  7.2447e-04,
         1.6538e-01,  1.0592e-01, -7.4874e-04,  3.5001e-03,  3.0900e-01,
        -4.0094e-03, -8.4942e-01,  1.0030e-03, -1.8284e-01, -1.9295e-01,
         4.5407e-02,  2.2242e-01, -1.5329e-02,  2.6259e-04,  3.2590e-02,
         4.7867e-01, -8.1054e-03, -2.1419e-02,  2.1114e-02, -1.4097e-03,
         3.9547e-01,  6.7029e-02, -7.0707e-03, -2.9653e-01,  3.1926e-01,
         2.9254e-01, -2.8872e-01, -1.9610e-02,  2.3829e-01,  6.3319e-03,
         1.2408e-01,  7.7772e-02,  1.1616e-01, -7.2632e-02,  4.4684e-02,
        -2.0923e-01, -5.5327e-03,  1.7412e-02, -8.6930e-03,  1.4219e-01,
        -3.1874e-01,  3.0855e-01,  1.1764e-01,  8.1276e-02, -2.7714e-03,
         3.9494e-01, -4.3484e-02,  1.8097e-01, -1.7396e-01,  1.0000e+00,
        -1.1112e-02, -5.0324e-03, -1.2712e-01,  7.4909e-03, -4.0420e-01,
        -6.3497e-02, -2.3460e-01, -5.4490e-03,  7.7608e-01,  7.3809e-01,
         1.8613e-01,  3.7175e-02, -6.1988e-03, -4.1727e-01, -1.9685e-02,
        -1.4673e-01,  3.0621e-02,  8.1144e-01, -1.4049e-01,  1.8670e-01,
        -1.4354e-01,  4.0347e-01, -3.4343e-01,  1.5191e-01,  1.3946e-02,
        -2.1194e-02, -2.7387e-01, -4.4525e-02, -3.5677e-02, -3.6691e-01,
         4.3799e-03, -7.8093e-02, -9.8441e-02,  5.9217e-01, -3.9111e-02,
         2.0596e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 1.6497e-03,  1.9875e-04,  4.3818e-04,  2.2631e-04,  9.2604e-01,
        -3.6322e-02,  1.4620e-03, -3.1491e-03, -2.0400e-04,  4.8712e-03,
        -7.3878e-05,  3.3166e-03,  2.5829e-03,  1.5562e-03, -1.4293e-04,
         2.2982e-03, -5.5295e-04,  4.8175e-02, -1.3645e-02, -1.6232e-02,
         1.1711e-03,  2.5179e-04,  9.7178e-03,  5.2320e-03,  3.9179e-03,
         9.3389e-03, -5.1052e-03, -3.9680e-03, -6.8696e-03,  1.2207e-03,
         1.0053e-03, -1.7635e-02, -1.7809e-02, -2.2217e-04, -5.8817e-03,
        -1.4746e-04,  7.9396e-02,  1.1548e-02,  8.5405e-05,  1.3117e-04,
         1.7224e-03,  1.2198e-02,  6.7023e-04,  5.8088e-05,  1.4750e-03,
         5.5404e-09, -6.1092e-02,  6.7793e-03, -8.7042e-03, -2.1595e-02,
         5.3426e-02, -1.3242e-05, -4.2702e-04,  1.2886e-04, -6.4118e-03,
         5.9787e-02,  8.0946e-02, -2.3455e-04,  1.0323e-02, -2.4025e-04,
         1.2172e-04, -9.7062e-02,  4.3293e-04, -1.1938e-02, -2.4034e-02,
        -1.6284e-05, -8.0959e-02,  2.6137e-04,  5.5975e-02, -2.1299e-03,
        -6.3298e-02, -2.1935e-03, -4.5828e-02,  3.1943e-01, -5.8868e-04,
         6.9664e-02,  2.8211e-01, -3.6960e-02, -4.1849e-02,  7.2758e-02,
        -2.0004e-03,  1.2947e-04,  3.4534e-04, -5.3514e-05,  2.8851e-02,
         1.8986e-05,  4.3380e-05, -1.6957e-01, -1.8001e-03,  8.1271e-06,
        -1.9054e-03,  1.8689e-04, -1.6949e-03,  5.8045e-05,  8.3057e-03,
        -9.1768e-03,  6.3129e-03,  7.7744e-04, -1.8647e-02,  2.0871e-03,
         2.8462e-01,  2.2765e-04,  1.6052e-04, -1.6849e-02, -9.9119e-03,
         8.0280e-03, -1.0734e-03, -4.7587e-03, -6.5447e-03,  6.2858e-02,
        -1.0420e-02, -4.2226e-05,  2.3345e-05, -2.5564e-02,  4.2997e-02,
        -1.9941e-03, -3.0231e-03,  8.8025e-02,  5.5176e-02, -5.8188e-03,
        -1.1201e-01, -3.4873e-01,  2.0603e-04,  1.3139e-02,  3.3899e-02,
        -1.8469e-01, -1.5970e-03, -4.6929e-02,  1.3180e-03,  1.0804e-04,
         7.1643e-05,  2.9450e-03,  1.0574e-03,  2.2648e-06,  9.5943e-03,
         1.7341e-03,  5.1794e-06, -2.7626e-04,  1.4144e-02,  6.8763e-04,
         2.9027e-02, -9.6313e-05,  6.2271e-03, -2.6152e-04,  4.8838e-02,
         1.3392e-04,  3.7421e-02, -1.0279e-03,  1.9578e-03, -2.4686e-06,
        -3.1391e-02, -2.7439e-03,  2.6205e-04,  4.4403e-04,  5.8927e-05,
        -2.0026e-04, -3.3617e-02, -2.0904e-01,  3.7527e-03, -6.1212e-02,
        -7.5932e-05,  3.4279e-04,  1.0103e-01, -7.2149e-02, -6.7910e-03,
         3.1017e-03, -4.9871e-03, -2.4827e-05,  5.7320e-03, -9.8641e-03,
        -1.0900e-02,  2.3263e-03,  3.6481e-03,  4.4674e-03,  1.8615e-02,
        -8.9003e-03,  7.2513e-03, -8.9819e-06,  1.4622e-03,  9.9692e-03,
        -6.1444e-01,  3.5636e-03,  4.6653e-03, -1.3793e-05, -6.2969e-04,
         3.4120e-05,  1.1020e-03, -3.0469e-05,  4.1266e-03, -3.6619e-02,
         3.8302e-01, -3.0779e-03,  1.7754e-04,  4.4800e-02,  3.6007e-04,
         6.4968e-01,  5.2967e-05,  1.0000e+00, -1.5822e-04,  3.0491e-05,
         2.4169e-02, -3.4941e-04, -1.9727e-02,  9.8392e-02, -4.4161e-03,
         1.1326e-03,  7.1525e-02, -1.4324e-05,  3.2677e-05,  5.8709e-03,
        -1.9198e-03,  1.7674e-03,  4.1423e-05, -3.1888e-02, -3.7341e-06,
         6.1054e-04,  4.8628e-04,  7.5724e-03, -4.3960e-02,  8.3225e-03,
         1.3550e-04,  1.1450e-02, -1.8919e-03, -2.6429e-04,  9.9447e-03,
        -1.3199e-02,  8.2843e-04, -3.4749e-02,  9.8738e-04,  1.5413e-03,
        -3.8671e-05, -2.3892e-05,  9.4244e-04,  6.9910e-04,  2.9414e-03,
        -2.7643e-03,  3.2199e-03,  4.3235e-05,  6.0167e-03,  2.1491e-03,
        -2.2352e-02, -1.3359e-02,  1.7401e-07, -2.3843e-01, -7.2115e-05,
        -3.0431e-03, -8.7497e-05, -4.1749e-02,  3.2445e-02, -1.0811e-04,
         1.9581e-02,  8.2049e-05, -1.3406e-03,  3.0483e-04, -2.3033e-02,
         2.2119e-03,  2.9305e-05,  3.3734e-04, -7.1218e-05,  6.1239e-05,
        -3.7863e-04, -4.0345e-05,  1.2659e-01,  7.5645e-04, -4.2602e-02,
         1.4669e-03, -9.1165e-01, -2.5074e-05,  1.2828e-05,  1.6369e-05,
        -9.4926e-03,  6.3717e-02, -3.8056e-05,  3.3859e-06,  7.6375e-02,
        -2.5977e-04,  1.9502e-03,  5.1995e-03,  1.3606e-02, -2.9814e-05,
        -1.2566e-03,  1.0382e-02,  8.9141e-03, -1.0568e-07,  2.4576e-04,
         1.1888e-02, -5.9848e-04, -7.8868e-03,  2.4651e-03, -5.5964e-03,
        -2.7051e-04, -1.6336e-01,  3.1006e-03, -3.7294e-04, -1.3225e-03,
        -4.1570e-02, -2.9464e-02, -1.0904e-03,  1.1279e-02,  2.5964e-03,
        -1.0496e-01, -4.4629e-02,  4.6441e-04, -1.4105e-03,  1.0779e-01,
         2.7615e-04,  4.9523e-02,  5.8190e-05,  1.2694e-04, -7.1184e-04,
        -1.0205e-03, -6.0117e-05,  7.1328e-03, -1.9950e-03,  3.5335e-03,
         1.7623e-05,  2.5985e-04,  1.6081e-03, -3.1294e-04, -1.7198e-06,
        -6.9620e-07,  1.9185e-04,  6.4374e-03, -3.3447e-03, -2.5361e-01,
         1.2631e-02,  9.3823e-03,  8.2233e-03, -4.7936e-02,  2.4008e-01,
        -1.3753e-01,  1.7838e-03, -1.1939e-01,  1.5563e-03, -2.1886e-05,
         2.6196e-03,  1.4795e-02, -3.5906e-05,  8.7130e-04,  3.6218e-04,
         2.7082e-04,  1.6384e-04, -2.2462e-04,  2.3106e-02, -6.3837e-03,
        -2.1288e-01,  6.5514e-04,  1.9160e-01,  2.7915e-03, -2.2252e-01,
         2.7847e-04, -1.4735e-04, -9.4040e-02,  1.4203e-03,  2.4017e-01,
        -1.9699e-01, -4.4020e-04,  8.0017e-02,  1.9920e-02, -3.3929e-04,
         6.6552e-05,  1.6553e-05, -2.8286e-03,  2.2965e-03,  6.0745e-03,
        -5.1032e-01, -2.2038e-05,  8.0501e-03,  3.5752e-03, -1.4031e-04,
         3.3737e-04,  2.2669e-02,  2.8834e-03, -8.0148e-02, -9.3541e-03,
         6.0478e-04, -9.2107e-04,  2.8046e-04,  4.2740e-04,  7.4344e-03,
         6.2248e-04,  5.2942e-03, -6.2590e-02,  3.1001e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0008,  0.0005, -0.0125,  ...,  0.0090,  0.0078,  0.0085],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.0956,  0.0150, -0.0350,  ..., -0.0298, -0.0049,  0.0246],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([-0.0106,  0.0003,  0.0154,  ...,  0.0085,  0.0011,  0.0535],
       device='cuda:0')}
topk_layer inside model after retrieval:  [307, 937, 662, 141, 1307, 1006, 869, 683, 1441, 1238, 348, 1086, 1278, 37, 1404, 427, 889, 311, 341, 1405, 796, 312, 1500, 794, 1486, 1370, 490, 304, 449, 835, 1297, 1100, 28, 942, 731, 537, 710, 380, 612, 451, 1159, 735, 1139, 1046, 142, 1385, 998, 176, 41, 295, 958, 346, 313, 1436, 1034, 167, 1360, 906, 529, 441, 214, 1306, 1395, 79, 1450, 544, 1412, 660, 542, 1000, 1372, 700, 405, 613, 61, 14, 1243, 711, 195, 673, 636, 1082, 107, 1317, 635, 590, 551, 997, 209, 207, 1474, 987, 1226, 865, 706, 1172, 421, 733, 70, 1143, 94, 959, 896, 547, 229, 423, 11, 838, 923, 856, 407, 851, 1345, 914, 1359, 622, 259, 1464, 1400, 460, 268, 730, 1532, 1266, 454, 634, 806, 450, 577, 35, 12, 1027, 1449, 1534, 1466, 872, 534, 752, 1489, 1101, 1230, 1134, 199, 434, 1185, 932, 915, 123, 766, 1429, 250, 1033, 155, 335, 1030, 276, 457, 953, 174, 799, 828, 1452, 891, 626, 496, 1149, 831, 456, 510, 194, 68, 810, 761, 981, 1242, 1291, 1294, 1535, 75, 499, 935, 1272, 501, 912, 1347, 921, 353, 859, 640, 1032, 1507, 1171, 373, 100, 287, 281, 1442, 944, 1083, 406, 356, 549, 187, 883, 1265, 919, 619, 109, 783, 870, 760, 927, 868, 904, 644, 1249, 420, 1113, 1455, 1211, 1183, 78, 95, 13, 1167, 261, 839, 817, 1274, 252, 994]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-1.4532e-01,  8.7761e-04,  5.7996e-05,  1.9021e-03, -3.7310e-01,
        -6.7027e-01,  9.7028e-02, -1.7941e-03, -4.0346e-01,  7.2447e-04,
         1.6538e-01,  1.0592e-01, -7.4874e-04,  3.5001e-03,  3.0900e-01,
        -4.0094e-03, -8.4942e-01,  1.0030e-03, -1.8284e-01, -1.9295e-01,
         4.5407e-02,  2.2242e-01, -1.5329e-02,  2.6259e-04,  3.2590e-02,
         4.7867e-01, -8.1054e-03, -2.1419e-02,  2.1114e-02, -1.4097e-03,
         3.9547e-01,  6.7029e-02, -7.0707e-03, -2.9653e-01,  3.1926e-01,
         2.9254e-01, -2.8872e-01, -1.9610e-02,  2.3829e-01,  6.3319e-03,
         1.2408e-01,  7.7772e-02,  1.1616e-01, -7.2632e-02,  4.4684e-02,
        -2.0923e-01, -5.5327e-03,  1.7412e-02, -8.6930e-03,  1.4219e-01,
        -3.1874e-01,  3.0855e-01,  1.1764e-01,  8.1276e-02, -2.7714e-03,
         3.9494e-01, -4.3484e-02,  1.8097e-01, -1.7396e-01,  1.0000e+00,
        -1.1112e-02, -5.0324e-03, -1.2712e-01,  7.4909e-03, -4.0420e-01,
        -6.3497e-02, -2.3460e-01, -5.4490e-03,  7.7608e-01,  7.3809e-01,
         1.8613e-01,  3.7175e-02, -6.1988e-03, -4.1727e-01, -1.9685e-02,
        -1.4673e-01,  3.0621e-02,  8.1144e-01, -1.4049e-01,  1.8670e-01,
        -1.4354e-01,  4.0347e-01, -3.4343e-01,  1.5191e-01,  1.3946e-02,
        -2.1194e-02, -2.7387e-01, -4.4525e-02, -3.5677e-02, -3.6691e-01,
         4.3799e-03, -7.8093e-02, -9.8441e-02,  5.9217e-01, -3.9111e-02,
         2.0596e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 1.6497e-03,  1.9875e-04,  4.3818e-04,  2.2631e-04,  9.2604e-01,
        -3.6322e-02,  1.4620e-03, -3.1491e-03, -2.0400e-04,  4.8712e-03,
        -7.3878e-05,  3.3166e-03,  2.5829e-03,  1.5562e-03, -1.4293e-04,
         2.2982e-03, -5.5295e-04,  4.8175e-02, -1.3645e-02, -1.6232e-02,
         1.1711e-03,  2.5179e-04,  9.7178e-03,  5.2320e-03,  3.9179e-03,
         9.3389e-03, -5.1052e-03, -3.9680e-03, -6.8696e-03,  1.2207e-03,
         1.0053e-03, -1.7635e-02, -1.7809e-02, -2.2217e-04, -5.8817e-03,
        -1.4746e-04,  7.9396e-02,  1.1548e-02,  8.5405e-05,  1.3117e-04,
         1.7224e-03,  1.2198e-02,  6.7023e-04,  5.8088e-05,  1.4750e-03,
         5.5404e-09, -6.1092e-02,  6.7793e-03, -8.7042e-03, -2.1595e-02,
         5.3426e-02, -1.3242e-05, -4.2702e-04,  1.2886e-04, -6.4118e-03,
         5.9787e-02,  8.0946e-02, -2.3455e-04,  1.0323e-02, -2.4025e-04,
         1.2172e-04, -9.7062e-02,  4.3293e-04, -1.1938e-02, -2.4034e-02,
        -1.6284e-05, -8.0959e-02,  2.6137e-04,  5.5975e-02, -2.1299e-03,
        -6.3298e-02, -2.1935e-03, -4.5828e-02,  3.1943e-01, -5.8868e-04,
         6.9664e-02,  2.8211e-01, -3.6960e-02, -4.1849e-02,  7.2758e-02,
        -2.0004e-03,  1.2947e-04,  3.4534e-04, -5.3514e-05,  2.8851e-02,
         1.8986e-05,  4.3380e-05, -1.6957e-01, -1.8001e-03,  8.1271e-06,
        -1.9054e-03,  1.8689e-04, -1.6949e-03,  5.8045e-05,  8.3057e-03,
        -9.1768e-03,  6.3129e-03,  7.7744e-04, -1.8647e-02,  2.0871e-03,
         2.8462e-01,  2.2765e-04,  1.6052e-04, -1.6849e-02, -9.9119e-03,
         8.0280e-03, -1.0734e-03, -4.7587e-03, -6.5447e-03,  6.2858e-02,
        -1.0420e-02, -4.2226e-05,  2.3345e-05, -2.5564e-02,  4.2997e-02,
        -1.9941e-03, -3.0231e-03,  8.8025e-02,  5.5176e-02, -5.8188e-03,
        -1.1201e-01, -3.4873e-01,  2.0603e-04,  1.3139e-02,  3.3899e-02,
        -1.8469e-01, -1.5970e-03, -4.6929e-02,  1.3180e-03,  1.0804e-04,
         7.1643e-05,  2.9450e-03,  1.0574e-03,  2.2648e-06,  9.5943e-03,
         1.7341e-03,  5.1794e-06, -2.7626e-04,  1.4144e-02,  6.8763e-04,
         2.9027e-02, -9.6313e-05,  6.2271e-03, -2.6152e-04,  4.8838e-02,
         1.3392e-04,  3.7421e-02, -1.0279e-03,  1.9578e-03, -2.4686e-06,
        -3.1391e-02, -2.7439e-03,  2.6205e-04,  4.4403e-04,  5.8927e-05,
        -2.0026e-04, -3.3617e-02, -2.0904e-01,  3.7527e-03, -6.1212e-02,
        -7.5932e-05,  3.4279e-04,  1.0103e-01, -7.2149e-02, -6.7910e-03,
         3.1017e-03, -4.9871e-03, -2.4827e-05,  5.7320e-03, -9.8641e-03,
        -1.0900e-02,  2.3263e-03,  3.6481e-03,  4.4674e-03,  1.8615e-02,
        -8.9003e-03,  7.2513e-03, -8.9819e-06,  1.4622e-03,  9.9692e-03,
        -6.1444e-01,  3.5636e-03,  4.6653e-03, -1.3793e-05, -6.2969e-04,
         3.4120e-05,  1.1020e-03, -3.0469e-05,  4.1266e-03, -3.6619e-02,
         3.8302e-01, -3.0779e-03,  1.7754e-04,  4.4800e-02,  3.6007e-04,
         6.4968e-01,  5.2967e-05,  1.0000e+00, -1.5822e-04,  3.0491e-05,
         2.4169e-02, -3.4941e-04, -1.9727e-02,  9.8392e-02, -4.4161e-03,
         1.1326e-03,  7.1525e-02, -1.4324e-05,  3.2677e-05,  5.8709e-03,
        -1.9198e-03,  1.7674e-03,  4.1423e-05, -3.1888e-02, -3.7341e-06,
         6.1054e-04,  4.8628e-04,  7.5724e-03, -4.3960e-02,  8.3225e-03,
         1.3550e-04,  1.1450e-02, -1.8919e-03, -2.6429e-04,  9.9447e-03,
        -1.3199e-02,  8.2843e-04, -3.4749e-02,  9.8738e-04,  1.5413e-03,
        -3.8671e-05, -2.3892e-05,  9.4244e-04,  6.9910e-04,  2.9414e-03,
        -2.7643e-03,  3.2199e-03,  4.3235e-05,  6.0167e-03,  2.1491e-03,
        -2.2352e-02, -1.3359e-02,  1.7401e-07, -2.3843e-01, -7.2115e-05,
        -3.0431e-03, -8.7497e-05, -4.1749e-02,  3.2445e-02, -1.0811e-04,
         1.9581e-02,  8.2049e-05, -1.3406e-03,  3.0483e-04, -2.3033e-02,
         2.2119e-03,  2.9305e-05,  3.3734e-04, -7.1218e-05,  6.1239e-05,
        -3.7863e-04, -4.0345e-05,  1.2659e-01,  7.5645e-04, -4.2602e-02,
         1.4669e-03, -9.1165e-01, -2.5074e-05,  1.2828e-05,  1.6369e-05,
        -9.4926e-03,  6.3717e-02, -3.8056e-05,  3.3859e-06,  7.6375e-02,
        -2.5977e-04,  1.9502e-03,  5.1995e-03,  1.3606e-02, -2.9814e-05,
        -1.2566e-03,  1.0382e-02,  8.9141e-03, -1.0568e-07,  2.4576e-04,
         1.1888e-02, -5.9848e-04, -7.8868e-03,  2.4651e-03, -5.5964e-03,
        -2.7051e-04, -1.6336e-01,  3.1006e-03, -3.7294e-04, -1.3225e-03,
        -4.1570e-02, -2.9464e-02, -1.0904e-03,  1.1279e-02,  2.5964e-03,
        -1.0496e-01, -4.4629e-02,  4.6441e-04, -1.4105e-03,  1.0779e-01,
         2.7615e-04,  4.9523e-02,  5.8190e-05,  1.2694e-04, -7.1184e-04,
        -1.0205e-03, -6.0117e-05,  7.1328e-03, -1.9950e-03,  3.5335e-03,
         1.7623e-05,  2.5985e-04,  1.6081e-03, -3.1294e-04, -1.7198e-06,
        -6.9620e-07,  1.9185e-04,  6.4374e-03, -3.3447e-03, -2.5361e-01,
         1.2631e-02,  9.3823e-03,  8.2233e-03, -4.7936e-02,  2.4008e-01,
        -1.3753e-01,  1.7838e-03, -1.1939e-01,  1.5563e-03, -2.1886e-05,
         2.6196e-03,  1.4795e-02, -3.5906e-05,  8.7130e-04,  3.6218e-04,
         2.7082e-04,  1.6384e-04, -2.2462e-04,  2.3106e-02, -6.3837e-03,
        -2.1288e-01,  6.5514e-04,  1.9160e-01,  2.7915e-03, -2.2252e-01,
         2.7847e-04, -1.4735e-04, -9.4040e-02,  1.4203e-03,  2.4017e-01,
        -1.9699e-01, -4.4020e-04,  8.0017e-02,  1.9920e-02, -3.3929e-04,
         6.6552e-05,  1.6553e-05, -2.8286e-03,  2.2965e-03,  6.0745e-03,
        -5.1032e-01, -2.2038e-05,  8.0501e-03,  3.5752e-03, -1.4031e-04,
         3.3737e-04,  2.2669e-02,  2.8834e-03, -8.0148e-02, -9.3541e-03,
         6.0478e-04, -9.2107e-04,  2.8046e-04,  4.2740e-04,  7.4344e-03,
         6.2248e-04,  5.2942e-03, -6.2590e-02,  3.1001e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0008,  0.0005, -0.0125,  ...,  0.0090,  0.0078,  0.0085],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.0956,  0.0150, -0.0350,  ..., -0.0298, -0.0049,  0.0246],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([-0.0106,  0.0003,  0.0154,  ...,  0.0085,  0.0011,  0.0535],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.3.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([-0.0956,  0.0150, -0.0350,  ..., -0.0298, -0.0049,  0.0246],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-1.4532e-01,  8.7761e-04,  5.7996e-05,  1.9021e-03, -3.7310e-01,
        -6.7027e-01,  9.7028e-02, -1.7941e-03, -4.0346e-01,  7.2447e-04,
         1.6538e-01,  1.0592e-01, -7.4874e-04,  3.5001e-03,  3.0900e-01,
        -4.0094e-03, -8.4942e-01,  1.0030e-03, -1.8284e-01, -1.9295e-01,
         4.5407e-02,  2.2242e-01, -1.5329e-02,  2.6259e-04,  3.2590e-02,
         4.7867e-01, -8.1054e-03, -2.1419e-02,  2.1114e-02, -1.4097e-03,
         3.9547e-01,  6.7029e-02, -7.0707e-03, -2.9653e-01,  3.1926e-01,
         2.9254e-01, -2.8872e-01, -1.9610e-02,  2.3829e-01,  6.3319e-03,
         1.2408e-01,  7.7772e-02,  1.1616e-01, -7.2632e-02,  4.4684e-02,
        -2.0923e-01, -5.5327e-03,  1.7412e-02, -8.6930e-03,  1.4219e-01,
        -3.1874e-01,  3.0855e-01,  1.1764e-01,  8.1276e-02, -2.7714e-03,
         3.9494e-01, -4.3484e-02,  1.8097e-01, -1.7396e-01,  1.0000e+00,
        -1.1112e-02, -5.0324e-03, -1.2712e-01,  7.4909e-03, -4.0420e-01,
        -6.3497e-02, -2.3460e-01, -5.4490e-03,  7.7608e-01,  7.3809e-01,
         1.8613e-01,  3.7175e-02, -6.1988e-03, -4.1727e-01, -1.9685e-02,
        -1.4673e-01,  3.0621e-02,  8.1144e-01, -1.4049e-01,  1.8670e-01,
        -1.4354e-01,  4.0347e-01, -3.4343e-01,  1.5191e-01,  1.3946e-02,
        -2.1194e-02, -2.7387e-01, -4.4525e-02, -3.5677e-02, -3.6691e-01,
         4.3799e-03, -7.8093e-02, -9.8441e-02,  5.9217e-01, -3.9111e-02,
         2.0596e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 1.6497e-03,  1.9875e-04,  4.3818e-04,  2.2631e-04,  9.2604e-01,
        -3.6322e-02,  1.4620e-03, -3.1491e-03, -2.0400e-04,  4.8712e-03,
        -7.3878e-05,  3.3166e-03,  2.5829e-03,  1.5562e-03, -1.4293e-04,
         2.2982e-03, -5.5295e-04,  4.8175e-02, -1.3645e-02, -1.6232e-02,
         1.1711e-03,  2.5179e-04,  9.7178e-03,  5.2320e-03,  3.9179e-03,
         9.3389e-03, -5.1052e-03, -3.9680e-03, -6.8696e-03,  1.2207e-03,
         1.0053e-03, -1.7635e-02, -1.7809e-02, -2.2217e-04, -5.8817e-03,
        -1.4746e-04,  7.9396e-02,  1.1548e-02,  8.5405e-05,  1.3117e-04,
         1.7224e-03,  1.2198e-02,  6.7023e-04,  5.8088e-05,  1.4750e-03,
         5.5404e-09, -6.1092e-02,  6.7793e-03, -8.7042e-03, -2.1595e-02,
         5.3426e-02, -1.3242e-05, -4.2702e-04,  1.2886e-04, -6.4118e-03,
         5.9787e-02,  8.0946e-02, -2.3455e-04,  1.0323e-02, -2.4025e-04,
         1.2172e-04, -9.7062e-02,  4.3293e-04, -1.1938e-02, -2.4034e-02,
        -1.6284e-05, -8.0959e-02,  2.6137e-04,  5.5975e-02, -2.1299e-03,
        -6.3298e-02, -2.1935e-03, -4.5828e-02,  3.1943e-01, -5.8868e-04,
         6.9664e-02,  2.8211e-01, -3.6960e-02, -4.1849e-02,  7.2758e-02,
        -2.0004e-03,  1.2947e-04,  3.4534e-04, -5.3514e-05,  2.8851e-02,
         1.8986e-05,  4.3380e-05, -1.6957e-01, -1.8001e-03,  8.1271e-06,
        -1.9054e-03,  1.8689e-04, -1.6949e-03,  5.8045e-05,  8.3057e-03,
        -9.1768e-03,  6.3129e-03,  7.7744e-04, -1.8647e-02,  2.0871e-03,
         2.8462e-01,  2.2765e-04,  1.6052e-04, -1.6849e-02, -9.9119e-03,
         8.0280e-03, -1.0734e-03, -4.7587e-03, -6.5447e-03,  6.2858e-02,
        -1.0420e-02, -4.2226e-05,  2.3345e-05, -2.5564e-02,  4.2997e-02,
        -1.9941e-03, -3.0231e-03,  8.8025e-02,  5.5176e-02, -5.8188e-03,
        -1.1201e-01, -3.4873e-01,  2.0603e-04,  1.3139e-02,  3.3899e-02,
        -1.8469e-01, -1.5970e-03, -4.6929e-02,  1.3180e-03,  1.0804e-04,
         7.1643e-05,  2.9450e-03,  1.0574e-03,  2.2648e-06,  9.5943e-03,
         1.7341e-03,  5.1794e-06, -2.7626e-04,  1.4144e-02,  6.8763e-04,
         2.9027e-02, -9.6313e-05,  6.2271e-03, -2.6152e-04,  4.8838e-02,
         1.3392e-04,  3.7421e-02, -1.0279e-03,  1.9578e-03, -2.4686e-06,
        -3.1391e-02, -2.7439e-03,  2.6205e-04,  4.4403e-04,  5.8927e-05,
        -2.0026e-04, -3.3617e-02, -2.0904e-01,  3.7527e-03, -6.1212e-02,
        -7.5932e-05,  3.4279e-04,  1.0103e-01, -7.2149e-02, -6.7910e-03,
         3.1017e-03, -4.9871e-03, -2.4827e-05,  5.7320e-03, -9.8641e-03,
        -1.0900e-02,  2.3263e-03,  3.6481e-03,  4.4674e-03,  1.8615e-02,
        -8.9003e-03,  7.2513e-03, -8.9819e-06,  1.4622e-03,  9.9692e-03,
        -6.1444e-01,  3.5636e-03,  4.6653e-03, -1.3793e-05, -6.2969e-04,
         3.4120e-05,  1.1020e-03, -3.0469e-05,  4.1266e-03, -3.6619e-02,
         3.8302e-01, -3.0779e-03,  1.7754e-04,  4.4800e-02,  3.6007e-04,
         6.4968e-01,  5.2967e-05,  1.0000e+00, -1.5822e-04,  3.0491e-05,
         2.4169e-02, -3.4941e-04, -1.9727e-02,  9.8392e-02, -4.4161e-03,
         1.1326e-03,  7.1525e-02, -1.4324e-05,  3.2677e-05,  5.8709e-03,
        -1.9198e-03,  1.7674e-03,  4.1423e-05, -3.1888e-02, -3.7341e-06,
         6.1054e-04,  4.8628e-04,  7.5724e-03, -4.3960e-02,  8.3225e-03,
         1.3550e-04,  1.1450e-02, -1.8919e-03, -2.6429e-04,  9.9447e-03,
        -1.3199e-02,  8.2843e-04, -3.4749e-02,  9.8738e-04,  1.5413e-03,
        -3.8671e-05, -2.3892e-05,  9.4244e-04,  6.9910e-04,  2.9414e-03,
        -2.7643e-03,  3.2199e-03,  4.3235e-05,  6.0167e-03,  2.1491e-03,
        -2.2352e-02, -1.3359e-02,  1.7401e-07, -2.3843e-01, -7.2115e-05,
        -3.0431e-03, -8.7497e-05, -4.1749e-02,  3.2445e-02, -1.0811e-04,
         1.9581e-02,  8.2049e-05, -1.3406e-03,  3.0483e-04, -2.3033e-02,
         2.2119e-03,  2.9305e-05,  3.3734e-04, -7.1218e-05,  6.1239e-05,
        -3.7863e-04, -4.0345e-05,  1.2659e-01,  7.5645e-04, -4.2602e-02,
         1.4669e-03, -9.1165e-01, -2.5074e-05,  1.2828e-05,  1.6369e-05,
        -9.4926e-03,  6.3717e-02, -3.8056e-05,  3.3859e-06,  7.6375e-02,
        -2.5977e-04,  1.9502e-03,  5.1995e-03,  1.3606e-02, -2.9814e-05,
        -1.2566e-03,  1.0382e-02,  8.9141e-03, -1.0568e-07,  2.4576e-04,
         1.1888e-02, -5.9848e-04, -7.8868e-03,  2.4651e-03, -5.5964e-03,
        -2.7051e-04, -1.6336e-01,  3.1006e-03, -3.7294e-04, -1.3225e-03,
        -4.1570e-02, -2.9464e-02, -1.0904e-03,  1.1279e-02,  2.5964e-03,
        -1.0496e-01, -4.4629e-02,  4.6441e-04, -1.4105e-03,  1.0779e-01,
         2.7615e-04,  4.9523e-02,  5.8190e-05,  1.2694e-04, -7.1184e-04,
        -1.0205e-03, -6.0117e-05,  7.1328e-03, -1.9950e-03,  3.5335e-03,
         1.7623e-05,  2.5985e-04,  1.6081e-03, -3.1294e-04, -1.7198e-06,
        -6.9620e-07,  1.9185e-04,  6.4374e-03, -3.3447e-03, -2.5361e-01,
         1.2631e-02,  9.3823e-03,  8.2233e-03, -4.7936e-02,  2.4008e-01,
        -1.3753e-01,  1.7838e-03, -1.1939e-01,  1.5563e-03, -2.1886e-05,
         2.6196e-03,  1.4795e-02, -3.5906e-05,  8.7130e-04,  3.6218e-04,
         2.7082e-04,  1.6384e-04, -2.2462e-04,  2.3106e-02, -6.3837e-03,
        -2.1288e-01,  6.5514e-04,  1.9160e-01,  2.7915e-03, -2.2252e-01,
         2.7847e-04, -1.4735e-04, -9.4040e-02,  1.4203e-03,  2.4017e-01,
        -1.9699e-01, -4.4020e-04,  8.0017e-02,  1.9920e-02, -3.3929e-04,
         6.6552e-05,  1.6553e-05, -2.8286e-03,  2.2965e-03,  6.0745e-03,
        -5.1032e-01, -2.2038e-05,  8.0501e-03,  3.5752e-03, -1.4031e-04,
         3.3737e-04,  2.2669e-02,  2.8834e-03, -8.0148e-02, -9.3541e-03,
         6.0478e-04, -9.2107e-04,  2.8046e-04,  4.2740e-04,  7.4344e-03,
         6.2248e-04,  5.2942e-03, -6.2590e-02,  3.1001e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0008,  0.0005, -0.0125,  ...,  0.0090,  0.0078,  0.0085],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.0956,  0.0150, -0.0350,  ..., -0.0298, -0.0049,  0.0246],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([-0.0106,  0.0003,  0.0154,  ...,  0.0085,  0.0011,  0.0535],
       device='cuda:0')}
topk_layer inside model after retrieval:  [1422, 43, 1677, 114, 1319, 2000, 471, 2051, 165, 909, 1874, 1660, 934, 1549, 1608, 2302, 2494, 1312, 2352, 1548, 589, 2069, 1083, 499, 1520, 1988, 1873, 1187, 1110, 195, 2438, 1168, 167, 2138, 1090, 882, 512, 505, 398, 1938, 1827, 648, 1127, 1776, 468, 423, 1331, 1109, 2448, 784, 1495, 1554, 2081, 907, 1558, 2533, 2281, 781, 1616, 694, 278, 1063, 980, 2247, 249, 1245, 305, 1857, 867, 443, 2556, 1709, 359, 178, 1164, 419, 841, 2095, 1049, 95, 28, 137, 2181, 1403, 1359, 377, 231, 136, 2001, 1447, 1744, 2315, 1443, 2067, 2101, 1723, 1990, 210, 1866, 1926, 1303, 1887, 2515, 646, 356, 2391, 129, 667, 2037, 1038, 2049, 2152, 1783, 2460, 103, 1641, 878, 757, 272, 1073, 2221, 1417, 565, 2007, 1088, 192, 1506, 2085, 1930, 1435, 2170, 251, 2342, 327, 452, 2241, 1045, 1789, 2244, 2185, 1468, 148, 932, 147, 2008, 2248, 303, 1774, 2500, 1651, 507, 923, 339, 792, 1522, 1879, 2300, 2374, 1280, 2154, 334, 564, 2334, 2151, 1177, 68, 1854, 2435, 1865, 1955, 2131, 533, 611, 446, 1689, 939, 2317, 540, 689, 922, 1843, 450, 1036, 261, 1798, 1260, 2278, 1801, 1497, 372, 1452, 1324, 1614, 1612, 347, 529, 621, 1446, 2471, 2146, 2249, 2456, 892, 514, 555, 2290, 2429, 1043, 1720, 633, 1363, 894, 634, 2021, 477, 1591, 2106, 2193, 1232, 739, 1269, 185, 674, 2102, 2511, 1758, 828, 1845, 1824, 2547, 1105, 780, 750, 350, 1395, 21, 526, 2549, 233, 2479, 1188, 33, 297, 2100, 1205, 1211, 1346, 709, 881, 54, 861, 1029, 1283, 819, 534, 299, 1670, 1698, 652, 245, 46, 2119, 1982, 2378, 556, 1183, 1242, 1527, 940, 1054, 2279, 497, 2497, 196, 1344, 949, 2018, 2414, 1009, 2215, 1394, 1958, 1226, 2475, 42, 989, 1872, 1089, 863, 1685, 2309, 454, 2242, 1256, 2243, 2161, 1425, 361, 1559, 1567, 2112, 187, 697, 1112, 1037, 1784, 1898, 1378, 1348, 193, 241, 627, 2401, 2145, 481, 2283, 1333, 1894, 1146, 2125, 1326, 1411, 1057, 1924, 2338, 1438, 679, 653, 518, 436, 1998, 1968, 994, 1252, 1978, 2030, 465, 328, 494, 1599, 2450, 660, 664, 197, 1578, 1735, 506, 2465, 2276, 225, 1141, 202, 2316, 106, 389, 80, 1584, 1085, 2331, 2072, 1362, 378, 1208, 2427, 1385, 1755, 1684, 2090, 2261, 2122, 1066, 544, 1266, 1349, 1664, 2183, 2293, 1131, 945, 2282, 122, 982, 1413, 1258, 1995]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-1.4532e-01,  8.7761e-04,  5.7996e-05,  1.9021e-03, -3.7310e-01,
        -6.7027e-01,  9.7028e-02, -1.7941e-03, -4.0346e-01,  7.2447e-04,
         1.6538e-01,  1.0592e-01, -7.4874e-04,  3.5001e-03,  3.0900e-01,
        -4.0094e-03, -8.4942e-01,  1.0030e-03, -1.8284e-01, -1.9295e-01,
         4.5407e-02,  2.2242e-01, -1.5329e-02,  2.6259e-04,  3.2590e-02,
         4.7867e-01, -8.1054e-03, -2.1419e-02,  2.1114e-02, -1.4097e-03,
         3.9547e-01,  6.7029e-02, -7.0707e-03, -2.9653e-01,  3.1926e-01,
         2.9254e-01, -2.8872e-01, -1.9610e-02,  2.3829e-01,  6.3319e-03,
         1.2408e-01,  7.7772e-02,  1.1616e-01, -7.2632e-02,  4.4684e-02,
        -2.0923e-01, -5.5327e-03,  1.7412e-02, -8.6930e-03,  1.4219e-01,
        -3.1874e-01,  3.0855e-01,  1.1764e-01,  8.1276e-02, -2.7714e-03,
         3.9494e-01, -4.3484e-02,  1.8097e-01, -1.7396e-01,  1.0000e+00,
        -1.1112e-02, -5.0324e-03, -1.2712e-01,  7.4909e-03, -4.0420e-01,
        -6.3497e-02, -2.3460e-01, -5.4490e-03,  7.7608e-01,  7.3809e-01,
         1.8613e-01,  3.7175e-02, -6.1988e-03, -4.1727e-01, -1.9685e-02,
        -1.4673e-01,  3.0621e-02,  8.1144e-01, -1.4049e-01,  1.8670e-01,
        -1.4354e-01,  4.0347e-01, -3.4343e-01,  1.5191e-01,  1.3946e-02,
        -2.1194e-02, -2.7387e-01, -4.4525e-02, -3.5677e-02, -3.6691e-01,
         4.3799e-03, -7.8093e-02, -9.8441e-02,  5.9217e-01, -3.9111e-02,
         2.0596e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 1.6497e-03,  1.9875e-04,  4.3818e-04,  2.2631e-04,  9.2604e-01,
        -3.6322e-02,  1.4620e-03, -3.1491e-03, -2.0400e-04,  4.8712e-03,
        -7.3878e-05,  3.3166e-03,  2.5829e-03,  1.5562e-03, -1.4293e-04,
         2.2982e-03, -5.5295e-04,  4.8175e-02, -1.3645e-02, -1.6232e-02,
         1.1711e-03,  2.5179e-04,  9.7178e-03,  5.2320e-03,  3.9179e-03,
         9.3389e-03, -5.1052e-03, -3.9680e-03, -6.8696e-03,  1.2207e-03,
         1.0053e-03, -1.7635e-02, -1.7809e-02, -2.2217e-04, -5.8817e-03,
        -1.4746e-04,  7.9396e-02,  1.1548e-02,  8.5405e-05,  1.3117e-04,
         1.7224e-03,  1.2198e-02,  6.7023e-04,  5.8088e-05,  1.4750e-03,
         5.5404e-09, -6.1092e-02,  6.7793e-03, -8.7042e-03, -2.1595e-02,
         5.3426e-02, -1.3242e-05, -4.2702e-04,  1.2886e-04, -6.4118e-03,
         5.9787e-02,  8.0946e-02, -2.3455e-04,  1.0323e-02, -2.4025e-04,
         1.2172e-04, -9.7062e-02,  4.3293e-04, -1.1938e-02, -2.4034e-02,
        -1.6284e-05, -8.0959e-02,  2.6137e-04,  5.5975e-02, -2.1299e-03,
        -6.3298e-02, -2.1935e-03, -4.5828e-02,  3.1943e-01, -5.8868e-04,
         6.9664e-02,  2.8211e-01, -3.6960e-02, -4.1849e-02,  7.2758e-02,
        -2.0004e-03,  1.2947e-04,  3.4534e-04, -5.3514e-05,  2.8851e-02,
         1.8986e-05,  4.3380e-05, -1.6957e-01, -1.8001e-03,  8.1271e-06,
        -1.9054e-03,  1.8689e-04, -1.6949e-03,  5.8045e-05,  8.3057e-03,
        -9.1768e-03,  6.3129e-03,  7.7744e-04, -1.8647e-02,  2.0871e-03,
         2.8462e-01,  2.2765e-04,  1.6052e-04, -1.6849e-02, -9.9119e-03,
         8.0280e-03, -1.0734e-03, -4.7587e-03, -6.5447e-03,  6.2858e-02,
        -1.0420e-02, -4.2226e-05,  2.3345e-05, -2.5564e-02,  4.2997e-02,
        -1.9941e-03, -3.0231e-03,  8.8025e-02,  5.5176e-02, -5.8188e-03,
        -1.1201e-01, -3.4873e-01,  2.0603e-04,  1.3139e-02,  3.3899e-02,
        -1.8469e-01, -1.5970e-03, -4.6929e-02,  1.3180e-03,  1.0804e-04,
         7.1643e-05,  2.9450e-03,  1.0574e-03,  2.2648e-06,  9.5943e-03,
         1.7341e-03,  5.1794e-06, -2.7626e-04,  1.4144e-02,  6.8763e-04,
         2.9027e-02, -9.6313e-05,  6.2271e-03, -2.6152e-04,  4.8838e-02,
         1.3392e-04,  3.7421e-02, -1.0279e-03,  1.9578e-03, -2.4686e-06,
        -3.1391e-02, -2.7439e-03,  2.6205e-04,  4.4403e-04,  5.8927e-05,
        -2.0026e-04, -3.3617e-02, -2.0904e-01,  3.7527e-03, -6.1212e-02,
        -7.5932e-05,  3.4279e-04,  1.0103e-01, -7.2149e-02, -6.7910e-03,
         3.1017e-03, -4.9871e-03, -2.4827e-05,  5.7320e-03, -9.8641e-03,
        -1.0900e-02,  2.3263e-03,  3.6481e-03,  4.4674e-03,  1.8615e-02,
        -8.9003e-03,  7.2513e-03, -8.9819e-06,  1.4622e-03,  9.9692e-03,
        -6.1444e-01,  3.5636e-03,  4.6653e-03, -1.3793e-05, -6.2969e-04,
         3.4120e-05,  1.1020e-03, -3.0469e-05,  4.1266e-03, -3.6619e-02,
         3.8302e-01, -3.0779e-03,  1.7754e-04,  4.4800e-02,  3.6007e-04,
         6.4968e-01,  5.2967e-05,  1.0000e+00, -1.5822e-04,  3.0491e-05,
         2.4169e-02, -3.4941e-04, -1.9727e-02,  9.8392e-02, -4.4161e-03,
         1.1326e-03,  7.1525e-02, -1.4324e-05,  3.2677e-05,  5.8709e-03,
        -1.9198e-03,  1.7674e-03,  4.1423e-05, -3.1888e-02, -3.7341e-06,
         6.1054e-04,  4.8628e-04,  7.5724e-03, -4.3960e-02,  8.3225e-03,
         1.3550e-04,  1.1450e-02, -1.8919e-03, -2.6429e-04,  9.9447e-03,
        -1.3199e-02,  8.2843e-04, -3.4749e-02,  9.8738e-04,  1.5413e-03,
        -3.8671e-05, -2.3892e-05,  9.4244e-04,  6.9910e-04,  2.9414e-03,
        -2.7643e-03,  3.2199e-03,  4.3235e-05,  6.0167e-03,  2.1491e-03,
        -2.2352e-02, -1.3359e-02,  1.7401e-07, -2.3843e-01, -7.2115e-05,
        -3.0431e-03, -8.7497e-05, -4.1749e-02,  3.2445e-02, -1.0811e-04,
         1.9581e-02,  8.2049e-05, -1.3406e-03,  3.0483e-04, -2.3033e-02,
         2.2119e-03,  2.9305e-05,  3.3734e-04, -7.1218e-05,  6.1239e-05,
        -3.7863e-04, -4.0345e-05,  1.2659e-01,  7.5645e-04, -4.2602e-02,
         1.4669e-03, -9.1165e-01, -2.5074e-05,  1.2828e-05,  1.6369e-05,
        -9.4926e-03,  6.3717e-02, -3.8056e-05,  3.3859e-06,  7.6375e-02,
        -2.5977e-04,  1.9502e-03,  5.1995e-03,  1.3606e-02, -2.9814e-05,
        -1.2566e-03,  1.0382e-02,  8.9141e-03, -1.0568e-07,  2.4576e-04,
         1.1888e-02, -5.9848e-04, -7.8868e-03,  2.4651e-03, -5.5964e-03,
        -2.7051e-04, -1.6336e-01,  3.1006e-03, -3.7294e-04, -1.3225e-03,
        -4.1570e-02, -2.9464e-02, -1.0904e-03,  1.1279e-02,  2.5964e-03,
        -1.0496e-01, -4.4629e-02,  4.6441e-04, -1.4105e-03,  1.0779e-01,
         2.7615e-04,  4.9523e-02,  5.8190e-05,  1.2694e-04, -7.1184e-04,
        -1.0205e-03, -6.0117e-05,  7.1328e-03, -1.9950e-03,  3.5335e-03,
         1.7623e-05,  2.5985e-04,  1.6081e-03, -3.1294e-04, -1.7198e-06,
        -6.9620e-07,  1.9185e-04,  6.4374e-03, -3.3447e-03, -2.5361e-01,
         1.2631e-02,  9.3823e-03,  8.2233e-03, -4.7936e-02,  2.4008e-01,
        -1.3753e-01,  1.7838e-03, -1.1939e-01,  1.5563e-03, -2.1886e-05,
         2.6196e-03,  1.4795e-02, -3.5906e-05,  8.7130e-04,  3.6218e-04,
         2.7082e-04,  1.6384e-04, -2.2462e-04,  2.3106e-02, -6.3837e-03,
        -2.1288e-01,  6.5514e-04,  1.9160e-01,  2.7915e-03, -2.2252e-01,
         2.7847e-04, -1.4735e-04, -9.4040e-02,  1.4203e-03,  2.4017e-01,
        -1.9699e-01, -4.4020e-04,  8.0017e-02,  1.9920e-02, -3.3929e-04,
         6.6552e-05,  1.6553e-05, -2.8286e-03,  2.2965e-03,  6.0745e-03,
        -5.1032e-01, -2.2038e-05,  8.0501e-03,  3.5752e-03, -1.4031e-04,
         3.3737e-04,  2.2669e-02,  2.8834e-03, -8.0148e-02, -9.3541e-03,
         6.0478e-04, -9.2107e-04,  2.8046e-04,  4.2740e-04,  7.4344e-03,
         6.2248e-04,  5.2942e-03, -6.2590e-02,  3.1001e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0008,  0.0005, -0.0125,  ...,  0.0090,  0.0078,  0.0085],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.0956,  0.0150, -0.0350,  ..., -0.0298, -0.0049,  0.0246],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([-0.0106,  0.0003,  0.0154,  ...,  0.0085,  0.0011,  0.0535],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.4.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([-0.0106,  0.0003,  0.0154,  ...,  0.0085,  0.0011,  0.0535],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-1.4532e-01,  8.7761e-04,  5.7996e-05,  1.9021e-03, -3.7310e-01,
        -6.7027e-01,  9.7028e-02, -1.7941e-03, -4.0346e-01,  7.2447e-04,
         1.6538e-01,  1.0592e-01, -7.4874e-04,  3.5001e-03,  3.0900e-01,
        -4.0094e-03, -8.4942e-01,  1.0030e-03, -1.8284e-01, -1.9295e-01,
         4.5407e-02,  2.2242e-01, -1.5329e-02,  2.6259e-04,  3.2590e-02,
         4.7867e-01, -8.1054e-03, -2.1419e-02,  2.1114e-02, -1.4097e-03,
         3.9547e-01,  6.7029e-02, -7.0707e-03, -2.9653e-01,  3.1926e-01,
         2.9254e-01, -2.8872e-01, -1.9610e-02,  2.3829e-01,  6.3319e-03,
         1.2408e-01,  7.7772e-02,  1.1616e-01, -7.2632e-02,  4.4684e-02,
        -2.0923e-01, -5.5327e-03,  1.7412e-02, -8.6930e-03,  1.4219e-01,
        -3.1874e-01,  3.0855e-01,  1.1764e-01,  8.1276e-02, -2.7714e-03,
         3.9494e-01, -4.3484e-02,  1.8097e-01, -1.7396e-01,  1.0000e+00,
        -1.1112e-02, -5.0324e-03, -1.2712e-01,  7.4909e-03, -4.0420e-01,
        -6.3497e-02, -2.3460e-01, -5.4490e-03,  7.7608e-01,  7.3809e-01,
         1.8613e-01,  3.7175e-02, -6.1988e-03, -4.1727e-01, -1.9685e-02,
        -1.4673e-01,  3.0621e-02,  8.1144e-01, -1.4049e-01,  1.8670e-01,
        -1.4354e-01,  4.0347e-01, -3.4343e-01,  1.5191e-01,  1.3946e-02,
        -2.1194e-02, -2.7387e-01, -4.4525e-02, -3.5677e-02, -3.6691e-01,
         4.3799e-03, -7.8093e-02, -9.8441e-02,  5.9217e-01, -3.9111e-02,
         2.0596e-02], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 1.6497e-03,  1.9875e-04,  4.3818e-04,  2.2631e-04,  9.2604e-01,
        -3.6322e-02,  1.4620e-03, -3.1491e-03, -2.0400e-04,  4.8712e-03,
        -7.3878e-05,  3.3166e-03,  2.5829e-03,  1.5562e-03, -1.4293e-04,
         2.2982e-03, -5.5295e-04,  4.8175e-02, -1.3645e-02, -1.6232e-02,
         1.1711e-03,  2.5179e-04,  9.7178e-03,  5.2320e-03,  3.9179e-03,
         9.3389e-03, -5.1052e-03, -3.9680e-03, -6.8696e-03,  1.2207e-03,
         1.0053e-03, -1.7635e-02, -1.7809e-02, -2.2217e-04, -5.8817e-03,
        -1.4746e-04,  7.9396e-02,  1.1548e-02,  8.5405e-05,  1.3117e-04,
         1.7224e-03,  1.2198e-02,  6.7023e-04,  5.8088e-05,  1.4750e-03,
         5.5404e-09, -6.1092e-02,  6.7793e-03, -8.7042e-03, -2.1595e-02,
         5.3426e-02, -1.3242e-05, -4.2702e-04,  1.2886e-04, -6.4118e-03,
         5.9787e-02,  8.0946e-02, -2.3455e-04,  1.0323e-02, -2.4025e-04,
         1.2172e-04, -9.7062e-02,  4.3293e-04, -1.1938e-02, -2.4034e-02,
        -1.6284e-05, -8.0959e-02,  2.6137e-04,  5.5975e-02, -2.1299e-03,
        -6.3298e-02, -2.1935e-03, -4.5828e-02,  3.1943e-01, -5.8868e-04,
         6.9664e-02,  2.8211e-01, -3.6960e-02, -4.1849e-02,  7.2758e-02,
        -2.0004e-03,  1.2947e-04,  3.4534e-04, -5.3514e-05,  2.8851e-02,
         1.8986e-05,  4.3380e-05, -1.6957e-01, -1.8001e-03,  8.1271e-06,
        -1.9054e-03,  1.8689e-04, -1.6949e-03,  5.8045e-05,  8.3057e-03,
        -9.1768e-03,  6.3129e-03,  7.7744e-04, -1.8647e-02,  2.0871e-03,
         2.8462e-01,  2.2765e-04,  1.6052e-04, -1.6849e-02, -9.9119e-03,
         8.0280e-03, -1.0734e-03, -4.7587e-03, -6.5447e-03,  6.2858e-02,
        -1.0420e-02, -4.2226e-05,  2.3345e-05, -2.5564e-02,  4.2997e-02,
        -1.9941e-03, -3.0231e-03,  8.8025e-02,  5.5176e-02, -5.8188e-03,
        -1.1201e-01, -3.4873e-01,  2.0603e-04,  1.3139e-02,  3.3899e-02,
        -1.8469e-01, -1.5970e-03, -4.6929e-02,  1.3180e-03,  1.0804e-04,
         7.1643e-05,  2.9450e-03,  1.0574e-03,  2.2648e-06,  9.5943e-03,
         1.7341e-03,  5.1794e-06, -2.7626e-04,  1.4144e-02,  6.8763e-04,
         2.9027e-02, -9.6313e-05,  6.2271e-03, -2.6152e-04,  4.8838e-02,
         1.3392e-04,  3.7421e-02, -1.0279e-03,  1.9578e-03, -2.4686e-06,
        -3.1391e-02, -2.7439e-03,  2.6205e-04,  4.4403e-04,  5.8927e-05,
        -2.0026e-04, -3.3617e-02, -2.0904e-01,  3.7527e-03, -6.1212e-02,
        -7.5932e-05,  3.4279e-04,  1.0103e-01, -7.2149e-02, -6.7910e-03,
         3.1017e-03, -4.9871e-03, -2.4827e-05,  5.7320e-03, -9.8641e-03,
        -1.0900e-02,  2.3263e-03,  3.6481e-03,  4.4674e-03,  1.8615e-02,
        -8.9003e-03,  7.2513e-03, -8.9819e-06,  1.4622e-03,  9.9692e-03,
        -6.1444e-01,  3.5636e-03,  4.6653e-03, -1.3793e-05, -6.2969e-04,
         3.4120e-05,  1.1020e-03, -3.0469e-05,  4.1266e-03, -3.6619e-02,
         3.8302e-01, -3.0779e-03,  1.7754e-04,  4.4800e-02,  3.6007e-04,
         6.4968e-01,  5.2967e-05,  1.0000e+00, -1.5822e-04,  3.0491e-05,
         2.4169e-02, -3.4941e-04, -1.9727e-02,  9.8392e-02, -4.4161e-03,
         1.1326e-03,  7.1525e-02, -1.4324e-05,  3.2677e-05,  5.8709e-03,
        -1.9198e-03,  1.7674e-03,  4.1423e-05, -3.1888e-02, -3.7341e-06,
         6.1054e-04,  4.8628e-04,  7.5724e-03, -4.3960e-02,  8.3225e-03,
         1.3550e-04,  1.1450e-02, -1.8919e-03, -2.6429e-04,  9.9447e-03,
        -1.3199e-02,  8.2843e-04, -3.4749e-02,  9.8738e-04,  1.5413e-03,
        -3.8671e-05, -2.3892e-05,  9.4244e-04,  6.9910e-04,  2.9414e-03,
        -2.7643e-03,  3.2199e-03,  4.3235e-05,  6.0167e-03,  2.1491e-03,
        -2.2352e-02, -1.3359e-02,  1.7401e-07, -2.3843e-01, -7.2115e-05,
        -3.0431e-03, -8.7497e-05, -4.1749e-02,  3.2445e-02, -1.0811e-04,
         1.9581e-02,  8.2049e-05, -1.3406e-03,  3.0483e-04, -2.3033e-02,
         2.2119e-03,  2.9305e-05,  3.3734e-04, -7.1218e-05,  6.1239e-05,
        -3.7863e-04, -4.0345e-05,  1.2659e-01,  7.5645e-04, -4.2602e-02,
         1.4669e-03, -9.1165e-01, -2.5074e-05,  1.2828e-05,  1.6369e-05,
        -9.4926e-03,  6.3717e-02, -3.8056e-05,  3.3859e-06,  7.6375e-02,
        -2.5977e-04,  1.9502e-03,  5.1995e-03,  1.3606e-02, -2.9814e-05,
        -1.2566e-03,  1.0382e-02,  8.9141e-03, -1.0568e-07,  2.4576e-04,
         1.1888e-02, -5.9848e-04, -7.8868e-03,  2.4651e-03, -5.5964e-03,
        -2.7051e-04, -1.6336e-01,  3.1006e-03, -3.7294e-04, -1.3225e-03,
        -4.1570e-02, -2.9464e-02, -1.0904e-03,  1.1279e-02,  2.5964e-03,
        -1.0496e-01, -4.4629e-02,  4.6441e-04, -1.4105e-03,  1.0779e-01,
         2.7615e-04,  4.9523e-02,  5.8190e-05,  1.2694e-04, -7.1184e-04,
        -1.0205e-03, -6.0117e-05,  7.1328e-03, -1.9950e-03,  3.5335e-03,
         1.7623e-05,  2.5985e-04,  1.6081e-03, -3.1294e-04, -1.7198e-06,
        -6.9620e-07,  1.9185e-04,  6.4374e-03, -3.3447e-03, -2.5361e-01,
         1.2631e-02,  9.3823e-03,  8.2233e-03, -4.7936e-02,  2.4008e-01,
        -1.3753e-01,  1.7838e-03, -1.1939e-01,  1.5563e-03, -2.1886e-05,
         2.6196e-03,  1.4795e-02, -3.5906e-05,  8.7130e-04,  3.6218e-04,
         2.7082e-04,  1.6384e-04, -2.2462e-04,  2.3106e-02, -6.3837e-03,
        -2.1288e-01,  6.5514e-04,  1.9160e-01,  2.7915e-03, -2.2252e-01,
         2.7847e-04, -1.4735e-04, -9.4040e-02,  1.4203e-03,  2.4017e-01,
        -1.9699e-01, -4.4020e-04,  8.0017e-02,  1.9920e-02, -3.3929e-04,
         6.6552e-05,  1.6553e-05, -2.8286e-03,  2.2965e-03,  6.0745e-03,
        -5.1032e-01, -2.2038e-05,  8.0501e-03,  3.5752e-03, -1.4031e-04,
         3.3737e-04,  2.2669e-02,  2.8834e-03, -8.0148e-02, -9.3541e-03,
         6.0478e-04, -9.2107e-04,  2.8046e-04,  4.2740e-04,  7.4344e-03,
         6.2248e-04,  5.2942e-03, -6.2590e-02,  3.1001e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0008,  0.0005, -0.0125,  ...,  0.0090,  0.0078,  0.0085],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.0956,  0.0150, -0.0350,  ..., -0.0298, -0.0049,  0.0246],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([-0.0106,  0.0003,  0.0154,  ...,  0.0085,  0.0011,  0.0535],
       device='cuda:0')}
topk_layer inside model after retrieval:  [1575, 1644, 315, 2388, 4, 1323, 1622, 1859, 2228, 2029, 263, 770, 1131, 1289, 2356, 544, 1187, 2123, 1193, 188, 2237, 1610, 1746, 1064, 264, 1908, 806, 1444, 2413, 891, 1580, 412, 1288, 1993, 1706, 2207, 1402, 1757, 1185, 1109, 493, 502, 2285, 2353, 2475, 778, 1143, 2537, 2008, 615, 67, 1524, 2142, 1795, 1366, 2070, 2041, 448, 2064, 1654, 1167, 514, 549, 1975, 309, 1732, 1019, 1957, 1266, 1629, 1481, 1855, 893, 34, 1067, 1326, 231, 2346, 919, 1819, 366, 1667, 2268, 1082, 1620, 132, 6, 1521, 929, 1428, 1867, 1969, 1631, 621, 2463, 1585, 774, 277, 551, 2254, 783, 1659, 1684, 2131, 1813, 603, 792, 556, 288, 1604, 1041, 2209, 2292, 1499, 2510, 1641, 1749, 805, 2262, 1682, 1967, 2517, 1668, 1857, 593, 1676, 2524, 969, 2038, 802, 489, 670, 435, 1063, 1988, 2286, 1062, 829, 2225, 230, 1285, 687, 437, 2183, 568, 673, 2158, 2518, 99, 2396, 851, 1084, 1152, 895, 711, 397, 1833, 1922, 8, 2015, 2461, 1603, 272, 292, 1066, 1244, 2143, 2523, 21, 586, 1214, 1439, 648, 703, 1734, 1727, 401, 433, 2014, 1874, 2385, 718, 294, 26, 675, 2218, 704, 1490, 1468, 1267, 931, 1191, 1251, 2439, 331, 1212, 592, 558, 59, 1756, 139, 1674, 2335, 2528, 2539, 1538, 2210, 2383, 1529, 389, 2267, 1787, 2163, 1657, 1726, 2338, 527, 843, 91, 1254, 2121, 386, 1498, 2359, 1877, 865, 507, 1739, 1190, 2022, 1134, 7, 936, 1434, 1412, 1828, 150, 2152, 2224, 1974, 862, 1870, 1672, 1035, 725, 1043, 1539, 297, 339, 1070, 1261, 2336, 2078, 1764, 889, 1541, 2300, 1898, 1801, 599, 1221, 1208, 381, 1286, 1996, 1399, 671, 1218, 1618, 1257, 1688, 1375, 1534, 1730, 674, 233, 1429, 1454, 2249, 2415, 536, 1778, 2046, 987, 1448, 553, 2543, 473, 1296, 1130, 1204, 641, 2133, 1883, 1535, 70, 1355, 1354, 2174, 1452, 1991, 2053, 1656, 997, 508, 2227, 1832, 2089, 1445, 612, 1652, 182, 1338, 1115, 1471, 1483, 2557, 175, 761, 1685, 1313, 2211, 2496, 510, 455, 1772, 2472, 1472, 1998, 1596, 894, 1308, 1104, 1018, 859, 734, 1624, 1034, 1536, 945, 1514, 1613, 1915, 33, 343, 1518, 2203, 1540, 1865, 65, 1276, 1201, 172, 656, 460, 2555, 1560, 2259, 1731, 2404, 405, 135, 203, 1766, 149, 225, 1928, 2195, 1612, 457, 253, 208, 518, 487, 258, 1885, 1357, 1567, 2213, 1990, 1784, 1485, 2149, 2257, 1422]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
The device used will be: 
True
cuda:0
The device used will be: 
True
cuda:0
chosen_head:  {'blocks.5.layer.bias': tensor([ 1.5791e-05, -1.6506e-02], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0120,  0.0150,  0.0043,  ...,  0.0072,  0.0111, -0.0045],
        [ 0.0163,  0.0075,  0.0001,  ..., -0.0045,  0.0101,  0.0122]],
       device='cuda:0')}

 Model C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c loaded successfuly with best perf



 ----- Architecture Block BatchNorm2dSK3962(5, 5)1.0reflect, number 0 -----
- BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK963842(3, 3)0.6499999761581421reflect, number 1 -----
- BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.4)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK38415362(3, 3)0.25reflect, number 2 -----
- BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block BatchNorm2dSK153625602(3, 3)0.25reflect, number 3 -----
- BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block BatchNorm2dSK256025602(3, 3)0.25reflect, number 4 -----
- BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 5 -----
- Flatten(start_dim=1, end_dim=-1)
- Dropout(p=0.5, inplace=False)
- Linear(in_features=2560, out_features=2, bias=True)
model.heads:  [{'blocks.5.layer.bias': tensor([ 1.5791e-05, -1.6506e-02], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0120,  0.0150,  0.0043,  ...,  0.0072,  0.0111, -0.0045],
        [ 0.0163,  0.0075,  0.0001,  ..., -0.0045,  0.0101,  0.0122]],
       device='cuda:0')}]
[Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (5): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=2560, out_features=2, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)]
LAYER NAME:  Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (5): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=2560, out_features=2, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)
LAYER CHILDREN:  [BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=1.4)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
  )
  (layer): Linear(in_features=2560, out_features=2, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)]
subsubl NAME:  Sequential(
  (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=1.4)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): Flatten(start_dim=1, end_dim=-1)
)
subsubl NAME:  Linear(in_features=2560, out_features=2, bias=True)
subsubl NAME:  Dropout(p=0.5, inplace=False)
CONFIG MODE:  unsupervised

 ********** Hebbian Unsupervised learning of blocks [0, 1, 2, 3, 4] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([49, 33, 72, 51, 71, 92, 15, 14, 23,  0, 71, 75, 81, 69, 40, 43, 92, 97,
        70, 53], device='cuda:0')
TARGETS AFTER SUB:  tensor([92, 92, 35, 92, 35, 92, 92, 92, 35, 35, 35, 35, 92, 92, 35, 35, 35, 35,
        92, 92])
[35, 92]
TARGETS AFTER CLEANER:  tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1])
200
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 200
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               None
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([19, 29,  0, 11,  1, 86, 90, 28, 23, 31, 39, 96, 82, 17, 71, 39,  8, 97,
        80, 71], device='cuda:0')
TARGETS AFTER SUB:  tensor([35, 92, 35, 35, 92, 35, 92, 35, 92, 35, 92, 92, 35, 35, 92, 35, 35, 92,
        35, 35])
[35, 92]
TARGETS AFTER CLEANER:  tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0])
1000
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 1000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               None
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([19, 29,  0, 11,  1, 86, 90, 28, 23, 31, 39, 96, 82, 17, 71, 39,  8, 97,
        80, 71], device='cuda:0')
TARGETS AFTER SUB:  tensor([35, 92, 35, 35, 92, 35, 92, 35, 92, 35, 92, 92, 35, 35, 92, 35, 35, 92,
        35, 35])
[35, 92]
TARGETS AFTER CLEANER:  tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0])
1000
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 1000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
------------------------
INDICES:  1000
IMAGE SIZE: torch.Size([10, 3, 32, 32])
SAVING FOLDER FOR UNSUP:  C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c
prev_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight', 'blocks.4.layer.weight'])
activations_sum.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight', 'blocks.4.layer.weight'])
DEPTH:  5
ACTIVATIONS SHAPE:  tensor([[-152.2907, -403.4749, -356.2777, -586.3671, -580.8934, -223.1667,
          -10.3093, -258.2734, -516.2469, -749.0862, -757.7245, -975.6273,
         -889.4256, -631.1808, -556.7327, -446.1241],
        [  41.6813,  -76.1985,   58.1696,    3.9287,   41.4839,  215.4710,
          311.5930,  256.5968,  159.8901, -210.3577, -413.9152, -841.0543,
         -908.8751, -696.8343, -566.3557, -332.1948],
        [ 223.7924,   28.4885,  156.6260,  111.9260,  406.5362,  583.1995,
          796.1180,  436.6146,  240.9545,  145.6266, -241.9666, -800.4061,
         -868.8165, -749.6377, -633.1860, -403.4809],
        [ 366.0112,   61.0283,  361.6021,  371.9647,  645.4877,  792.5997,
          579.3175,  177.8482,  387.9334,  472.2398,  121.1713, -238.8552,
         -563.3005, -739.6911, -879.5386, -384.8432],
        [ 591.2642,  341.4478,  682.2150,  616.9742, 1025.2965,  878.9802,
          543.8317,  251.2975,  540.0398,  999.0717,  855.2768,  715.8584,
           49.2637, -532.4517, -940.9575, -351.5040],
        [ 527.2054,  306.8165,  843.2068, 1039.3364, 1289.6626, 1325.1401,
          923.9033,  574.7661,  536.8680,  964.2798, 1455.3677, 1251.8785,
          543.2627, -450.5767, -998.2334, -487.2592],
        [ 719.5217,  542.1281, 1210.9896, 1204.7703, 1293.7808, 1332.6094,
          854.9056,  301.6827,  227.9609,  773.9059, 1349.9639, 1444.5459,
          779.1287, -141.2250, -561.3367, -203.3311],
        [ 715.7137,  394.0821, 1105.2715, 1208.8802, 1142.0857,  954.8217,
          655.8871,  390.5947,  389.3092,  728.4358, 1049.5594,  983.6953,
          515.3995,   65.3734, -174.8686,  -91.7841],
        [ 856.9496,  548.3883, 1088.3214,  796.9083,  954.3324,  802.2096,
          630.6707,  567.6921,  521.9008,  606.9503,  676.7438,  517.7700,
           27.9522, -295.2770, -324.9792,  -12.0673],
        [ 506.0093,  401.9494,  991.6010,  653.3853,  925.3539,  580.2527,
          364.6240,  372.6086,  421.9473,  487.9566,  211.7101,   53.1164,
         -422.1057, -594.5148, -416.5235, -289.2726],
        [ 206.5716,  196.2495,  632.0266,  653.4379,  834.1378,  664.9602,
          536.8668,  511.8538,  327.9015,  152.3011,   67.7787, -223.2313,
         -348.6470, -676.9901, -560.8005, -408.6268],
        [ 107.8890,  127.1555,  519.8978,  638.5110,  692.1714,  653.7621,
          493.8723,  308.3265,  220.8760,   68.6073, -112.5496, -186.8682,
         -330.5761, -568.7615, -422.7990, -183.6491],
        [ -13.4347, -202.7676,  245.3792,  350.0161,  438.2267,  342.8839,
          230.4328,  -82.5404,  -31.9217, -172.6097,   48.7424,   16.5123,
         -182.4442, -191.8058,   35.2881,   42.2200],
        [ 109.1172, -398.9314,  -46.2222, -131.1700,   -6.1720,  104.2701,
           71.6403,  -34.8620,  -43.2667, -128.6241,  247.4235,  359.1992,
          174.0032,  -35.5982,  219.1813,  156.8530],
        [ 212.3900, -442.5314, -109.4035, -217.1404,  -28.2824,  245.2681,
          312.7322,  187.2620,   62.0855,  194.2123,  422.3405,  615.0320,
          475.2839,  338.2192,  427.5997,  379.1465],
        [ 111.5478, -490.7449, -226.0226, -215.6249, -122.2747, -121.7781,
           27.0753,  -45.3931, -251.5706, -342.3000, -326.4424, -179.5995,
         -160.1930, -259.3464, -149.6956,    9.9926]], device='cuda:0')
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.topk_layer:  <class 'list'>
CHECK_THRESHOLD:  tensor(1, device='cuda:0', dtype=torch.uint8) tensor(-0.0826) tensor(-0.1453)
topk_mask  tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 0., 0.])
threshold_mask  tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1],
       dtype=torch.uint8)
final_mask tensor([1., 1., 1., 1., 2., 2., 0., 0., 1., 1., 0., 0., 0., 1., 2., 0., 1., 0.,
        1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1.,
        1., 1., 0., 1., 0., 0., 0., 1., 1., 2., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 2., 0., 0., 0., 2., 0., 0., 1.,
        0., 1., 1., 1., 0., 1.])
lower_lr_mask  tensor([-0., -0., -0., -0., -1., -1., -0., -0., -0., -0., -0., -0., -0., -0.,
        -1., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -1., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0.,
        -0., -0., -1., -0., -0., -0., -0., -0., -0., -0., -0., -0.])
higher_lr_mask  tensor([0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.0000, 0.0000, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500])
lr_mask  tensor([[[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]]])
torch.Size([96, 3, 5, 5])
self.lr tensor([[[0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066]],

        [[0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066]],

        [[0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066]]], device='cuda:0')
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.topk_layer:  <class 'list'>
CHECK_THRESHOLD:  tensor(1, device='cuda:0', dtype=torch.uint8) tensor(0.0043) tensor(0.0016)
topk_mask  tensor([0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0.])
threshold_mask  tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0],
       dtype=torch.uint8)
final_mask tensor([1., 0., 0., 0., 1., 0., 2., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 2.,
        1., 0., 0., 0., 0., 1., 1., 0., 2., 1., 0., 0., 1., 0., 0., 2., 0., 1.,
        1., 0., 0., 0., 1., 2., 0., 0., 1., 1., 0., 0., 1., 0., 2., 0., 1., 0.,
        1., 0., 2., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,
        1., 1., 0., 2., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,
        1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0.,
        1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 2., 0., 0., 2., 1., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 2., 0., 2., 1., 1., 1.,
        2., 0., 2., 0., 0., 0., 2., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0.,
        0., 2., 0., 1., 0., 1., 1., 1., 2., 0., 0., 1., 1., 1., 1., 1., 0., 2.,
        2., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1.,
        1., 0., 0., 0., 1., 0., 1., 0., 2., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1.,
        1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,
        0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 2., 0., 2., 1., 0., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        1., 1., 0., 1., 1., 2., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 2., 1.,
        0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1.,
        2., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 0., 2., 1., 0., 1., 1., 1., 1., 2., 1., 2., 2., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 2., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0.,
        1., 0., 1., 0., 2., 0.])
lower_lr_mask  tensor([-0., -0., -0., -0., -0., -0., -1., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -1., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0.,
        -0., -0., -0., -0., -0., -1., -0., -0., -0., -0., -0., -0., -0., -1.,
        -0., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0., -0., -0., -0.,
        -1., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -1., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -1., -0., -0., -1., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0.,
        -1., -0., -0., -0., -1., -0., -1., -0., -0., -0., -1., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0., -0., -0.,
        -0., -0., -1., -0., -0., -0., -0., -0., -0., -0., -0., -1., -1., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0.,
        -1., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -1.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -1., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0., -0., -0.,
        -0., -0., -1., -0., -1., -1., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -1., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -1., -0.])
higher_lr_mask  tensor([0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.0000, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.0000,
        0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.0000, 0.1500, 0.0000, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.0000, 0.1500, 0.0000, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500,
        0.0000, 0.0000, 0.1500, 0.1500, 0.1500, 0.0000, 0.0000, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.0000, 0.0000, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.0000, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000,
        0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.0000, 0.0000, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.0000,
        0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.0000,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.0000, 0.1500, 0.0000, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.0000, 0.1500, 0.0000, 0.0000, 0.1500, 0.0000, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.0000,
        0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500])
lr_mask  tensor([[[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]]])
torch.Size([384, 96, 3, 3])
self.lr tensor([[[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]]], device='cuda:0')
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.topk_layer:  <class 'list'>
CHECK_THRESHOLD:  tensor(0, device='cuda:0', dtype=torch.uint8) tensor(7.0545e-05) tensor(0.0008)
topk_mask  tensor([0., 0., 0.,  ..., 0., 1., 1.])
threshold_mask  tensor([0, 1, 1,  ..., 0, 0, 0], dtype=torch.uint8)
final_mask tensor([0., 1., 1.,  ..., 0., 1., 1.])
lower_lr_mask  tensor([-0., -0., -0.,  ..., -0., -0., -0.])
higher_lr_mask  tensor([0.1500, 0.1500, 0.1500,  ..., 0.1500, 0.0000, 0.0000])
lr_mask  tensor([[[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        ...,


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.0000]]]])
torch.Size([1536, 384, 3, 3])
self.lr tensor([[[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]],

        [[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]],

        [[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]],

        ...,

        [[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]],

        [[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]],

        [[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]]], device='cuda:0')
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.topk_layer:  <class 'list'>
CHECK_THRESHOLD:  tensor(1, device='cuda:0', dtype=torch.uint8) tensor(0.0011) tensor(-0.0956)
topk_mask  tensor([0., 0., 0.,  ..., 0., 0., 0.])
threshold_mask  tensor([1, 0, 1,  ..., 1, 1, 0], dtype=torch.uint8)
final_mask tensor([1., 0., 1.,  ..., 1., 1., 0.])
lower_lr_mask  tensor([-0., -0., -0.,  ..., -0., -0., -0.])
higher_lr_mask  tensor([0.1500, 0.1500, 0.1500,  ..., 0.1500, 0.1500, 0.1500])
lr_mask  tensor([[[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        ...,


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]]])
torch.Size([2560, 1536, 3, 3])
self.lr tensor([[[0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488]],

        [[0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488]],

        [[0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488]],

        ...,

        [[0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488]],

        [[0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488]],

        [[0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488]]], device='cuda:0')
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.topk_layer:  <class 'list'>
CHECK_THRESHOLD:  tensor(1, device='cuda:0', dtype=torch.uint8) tensor(0.0288) tensor(-0.0106)
topk_mask  tensor([0., 0., 0.,  ..., 1., 0., 0.])
threshold_mask  tensor([1, 0, 0,  ..., 0, 0, 0], dtype=torch.uint8)
final_mask tensor([1., 0., 0.,  ..., 1., 0., 0.])
lower_lr_mask  tensor([-0., -0., -0.,  ..., -0., -0., -0.])
higher_lr_mask  tensor([0.1500, 0.1500, 0.1500,  ..., 0.0000, 0.1500, 0.1500])
lr_mask  tensor([[[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        ...,


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]]])
torch.Size([2560, 2560, 3, 3])
self.lr tensor([[[0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491]],

        [[0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491]],

        [[0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491]],

        ...,

        [[0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491]],

        [[0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491]],

        [[0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491]]], device='cuda:0')
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight', 'blocks.4.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight', 'blocks.4.layer.weight'])
activations_sum[k] 96
activations_sum[k]:  [25, 59, 30, 55, 42, 69, 5, 68, 93, 16]
topk_kernels len:  5
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3', 'conv4']
topk_kernels:  {'conv0': [25, 59, 30, 55, 42, 69, 5, 68, 93, 16, 57, 14, 36, 38, 77], 'conv1': [365, 50, 382, 75, 180, 264, 56, 345, 104, 352, 206, 100, 190, 324, 354, 99, 73, 140, 336, 355, 121, 191, 233, 144, 243, 163, 193, 150, 4, 26, 373, 266, 118, 92, 357, 117, 94, 90, 158, 68, 179, 202, 368, 76, 36, 170, 304, 146, 41, 35, 33, 138, 6, 105, 262, 197, 293, 317, 17], 'conv2': [307, 937, 662, 141, 1307, 1006, 869, 683, 1441, 1238, 348, 1086, 1278, 37, 1404, 427, 889, 311, 341, 1405, 796, 312, 1500, 794, 1486, 1370, 490, 304, 449, 835, 1297, 1100, 28, 942, 731, 537, 710, 380, 612, 451, 1159, 735, 1139, 1046, 142, 1385, 998, 176, 41, 295, 958, 346, 313, 1436, 1034, 167, 1360, 906, 529, 441, 214, 1306, 1395, 79, 1450, 544, 1412, 660, 542, 1000, 1372, 700, 405, 613, 61, 14, 1243, 711, 195, 673, 636, 1082, 107, 1317, 635, 590, 551, 997, 209, 207, 1474, 987, 1226, 865, 706, 1172, 421, 733, 70, 1143, 94, 959, 896, 547, 229, 423, 11, 838, 923, 856, 407, 851, 1345, 914, 1359, 622, 259, 1464, 1400, 460, 268, 730, 1532, 1266, 454, 634, 806, 450, 577, 35, 12, 1027, 1449, 1534, 1466, 872, 534, 752, 1489, 1101, 1230, 1134, 199, 434, 1185, 932, 915, 123, 766, 1429, 250, 1033, 155, 335, 1030, 276, 457, 953, 174, 799, 828, 1452, 891, 626, 496, 1149, 831, 456, 510, 194, 68, 810, 761, 981, 1242, 1291, 1294, 1535, 75, 499, 935, 1272, 501, 912, 1347, 921, 353, 859, 640, 1032, 1507, 1171, 373, 100, 287, 281, 1442, 944, 1083, 406, 356, 549, 187, 883, 1265, 919, 619, 109, 783, 870, 760, 927, 868, 904, 644, 1249, 420, 1113, 1455, 1211, 1183, 78, 95, 13, 1167, 261, 839, 817, 1274, 252, 994], 'conv3': [1422, 43, 1677, 114, 1319, 2000, 471, 2051, 165, 909, 1874, 1660, 934, 1549, 1608, 2302, 2494, 1312, 2352, 1548, 589, 2069, 1083, 499, 1520, 1988, 1873, 1187, 1110, 195, 2438, 1168, 167, 2138, 1090, 882, 512, 505, 398, 1938, 1827, 648, 1127, 1776, 468, 423, 1331, 1109, 2448, 784, 1495, 1554, 2081, 907, 1558, 2533, 2281, 781, 1616, 694, 278, 1063, 980, 2247, 249, 1245, 305, 1857, 867, 443, 2556, 1709, 359, 178, 1164, 419, 841, 2095, 1049, 95, 28, 137, 2181, 1403, 1359, 377, 231, 136, 2001, 1447, 1744, 2315, 1443, 2067, 2101, 1723, 1990, 210, 1866, 1926, 1303, 1887, 2515, 646, 356, 2391, 129, 667, 2037, 1038, 2049, 2152, 1783, 2460, 103, 1641, 878, 757, 272, 1073, 2221, 1417, 565, 2007, 1088, 192, 1506, 2085, 1930, 1435, 2170, 251, 2342, 327, 452, 2241, 1045, 1789, 2244, 2185, 1468, 148, 932, 147, 2008, 2248, 303, 1774, 2500, 1651, 507, 923, 339, 792, 1522, 1879, 2300, 2374, 1280, 2154, 334, 564, 2334, 2151, 1177, 68, 1854, 2435, 1865, 1955, 2131, 533, 611, 446, 1689, 939, 2317, 540, 689, 922, 1843, 450, 1036, 261, 1798, 1260, 2278, 1801, 1497, 372, 1452, 1324, 1614, 1612, 347, 529, 621, 1446, 2471, 2146, 2249, 2456, 892, 514, 555, 2290, 2429, 1043, 1720, 633, 1363, 894, 634, 2021, 477, 1591, 2106, 2193, 1232, 739, 1269, 185, 674, 2102, 2511, 1758, 828, 1845, 1824, 2547, 1105, 780, 750, 350, 1395, 21, 526, 2549, 233, 2479, 1188, 33, 297, 2100, 1205, 1211, 1346, 709, 881, 54, 861, 1029, 1283, 819, 534, 299, 1670, 1698, 652, 245, 46, 2119, 1982, 2378, 556, 1183, 1242, 1527, 940, 1054, 2279, 497, 2497, 196, 1344, 949, 2018, 2414, 1009, 2215, 1394, 1958, 1226, 2475, 42, 989, 1872, 1089, 863, 1685, 2309, 454, 2242, 1256, 2243, 2161, 1425, 361, 1559, 1567, 2112, 187, 697, 1112, 1037, 1784, 1898, 1378, 1348, 193, 241, 627, 2401, 2145, 481, 2283, 1333, 1894, 1146, 2125, 1326, 1411, 1057, 1924, 2338, 1438, 679, 653, 518, 436, 1998, 1968, 994, 1252, 1978, 2030, 465, 328, 494, 1599, 2450, 660, 664, 197, 1578, 1735, 506, 2465, 2276, 225, 1141, 202, 2316, 106, 389, 80, 1584, 1085, 2331, 2072, 1362, 378, 1208, 2427, 1385, 1755, 1684, 2090, 2261, 2122, 1066, 544, 1266, 1349, 1664, 2183, 2293, 1131, 945, 2282, 122, 982, 1413, 1258, 1995], 'conv4': [1575, 1644, 315, 2388, 4, 1323, 1622, 1859, 2228, 2029, 263, 770, 1131, 1289, 2356, 544, 1187, 2123, 1193, 188, 2237, 1610, 1746, 1064, 264, 1908, 806, 1444, 2413, 891, 1580, 412, 1288, 1993, 1706, 2207, 1402, 1757, 1185, 1109, 493, 502, 2285, 2353, 2475, 778, 1143, 2537, 2008, 615, 67, 1524, 2142, 1795, 1366, 2070, 2041, 448, 2064, 1654, 1167, 514, 549, 1975, 309, 1732, 1019, 1957, 1266, 1629, 1481, 1855, 893, 34, 1067, 1326, 231, 2346, 919, 1819, 366, 1667, 2268, 1082, 1620, 132, 6, 1521, 929, 1428, 1867, 1969, 1631, 621, 2463, 1585, 774, 277, 551, 2254, 783, 1659, 1684, 2131, 1813, 603, 792, 556, 288, 1604, 1041, 2209, 2292, 1499, 2510, 1641, 1749, 805, 2262, 1682, 1967, 2517, 1668, 1857, 593, 1676, 2524, 969, 2038, 802, 489, 670, 435, 1063, 1988, 2286, 1062, 829, 2225, 230, 1285, 687, 437, 2183, 568, 673, 2158, 2518, 99, 2396, 851, 1084, 1152, 895, 711, 397, 1833, 1922, 8, 2015, 2461, 1603, 272, 292, 1066, 1244, 2143, 2523, 21, 586, 1214, 1439, 648, 703, 1734, 1727, 401, 433, 2014, 1874, 2385, 718, 294, 26, 675, 2218, 704, 1490, 1468, 1267, 931, 1191, 1251, 2439, 331, 1212, 592, 558, 59, 1756, 139, 1674, 2335, 2528, 2539, 1538, 2210, 2383, 1529, 389, 2267, 1787, 2163, 1657, 1726, 2338, 527, 843, 91, 1254, 2121, 386, 1498, 2359, 1877, 865, 507, 1739, 1190, 2022, 1134, 7, 936, 1434, 1412, 1828, 150, 2152, 2224, 1974, 862, 1870, 1672, 1035, 725, 1043, 1539, 297, 339, 1070, 1261, 2336, 2078, 1764, 889, 1541, 2300, 1898, 1801, 599, 1221, 1208, 381, 1286, 1996, 1399, 671, 1218, 1618, 1257, 1688, 1375, 1534, 1730, 674, 233, 1429, 1454, 2249, 2415, 536, 1778, 2046, 987, 1448, 553, 2543, 473, 1296, 1130, 1204, 641, 2133, 1883, 1535, 70, 1355, 1354, 2174, 1452, 1991, 2053, 1656, 997, 508, 2227, 1832, 2089, 1445, 612, 1652, 182, 1338, 1115, 1471, 1483, 2557, 175, 761, 1685, 1313, 2211, 2496, 510, 455, 1772, 2472, 1472, 1998, 1596, 894, 1308, 1104, 1018, 859, 734, 1624, 1034, 1536, 945, 1514, 1613, 1915, 33, 343, 1518, 2203, 1540, 1865, 65, 1276, 1201, 172, 656, 460, 2555, 1560, 2259, 1731, 2404, 405, 135, 203, 1766, 149, 225, 1928, 2195, 1612, 457, 253, 208, 518, 487, 258, 1885, 1357, 1567, 2213, 1990, 1784, 1485, 2149, 2257, 1422]}
activations_sum[k] len:  96
activations_sum[k] 384
activations_sum[k]:  [197, 349, 72, 345, 121, 76, 124, 193, 109, 365]
topk_kernels len:  5
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3', 'conv4']
topk_kernels:  {'conv0': [25, 59, 30, 55, 42, 69, 5, 68, 93, 16, 57, 14, 36, 38, 77], 'conv1': [197, 349, 72, 345, 121, 76, 124, 193, 109, 365, 274, 296, 354, 92, 73, 56, 75, 105, 36, 188, 202, 343, 336, 304, 162, 41, 266, 159, 100, 104, 163, 148, 22, 9, 186, 229, 190, 379, 4, 174, 358, 114, 118, 373, 146, 217, 278, 347, 70, 191, 169, 99, 50, 180, 137, 26, 94, 383, 324], 'conv2': [307, 937, 662, 141, 1307, 1006, 869, 683, 1441, 1238, 348, 1086, 1278, 37, 1404, 427, 889, 311, 341, 1405, 796, 312, 1500, 794, 1486, 1370, 490, 304, 449, 835, 1297, 1100, 28, 942, 731, 537, 710, 380, 612, 451, 1159, 735, 1139, 1046, 142, 1385, 998, 176, 41, 295, 958, 346, 313, 1436, 1034, 167, 1360, 906, 529, 441, 214, 1306, 1395, 79, 1450, 544, 1412, 660, 542, 1000, 1372, 700, 405, 613, 61, 14, 1243, 711, 195, 673, 636, 1082, 107, 1317, 635, 590, 551, 997, 209, 207, 1474, 987, 1226, 865, 706, 1172, 421, 733, 70, 1143, 94, 959, 896, 547, 229, 423, 11, 838, 923, 856, 407, 851, 1345, 914, 1359, 622, 259, 1464, 1400, 460, 268, 730, 1532, 1266, 454, 634, 806, 450, 577, 35, 12, 1027, 1449, 1534, 1466, 872, 534, 752, 1489, 1101, 1230, 1134, 199, 434, 1185, 932, 915, 123, 766, 1429, 250, 1033, 155, 335, 1030, 276, 457, 953, 174, 799, 828, 1452, 891, 626, 496, 1149, 831, 456, 510, 194, 68, 810, 761, 981, 1242, 1291, 1294, 1535, 75, 499, 935, 1272, 501, 912, 1347, 921, 353, 859, 640, 1032, 1507, 1171, 373, 100, 287, 281, 1442, 944, 1083, 406, 356, 549, 187, 883, 1265, 919, 619, 109, 783, 870, 760, 927, 868, 904, 644, 1249, 420, 1113, 1455, 1211, 1183, 78, 95, 13, 1167, 261, 839, 817, 1274, 252, 994], 'conv3': [1422, 43, 1677, 114, 1319, 2000, 471, 2051, 165, 909, 1874, 1660, 934, 1549, 1608, 2302, 2494, 1312, 2352, 1548, 589, 2069, 1083, 499, 1520, 1988, 1873, 1187, 1110, 195, 2438, 1168, 167, 2138, 1090, 882, 512, 505, 398, 1938, 1827, 648, 1127, 1776, 468, 423, 1331, 1109, 2448, 784, 1495, 1554, 2081, 907, 1558, 2533, 2281, 781, 1616, 694, 278, 1063, 980, 2247, 249, 1245, 305, 1857, 867, 443, 2556, 1709, 359, 178, 1164, 419, 841, 2095, 1049, 95, 28, 137, 2181, 1403, 1359, 377, 231, 136, 2001, 1447, 1744, 2315, 1443, 2067, 2101, 1723, 1990, 210, 1866, 1926, 1303, 1887, 2515, 646, 356, 2391, 129, 667, 2037, 1038, 2049, 2152, 1783, 2460, 103, 1641, 878, 757, 272, 1073, 2221, 1417, 565, 2007, 1088, 192, 1506, 2085, 1930, 1435, 2170, 251, 2342, 327, 452, 2241, 1045, 1789, 2244, 2185, 1468, 148, 932, 147, 2008, 2248, 303, 1774, 2500, 1651, 507, 923, 339, 792, 1522, 1879, 2300, 2374, 1280, 2154, 334, 564, 2334, 2151, 1177, 68, 1854, 2435, 1865, 1955, 2131, 533, 611, 446, 1689, 939, 2317, 540, 689, 922, 1843, 450, 1036, 261, 1798, 1260, 2278, 1801, 1497, 372, 1452, 1324, 1614, 1612, 347, 529, 621, 1446, 2471, 2146, 2249, 2456, 892, 514, 555, 2290, 2429, 1043, 1720, 633, 1363, 894, 634, 2021, 477, 1591, 2106, 2193, 1232, 739, 1269, 185, 674, 2102, 2511, 1758, 828, 1845, 1824, 2547, 1105, 780, 750, 350, 1395, 21, 526, 2549, 233, 2479, 1188, 33, 297, 2100, 1205, 1211, 1346, 709, 881, 54, 861, 1029, 1283, 819, 534, 299, 1670, 1698, 652, 245, 46, 2119, 1982, 2378, 556, 1183, 1242, 1527, 940, 1054, 2279, 497, 2497, 196, 1344, 949, 2018, 2414, 1009, 2215, 1394, 1958, 1226, 2475, 42, 989, 1872, 1089, 863, 1685, 2309, 454, 2242, 1256, 2243, 2161, 1425, 361, 1559, 1567, 2112, 187, 697, 1112, 1037, 1784, 1898, 1378, 1348, 193, 241, 627, 2401, 2145, 481, 2283, 1333, 1894, 1146, 2125, 1326, 1411, 1057, 1924, 2338, 1438, 679, 653, 518, 436, 1998, 1968, 994, 1252, 1978, 2030, 465, 328, 494, 1599, 2450, 660, 664, 197, 1578, 1735, 506, 2465, 2276, 225, 1141, 202, 2316, 106, 389, 80, 1584, 1085, 2331, 2072, 1362, 378, 1208, 2427, 1385, 1755, 1684, 2090, 2261, 2122, 1066, 544, 1266, 1349, 1664, 2183, 2293, 1131, 945, 2282, 122, 982, 1413, 1258, 1995], 'conv4': [1575, 1644, 315, 2388, 4, 1323, 1622, 1859, 2228, 2029, 263, 770, 1131, 1289, 2356, 544, 1187, 2123, 1193, 188, 2237, 1610, 1746, 1064, 264, 1908, 806, 1444, 2413, 891, 1580, 412, 1288, 1993, 1706, 2207, 1402, 1757, 1185, 1109, 493, 502, 2285, 2353, 2475, 778, 1143, 2537, 2008, 615, 67, 1524, 2142, 1795, 1366, 2070, 2041, 448, 2064, 1654, 1167, 514, 549, 1975, 309, 1732, 1019, 1957, 1266, 1629, 1481, 1855, 893, 34, 1067, 1326, 231, 2346, 919, 1819, 366, 1667, 2268, 1082, 1620, 132, 6, 1521, 929, 1428, 1867, 1969, 1631, 621, 2463, 1585, 774, 277, 551, 2254, 783, 1659, 1684, 2131, 1813, 603, 792, 556, 288, 1604, 1041, 2209, 2292, 1499, 2510, 1641, 1749, 805, 2262, 1682, 1967, 2517, 1668, 1857, 593, 1676, 2524, 969, 2038, 802, 489, 670, 435, 1063, 1988, 2286, 1062, 829, 2225, 230, 1285, 687, 437, 2183, 568, 673, 2158, 2518, 99, 2396, 851, 1084, 1152, 895, 711, 397, 1833, 1922, 8, 2015, 2461, 1603, 272, 292, 1066, 1244, 2143, 2523, 21, 586, 1214, 1439, 648, 703, 1734, 1727, 401, 433, 2014, 1874, 2385, 718, 294, 26, 675, 2218, 704, 1490, 1468, 1267, 931, 1191, 1251, 2439, 331, 1212, 592, 558, 59, 1756, 139, 1674, 2335, 2528, 2539, 1538, 2210, 2383, 1529, 389, 2267, 1787, 2163, 1657, 1726, 2338, 527, 843, 91, 1254, 2121, 386, 1498, 2359, 1877, 865, 507, 1739, 1190, 2022, 1134, 7, 936, 1434, 1412, 1828, 150, 2152, 2224, 1974, 862, 1870, 1672, 1035, 725, 1043, 1539, 297, 339, 1070, 1261, 2336, 2078, 1764, 889, 1541, 2300, 1898, 1801, 599, 1221, 1208, 381, 1286, 1996, 1399, 671, 1218, 1618, 1257, 1688, 1375, 1534, 1730, 674, 233, 1429, 1454, 2249, 2415, 536, 1778, 2046, 987, 1448, 553, 2543, 473, 1296, 1130, 1204, 641, 2133, 1883, 1535, 70, 1355, 1354, 2174, 1452, 1991, 2053, 1656, 997, 508, 2227, 1832, 2089, 1445, 612, 1652, 182, 1338, 1115, 1471, 1483, 2557, 175, 761, 1685, 1313, 2211, 2496, 510, 455, 1772, 2472, 1472, 1998, 1596, 894, 1308, 1104, 1018, 859, 734, 1624, 1034, 1536, 945, 1514, 1613, 1915, 33, 343, 1518, 2203, 1540, 1865, 65, 1276, 1201, 172, 656, 460, 2555, 1560, 2259, 1731, 2404, 405, 135, 203, 1766, 149, 225, 1928, 2195, 1612, 457, 253, 208, 518, 487, 258, 1885, 1357, 1567, 2213, 1990, 1784, 1485, 2149, 2257, 1422]}
activations_sum[k] len:  384
activations_sum[k] 1536
activations_sum[k]:  [958, 733, 761, 250, 1436, 856, 14, 209, 1163, 176]
topk_kernels len:  5
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3', 'conv4']
topk_kernels:  {'conv0': [25, 59, 30, 55, 42, 69, 5, 68, 93, 16, 57, 14, 36, 38, 77], 'conv1': [197, 349, 72, 345, 121, 76, 124, 193, 109, 365, 274, 296, 354, 92, 73, 56, 75, 105, 36, 188, 202, 343, 336, 304, 162, 41, 266, 159, 100, 104, 163, 148, 22, 9, 186, 229, 190, 379, 4, 174, 358, 114, 118, 373, 146, 217, 278, 347, 70, 191, 169, 99, 50, 180, 137, 26, 94, 383, 324], 'conv2': [958, 733, 761, 250, 1436, 856, 14, 209, 1163, 176, 399, 801, 942, 95, 1532, 118, 622, 959, 1452, 1274, 324, 326, 835, 632, 639, 1172, 852, 1113, 35, 850, 730, 1486, 636, 869, 508, 155, 597, 173, 15, 423, 22, 312, 1278, 281, 12, 408, 783, 1159, 1345, 799, 746, 499, 933, 1126, 1178, 373, 395, 915, 868, 356, 773, 810, 1002, 923, 588, 580, 426, 519, 1238, 164, 1089, 42, 980, 441, 575, 1001, 612, 743, 353, 649, 779, 154, 523, 964, 905, 507, 461, 1046, 626, 294, 981, 1025, 937, 752, 1186, 1032, 405, 953, 313, 58, 542, 1206, 544, 667, 335, 754, 1155, 1508, 1317, 900, 311, 70, 1292, 1184, 707, 439, 682, 1522, 77, 93, 1203, 174, 115, 854, 1125, 1149, 960, 1494, 74, 952, 866, 134, 902, 921, 653, 590, 1038, 238, 872, 1282, 1340, 650, 1297, 1357, 1455, 307, 788, 265, 378, 75, 614, 1265, 243, 454, 406, 1396, 1260, 264, 212, 790, 119, 1461, 1520, 911, 961, 195, 994, 359, 16, 602, 760, 692, 1045, 457, 670, 1302, 732, 1281, 802, 1385, 1460, 728, 714, 889, 1324, 61, 208, 1261, 878, 562, 629, 936, 1132, 574, 257, 1463, 217, 1171, 851, 490, 175, 736, 1462, 1294, 1432, 295, 1242, 1031, 1450, 745, 1192, 41, 1349, 411, 1248, 1227, 1298, 132, 1426, 1379, 348, 105, 705, 1156, 846, 674, 1291, 171, 1442, 381, 188], 'conv3': [1422, 43, 1677, 114, 1319, 2000, 471, 2051, 165, 909, 1874, 1660, 934, 1549, 1608, 2302, 2494, 1312, 2352, 1548, 589, 2069, 1083, 499, 1520, 1988, 1873, 1187, 1110, 195, 2438, 1168, 167, 2138, 1090, 882, 512, 505, 398, 1938, 1827, 648, 1127, 1776, 468, 423, 1331, 1109, 2448, 784, 1495, 1554, 2081, 907, 1558, 2533, 2281, 781, 1616, 694, 278, 1063, 980, 2247, 249, 1245, 305, 1857, 867, 443, 2556, 1709, 359, 178, 1164, 419, 841, 2095, 1049, 95, 28, 137, 2181, 1403, 1359, 377, 231, 136, 2001, 1447, 1744, 2315, 1443, 2067, 2101, 1723, 1990, 210, 1866, 1926, 1303, 1887, 2515, 646, 356, 2391, 129, 667, 2037, 1038, 2049, 2152, 1783, 2460, 103, 1641, 878, 757, 272, 1073, 2221, 1417, 565, 2007, 1088, 192, 1506, 2085, 1930, 1435, 2170, 251, 2342, 327, 452, 2241, 1045, 1789, 2244, 2185, 1468, 148, 932, 147, 2008, 2248, 303, 1774, 2500, 1651, 507, 923, 339, 792, 1522, 1879, 2300, 2374, 1280, 2154, 334, 564, 2334, 2151, 1177, 68, 1854, 2435, 1865, 1955, 2131, 533, 611, 446, 1689, 939, 2317, 540, 689, 922, 1843, 450, 1036, 261, 1798, 1260, 2278, 1801, 1497, 372, 1452, 1324, 1614, 1612, 347, 529, 621, 1446, 2471, 2146, 2249, 2456, 892, 514, 555, 2290, 2429, 1043, 1720, 633, 1363, 894, 634, 2021, 477, 1591, 2106, 2193, 1232, 739, 1269, 185, 674, 2102, 2511, 1758, 828, 1845, 1824, 2547, 1105, 780, 750, 350, 1395, 21, 526, 2549, 233, 2479, 1188, 33, 297, 2100, 1205, 1211, 1346, 709, 881, 54, 861, 1029, 1283, 819, 534, 299, 1670, 1698, 652, 245, 46, 2119, 1982, 2378, 556, 1183, 1242, 1527, 940, 1054, 2279, 497, 2497, 196, 1344, 949, 2018, 2414, 1009, 2215, 1394, 1958, 1226, 2475, 42, 989, 1872, 1089, 863, 1685, 2309, 454, 2242, 1256, 2243, 2161, 1425, 361, 1559, 1567, 2112, 187, 697, 1112, 1037, 1784, 1898, 1378, 1348, 193, 241, 627, 2401, 2145, 481, 2283, 1333, 1894, 1146, 2125, 1326, 1411, 1057, 1924, 2338, 1438, 679, 653, 518, 436, 1998, 1968, 994, 1252, 1978, 2030, 465, 328, 494, 1599, 2450, 660, 664, 197, 1578, 1735, 506, 2465, 2276, 225, 1141, 202, 2316, 106, 389, 80, 1584, 1085, 2331, 2072, 1362, 378, 1208, 2427, 1385, 1755, 1684, 2090, 2261, 2122, 1066, 544, 1266, 1349, 1664, 2183, 2293, 1131, 945, 2282, 122, 982, 1413, 1258, 1995], 'conv4': [1575, 1644, 315, 2388, 4, 1323, 1622, 1859, 2228, 2029, 263, 770, 1131, 1289, 2356, 544, 1187, 2123, 1193, 188, 2237, 1610, 1746, 1064, 264, 1908, 806, 1444, 2413, 891, 1580, 412, 1288, 1993, 1706, 2207, 1402, 1757, 1185, 1109, 493, 502, 2285, 2353, 2475, 778, 1143, 2537, 2008, 615, 67, 1524, 2142, 1795, 1366, 2070, 2041, 448, 2064, 1654, 1167, 514, 549, 1975, 309, 1732, 1019, 1957, 1266, 1629, 1481, 1855, 893, 34, 1067, 1326, 231, 2346, 919, 1819, 366, 1667, 2268, 1082, 1620, 132, 6, 1521, 929, 1428, 1867, 1969, 1631, 621, 2463, 1585, 774, 277, 551, 2254, 783, 1659, 1684, 2131, 1813, 603, 792, 556, 288, 1604, 1041, 2209, 2292, 1499, 2510, 1641, 1749, 805, 2262, 1682, 1967, 2517, 1668, 1857, 593, 1676, 2524, 969, 2038, 802, 489, 670, 435, 1063, 1988, 2286, 1062, 829, 2225, 230, 1285, 687, 437, 2183, 568, 673, 2158, 2518, 99, 2396, 851, 1084, 1152, 895, 711, 397, 1833, 1922, 8, 2015, 2461, 1603, 272, 292, 1066, 1244, 2143, 2523, 21, 586, 1214, 1439, 648, 703, 1734, 1727, 401, 433, 2014, 1874, 2385, 718, 294, 26, 675, 2218, 704, 1490, 1468, 1267, 931, 1191, 1251, 2439, 331, 1212, 592, 558, 59, 1756, 139, 1674, 2335, 2528, 2539, 1538, 2210, 2383, 1529, 389, 2267, 1787, 2163, 1657, 1726, 2338, 527, 843, 91, 1254, 2121, 386, 1498, 2359, 1877, 865, 507, 1739, 1190, 2022, 1134, 7, 936, 1434, 1412, 1828, 150, 2152, 2224, 1974, 862, 1870, 1672, 1035, 725, 1043, 1539, 297, 339, 1070, 1261, 2336, 2078, 1764, 889, 1541, 2300, 1898, 1801, 599, 1221, 1208, 381, 1286, 1996, 1399, 671, 1218, 1618, 1257, 1688, 1375, 1534, 1730, 674, 233, 1429, 1454, 2249, 2415, 536, 1778, 2046, 987, 1448, 553, 2543, 473, 1296, 1130, 1204, 641, 2133, 1883, 1535, 70, 1355, 1354, 2174, 1452, 1991, 2053, 1656, 997, 508, 2227, 1832, 2089, 1445, 612, 1652, 182, 1338, 1115, 1471, 1483, 2557, 175, 761, 1685, 1313, 2211, 2496, 510, 455, 1772, 2472, 1472, 1998, 1596, 894, 1308, 1104, 1018, 859, 734, 1624, 1034, 1536, 945, 1514, 1613, 1915, 33, 343, 1518, 2203, 1540, 1865, 65, 1276, 1201, 172, 656, 460, 2555, 1560, 2259, 1731, 2404, 405, 135, 203, 1766, 149, 225, 1928, 2195, 1612, 457, 253, 208, 518, 487, 258, 1885, 1357, 1567, 2213, 1990, 1784, 1485, 2149, 2257, 1422]}
activations_sum[k] len:  1536
activations_sum[k] 2560
activations_sum[k]:  [1447, 129, 2500, 2420, 2056, 2414, 2181, 564, 370, 746]
topk_kernels len:  5
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3', 'conv4']
topk_kernels:  {'conv0': [25, 59, 30, 55, 42, 69, 5, 68, 93, 16, 57, 14, 36, 38, 77], 'conv1': [197, 349, 72, 345, 121, 76, 124, 193, 109, 365, 274, 296, 354, 92, 73, 56, 75, 105, 36, 188, 202, 343, 336, 304, 162, 41, 266, 159, 100, 104, 163, 148, 22, 9, 186, 229, 190, 379, 4, 174, 358, 114, 118, 373, 146, 217, 278, 347, 70, 191, 169, 99, 50, 180, 137, 26, 94, 383, 324], 'conv2': [958, 733, 761, 250, 1436, 856, 14, 209, 1163, 176, 399, 801, 942, 95, 1532, 118, 622, 959, 1452, 1274, 324, 326, 835, 632, 639, 1172, 852, 1113, 35, 850, 730, 1486, 636, 869, 508, 155, 597, 173, 15, 423, 22, 312, 1278, 281, 12, 408, 783, 1159, 1345, 799, 746, 499, 933, 1126, 1178, 373, 395, 915, 868, 356, 773, 810, 1002, 923, 588, 580, 426, 519, 1238, 164, 1089, 42, 980, 441, 575, 1001, 612, 743, 353, 649, 779, 154, 523, 964, 905, 507, 461, 1046, 626, 294, 981, 1025, 937, 752, 1186, 1032, 405, 953, 313, 58, 542, 1206, 544, 667, 335, 754, 1155, 1508, 1317, 900, 311, 70, 1292, 1184, 707, 439, 682, 1522, 77, 93, 1203, 174, 115, 854, 1125, 1149, 960, 1494, 74, 952, 866, 134, 902, 921, 653, 590, 1038, 238, 872, 1282, 1340, 650, 1297, 1357, 1455, 307, 788, 265, 378, 75, 614, 1265, 243, 454, 406, 1396, 1260, 264, 212, 790, 119, 1461, 1520, 911, 961, 195, 994, 359, 16, 602, 760, 692, 1045, 457, 670, 1302, 732, 1281, 802, 1385, 1460, 728, 714, 889, 1324, 61, 208, 1261, 878, 562, 629, 936, 1132, 574, 257, 1463, 217, 1171, 851, 490, 175, 736, 1462, 1294, 1432, 295, 1242, 1031, 1450, 745, 1192, 41, 1349, 411, 1248, 1227, 1298, 132, 1426, 1379, 348, 105, 705, 1156, 846, 674, 1291, 171, 1442, 381, 188], 'conv3': [1447, 129, 2500, 2420, 2056, 2414, 2181, 564, 370, 746, 398, 646, 35, 1720, 922, 2523, 199, 419, 696, 1843, 1755, 1025, 2317, 305, 1424, 2193, 1794, 1993, 583, 472, 1926, 534, 640, 1380, 1394, 708, 68, 1292, 1304, 533, 1198, 837, 2331, 1958, 653, 1409, 2085, 162, 1830, 658, 1175, 2447, 2000, 964, 1558, 1280, 436, 1992, 2259, 2237, 888, 1746, 1953, 1637, 1051, 2070, 1506, 1141, 2, 2093, 732, 299, 2069, 1608, 33, 1666, 2390, 828, 2151, 2536, 72, 1003, 2342, 2179, 1887, 1550, 454, 1873, 2547, 1909, 2022, 1001, 1123, 1248, 2057, 810, 34, 1403, 2176, 1443, 1824, 2453, 421, 2185, 1753, 1938, 638, 796, 1385, 1650, 2094, 1045, 561, 507, 720, 1831, 146, 1536, 210, 1457, 1139, 1689, 2460, 2095, 1162, 1524, 302, 800, 1451, 1449, 669, 1783, 365, 1245, 1406, 2174, 1931, 509, 674, 2545, 1056, 425, 1153, 1776, 63, 2283, 935, 586, 2300, 1376, 2162, 2060, 2305, 437, 1555, 1567, 2168, 283, 1283, 1415, 838, 80, 1332, 2112, 481, 1659, 1049, 1638, 43, 1221, 291, 340, 2322, 2272, 404, 450, 575, 1789, 1809, 867, 2394, 1572, 1986, 2515, 1970, 122, 1886, 2462, 491, 1929, 2320, 557, 2227, 1160, 1266, 225, 1960, 1845, 1548, 336, 1157, 1781, 1439, 1074, 2187, 2197, 792, 2285, 2511, 687, 167, 1021, 632, 788, 1864, 1799, 1435, 1591, 1458, 2208, 267, 13, 1088, 2008, 2252, 2387, 2161, 304, 895, 1512, 465, 968, 1199, 1271, 306, 1742, 505, 130, 220, 926, 781, 662, 1554, 855, 1733, 698, 1839, 2062, 499, 1052, 2081, 258, 2308, 357, 468, 1062, 24, 521, 1645, 1375, 768, 2121, 411, 2042, 616, 96, 1890, 242, 2418, 1196, 276, 2028, 2378, 467, 2173, 1066, 772, 366, 2079, 761, 1874, 1446, 362, 284, 1699, 884, 1226, 252, 1054, 543, 597, 1108, 2527, 2310, 1966, 1325, 2456, 203, 27, 959, 2192, 962, 992, 2050, 1253, 1531, 1227, 2518, 1961, 192, 124, 259, 6, 1827, 944, 239, 2157, 885, 2167, 1527, 576, 706, 399, 2484, 1971, 555, 943, 82, 1194, 1694, 1029, 2130, 1633, 1267, 107, 2293, 2508, 2532, 2256, 2011, 316, 1112, 240, 151, 2436, 634, 1206, 1549, 54, 2242, 300, 2309, 689, 1782, 899, 1448, 2113, 2030, 2033, 1107, 1647, 1894, 1968, 1646, 1670, 1326, 2164, 937, 298, 2441, 1138, 1578, 1811, 1622, 980, 1055, 1351, 1837, 2284, 331, 2446, 939, 1697, 1908, 2018], 'conv4': [1575, 1644, 315, 2388, 4, 1323, 1622, 1859, 2228, 2029, 263, 770, 1131, 1289, 2356, 544, 1187, 2123, 1193, 188, 2237, 1610, 1746, 1064, 264, 1908, 806, 1444, 2413, 891, 1580, 412, 1288, 1993, 1706, 2207, 1402, 1757, 1185, 1109, 493, 502, 2285, 2353, 2475, 778, 1143, 2537, 2008, 615, 67, 1524, 2142, 1795, 1366, 2070, 2041, 448, 2064, 1654, 1167, 514, 549, 1975, 309, 1732, 1019, 1957, 1266, 1629, 1481, 1855, 893, 34, 1067, 1326, 231, 2346, 919, 1819, 366, 1667, 2268, 1082, 1620, 132, 6, 1521, 929, 1428, 1867, 1969, 1631, 621, 2463, 1585, 774, 277, 551, 2254, 783, 1659, 1684, 2131, 1813, 603, 792, 556, 288, 1604, 1041, 2209, 2292, 1499, 2510, 1641, 1749, 805, 2262, 1682, 1967, 2517, 1668, 1857, 593, 1676, 2524, 969, 2038, 802, 489, 670, 435, 1063, 1988, 2286, 1062, 829, 2225, 230, 1285, 687, 437, 2183, 568, 673, 2158, 2518, 99, 2396, 851, 1084, 1152, 895, 711, 397, 1833, 1922, 8, 2015, 2461, 1603, 272, 292, 1066, 1244, 2143, 2523, 21, 586, 1214, 1439, 648, 703, 1734, 1727, 401, 433, 2014, 1874, 2385, 718, 294, 26, 675, 2218, 704, 1490, 1468, 1267, 931, 1191, 1251, 2439, 331, 1212, 592, 558, 59, 1756, 139, 1674, 2335, 2528, 2539, 1538, 2210, 2383, 1529, 389, 2267, 1787, 2163, 1657, 1726, 2338, 527, 843, 91, 1254, 2121, 386, 1498, 2359, 1877, 865, 507, 1739, 1190, 2022, 1134, 7, 936, 1434, 1412, 1828, 150, 2152, 2224, 1974, 862, 1870, 1672, 1035, 725, 1043, 1539, 297, 339, 1070, 1261, 2336, 2078, 1764, 889, 1541, 2300, 1898, 1801, 599, 1221, 1208, 381, 1286, 1996, 1399, 671, 1218, 1618, 1257, 1688, 1375, 1534, 1730, 674, 233, 1429, 1454, 2249, 2415, 536, 1778, 2046, 987, 1448, 553, 2543, 473, 1296, 1130, 1204, 641, 2133, 1883, 1535, 70, 1355, 1354, 2174, 1452, 1991, 2053, 1656, 997, 508, 2227, 1832, 2089, 1445, 612, 1652, 182, 1338, 1115, 1471, 1483, 2557, 175, 761, 1685, 1313, 2211, 2496, 510, 455, 1772, 2472, 1472, 1998, 1596, 894, 1308, 1104, 1018, 859, 734, 1624, 1034, 1536, 945, 1514, 1613, 1915, 33, 343, 1518, 2203, 1540, 1865, 65, 1276, 1201, 172, 656, 460, 2555, 1560, 2259, 1731, 2404, 405, 135, 203, 1766, 149, 225, 1928, 2195, 1612, 457, 253, 208, 518, 487, 258, 1885, 1357, 1567, 2213, 1990, 1784, 1485, 2149, 2257, 1422]}
activations_sum[k] len:  2560
activations_sum[k] 2560
activations_sum[k]:  [139, 2517, 93, 686, 1471, 530, 1657, 1640, 2523, 1571]
topk_kernels len:  5
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3', 'conv4']
topk_kernels:  {'conv0': [25, 59, 30, 55, 42, 69, 5, 68, 93, 16, 57, 14, 36, 38, 77], 'conv1': [197, 349, 72, 345, 121, 76, 124, 193, 109, 365, 274, 296, 354, 92, 73, 56, 75, 105, 36, 188, 202, 343, 336, 304, 162, 41, 266, 159, 100, 104, 163, 148, 22, 9, 186, 229, 190, 379, 4, 174, 358, 114, 118, 373, 146, 217, 278, 347, 70, 191, 169, 99, 50, 180, 137, 26, 94, 383, 324], 'conv2': [958, 733, 761, 250, 1436, 856, 14, 209, 1163, 176, 399, 801, 942, 95, 1532, 118, 622, 959, 1452, 1274, 324, 326, 835, 632, 639, 1172, 852, 1113, 35, 850, 730, 1486, 636, 869, 508, 155, 597, 173, 15, 423, 22, 312, 1278, 281, 12, 408, 783, 1159, 1345, 799, 746, 499, 933, 1126, 1178, 373, 395, 915, 868, 356, 773, 810, 1002, 923, 588, 580, 426, 519, 1238, 164, 1089, 42, 980, 441, 575, 1001, 612, 743, 353, 649, 779, 154, 523, 964, 905, 507, 461, 1046, 626, 294, 981, 1025, 937, 752, 1186, 1032, 405, 953, 313, 58, 542, 1206, 544, 667, 335, 754, 1155, 1508, 1317, 900, 311, 70, 1292, 1184, 707, 439, 682, 1522, 77, 93, 1203, 174, 115, 854, 1125, 1149, 960, 1494, 74, 952, 866, 134, 902, 921, 653, 590, 1038, 238, 872, 1282, 1340, 650, 1297, 1357, 1455, 307, 788, 265, 378, 75, 614, 1265, 243, 454, 406, 1396, 1260, 264, 212, 790, 119, 1461, 1520, 911, 961, 195, 994, 359, 16, 602, 760, 692, 1045, 457, 670, 1302, 732, 1281, 802, 1385, 1460, 728, 714, 889, 1324, 61, 208, 1261, 878, 562, 629, 936, 1132, 574, 257, 1463, 217, 1171, 851, 490, 175, 736, 1462, 1294, 1432, 295, 1242, 1031, 1450, 745, 1192, 41, 1349, 411, 1248, 1227, 1298, 132, 1426, 1379, 348, 105, 705, 1156, 846, 674, 1291, 171, 1442, 381, 188], 'conv3': [1447, 129, 2500, 2420, 2056, 2414, 2181, 564, 370, 746, 398, 646, 35, 1720, 922, 2523, 199, 419, 696, 1843, 1755, 1025, 2317, 305, 1424, 2193, 1794, 1993, 583, 472, 1926, 534, 640, 1380, 1394, 708, 68, 1292, 1304, 533, 1198, 837, 2331, 1958, 653, 1409, 2085, 162, 1830, 658, 1175, 2447, 2000, 964, 1558, 1280, 436, 1992, 2259, 2237, 888, 1746, 1953, 1637, 1051, 2070, 1506, 1141, 2, 2093, 732, 299, 2069, 1608, 33, 1666, 2390, 828, 2151, 2536, 72, 1003, 2342, 2179, 1887, 1550, 454, 1873, 2547, 1909, 2022, 1001, 1123, 1248, 2057, 810, 34, 1403, 2176, 1443, 1824, 2453, 421, 2185, 1753, 1938, 638, 796, 1385, 1650, 2094, 1045, 561, 507, 720, 1831, 146, 1536, 210, 1457, 1139, 1689, 2460, 2095, 1162, 1524, 302, 800, 1451, 1449, 669, 1783, 365, 1245, 1406, 2174, 1931, 509, 674, 2545, 1056, 425, 1153, 1776, 63, 2283, 935, 586, 2300, 1376, 2162, 2060, 2305, 437, 1555, 1567, 2168, 283, 1283, 1415, 838, 80, 1332, 2112, 481, 1659, 1049, 1638, 43, 1221, 291, 340, 2322, 2272, 404, 450, 575, 1789, 1809, 867, 2394, 1572, 1986, 2515, 1970, 122, 1886, 2462, 491, 1929, 2320, 557, 2227, 1160, 1266, 225, 1960, 1845, 1548, 336, 1157, 1781, 1439, 1074, 2187, 2197, 792, 2285, 2511, 687, 167, 1021, 632, 788, 1864, 1799, 1435, 1591, 1458, 2208, 267, 13, 1088, 2008, 2252, 2387, 2161, 304, 895, 1512, 465, 968, 1199, 1271, 306, 1742, 505, 130, 220, 926, 781, 662, 1554, 855, 1733, 698, 1839, 2062, 499, 1052, 2081, 258, 2308, 357, 468, 1062, 24, 521, 1645, 1375, 768, 2121, 411, 2042, 616, 96, 1890, 242, 2418, 1196, 276, 2028, 2378, 467, 2173, 1066, 772, 366, 2079, 761, 1874, 1446, 362, 284, 1699, 884, 1226, 252, 1054, 543, 597, 1108, 2527, 2310, 1966, 1325, 2456, 203, 27, 959, 2192, 962, 992, 2050, 1253, 1531, 1227, 2518, 1961, 192, 124, 259, 6, 1827, 944, 239, 2157, 885, 2167, 1527, 576, 706, 399, 2484, 1971, 555, 943, 82, 1194, 1694, 1029, 2130, 1633, 1267, 107, 2293, 2508, 2532, 2256, 2011, 316, 1112, 240, 151, 2436, 634, 1206, 1549, 54, 2242, 300, 2309, 689, 1782, 899, 1448, 2113, 2030, 2033, 1107, 1647, 1894, 1968, 1646, 1670, 1326, 2164, 937, 298, 2441, 1138, 1578, 1811, 1622, 980, 1055, 1351, 1837, 2284, 331, 2446, 939, 1697, 1908, 2018], 'conv4': [139, 2517, 93, 686, 1471, 530, 1657, 1640, 2523, 1571, 1262, 888, 2015, 402, 494, 400, 150, 825, 797, 1877, 653, 2265, 1504, 2188, 625, 1375, 1565, 2326, 1556, 768, 1489, 1250, 654, 1306, 439, 1016, 282, 782, 120, 233, 1597, 2078, 1134, 2507, 959, 1406, 15, 2415, 126, 1387, 2335, 864, 1676, 30, 1548, 172, 1100, 388, 1092, 1010, 1285, 1730, 455, 1987, 1215, 792, 826, 2035, 557, 1719, 607, 660, 1390, 2385, 1269, 1995, 1118, 424, 382, 1477, 412, 2404, 2118, 1152, 627, 798, 2315, 612, 1869, 1397, 2368, 74, 2285, 257, 941, 1167, 350, 616, 2223, 571, 2226, 2224, 838, 929, 2171, 1937, 591, 2334, 725, 735, 2531, 1854, 975, 1892, 4, 1908, 1852, 486, 1639, 1431, 1169, 889, 1140, 287, 2559, 731, 2101, 161, 1867, 1555, 1912, 606, 565, 1156, 1572, 1080, 2141, 605, 966, 883, 1424, 1700, 757, 296, 1138, 703, 674, 2186, 2271, 1707, 1806, 2038, 1518, 1698, 1587, 2558, 6, 145, 2121, 401, 1759, 5, 748, 851, 1338, 1075, 246, 2457, 63, 1583, 1223, 80, 581, 1989, 3, 770, 1524, 1831, 476, 24, 1362, 910, 2311, 460, 2006, 1013, 1513, 114, 1582, 2509, 859, 1230, 1819, 531, 1625, 347, 1957, 2207, 426, 1883, 1113, 1718, 539, 1415, 1359, 593, 520, 2328, 1241, 1946, 808, 383, 2211, 1723, 2485, 844, 863, 1441, 2492, 881, 1366, 1226, 214, 2337, 2268, 2261, 562, 1844, 2248, 361, 564, 1034, 270, 1512, 1323, 1795, 1836, 2149, 1272, 1015, 578, 2218, 1970, 1887, 381, 1645, 271, 743, 718, 1840, 1658, 990, 69, 2554, 2355, 789, 949, 922, 1263, 309, 2172, 1040, 2049, 1696, 1019, 1620, 1898, 1574, 2133, 1143, 40, 1847, 34, 2475, 1198, 2487, 2506, 1439, 54, 36, 2199, 2299, 1289, 2257, 1117, 2418, 1025, 1893, 499, 778, 364, 1654, 82, 945, 2524, 1713, 372, 239, 1545, 2264, 928, 1779, 1811, 704, 689, 2044, 470, 1981, 1172, 137, 2045, 1246, 1705, 1820, 2012, 2495, 1634, 1522, 2322, 1239, 746, 978, 992, 525, 26, 2425, 646, 1420, 1740, 2258, 2538, 832, 1526, 648, 1641, 284, 1660, 1931, 1445, 2246, 1787, 335, 2138, 777, 1558, 1868, 29, 895, 344, 33, 2053, 1460, 2276, 741, 1951, 1098, 2459, 790, 997, 1499, 722, 2455, 2518, 515, 1998, 1474, 2134, 874, 2510, 536, 2163, 1224, 1007, 2117, 664, 1686, 552, 411, 2184, 1914, 784, 2323, 1491, 1035, 712]}
activations_sum[k] len:  2560
['conv0', 'conv1', 'conv2', 'conv3', 'conv4']
delta_weights INFO: 
NUM OF TRACKED COV LAYERS:  5
NUM OF TRACKED WEIGHTS CHANGES PER LAYER:  2
avg_deltas INFO:  <class 'dict'>
avg_deltas keys:  ['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight', 'blocks.4.layer.weight']
################################################
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 4.80e-02	time: 00:00:20	Acc_train 0.00	Acc_test 0.00	convergence: 2.33e+01	R1: 0	Info MB:0.000e+00/SB:0.000e+00/MW:1.562e-07/SW:1.608e-01/MR:2.433e+01/SR:1.996e+00/MeD:1.207e+00/MaD:2.229e+01/MW:0.583/MAW:0.417
|        0 |        1 |        2 |        3 |        4 |        5 |        6 |        7 |        8 |        9 |       10 |       11 |       12 |      13 |       14 |       15 |       16 |       17 |       18 |       19 |       20 |       21 |       22 |       23 |       24 |       25 |       26 |      27 |       28 |       29 |
|----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------|
|   0.0486 |   0.0473 |   0.0498 |   0.0494 |   0.0491 |   0.0441 |   0.0485 |   0.0487 |   0.0481 |   0.0455 |   0.0482 |   0.0499 |   0.0502 |   0.052 |   0.0435 |   0.0481 |   0.0488 |   0.0487 |   0.0493 |   0.0492 |   0.0496 |   0.0506 |   0.0489 |   0.0481 |   0.0492 |   0.0487 |   0.0493 |   0.049 |   0.0493 |   0.0492 |
|  24.65   |  23.33   |  25.81   |  25.36   |  25.15   |  20.48   |  24.48   |  24.73   |  24.12   |  21.7    |  24.27   |  25.87   |  26.18   |  28.07  |  19.9    |  24.13   |  24.83   |  24.74   |  25.34   |  25.19   |  25.64   |  26.59   |  24.87   |  24.09   |  25.25   |  24.68   |  25.34   |  25.04  |  25.28   |  25.16   |
|   0.07   |   0.12   |   0.01   |   0.01   |   0.03   |   0.12   |   0.02   |   0.07   |   0.03   |   0.12   |   0.02   |   0.01   |   0.02   |   0.09  |   0.19   |   0.01   |   0.01   |   0      |   0.02   |   0      |   0.01   |   0.03   |   0.01   |   0.03   |   0      |   0.01   |   0.01   |   0     |   0.05   |   0.01   |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:-4.275e-05/SW:2.050e-01/MR:2.404e+01/SR:1.806e+00/MeD:1.293e+00/MaD:1.506e+01/MW:0.546/MAW:0.454
|       0 |        1 |        2 |        3 |        4 |        5 |        6 |        7 |       8 |       9 |       10 |       11 |       12 |       13 |       14 |       15 |     16 |       17 |       18 |       19 |       20 |       21 |       22 |       23 |       24 |       25 |      26 |       27 |       28 |       29 |
|---------+----------+----------+----------+----------+----------+----------+----------+---------+---------+----------+----------+----------+----------+----------+----------+--------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------|
|   0.044 |   0.0499 |   0.0486 |   0.0491 |   0.0482 |   0.0485 |   0.0506 |   0.0484 |   0.039 |   0.049 |   0.0478 |   0.0462 |   0.0496 |   0.0496 |   0.0495 |   0.0495 |   0.05 |   0.0491 |   0.0475 |   0.0496 |   0.0487 |   0.0459 |   0.0488 |   0.0501 |   0.0491 |   0.0452 |   0.049 |   0.0485 |   0.0488 |   0.0482 |
|  20.34  |  25.94   |  24.62   |  25.15   |  24.28   |  24.49   |  26.6    |  24.47   |  16.21  |  25.03  |  23.82   |  22.35   |  25.56   |  25.63   |  25.55   |  25.49   |  26.03 |  25.06   |  23.54   |  25.64   |  24.67   |  22.09   |  24.78   |  26.07   |  25.08   |  21.46   |  25.02  |  24.51   |  24.81   |  24.23   |
|   0.08  |   0.05   |   0.04   |   0.03   |   0.02   |   0.01   |   0.05   |   0.01   |   0.04  |   0     |   0.03   |   0.04   |   0.01   |   0.01   |   0.01   |   0.02   |   0.02 |   0.02   |   0.02   |   0.01   |   0.02   |   0.05   |   0.01   |   0.01   |   0.02   |   0.02   |   0.05  |   0.02   |   0.03   |   0.04   |
| nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan    | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      |
| nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan    | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      |
| nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan    | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:8.184e-05/SW:4.113e-01/MR:2.411e+01/SR:1.904e+00/MeD:1.448e+00/MaD:8.701e+00/MW:0.516/MAW:0.484
|        0 |        1 |        2 |        3 |        4 |        5 |        6 |        7 |        8 |       9 |       10 |       11 |       12 |       13 |       14 |       15 |       16 |       17 |      18 |       19 |       20 |       21 |       22 |      23 |       24 |      25 |       26 |      27 |       28 |       29 |
|----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------+---------+----------+---------+----------+---------+----------+----------|
|   0.0492 |   0.0487 |   0.0493 |   0.0463 |   0.0471 |   0.0486 |   0.0494 |   0.0438 |   0.0482 |   0.049 |   0.0486 |   0.0498 |   0.0501 |   0.0441 |   0.0511 |   0.0479 |   0.0451 |   0.0496 |   0.048 |   0.0493 |   0.0419 |   0.0491 |   0.0501 |   0.048 |   0.0476 |   0.049 |   0.0492 |   0.047 |   0.0447 |   0.0482 |
|  25.23   |  24.71   |  25.33   |  22.46   |  23.18   |  24.59   |  25.45   |  20.18   |  24.25   |  25     |  24.66   |  25.83   |  26.09   |  20.42   |  27.09   |  23.95   |  21.32   |  25.64   |  24.04  |  25.34   |  18.58   |  25.09   |  26.06   |  24.07  |  23.63   |  25.01  |  25.17   |  23.13  |  20.94   |  24.28   |
|   0      |   0.03   |   0.07   |   0.06   |   0.05   |   0.04   |   0.01   |   0.07   |   0.04   |   0.01  |   0.04   |   0.1    |   0.11   |   0.09   |   0.26   |   0.06   |   0.19   |   0.01   |   0.06  |   0.01   |   0.27   |   0.03   |   0.16   |   0.06  |   0.04   |   0.04  |   0.03   |   0.05  |   0.19   |   0.03   |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan     | nan      | nan     | nan      | nan     | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan     | nan      | nan     | nan      | nan     | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan     | nan      | nan     | nan      | nan     | nan      | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:-3.220e-03/SW:7.867e-01/MR:2.303e+01/SR:2.139e+00/MeD:1.650e+00/MaD:8.413e+00/MW:0.788/MAW:0.212
|        0 |        1 |        2 |        3 |        4 |        5 |        6 |        7 |        8 |        9 |       10 |       11 |       12 |       13 |       14 |      15 |       16 |       17 |       18 |       19 |       20 |       21 |       22 |       23 |       24 |       25 |       26 |       27 |       28 |      29 |
|----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------|
|   0.0241 |   0.0245 |   0.0242 |   0.0241 |   0.0195 |   0.0221 |   0.0244 |   0.0246 |   0.0243 |   0.0231 |   0.0248 |   0.0239 |   0.0244 |   0.0243 |   0.0242 |   0.024 |   0.0236 |   0.0235 |   0.0236 |   0.0236 |   0.0233 |   0.0248 |   0.0235 |   0.0232 |   0.0238 |   0.0236 |   0.0227 |   0.0238 |   0.0229 |   0.025 |
|  24.15   |  24.98   |  24.42   |  24.22   |  16.28   |  20.46   |  24.88   |  25.21   |  24.68   |  22.39   |  25.58   |  23.89   |  24.84   |  24.56   |  24.38   |  24.11  |  23.31   |  23.07   |  23.2    |  23.2    |  22.78   |  25.68   |  23.05   |  22.6    |  23.72   |  23.18   |  21.61   |  23.73   |  21.91   |  25.96  |
|   0.17   |   0.03   |   0.02   |   0.03   |   0.83   |   0.52   |   0.1    |   0.12   |   0.04   |   0.26   |   0.01   |   0.06   |   0.14   |   0.13   |   0.02   |   0.07  |   0.13   |   0.72   |   0.17   |   0.29   |   0.15   |   0.01   |   0.4    |   0.45   |   0.15   |   0.18   |   0.54   |   0.08   |   0.31   |   0.04  |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:-6.317e-03/SW:9.661e-01/MR:8.228e+00/SR:1.520e+00/MeD:1.209e+00/MaD:4.454e+00/MW:0.589/MAW:0.411
|       0 |       1 |       2 |       3 |       4 |       5 |       6 |       7 |       8 |       9 |      10 |      11 |      12 |      13 |      14 |      15 |     16 |      17 |      18 |      19 |      20 |      21 |      22 |      23 |      24 |      25 |      26 |      27 |     28 |      29 |
|---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------+---------|
|   0.192 |   0.253 |   0.249 |   0.236 |   0.211 |   0.192 |   0.201 |   0.251 |   0.196 |   0.273 |   0.191 |   0.205 |   0.216 |   0.233 |   0.202 |   0.242 |   0.18 |   0.211 |   0.218 |   0.209 |   0.232 |   0.215 |   0.225 |   0.254 |   0.244 |   0.213 |   0.216 |   0.214 |   0.21 |   0.256 |
|   6.74  |  11     |  10.67  |   9.69  |   7.99  |   6.78  |   7.29  |  10.85  |   7     |  12.68  |   6.7   |   7.59  |   8.29  |   9.5   |   7.37  |  10.16  |   6.05 |   7.95  |   8.41  |   7.83  |   9.38  |   8.23  |   8.88  |  11.11  |  10.32  |   8.06  |   8.3   |   8.14  |   7.9  |  11.27  |
|   0.63  |   0.51  |   0.39  |   0.55  |   2.32  |   3.84  |   0.74  |   0.52  |   0.83  |   0.42  |   0.66  |   0.63  |   0.58  |   0.52  |   1.4   |   0.48  |   2.27 |   0.58  |   0.59  |   0.83  |   0.57  |   0.79  |   0.55  |   0.57  |   0.49  |   9.53  |   0.9   |   0.55  |   0.55 |   0.49  |
| nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     |
| nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     |
| nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [25, 59, 30, 55, 42, 69, 5, 68, 93, 16, 57, 14, 36, 38, 77], 'conv1': [197, 349, 72, 345, 121, 76, 124, 193, 109, 365, 274, 296, 354, 92, 73, 56, 75, 105, 36, 188, 202, 343, 336, 304, 162, 41, 266, 159, 100, 104, 163, 148, 22, 9, 186, 229, 190, 379, 4, 174, 358, 114, 118, 373, 146, 217, 278, 347, 70, 191, 169, 99, 50, 180, 137, 26, 94, 383, 324], 'conv2': [958, 733, 761, 250, 1436, 856, 14, 209, 1163, 176, 399, 801, 942, 95, 1532, 118, 622, 959, 1452, 1274, 324, 326, 835, 632, 639, 1172, 852, 1113, 35, 850, 730, 1486, 636, 869, 508, 155, 597, 173, 15, 423, 22, 312, 1278, 281, 12, 408, 783, 1159, 1345, 799, 746, 499, 933, 1126, 1178, 373, 395, 915, 868, 356, 773, 810, 1002, 923, 588, 580, 426, 519, 1238, 164, 1089, 42, 980, 441, 575, 1001, 612, 743, 353, 649, 779, 154, 523, 964, 905, 507, 461, 1046, 626, 294, 981, 1025, 937, 752, 1186, 1032, 405, 953, 313, 58, 542, 1206, 544, 667, 335, 754, 1155, 1508, 1317, 900, 311, 70, 1292, 1184, 707, 439, 682, 1522, 77, 93, 1203, 174, 115, 854, 1125, 1149, 960, 1494, 74, 952, 866, 134, 902, 921, 653, 590, 1038, 238, 872, 1282, 1340, 650, 1297, 1357, 1455, 307, 788, 265, 378, 75, 614, 1265, 243, 454, 406, 1396, 1260, 264, 212, 790, 119, 1461, 1520, 911, 961, 195, 994, 359, 16, 602, 760, 692, 1045, 457, 670, 1302, 732, 1281, 802, 1385, 1460, 728, 714, 889, 1324, 61, 208, 1261, 878, 562, 629, 936, 1132, 574, 257, 1463, 217, 1171, 851, 490, 175, 736, 1462, 1294, 1432, 295, 1242, 1031, 1450, 745, 1192, 41, 1349, 411, 1248, 1227, 1298, 132, 1426, 1379, 348, 105, 705, 1156, 846, 674, 1291, 171, 1442, 381, 188], 'conv3': [1447, 129, 2500, 2420, 2056, 2414, 2181, 564, 370, 746, 398, 646, 35, 1720, 922, 2523, 199, 419, 696, 1843, 1755, 1025, 2317, 305, 1424, 2193, 1794, 1993, 583, 472, 1926, 534, 640, 1380, 1394, 708, 68, 1292, 1304, 533, 1198, 837, 2331, 1958, 653, 1409, 2085, 162, 1830, 658, 1175, 2447, 2000, 964, 1558, 1280, 436, 1992, 2259, 2237, 888, 1746, 1953, 1637, 1051, 2070, 1506, 1141, 2, 2093, 732, 299, 2069, 1608, 33, 1666, 2390, 828, 2151, 2536, 72, 1003, 2342, 2179, 1887, 1550, 454, 1873, 2547, 1909, 2022, 1001, 1123, 1248, 2057, 810, 34, 1403, 2176, 1443, 1824, 2453, 421, 2185, 1753, 1938, 638, 796, 1385, 1650, 2094, 1045, 561, 507, 720, 1831, 146, 1536, 210, 1457, 1139, 1689, 2460, 2095, 1162, 1524, 302, 800, 1451, 1449, 669, 1783, 365, 1245, 1406, 2174, 1931, 509, 674, 2545, 1056, 425, 1153, 1776, 63, 2283, 935, 586, 2300, 1376, 2162, 2060, 2305, 437, 1555, 1567, 2168, 283, 1283, 1415, 838, 80, 1332, 2112, 481, 1659, 1049, 1638, 43, 1221, 291, 340, 2322, 2272, 404, 450, 575, 1789, 1809, 867, 2394, 1572, 1986, 2515, 1970, 122, 1886, 2462, 491, 1929, 2320, 557, 2227, 1160, 1266, 225, 1960, 1845, 1548, 336, 1157, 1781, 1439, 1074, 2187, 2197, 792, 2285, 2511, 687, 167, 1021, 632, 788, 1864, 1799, 1435, 1591, 1458, 2208, 267, 13, 1088, 2008, 2252, 2387, 2161, 304, 895, 1512, 465, 968, 1199, 1271, 306, 1742, 505, 130, 220, 926, 781, 662, 1554, 855, 1733, 698, 1839, 2062, 499, 1052, 2081, 258, 2308, 357, 468, 1062, 24, 521, 1645, 1375, 768, 2121, 411, 2042, 616, 96, 1890, 242, 2418, 1196, 276, 2028, 2378, 467, 2173, 1066, 772, 366, 2079, 761, 1874, 1446, 362, 284, 1699, 884, 1226, 252, 1054, 543, 597, 1108, 2527, 2310, 1966, 1325, 2456, 203, 27, 959, 2192, 962, 992, 2050, 1253, 1531, 1227, 2518, 1961, 192, 124, 259, 6, 1827, 944, 239, 2157, 885, 2167, 1527, 576, 706, 399, 2484, 1971, 555, 943, 82, 1194, 1694, 1029, 2130, 1633, 1267, 107, 2293, 2508, 2532, 2256, 2011, 316, 1112, 240, 151, 2436, 634, 1206, 1549, 54, 2242, 300, 2309, 689, 1782, 899, 1448, 2113, 2030, 2033, 1107, 1647, 1894, 1968, 1646, 1670, 1326, 2164, 937, 298, 2441, 1138, 1578, 1811, 1622, 980, 1055, 1351, 1837, 2284, 331, 2446, 939, 1697, 1908, 2018], 'conv4': [139, 2517, 93, 686, 1471, 530, 1657, 1640, 2523, 1571, 1262, 888, 2015, 402, 494, 400, 150, 825, 797, 1877, 653, 2265, 1504, 2188, 625, 1375, 1565, 2326, 1556, 768, 1489, 1250, 654, 1306, 439, 1016, 282, 782, 120, 233, 1597, 2078, 1134, 2507, 959, 1406, 15, 2415, 126, 1387, 2335, 864, 1676, 30, 1548, 172, 1100, 388, 1092, 1010, 1285, 1730, 455, 1987, 1215, 792, 826, 2035, 557, 1719, 607, 660, 1390, 2385, 1269, 1995, 1118, 424, 382, 1477, 412, 2404, 2118, 1152, 627, 798, 2315, 612, 1869, 1397, 2368, 74, 2285, 257, 941, 1167, 350, 616, 2223, 571, 2226, 2224, 838, 929, 2171, 1937, 591, 2334, 725, 735, 2531, 1854, 975, 1892, 4, 1908, 1852, 486, 1639, 1431, 1169, 889, 1140, 287, 2559, 731, 2101, 161, 1867, 1555, 1912, 606, 565, 1156, 1572, 1080, 2141, 605, 966, 883, 1424, 1700, 757, 296, 1138, 703, 674, 2186, 2271, 1707, 1806, 2038, 1518, 1698, 1587, 2558, 6, 145, 2121, 401, 1759, 5, 748, 851, 1338, 1075, 246, 2457, 63, 1583, 1223, 80, 581, 1989, 3, 770, 1524, 1831, 476, 24, 1362, 910, 2311, 460, 2006, 1013, 1513, 114, 1582, 2509, 859, 1230, 1819, 531, 1625, 347, 1957, 2207, 426, 1883, 1113, 1718, 539, 1415, 1359, 593, 520, 2328, 1241, 1946, 808, 383, 2211, 1723, 2485, 844, 863, 1441, 2492, 881, 1366, 1226, 214, 2337, 2268, 2261, 562, 1844, 2248, 361, 564, 1034, 270, 1512, 1323, 1795, 1836, 2149, 1272, 1015, 578, 2218, 1970, 1887, 381, 1645, 271, 743, 718, 1840, 1658, 990, 69, 2554, 2355, 789, 949, 922, 1263, 309, 2172, 1040, 2049, 1696, 1019, 1620, 1898, 1574, 2133, 1143, 40, 1847, 34, 2475, 1198, 2487, 2506, 1439, 54, 36, 2199, 2299, 1289, 2257, 1117, 2418, 1025, 1893, 499, 778, 364, 1654, 82, 945, 2524, 1713, 372, 239, 1545, 2264, 928, 1779, 1811, 704, 689, 2044, 470, 1981, 1172, 137, 2045, 1246, 1705, 1820, 2012, 2495, 1634, 1522, 2322, 1239, 746, 978, 992, 525, 26, 2425, 646, 1420, 1740, 2258, 2538, 832, 1526, 648, 1641, 284, 1660, 1931, 1445, 2246, 1787, 335, 2138, 777, 1558, 1868, 29, 895, 344, 33, 2053, 1460, 2276, 741, 1951, 1098, 2459, 790, 997, 1499, 722, 2455, 2518, 515, 1998, 1474, 2134, 874, 2510, 536, 2163, 1224, 1007, 2117, 664, 1686, 552, 411, 2184, 1914, 784, 2323, 1491, 1035, 712]}
SAVED model.heads_thresh 0.9
CONFIG MODE:  supervised

 ********** Supervised learning of blocks [5] **********
SAVING FOLDER FOR SUP:  C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([49, 33, 72, 51, 71, 92, 15, 14, 23,  0, 71, 75, 81, 69, 40, 43, 92, 97,
        70, 53], device='cuda:0')
TARGETS AFTER SUB:  tensor([92, 92, 35, 92, 35, 92, 92, 92, 35, 35, 35, 35, 92, 92, 35, 35, 35, 35,
        92, 92])
[35, 92]
TARGETS AFTER CLEANER:  tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1])
200
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 200
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               None
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([19, 29,  0, 11,  1, 86, 90, 28, 23, 31, 39, 96, 82, 17, 71, 39,  8, 97,
        80, 71], device='cuda:0')
TARGETS AFTER SUB:  tensor([35, 92, 35, 35, 92, 35, 92, 35, 92, 35, 92, 92, 35, 35, 92, 35, 35, 92,
        35, 35])
[35, 92]
TARGETS AFTER CLEANER:  tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0])
1000
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 1000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               None
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([19, 29,  0, 11,  1, 86, 90, 28, 23, 31, 39, 96, 82, 17, 71, 39,  8, 97,
        80, 71], device='cuda:0')
TARGETS AFTER SUB:  tensor([35, 92, 35, 35, 92, 35, 92, 35, 92, 35, 92, 92, 35, 35, 92, 35, 35, 92,
        35, 35])
[35, 92]
TARGETS AFTER CLEANER:  tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0])
1000
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 1000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
------------------------
INDICES:  1000
IMAGE SIZE: torch.Size([64, 3, 32, 32])
num_blocks:  6
Epoch: [1/50]	lr: 1.00e-03	time: 00:00:29	Loss_train 0.07271	Acc_train 69.40	/	Loss_test 0.01178	Acc_test 83.00
BLOCKS IN SUP:  [5]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [25, 59, 30, 55, 42, 69, 5, 68, 93, 16, 57, 14, 36, 38, 77], 'conv1': [197, 349, 72, 345, 121, 76, 124, 193, 109, 365, 274, 296, 354, 92, 73, 56, 75, 105, 36, 188, 202, 343, 336, 304, 162, 41, 266, 159, 100, 104, 163, 148, 22, 9, 186, 229, 190, 379, 4, 174, 358, 114, 118, 373, 146, 217, 278, 347, 70, 191, 169, 99, 50, 180, 137, 26, 94, 383, 324], 'conv2': [958, 733, 761, 250, 1436, 856, 14, 209, 1163, 176, 399, 801, 942, 95, 1532, 118, 622, 959, 1452, 1274, 324, 326, 835, 632, 639, 1172, 852, 1113, 35, 850, 730, 1486, 636, 869, 508, 155, 597, 173, 15, 423, 22, 312, 1278, 281, 12, 408, 783, 1159, 1345, 799, 746, 499, 933, 1126, 1178, 373, 395, 915, 868, 356, 773, 810, 1002, 923, 588, 580, 426, 519, 1238, 164, 1089, 42, 980, 441, 575, 1001, 612, 743, 353, 649, 779, 154, 523, 964, 905, 507, 461, 1046, 626, 294, 981, 1025, 937, 752, 1186, 1032, 405, 953, 313, 58, 542, 1206, 544, 667, 335, 754, 1155, 1508, 1317, 900, 311, 70, 1292, 1184, 707, 439, 682, 1522, 77, 93, 1203, 174, 115, 854, 1125, 1149, 960, 1494, 74, 952, 866, 134, 902, 921, 653, 590, 1038, 238, 872, 1282, 1340, 650, 1297, 1357, 1455, 307, 788, 265, 378, 75, 614, 1265, 243, 454, 406, 1396, 1260, 264, 212, 790, 119, 1461, 1520, 911, 961, 195, 994, 359, 16, 602, 760, 692, 1045, 457, 670, 1302, 732, 1281, 802, 1385, 1460, 728, 714, 889, 1324, 61, 208, 1261, 878, 562, 629, 936, 1132, 574, 257, 1463, 217, 1171, 851, 490, 175, 736, 1462, 1294, 1432, 295, 1242, 1031, 1450, 745, 1192, 41, 1349, 411, 1248, 1227, 1298, 132, 1426, 1379, 348, 105, 705, 1156, 846, 674, 1291, 171, 1442, 381, 188], 'conv3': [1447, 129, 2500, 2420, 2056, 2414, 2181, 564, 370, 746, 398, 646, 35, 1720, 922, 2523, 199, 419, 696, 1843, 1755, 1025, 2317, 305, 1424, 2193, 1794, 1993, 583, 472, 1926, 534, 640, 1380, 1394, 708, 68, 1292, 1304, 533, 1198, 837, 2331, 1958, 653, 1409, 2085, 162, 1830, 658, 1175, 2447, 2000, 964, 1558, 1280, 436, 1992, 2259, 2237, 888, 1746, 1953, 1637, 1051, 2070, 1506, 1141, 2, 2093, 732, 299, 2069, 1608, 33, 1666, 2390, 828, 2151, 2536, 72, 1003, 2342, 2179, 1887, 1550, 454, 1873, 2547, 1909, 2022, 1001, 1123, 1248, 2057, 810, 34, 1403, 2176, 1443, 1824, 2453, 421, 2185, 1753, 1938, 638, 796, 1385, 1650, 2094, 1045, 561, 507, 720, 1831, 146, 1536, 210, 1457, 1139, 1689, 2460, 2095, 1162, 1524, 302, 800, 1451, 1449, 669, 1783, 365, 1245, 1406, 2174, 1931, 509, 674, 2545, 1056, 425, 1153, 1776, 63, 2283, 935, 586, 2300, 1376, 2162, 2060, 2305, 437, 1555, 1567, 2168, 283, 1283, 1415, 838, 80, 1332, 2112, 481, 1659, 1049, 1638, 43, 1221, 291, 340, 2322, 2272, 404, 450, 575, 1789, 1809, 867, 2394, 1572, 1986, 2515, 1970, 122, 1886, 2462, 491, 1929, 2320, 557, 2227, 1160, 1266, 225, 1960, 1845, 1548, 336, 1157, 1781, 1439, 1074, 2187, 2197, 792, 2285, 2511, 687, 167, 1021, 632, 788, 1864, 1799, 1435, 1591, 1458, 2208, 267, 13, 1088, 2008, 2252, 2387, 2161, 304, 895, 1512, 465, 968, 1199, 1271, 306, 1742, 505, 130, 220, 926, 781, 662, 1554, 855, 1733, 698, 1839, 2062, 499, 1052, 2081, 258, 2308, 357, 468, 1062, 24, 521, 1645, 1375, 768, 2121, 411, 2042, 616, 96, 1890, 242, 2418, 1196, 276, 2028, 2378, 467, 2173, 1066, 772, 366, 2079, 761, 1874, 1446, 362, 284, 1699, 884, 1226, 252, 1054, 543, 597, 1108, 2527, 2310, 1966, 1325, 2456, 203, 27, 959, 2192, 962, 992, 2050, 1253, 1531, 1227, 2518, 1961, 192, 124, 259, 6, 1827, 944, 239, 2157, 885, 2167, 1527, 576, 706, 399, 2484, 1971, 555, 943, 82, 1194, 1694, 1029, 2130, 1633, 1267, 107, 2293, 2508, 2532, 2256, 2011, 316, 1112, 240, 151, 2436, 634, 1206, 1549, 54, 2242, 300, 2309, 689, 1782, 899, 1448, 2113, 2030, 2033, 1107, 1647, 1894, 1968, 1646, 1670, 1326, 2164, 937, 298, 2441, 1138, 1578, 1811, 1622, 980, 1055, 1351, 1837, 2284, 331, 2446, 939, 1697, 1908, 2018], 'conv4': [139, 2517, 93, 686, 1471, 530, 1657, 1640, 2523, 1571, 1262, 888, 2015, 402, 494, 400, 150, 825, 797, 1877, 653, 2265, 1504, 2188, 625, 1375, 1565, 2326, 1556, 768, 1489, 1250, 654, 1306, 439, 1016, 282, 782, 120, 233, 1597, 2078, 1134, 2507, 959, 1406, 15, 2415, 126, 1387, 2335, 864, 1676, 30, 1548, 172, 1100, 388, 1092, 1010, 1285, 1730, 455, 1987, 1215, 792, 826, 2035, 557, 1719, 607, 660, 1390, 2385, 1269, 1995, 1118, 424, 382, 1477, 412, 2404, 2118, 1152, 627, 798, 2315, 612, 1869, 1397, 2368, 74, 2285, 257, 941, 1167, 350, 616, 2223, 571, 2226, 2224, 838, 929, 2171, 1937, 591, 2334, 725, 735, 2531, 1854, 975, 1892, 4, 1908, 1852, 486, 1639, 1431, 1169, 889, 1140, 287, 2559, 731, 2101, 161, 1867, 1555, 1912, 606, 565, 1156, 1572, 1080, 2141, 605, 966, 883, 1424, 1700, 757, 296, 1138, 703, 674, 2186, 2271, 1707, 1806, 2038, 1518, 1698, 1587, 2558, 6, 145, 2121, 401, 1759, 5, 748, 851, 1338, 1075, 246, 2457, 63, 1583, 1223, 80, 581, 1989, 3, 770, 1524, 1831, 476, 24, 1362, 910, 2311, 460, 2006, 1013, 1513, 114, 1582, 2509, 859, 1230, 1819, 531, 1625, 347, 1957, 2207, 426, 1883, 1113, 1718, 539, 1415, 1359, 593, 520, 2328, 1241, 1946, 808, 383, 2211, 1723, 2485, 844, 863, 1441, 2492, 881, 1366, 1226, 214, 2337, 2268, 2261, 562, 1844, 2248, 361, 564, 1034, 270, 1512, 1323, 1795, 1836, 2149, 1272, 1015, 578, 2218, 1970, 1887, 381, 1645, 271, 743, 718, 1840, 1658, 990, 69, 2554, 2355, 789, 949, 922, 1263, 309, 2172, 1040, 2049, 1696, 1019, 1620, 1898, 1574, 2133, 1143, 40, 1847, 34, 2475, 1198, 2487, 2506, 1439, 54, 36, 2199, 2299, 1289, 2257, 1117, 2418, 1025, 1893, 499, 778, 364, 1654, 82, 945, 2524, 1713, 372, 239, 1545, 2264, 928, 1779, 1811, 704, 689, 2044, 470, 1981, 1172, 137, 2045, 1246, 1705, 1820, 2012, 2495, 1634, 1522, 2322, 1239, 746, 978, 992, 525, 26, 2425, 646, 1420, 1740, 2258, 2538, 832, 1526, 648, 1641, 284, 1660, 1931, 1445, 2246, 1787, 335, 2138, 777, 1558, 1868, 29, 895, 344, 33, 2053, 1460, 2276, 741, 1951, 1098, 2459, 790, 997, 1499, 722, 2455, 2518, 515, 1998, 1474, 2134, 874, 2510, 536, 2163, 1224, 1007, 2117, 664, 1686, 552, 411, 2184, 1914, 784, 2323, 1491, 1035, 712]}
SAVED model.heads_thresh 0.9
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
Epoch: [10/50]	lr: 1.00e-03	time: 00:00:31	Loss_train 0.02117	Acc_train 87.93	/	Loss_test 0.01091	Acc_test 87.00
BLOCKS IN SUP:  [5]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [25, 59, 30, 55, 42, 69, 5, 68, 93, 16, 57, 14, 36, 38, 77], 'conv1': [197, 349, 72, 345, 121, 76, 124, 193, 109, 365, 274, 296, 354, 92, 73, 56, 75, 105, 36, 188, 202, 343, 336, 304, 162, 41, 266, 159, 100, 104, 163, 148, 22, 9, 186, 229, 190, 379, 4, 174, 358, 114, 118, 373, 146, 217, 278, 347, 70, 191, 169, 99, 50, 180, 137, 26, 94, 383, 324], 'conv2': [958, 733, 761, 250, 1436, 856, 14, 209, 1163, 176, 399, 801, 942, 95, 1532, 118, 622, 959, 1452, 1274, 324, 326, 835, 632, 639, 1172, 852, 1113, 35, 850, 730, 1486, 636, 869, 508, 155, 597, 173, 15, 423, 22, 312, 1278, 281, 12, 408, 783, 1159, 1345, 799, 746, 499, 933, 1126, 1178, 373, 395, 915, 868, 356, 773, 810, 1002, 923, 588, 580, 426, 519, 1238, 164, 1089, 42, 980, 441, 575, 1001, 612, 743, 353, 649, 779, 154, 523, 964, 905, 507, 461, 1046, 626, 294, 981, 1025, 937, 752, 1186, 1032, 405, 953, 313, 58, 542, 1206, 544, 667, 335, 754, 1155, 1508, 1317, 900, 311, 70, 1292, 1184, 707, 439, 682, 1522, 77, 93, 1203, 174, 115, 854, 1125, 1149, 960, 1494, 74, 952, 866, 134, 902, 921, 653, 590, 1038, 238, 872, 1282, 1340, 650, 1297, 1357, 1455, 307, 788, 265, 378, 75, 614, 1265, 243, 454, 406, 1396, 1260, 264, 212, 790, 119, 1461, 1520, 911, 961, 195, 994, 359, 16, 602, 760, 692, 1045, 457, 670, 1302, 732, 1281, 802, 1385, 1460, 728, 714, 889, 1324, 61, 208, 1261, 878, 562, 629, 936, 1132, 574, 257, 1463, 217, 1171, 851, 490, 175, 736, 1462, 1294, 1432, 295, 1242, 1031, 1450, 745, 1192, 41, 1349, 411, 1248, 1227, 1298, 132, 1426, 1379, 348, 105, 705, 1156, 846, 674, 1291, 171, 1442, 381, 188], 'conv3': [1447, 129, 2500, 2420, 2056, 2414, 2181, 564, 370, 746, 398, 646, 35, 1720, 922, 2523, 199, 419, 696, 1843, 1755, 1025, 2317, 305, 1424, 2193, 1794, 1993, 583, 472, 1926, 534, 640, 1380, 1394, 708, 68, 1292, 1304, 533, 1198, 837, 2331, 1958, 653, 1409, 2085, 162, 1830, 658, 1175, 2447, 2000, 964, 1558, 1280, 436, 1992, 2259, 2237, 888, 1746, 1953, 1637, 1051, 2070, 1506, 1141, 2, 2093, 732, 299, 2069, 1608, 33, 1666, 2390, 828, 2151, 2536, 72, 1003, 2342, 2179, 1887, 1550, 454, 1873, 2547, 1909, 2022, 1001, 1123, 1248, 2057, 810, 34, 1403, 2176, 1443, 1824, 2453, 421, 2185, 1753, 1938, 638, 796, 1385, 1650, 2094, 1045, 561, 507, 720, 1831, 146, 1536, 210, 1457, 1139, 1689, 2460, 2095, 1162, 1524, 302, 800, 1451, 1449, 669, 1783, 365, 1245, 1406, 2174, 1931, 509, 674, 2545, 1056, 425, 1153, 1776, 63, 2283, 935, 586, 2300, 1376, 2162, 2060, 2305, 437, 1555, 1567, 2168, 283, 1283, 1415, 838, 80, 1332, 2112, 481, 1659, 1049, 1638, 43, 1221, 291, 340, 2322, 2272, 404, 450, 575, 1789, 1809, 867, 2394, 1572, 1986, 2515, 1970, 122, 1886, 2462, 491, 1929, 2320, 557, 2227, 1160, 1266, 225, 1960, 1845, 1548, 336, 1157, 1781, 1439, 1074, 2187, 2197, 792, 2285, 2511, 687, 167, 1021, 632, 788, 1864, 1799, 1435, 1591, 1458, 2208, 267, 13, 1088, 2008, 2252, 2387, 2161, 304, 895, 1512, 465, 968, 1199, 1271, 306, 1742, 505, 130, 220, 926, 781, 662, 1554, 855, 1733, 698, 1839, 2062, 499, 1052, 2081, 258, 2308, 357, 468, 1062, 24, 521, 1645, 1375, 768, 2121, 411, 2042, 616, 96, 1890, 242, 2418, 1196, 276, 2028, 2378, 467, 2173, 1066, 772, 366, 2079, 761, 1874, 1446, 362, 284, 1699, 884, 1226, 252, 1054, 543, 597, 1108, 2527, 2310, 1966, 1325, 2456, 203, 27, 959, 2192, 962, 992, 2050, 1253, 1531, 1227, 2518, 1961, 192, 124, 259, 6, 1827, 944, 239, 2157, 885, 2167, 1527, 576, 706, 399, 2484, 1971, 555, 943, 82, 1194, 1694, 1029, 2130, 1633, 1267, 107, 2293, 2508, 2532, 2256, 2011, 316, 1112, 240, 151, 2436, 634, 1206, 1549, 54, 2242, 300, 2309, 689, 1782, 899, 1448, 2113, 2030, 2033, 1107, 1647, 1894, 1968, 1646, 1670, 1326, 2164, 937, 298, 2441, 1138, 1578, 1811, 1622, 980, 1055, 1351, 1837, 2284, 331, 2446, 939, 1697, 1908, 2018], 'conv4': [139, 2517, 93, 686, 1471, 530, 1657, 1640, 2523, 1571, 1262, 888, 2015, 402, 494, 400, 150, 825, 797, 1877, 653, 2265, 1504, 2188, 625, 1375, 1565, 2326, 1556, 768, 1489, 1250, 654, 1306, 439, 1016, 282, 782, 120, 233, 1597, 2078, 1134, 2507, 959, 1406, 15, 2415, 126, 1387, 2335, 864, 1676, 30, 1548, 172, 1100, 388, 1092, 1010, 1285, 1730, 455, 1987, 1215, 792, 826, 2035, 557, 1719, 607, 660, 1390, 2385, 1269, 1995, 1118, 424, 382, 1477, 412, 2404, 2118, 1152, 627, 798, 2315, 612, 1869, 1397, 2368, 74, 2285, 257, 941, 1167, 350, 616, 2223, 571, 2226, 2224, 838, 929, 2171, 1937, 591, 2334, 725, 735, 2531, 1854, 975, 1892, 4, 1908, 1852, 486, 1639, 1431, 1169, 889, 1140, 287, 2559, 731, 2101, 161, 1867, 1555, 1912, 606, 565, 1156, 1572, 1080, 2141, 605, 966, 883, 1424, 1700, 757, 296, 1138, 703, 674, 2186, 2271, 1707, 1806, 2038, 1518, 1698, 1587, 2558, 6, 145, 2121, 401, 1759, 5, 748, 851, 1338, 1075, 246, 2457, 63, 1583, 1223, 80, 581, 1989, 3, 770, 1524, 1831, 476, 24, 1362, 910, 2311, 460, 2006, 1013, 1513, 114, 1582, 2509, 859, 1230, 1819, 531, 1625, 347, 1957, 2207, 426, 1883, 1113, 1718, 539, 1415, 1359, 593, 520, 2328, 1241, 1946, 808, 383, 2211, 1723, 2485, 844, 863, 1441, 2492, 881, 1366, 1226, 214, 2337, 2268, 2261, 562, 1844, 2248, 361, 564, 1034, 270, 1512, 1323, 1795, 1836, 2149, 1272, 1015, 578, 2218, 1970, 1887, 381, 1645, 271, 743, 718, 1840, 1658, 990, 69, 2554, 2355, 789, 949, 922, 1263, 309, 2172, 1040, 2049, 1696, 1019, 1620, 1898, 1574, 2133, 1143, 40, 1847, 34, 2475, 1198, 2487, 2506, 1439, 54, 36, 2199, 2299, 1289, 2257, 1117, 2418, 1025, 1893, 499, 778, 364, 1654, 82, 945, 2524, 1713, 372, 239, 1545, 2264, 928, 1779, 1811, 704, 689, 2044, 470, 1981, 1172, 137, 2045, 1246, 1705, 1820, 2012, 2495, 1634, 1522, 2322, 1239, 746, 978, 992, 525, 26, 2425, 646, 1420, 1740, 2258, 2538, 832, 1526, 648, 1641, 284, 1660, 1931, 1445, 2246, 1787, 335, 2138, 777, 1558, 1868, 29, 895, 344, 33, 2053, 1460, 2276, 741, 1951, 1098, 2459, 790, 997, 1499, 722, 2455, 2518, 515, 1998, 1474, 2134, 874, 2510, 536, 2163, 1224, 1007, 2117, 664, 1686, 552, 411, 2184, 1914, 784, 2323, 1491, 1035, 712]}
SAVED model.heads_thresh 0.9
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
Epoch: [20/50]	lr: 2.50e-04	time: 00:00:33	Loss_train 0.01122	Acc_train 91.75	/	Loss_test 0.00751	Acc_test 85.50
BLOCKS IN SUP:  [5]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [25, 59, 30, 55, 42, 69, 5, 68, 93, 16, 57, 14, 36, 38, 77], 'conv1': [197, 349, 72, 345, 121, 76, 124, 193, 109, 365, 274, 296, 354, 92, 73, 56, 75, 105, 36, 188, 202, 343, 336, 304, 162, 41, 266, 159, 100, 104, 163, 148, 22, 9, 186, 229, 190, 379, 4, 174, 358, 114, 118, 373, 146, 217, 278, 347, 70, 191, 169, 99, 50, 180, 137, 26, 94, 383, 324], 'conv2': [958, 733, 761, 250, 1436, 856, 14, 209, 1163, 176, 399, 801, 942, 95, 1532, 118, 622, 959, 1452, 1274, 324, 326, 835, 632, 639, 1172, 852, 1113, 35, 850, 730, 1486, 636, 869, 508, 155, 597, 173, 15, 423, 22, 312, 1278, 281, 12, 408, 783, 1159, 1345, 799, 746, 499, 933, 1126, 1178, 373, 395, 915, 868, 356, 773, 810, 1002, 923, 588, 580, 426, 519, 1238, 164, 1089, 42, 980, 441, 575, 1001, 612, 743, 353, 649, 779, 154, 523, 964, 905, 507, 461, 1046, 626, 294, 981, 1025, 937, 752, 1186, 1032, 405, 953, 313, 58, 542, 1206, 544, 667, 335, 754, 1155, 1508, 1317, 900, 311, 70, 1292, 1184, 707, 439, 682, 1522, 77, 93, 1203, 174, 115, 854, 1125, 1149, 960, 1494, 74, 952, 866, 134, 902, 921, 653, 590, 1038, 238, 872, 1282, 1340, 650, 1297, 1357, 1455, 307, 788, 265, 378, 75, 614, 1265, 243, 454, 406, 1396, 1260, 264, 212, 790, 119, 1461, 1520, 911, 961, 195, 994, 359, 16, 602, 760, 692, 1045, 457, 670, 1302, 732, 1281, 802, 1385, 1460, 728, 714, 889, 1324, 61, 208, 1261, 878, 562, 629, 936, 1132, 574, 257, 1463, 217, 1171, 851, 490, 175, 736, 1462, 1294, 1432, 295, 1242, 1031, 1450, 745, 1192, 41, 1349, 411, 1248, 1227, 1298, 132, 1426, 1379, 348, 105, 705, 1156, 846, 674, 1291, 171, 1442, 381, 188], 'conv3': [1447, 129, 2500, 2420, 2056, 2414, 2181, 564, 370, 746, 398, 646, 35, 1720, 922, 2523, 199, 419, 696, 1843, 1755, 1025, 2317, 305, 1424, 2193, 1794, 1993, 583, 472, 1926, 534, 640, 1380, 1394, 708, 68, 1292, 1304, 533, 1198, 837, 2331, 1958, 653, 1409, 2085, 162, 1830, 658, 1175, 2447, 2000, 964, 1558, 1280, 436, 1992, 2259, 2237, 888, 1746, 1953, 1637, 1051, 2070, 1506, 1141, 2, 2093, 732, 299, 2069, 1608, 33, 1666, 2390, 828, 2151, 2536, 72, 1003, 2342, 2179, 1887, 1550, 454, 1873, 2547, 1909, 2022, 1001, 1123, 1248, 2057, 810, 34, 1403, 2176, 1443, 1824, 2453, 421, 2185, 1753, 1938, 638, 796, 1385, 1650, 2094, 1045, 561, 507, 720, 1831, 146, 1536, 210, 1457, 1139, 1689, 2460, 2095, 1162, 1524, 302, 800, 1451, 1449, 669, 1783, 365, 1245, 1406, 2174, 1931, 509, 674, 2545, 1056, 425, 1153, 1776, 63, 2283, 935, 586, 2300, 1376, 2162, 2060, 2305, 437, 1555, 1567, 2168, 283, 1283, 1415, 838, 80, 1332, 2112, 481, 1659, 1049, 1638, 43, 1221, 291, 340, 2322, 2272, 404, 450, 575, 1789, 1809, 867, 2394, 1572, 1986, 2515, 1970, 122, 1886, 2462, 491, 1929, 2320, 557, 2227, 1160, 1266, 225, 1960, 1845, 1548, 336, 1157, 1781, 1439, 1074, 2187, 2197, 792, 2285, 2511, 687, 167, 1021, 632, 788, 1864, 1799, 1435, 1591, 1458, 2208, 267, 13, 1088, 2008, 2252, 2387, 2161, 304, 895, 1512, 465, 968, 1199, 1271, 306, 1742, 505, 130, 220, 926, 781, 662, 1554, 855, 1733, 698, 1839, 2062, 499, 1052, 2081, 258, 2308, 357, 468, 1062, 24, 521, 1645, 1375, 768, 2121, 411, 2042, 616, 96, 1890, 242, 2418, 1196, 276, 2028, 2378, 467, 2173, 1066, 772, 366, 2079, 761, 1874, 1446, 362, 284, 1699, 884, 1226, 252, 1054, 543, 597, 1108, 2527, 2310, 1966, 1325, 2456, 203, 27, 959, 2192, 962, 992, 2050, 1253, 1531, 1227, 2518, 1961, 192, 124, 259, 6, 1827, 944, 239, 2157, 885, 2167, 1527, 576, 706, 399, 2484, 1971, 555, 943, 82, 1194, 1694, 1029, 2130, 1633, 1267, 107, 2293, 2508, 2532, 2256, 2011, 316, 1112, 240, 151, 2436, 634, 1206, 1549, 54, 2242, 300, 2309, 689, 1782, 899, 1448, 2113, 2030, 2033, 1107, 1647, 1894, 1968, 1646, 1670, 1326, 2164, 937, 298, 2441, 1138, 1578, 1811, 1622, 980, 1055, 1351, 1837, 2284, 331, 2446, 939, 1697, 1908, 2018], 'conv4': [139, 2517, 93, 686, 1471, 530, 1657, 1640, 2523, 1571, 1262, 888, 2015, 402, 494, 400, 150, 825, 797, 1877, 653, 2265, 1504, 2188, 625, 1375, 1565, 2326, 1556, 768, 1489, 1250, 654, 1306, 439, 1016, 282, 782, 120, 233, 1597, 2078, 1134, 2507, 959, 1406, 15, 2415, 126, 1387, 2335, 864, 1676, 30, 1548, 172, 1100, 388, 1092, 1010, 1285, 1730, 455, 1987, 1215, 792, 826, 2035, 557, 1719, 607, 660, 1390, 2385, 1269, 1995, 1118, 424, 382, 1477, 412, 2404, 2118, 1152, 627, 798, 2315, 612, 1869, 1397, 2368, 74, 2285, 257, 941, 1167, 350, 616, 2223, 571, 2226, 2224, 838, 929, 2171, 1937, 591, 2334, 725, 735, 2531, 1854, 975, 1892, 4, 1908, 1852, 486, 1639, 1431, 1169, 889, 1140, 287, 2559, 731, 2101, 161, 1867, 1555, 1912, 606, 565, 1156, 1572, 1080, 2141, 605, 966, 883, 1424, 1700, 757, 296, 1138, 703, 674, 2186, 2271, 1707, 1806, 2038, 1518, 1698, 1587, 2558, 6, 145, 2121, 401, 1759, 5, 748, 851, 1338, 1075, 246, 2457, 63, 1583, 1223, 80, 581, 1989, 3, 770, 1524, 1831, 476, 24, 1362, 910, 2311, 460, 2006, 1013, 1513, 114, 1582, 2509, 859, 1230, 1819, 531, 1625, 347, 1957, 2207, 426, 1883, 1113, 1718, 539, 1415, 1359, 593, 520, 2328, 1241, 1946, 808, 383, 2211, 1723, 2485, 844, 863, 1441, 2492, 881, 1366, 1226, 214, 2337, 2268, 2261, 562, 1844, 2248, 361, 564, 1034, 270, 1512, 1323, 1795, 1836, 2149, 1272, 1015, 578, 2218, 1970, 1887, 381, 1645, 271, 743, 718, 1840, 1658, 990, 69, 2554, 2355, 789, 949, 922, 1263, 309, 2172, 1040, 2049, 1696, 1019, 1620, 1898, 1574, 2133, 1143, 40, 1847, 34, 2475, 1198, 2487, 2506, 1439, 54, 36, 2199, 2299, 1289, 2257, 1117, 2418, 1025, 1893, 499, 778, 364, 1654, 82, 945, 2524, 1713, 372, 239, 1545, 2264, 928, 1779, 1811, 704, 689, 2044, 470, 1981, 1172, 137, 2045, 1246, 1705, 1820, 2012, 2495, 1634, 1522, 2322, 1239, 746, 978, 992, 525, 26, 2425, 646, 1420, 1740, 2258, 2538, 832, 1526, 648, 1641, 284, 1660, 1931, 1445, 2246, 1787, 335, 2138, 777, 1558, 1868, 29, 895, 344, 33, 2053, 1460, 2276, 741, 1951, 1098, 2459, 790, 997, 1499, 722, 2455, 2518, 515, 1998, 1474, 2134, 874, 2510, 536, 2163, 1224, 1007, 2117, 664, 1686, 552, 411, 2184, 1914, 784, 2323, 1491, 1035, 712]}
SAVED model.heads_thresh 0.9
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
Epoch: [30/50]	lr: 1.25e-04	time: 00:00:34	Loss_train 0.00761	Acc_train 93.56	/	Loss_test 0.00757	Acc_test 87.00
BLOCKS IN SUP:  [5]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [25, 59, 30, 55, 42, 69, 5, 68, 93, 16, 57, 14, 36, 38, 77], 'conv1': [197, 349, 72, 345, 121, 76, 124, 193, 109, 365, 274, 296, 354, 92, 73, 56, 75, 105, 36, 188, 202, 343, 336, 304, 162, 41, 266, 159, 100, 104, 163, 148, 22, 9, 186, 229, 190, 379, 4, 174, 358, 114, 118, 373, 146, 217, 278, 347, 70, 191, 169, 99, 50, 180, 137, 26, 94, 383, 324], 'conv2': [958, 733, 761, 250, 1436, 856, 14, 209, 1163, 176, 399, 801, 942, 95, 1532, 118, 622, 959, 1452, 1274, 324, 326, 835, 632, 639, 1172, 852, 1113, 35, 850, 730, 1486, 636, 869, 508, 155, 597, 173, 15, 423, 22, 312, 1278, 281, 12, 408, 783, 1159, 1345, 799, 746, 499, 933, 1126, 1178, 373, 395, 915, 868, 356, 773, 810, 1002, 923, 588, 580, 426, 519, 1238, 164, 1089, 42, 980, 441, 575, 1001, 612, 743, 353, 649, 779, 154, 523, 964, 905, 507, 461, 1046, 626, 294, 981, 1025, 937, 752, 1186, 1032, 405, 953, 313, 58, 542, 1206, 544, 667, 335, 754, 1155, 1508, 1317, 900, 311, 70, 1292, 1184, 707, 439, 682, 1522, 77, 93, 1203, 174, 115, 854, 1125, 1149, 960, 1494, 74, 952, 866, 134, 902, 921, 653, 590, 1038, 238, 872, 1282, 1340, 650, 1297, 1357, 1455, 307, 788, 265, 378, 75, 614, 1265, 243, 454, 406, 1396, 1260, 264, 212, 790, 119, 1461, 1520, 911, 961, 195, 994, 359, 16, 602, 760, 692, 1045, 457, 670, 1302, 732, 1281, 802, 1385, 1460, 728, 714, 889, 1324, 61, 208, 1261, 878, 562, 629, 936, 1132, 574, 257, 1463, 217, 1171, 851, 490, 175, 736, 1462, 1294, 1432, 295, 1242, 1031, 1450, 745, 1192, 41, 1349, 411, 1248, 1227, 1298, 132, 1426, 1379, 348, 105, 705, 1156, 846, 674, 1291, 171, 1442, 381, 188], 'conv3': [1447, 129, 2500, 2420, 2056, 2414, 2181, 564, 370, 746, 398, 646, 35, 1720, 922, 2523, 199, 419, 696, 1843, 1755, 1025, 2317, 305, 1424, 2193, 1794, 1993, 583, 472, 1926, 534, 640, 1380, 1394, 708, 68, 1292, 1304, 533, 1198, 837, 2331, 1958, 653, 1409, 2085, 162, 1830, 658, 1175, 2447, 2000, 964, 1558, 1280, 436, 1992, 2259, 2237, 888, 1746, 1953, 1637, 1051, 2070, 1506, 1141, 2, 2093, 732, 299, 2069, 1608, 33, 1666, 2390, 828, 2151, 2536, 72, 1003, 2342, 2179, 1887, 1550, 454, 1873, 2547, 1909, 2022, 1001, 1123, 1248, 2057, 810, 34, 1403, 2176, 1443, 1824, 2453, 421, 2185, 1753, 1938, 638, 796, 1385, 1650, 2094, 1045, 561, 507, 720, 1831, 146, 1536, 210, 1457, 1139, 1689, 2460, 2095, 1162, 1524, 302, 800, 1451, 1449, 669, 1783, 365, 1245, 1406, 2174, 1931, 509, 674, 2545, 1056, 425, 1153, 1776, 63, 2283, 935, 586, 2300, 1376, 2162, 2060, 2305, 437, 1555, 1567, 2168, 283, 1283, 1415, 838, 80, 1332, 2112, 481, 1659, 1049, 1638, 43, 1221, 291, 340, 2322, 2272, 404, 450, 575, 1789, 1809, 867, 2394, 1572, 1986, 2515, 1970, 122, 1886, 2462, 491, 1929, 2320, 557, 2227, 1160, 1266, 225, 1960, 1845, 1548, 336, 1157, 1781, 1439, 1074, 2187, 2197, 792, 2285, 2511, 687, 167, 1021, 632, 788, 1864, 1799, 1435, 1591, 1458, 2208, 267, 13, 1088, 2008, 2252, 2387, 2161, 304, 895, 1512, 465, 968, 1199, 1271, 306, 1742, 505, 130, 220, 926, 781, 662, 1554, 855, 1733, 698, 1839, 2062, 499, 1052, 2081, 258, 2308, 357, 468, 1062, 24, 521, 1645, 1375, 768, 2121, 411, 2042, 616, 96, 1890, 242, 2418, 1196, 276, 2028, 2378, 467, 2173, 1066, 772, 366, 2079, 761, 1874, 1446, 362, 284, 1699, 884, 1226, 252, 1054, 543, 597, 1108, 2527, 2310, 1966, 1325, 2456, 203, 27, 959, 2192, 962, 992, 2050, 1253, 1531, 1227, 2518, 1961, 192, 124, 259, 6, 1827, 944, 239, 2157, 885, 2167, 1527, 576, 706, 399, 2484, 1971, 555, 943, 82, 1194, 1694, 1029, 2130, 1633, 1267, 107, 2293, 2508, 2532, 2256, 2011, 316, 1112, 240, 151, 2436, 634, 1206, 1549, 54, 2242, 300, 2309, 689, 1782, 899, 1448, 2113, 2030, 2033, 1107, 1647, 1894, 1968, 1646, 1670, 1326, 2164, 937, 298, 2441, 1138, 1578, 1811, 1622, 980, 1055, 1351, 1837, 2284, 331, 2446, 939, 1697, 1908, 2018], 'conv4': [139, 2517, 93, 686, 1471, 530, 1657, 1640, 2523, 1571, 1262, 888, 2015, 402, 494, 400, 150, 825, 797, 1877, 653, 2265, 1504, 2188, 625, 1375, 1565, 2326, 1556, 768, 1489, 1250, 654, 1306, 439, 1016, 282, 782, 120, 233, 1597, 2078, 1134, 2507, 959, 1406, 15, 2415, 126, 1387, 2335, 864, 1676, 30, 1548, 172, 1100, 388, 1092, 1010, 1285, 1730, 455, 1987, 1215, 792, 826, 2035, 557, 1719, 607, 660, 1390, 2385, 1269, 1995, 1118, 424, 382, 1477, 412, 2404, 2118, 1152, 627, 798, 2315, 612, 1869, 1397, 2368, 74, 2285, 257, 941, 1167, 350, 616, 2223, 571, 2226, 2224, 838, 929, 2171, 1937, 591, 2334, 725, 735, 2531, 1854, 975, 1892, 4, 1908, 1852, 486, 1639, 1431, 1169, 889, 1140, 287, 2559, 731, 2101, 161, 1867, 1555, 1912, 606, 565, 1156, 1572, 1080, 2141, 605, 966, 883, 1424, 1700, 757, 296, 1138, 703, 674, 2186, 2271, 1707, 1806, 2038, 1518, 1698, 1587, 2558, 6, 145, 2121, 401, 1759, 5, 748, 851, 1338, 1075, 246, 2457, 63, 1583, 1223, 80, 581, 1989, 3, 770, 1524, 1831, 476, 24, 1362, 910, 2311, 460, 2006, 1013, 1513, 114, 1582, 2509, 859, 1230, 1819, 531, 1625, 347, 1957, 2207, 426, 1883, 1113, 1718, 539, 1415, 1359, 593, 520, 2328, 1241, 1946, 808, 383, 2211, 1723, 2485, 844, 863, 1441, 2492, 881, 1366, 1226, 214, 2337, 2268, 2261, 562, 1844, 2248, 361, 564, 1034, 270, 1512, 1323, 1795, 1836, 2149, 1272, 1015, 578, 2218, 1970, 1887, 381, 1645, 271, 743, 718, 1840, 1658, 990, 69, 2554, 2355, 789, 949, 922, 1263, 309, 2172, 1040, 2049, 1696, 1019, 1620, 1898, 1574, 2133, 1143, 40, 1847, 34, 2475, 1198, 2487, 2506, 1439, 54, 36, 2199, 2299, 1289, 2257, 1117, 2418, 1025, 1893, 499, 778, 364, 1654, 82, 945, 2524, 1713, 372, 239, 1545, 2264, 928, 1779, 1811, 704, 689, 2044, 470, 1981, 1172, 137, 2045, 1246, 1705, 1820, 2012, 2495, 1634, 1522, 2322, 1239, 746, 978, 992, 525, 26, 2425, 646, 1420, 1740, 2258, 2538, 832, 1526, 648, 1641, 284, 1660, 1931, 1445, 2246, 1787, 335, 2138, 777, 1558, 1868, 29, 895, 344, 33, 2053, 1460, 2276, 741, 1951, 1098, 2459, 790, 997, 1499, 722, 2455, 2518, 515, 1998, 1474, 2134, 874, 2510, 536, 2163, 1224, 1007, 2117, 664, 1686, 552, 411, 2184, 1914, 784, 2323, 1491, 1035, 712]}
SAVED model.heads_thresh 0.9
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
Epoch: [40/50]	lr: 3.13e-05	time: 00:00:36	Loss_train 0.00689	Acc_train 94.04	/	Loss_test 0.00737	Acc_test 86.50
BLOCKS IN SUP:  [5]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c/models
SAVED HEADS THRESHOLD:  0.9
SAVED model.topk_kernels {'conv0': [25, 59, 30, 55, 42, 69, 5, 68, 93, 16, 57, 14, 36, 38, 77], 'conv1': [197, 349, 72, 345, 121, 76, 124, 193, 109, 365, 274, 296, 354, 92, 73, 56, 75, 105, 36, 188, 202, 343, 336, 304, 162, 41, 266, 159, 100, 104, 163, 148, 22, 9, 186, 229, 190, 379, 4, 174, 358, 114, 118, 373, 146, 217, 278, 347, 70, 191, 169, 99, 50, 180, 137, 26, 94, 383, 324], 'conv2': [958, 733, 761, 250, 1436, 856, 14, 209, 1163, 176, 399, 801, 942, 95, 1532, 118, 622, 959, 1452, 1274, 324, 326, 835, 632, 639, 1172, 852, 1113, 35, 850, 730, 1486, 636, 869, 508, 155, 597, 173, 15, 423, 22, 312, 1278, 281, 12, 408, 783, 1159, 1345, 799, 746, 499, 933, 1126, 1178, 373, 395, 915, 868, 356, 773, 810, 1002, 923, 588, 580, 426, 519, 1238, 164, 1089, 42, 980, 441, 575, 1001, 612, 743, 353, 649, 779, 154, 523, 964, 905, 507, 461, 1046, 626, 294, 981, 1025, 937, 752, 1186, 1032, 405, 953, 313, 58, 542, 1206, 544, 667, 335, 754, 1155, 1508, 1317, 900, 311, 70, 1292, 1184, 707, 439, 682, 1522, 77, 93, 1203, 174, 115, 854, 1125, 1149, 960, 1494, 74, 952, 866, 134, 902, 921, 653, 590, 1038, 238, 872, 1282, 1340, 650, 1297, 1357, 1455, 307, 788, 265, 378, 75, 614, 1265, 243, 454, 406, 1396, 1260, 264, 212, 790, 119, 1461, 1520, 911, 961, 195, 994, 359, 16, 602, 760, 692, 1045, 457, 670, 1302, 732, 1281, 802, 1385, 1460, 728, 714, 889, 1324, 61, 208, 1261, 878, 562, 629, 936, 1132, 574, 257, 1463, 217, 1171, 851, 490, 175, 736, 1462, 1294, 1432, 295, 1242, 1031, 1450, 745, 1192, 41, 1349, 411, 1248, 1227, 1298, 132, 1426, 1379, 348, 105, 705, 1156, 846, 674, 1291, 171, 1442, 381, 188], 'conv3': [1447, 129, 2500, 2420, 2056, 2414, 2181, 564, 370, 746, 398, 646, 35, 1720, 922, 2523, 199, 419, 696, 1843, 1755, 1025, 2317, 305, 1424, 2193, 1794, 1993, 583, 472, 1926, 534, 640, 1380, 1394, 708, 68, 1292, 1304, 533, 1198, 837, 2331, 1958, 653, 1409, 2085, 162, 1830, 658, 1175, 2447, 2000, 964, 1558, 1280, 436, 1992, 2259, 2237, 888, 1746, 1953, 1637, 1051, 2070, 1506, 1141, 2, 2093, 732, 299, 2069, 1608, 33, 1666, 2390, 828, 2151, 2536, 72, 1003, 2342, 2179, 1887, 1550, 454, 1873, 2547, 1909, 2022, 1001, 1123, 1248, 2057, 810, 34, 1403, 2176, 1443, 1824, 2453, 421, 2185, 1753, 1938, 638, 796, 1385, 1650, 2094, 1045, 561, 507, 720, 1831, 146, 1536, 210, 1457, 1139, 1689, 2460, 2095, 1162, 1524, 302, 800, 1451, 1449, 669, 1783, 365, 1245, 1406, 2174, 1931, 509, 674, 2545, 1056, 425, 1153, 1776, 63, 2283, 935, 586, 2300, 1376, 2162, 2060, 2305, 437, 1555, 1567, 2168, 283, 1283, 1415, 838, 80, 1332, 2112, 481, 1659, 1049, 1638, 43, 1221, 291, 340, 2322, 2272, 404, 450, 575, 1789, 1809, 867, 2394, 1572, 1986, 2515, 1970, 122, 1886, 2462, 491, 1929, 2320, 557, 2227, 1160, 1266, 225, 1960, 1845, 1548, 336, 1157, 1781, 1439, 1074, 2187, 2197, 792, 2285, 2511, 687, 167, 1021, 632, 788, 1864, 1799, 1435, 1591, 1458, 2208, 267, 13, 1088, 2008, 2252, 2387, 2161, 304, 895, 1512, 465, 968, 1199, 1271, 306, 1742, 505, 130, 220, 926, 781, 662, 1554, 855, 1733, 698, 1839, 2062, 499, 1052, 2081, 258, 2308, 357, 468, 1062, 24, 521, 1645, 1375, 768, 2121, 411, 2042, 616, 96, 1890, 242, 2418, 1196, 276, 2028, 2378, 467, 2173, 1066, 772, 366, 2079, 761, 1874, 1446, 362, 284, 1699, 884, 1226, 252, 1054, 543, 597, 1108, 2527, 2310, 1966, 1325, 2456, 203, 27, 959, 2192, 962, 992, 2050, 1253, 1531, 1227, 2518, 1961, 192, 124, 259, 6, 1827, 944, 239, 2157, 885, 2167, 1527, 576, 706, 399, 2484, 1971, 555, 943, 82, 1194, 1694, 1029, 2130, 1633, 1267, 107, 2293, 2508, 2532, 2256, 2011, 316, 1112, 240, 151, 2436, 634, 1206, 1549, 54, 2242, 300, 2309, 689, 1782, 899, 1448, 2113, 2030, 2033, 1107, 1647, 1894, 1968, 1646, 1670, 1326, 2164, 937, 298, 2441, 1138, 1578, 1811, 1622, 980, 1055, 1351, 1837, 2284, 331, 2446, 939, 1697, 1908, 2018], 'conv4': [139, 2517, 93, 686, 1471, 530, 1657, 1640, 2523, 1571, 1262, 888, 2015, 402, 494, 400, 150, 825, 797, 1877, 653, 2265, 1504, 2188, 625, 1375, 1565, 2326, 1556, 768, 1489, 1250, 654, 1306, 439, 1016, 282, 782, 120, 233, 1597, 2078, 1134, 2507, 959, 1406, 15, 2415, 126, 1387, 2335, 864, 1676, 30, 1548, 172, 1100, 388, 1092, 1010, 1285, 1730, 455, 1987, 1215, 792, 826, 2035, 557, 1719, 607, 660, 1390, 2385, 1269, 1995, 1118, 424, 382, 1477, 412, 2404, 2118, 1152, 627, 798, 2315, 612, 1869, 1397, 2368, 74, 2285, 257, 941, 1167, 350, 616, 2223, 571, 2226, 2224, 838, 929, 2171, 1937, 591, 2334, 725, 735, 2531, 1854, 975, 1892, 4, 1908, 1852, 486, 1639, 1431, 1169, 889, 1140, 287, 2559, 731, 2101, 161, 1867, 1555, 1912, 606, 565, 1156, 1572, 1080, 2141, 605, 966, 883, 1424, 1700, 757, 296, 1138, 703, 674, 2186, 2271, 1707, 1806, 2038, 1518, 1698, 1587, 2558, 6, 145, 2121, 401, 1759, 5, 748, 851, 1338, 1075, 246, 2457, 63, 1583, 1223, 80, 581, 1989, 3, 770, 1524, 1831, 476, 24, 1362, 910, 2311, 460, 2006, 1013, 1513, 114, 1582, 2509, 859, 1230, 1819, 531, 1625, 347, 1957, 2207, 426, 1883, 1113, 1718, 539, 1415, 1359, 593, 520, 2328, 1241, 1946, 808, 383, 2211, 1723, 2485, 844, 863, 1441, 2492, 881, 1366, 1226, 214, 2337, 2268, 2261, 562, 1844, 2248, 361, 564, 1034, 270, 1512, 1323, 1795, 1836, 2149, 1272, 1015, 578, 2218, 1970, 1887, 381, 1645, 271, 743, 718, 1840, 1658, 990, 69, 2554, 2355, 789, 949, 922, 1263, 309, 2172, 1040, 2049, 1696, 1019, 1620, 1898, 1574, 2133, 1143, 40, 1847, 34, 2475, 1198, 2487, 2506, 1439, 54, 36, 2199, 2299, 1289, 2257, 1117, 2418, 1025, 1893, 499, 778, 364, 1654, 82, 945, 2524, 1713, 372, 239, 1545, 2264, 928, 1779, 1811, 704, 689, 2044, 470, 1981, 1172, 137, 2045, 1246, 1705, 1820, 2012, 2495, 1634, 1522, 2322, 1239, 746, 978, 992, 525, 26, 2425, 646, 1420, 1740, 2258, 2538, 832, 1526, 648, 1641, 284, 1660, 1931, 1445, 2246, 1787, 335, 2138, 777, 1558, 1868, 29, 895, 344, 33, 2053, 1460, 2276, 741, 1951, 1098, 2459, 790, 997, 1499, 722, 2455, 2518, 515, 1998, 1474, 2134, 874, 2510, 536, 2163, 1224, 1007, 2117, 664, 1686, 552, 411, 2184, 1914, 784, 2323, 1491, 1035, 712]}
SAVED model.heads_thresh 0.9
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
Epoch: [50/50]	lr: 7.81e-06	time: 00:00:38	Loss_train 0.00615	Acc_train 94.37	/	Loss_test 0.00718	Acc_test 88.00
new_head:  {'blocks.5.layer.bias': tensor([ 0.0040, -0.0205], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 1.5765e-02,  1.0660e-02,  4.3432e-03,  ...,  1.0930e-02,
          1.1431e-02, -3.5607e-03],
        [ 1.2527e-02,  1.1907e-02,  8.8815e-05,  ..., -8.2031e-03,
          9.7858e-03,  1.1328e-02]], device='cuda:0')}
BLOCKS IN SUP:  [5]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c/models
SAVED HEADS THRESHOLD:  0.89
SAVED model.topk_kernels {'conv0': [25, 59, 30, 55, 42, 69, 5, 68, 93, 16, 57, 14, 36, 38, 77], 'conv1': [197, 349, 72, 345, 121, 76, 124, 193, 109, 365, 274, 296, 354, 92, 73, 56, 75, 105, 36, 188, 202, 343, 336, 304, 162, 41, 266, 159, 100, 104, 163, 148, 22, 9, 186, 229, 190, 379, 4, 174, 358, 114, 118, 373, 146, 217, 278, 347, 70, 191, 169, 99, 50, 180, 137, 26, 94, 383, 324], 'conv2': [958, 733, 761, 250, 1436, 856, 14, 209, 1163, 176, 399, 801, 942, 95, 1532, 118, 622, 959, 1452, 1274, 324, 326, 835, 632, 639, 1172, 852, 1113, 35, 850, 730, 1486, 636, 869, 508, 155, 597, 173, 15, 423, 22, 312, 1278, 281, 12, 408, 783, 1159, 1345, 799, 746, 499, 933, 1126, 1178, 373, 395, 915, 868, 356, 773, 810, 1002, 923, 588, 580, 426, 519, 1238, 164, 1089, 42, 980, 441, 575, 1001, 612, 743, 353, 649, 779, 154, 523, 964, 905, 507, 461, 1046, 626, 294, 981, 1025, 937, 752, 1186, 1032, 405, 953, 313, 58, 542, 1206, 544, 667, 335, 754, 1155, 1508, 1317, 900, 311, 70, 1292, 1184, 707, 439, 682, 1522, 77, 93, 1203, 174, 115, 854, 1125, 1149, 960, 1494, 74, 952, 866, 134, 902, 921, 653, 590, 1038, 238, 872, 1282, 1340, 650, 1297, 1357, 1455, 307, 788, 265, 378, 75, 614, 1265, 243, 454, 406, 1396, 1260, 264, 212, 790, 119, 1461, 1520, 911, 961, 195, 994, 359, 16, 602, 760, 692, 1045, 457, 670, 1302, 732, 1281, 802, 1385, 1460, 728, 714, 889, 1324, 61, 208, 1261, 878, 562, 629, 936, 1132, 574, 257, 1463, 217, 1171, 851, 490, 175, 736, 1462, 1294, 1432, 295, 1242, 1031, 1450, 745, 1192, 41, 1349, 411, 1248, 1227, 1298, 132, 1426, 1379, 348, 105, 705, 1156, 846, 674, 1291, 171, 1442, 381, 188], 'conv3': [1447, 129, 2500, 2420, 2056, 2414, 2181, 564, 370, 746, 398, 646, 35, 1720, 922, 2523, 199, 419, 696, 1843, 1755, 1025, 2317, 305, 1424, 2193, 1794, 1993, 583, 472, 1926, 534, 640, 1380, 1394, 708, 68, 1292, 1304, 533, 1198, 837, 2331, 1958, 653, 1409, 2085, 162, 1830, 658, 1175, 2447, 2000, 964, 1558, 1280, 436, 1992, 2259, 2237, 888, 1746, 1953, 1637, 1051, 2070, 1506, 1141, 2, 2093, 732, 299, 2069, 1608, 33, 1666, 2390, 828, 2151, 2536, 72, 1003, 2342, 2179, 1887, 1550, 454, 1873, 2547, 1909, 2022, 1001, 1123, 1248, 2057, 810, 34, 1403, 2176, 1443, 1824, 2453, 421, 2185, 1753, 1938, 638, 796, 1385, 1650, 2094, 1045, 561, 507, 720, 1831, 146, 1536, 210, 1457, 1139, 1689, 2460, 2095, 1162, 1524, 302, 800, 1451, 1449, 669, 1783, 365, 1245, 1406, 2174, 1931, 509, 674, 2545, 1056, 425, 1153, 1776, 63, 2283, 935, 586, 2300, 1376, 2162, 2060, 2305, 437, 1555, 1567, 2168, 283, 1283, 1415, 838, 80, 1332, 2112, 481, 1659, 1049, 1638, 43, 1221, 291, 340, 2322, 2272, 404, 450, 575, 1789, 1809, 867, 2394, 1572, 1986, 2515, 1970, 122, 1886, 2462, 491, 1929, 2320, 557, 2227, 1160, 1266, 225, 1960, 1845, 1548, 336, 1157, 1781, 1439, 1074, 2187, 2197, 792, 2285, 2511, 687, 167, 1021, 632, 788, 1864, 1799, 1435, 1591, 1458, 2208, 267, 13, 1088, 2008, 2252, 2387, 2161, 304, 895, 1512, 465, 968, 1199, 1271, 306, 1742, 505, 130, 220, 926, 781, 662, 1554, 855, 1733, 698, 1839, 2062, 499, 1052, 2081, 258, 2308, 357, 468, 1062, 24, 521, 1645, 1375, 768, 2121, 411, 2042, 616, 96, 1890, 242, 2418, 1196, 276, 2028, 2378, 467, 2173, 1066, 772, 366, 2079, 761, 1874, 1446, 362, 284, 1699, 884, 1226, 252, 1054, 543, 597, 1108, 2527, 2310, 1966, 1325, 2456, 203, 27, 959, 2192, 962, 992, 2050, 1253, 1531, 1227, 2518, 1961, 192, 124, 259, 6, 1827, 944, 239, 2157, 885, 2167, 1527, 576, 706, 399, 2484, 1971, 555, 943, 82, 1194, 1694, 1029, 2130, 1633, 1267, 107, 2293, 2508, 2532, 2256, 2011, 316, 1112, 240, 151, 2436, 634, 1206, 1549, 54, 2242, 300, 2309, 689, 1782, 899, 1448, 2113, 2030, 2033, 1107, 1647, 1894, 1968, 1646, 1670, 1326, 2164, 937, 298, 2441, 1138, 1578, 1811, 1622, 980, 1055, 1351, 1837, 2284, 331, 2446, 939, 1697, 1908, 2018], 'conv4': [139, 2517, 93, 686, 1471, 530, 1657, 1640, 2523, 1571, 1262, 888, 2015, 402, 494, 400, 150, 825, 797, 1877, 653, 2265, 1504, 2188, 625, 1375, 1565, 2326, 1556, 768, 1489, 1250, 654, 1306, 439, 1016, 282, 782, 120, 233, 1597, 2078, 1134, 2507, 959, 1406, 15, 2415, 126, 1387, 2335, 864, 1676, 30, 1548, 172, 1100, 388, 1092, 1010, 1285, 1730, 455, 1987, 1215, 792, 826, 2035, 557, 1719, 607, 660, 1390, 2385, 1269, 1995, 1118, 424, 382, 1477, 412, 2404, 2118, 1152, 627, 798, 2315, 612, 1869, 1397, 2368, 74, 2285, 257, 941, 1167, 350, 616, 2223, 571, 2226, 2224, 838, 929, 2171, 1937, 591, 2334, 725, 735, 2531, 1854, 975, 1892, 4, 1908, 1852, 486, 1639, 1431, 1169, 889, 1140, 287, 2559, 731, 2101, 161, 1867, 1555, 1912, 606, 565, 1156, 1572, 1080, 2141, 605, 966, 883, 1424, 1700, 757, 296, 1138, 703, 674, 2186, 2271, 1707, 1806, 2038, 1518, 1698, 1587, 2558, 6, 145, 2121, 401, 1759, 5, 748, 851, 1338, 1075, 246, 2457, 63, 1583, 1223, 80, 581, 1989, 3, 770, 1524, 1831, 476, 24, 1362, 910, 2311, 460, 2006, 1013, 1513, 114, 1582, 2509, 859, 1230, 1819, 531, 1625, 347, 1957, 2207, 426, 1883, 1113, 1718, 539, 1415, 1359, 593, 520, 2328, 1241, 1946, 808, 383, 2211, 1723, 2485, 844, 863, 1441, 2492, 881, 1366, 1226, 214, 2337, 2268, 2261, 562, 1844, 2248, 361, 564, 1034, 270, 1512, 1323, 1795, 1836, 2149, 1272, 1015, 578, 2218, 1970, 1887, 381, 1645, 271, 743, 718, 1840, 1658, 990, 69, 2554, 2355, 789, 949, 922, 1263, 309, 2172, 1040, 2049, 1696, 1019, 1620, 1898, 1574, 2133, 1143, 40, 1847, 34, 2475, 1198, 2487, 2506, 1439, 54, 36, 2199, 2299, 1289, 2257, 1117, 2418, 1025, 1893, 499, 778, 364, 1654, 82, 945, 2524, 1713, 372, 239, 1545, 2264, 928, 1779, 1811, 704, 689, 2044, 470, 1981, 1172, 137, 2045, 1246, 1705, 1820, 2012, 2495, 1634, 1522, 2322, 1239, 746, 978, 992, 525, 26, 2425, 646, 1420, 1740, 2258, 2538, 832, 1526, 648, 1641, 284, 1660, 1931, 1445, 2246, 1787, 335, 2138, 777, 1558, 1868, 29, 895, 344, 33, 2053, 1460, 2276, 741, 1951, 1098, 2459, 790, 997, 1499, 722, 2455, 2518, 515, 1998, 1474, 2134, 874, 2510, 536, 2163, 1224, 1007, 2117, 664, 1686, 552, 411, 2184, 1914, 784, 2323, 1491, 1035, 712]}
SAVED model.heads_thresh 0.89
RESULT:  {'train_loss': 0.0061482023447752, 'train_acc': 94.37000155448914, 'test_loss': 0.007179258391261101, 'test_acc': 88.0, 'convergence': 23.331533432006836, 'R1': 0, 'dataset_sup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [35, 92]}, 'dataset_unsup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 50, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [35, 92]}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3, 4], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 50, 'lr': 0.005}, 't1': {'blocks': [5], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}}
IN R1:  {'count': 1, 'R0': {'train_loss': 0.014464263804256916, 'train_acc': 84.64999794960022, 'test_loss': 0.005055953748524189, 'test_acc': 82.0, 'convergence': 23.65986442565918, 'R1': 0, 'dataset_sup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [32, 93]}, 'dataset_unsup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 50, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [32, 93]}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3, 4], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 50, 'lr': 0.005}, 't1': {'blocks': [5], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}}, 'model_config': {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}, 'R1': {'train_loss': 0.0061482023447752, 'train_acc': 94.37000155448914, 'test_loss': 0.007179258391261101, 'test_acc': 88.0, 'convergence': 23.331533432006836, 'R1': 0, 'dataset_sup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [35, 92]}, 'dataset_unsup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 50, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [35, 92]}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3, 4], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 50, 'lr': 0.005}, 't1': {'blocks': [5], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}}}
################################## TASK 2 ############################################
[16, 41]
SEED:  0
block 0, size : 96 16 16
96
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 2.886751345948129
block 1, size : 384 8 8
384
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.8505172717997146
block 2, size : 1536 4 4
1536
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.4252586358998573
block 3, size : 2560 2 2
2560
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.21262931794992865
block 4, size : 2560 1 1
2560
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.1647019614671031
NOWWWW
1 1
2
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
blocks['b%s' % 5] 2560 2560
range = 0.3423265984407288
train_layer_order: consecutive {'t0': {'blocks': [0, 1, 2, 3, 4], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 50, 'lr': 0.005}, 't1': {'blocks': [5], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}
The device used will be: 
True
cuda:0
PARAMSSSS IN LOAD:  {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
avg_deltas:  {'blocks.0.layer.weight': tensor([-0.5105,  0.1496,  0.0392,  0.0903, -0.1568, -0.4092,  0.3688, -0.2736,
        -0.7403,  0.3870,  0.5747,  0.4217, -0.0256,  0.1433,  0.9749, -0.3270,
        -0.4428,  0.0176, -0.6988, -0.6410,  0.4186,  0.6243, -0.1298,  0.0231,
         0.3447, -0.0524, -0.1776, -0.1404,  0.2666, -0.1201,  0.5107,  0.5493,
        -0.6482, -0.5930,  0.6337,  1.0000, -1.0838, -0.0476,  0.7847,  0.0733,
         0.4460,  0.1841,  0.7566, -0.3229,  0.4395, -0.4751, -0.4542,  0.5812,
        -0.0445,  0.4783, -0.7011,  0.4480,  0.2792,  0.7637, -0.0938,  0.6215,
        -0.6184,  0.7859, -0.4463,  0.3975, -0.1486, -0.0076, -0.6155,  0.1102,
        -0.6445, -0.3612, -0.5643, -0.1696,  0.9089,  0.4815,  0.4843,  0.3092,
        -0.0347, -0.5962, -0.0534, -0.7011,  0.1136,  0.2573, -0.9436,  0.5460,
        -0.4472,  0.7750, -0.3512,  0.9307,  0.0703, -0.3815, -0.4113, -0.5002,
        -0.3426, -0.6174,  0.0214, -0.2066, -0.3265,  0.8343, -0.6442,  0.4524],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.9976e-02,  6.1699e-03,  2.4019e-03,  6.0392e-03,  8.4756e-01,
        -7.4254e-01, -1.4550e-03, -6.1978e-02, -7.2665e-03,  1.2007e-01,
        -1.2048e-03,  4.2755e-02,  7.2891e-02,  3.6609e-02,  9.8713e-03,
         4.2635e-02, -1.7606e-02,  1.8680e-02, -1.8105e-01, -2.3585e-01,
         2.5812e-02,  3.5814e-03,  3.2552e-01,  2.9203e-01,  1.5664e-01,
         1.4485e-01, -4.5061e-02, -5.1993e-02, -1.1699e-01,  1.9132e-02,
         1.5974e-02, -2.6301e-01, -2.9754e-01, -2.3803e-02, -2.7128e-02,
        -2.1432e-02,  4.0495e-02,  5.0280e-02, -2.9940e-04,  1.6047e-02,
         1.4387e-01,  5.0591e-03, -4.5106e-03,  6.9360e-04,  4.6246e-02,
        -6.2170e-05, -5.3023e-01,  4.3946e-02, -8.4399e-02, -1.2614e-01,
         1.4879e-02,  4.3965e-03, -9.0257e-03, -6.0968e-03, -1.2920e-01,
         6.5336e-01,  6.4390e-02, -5.9877e-02,  4.4773e-02, -1.1032e-02,
         3.0916e-03, -5.4395e-01,  3.0280e-02, -3.1792e-01, -1.3500e-01,
        -2.9181e-04, -2.6110e-01,  6.3174e-02,  3.8673e-02, -8.6348e-02,
        -1.9835e-01, -3.5634e-02, -2.8475e-01,  2.6840e-01, -1.1208e-02,
         2.0462e-03,  2.4126e-01, -3.3351e-01, -5.8050e-01,  3.4679e-01,
        -1.8159e-02,  7.4245e-03,  1.3566e-02, -1.7060e-06,  3.4349e-01,
        -1.0565e-03,  7.3623e-03, -8.6841e-02, -3.2311e-02,  7.5918e-03,
        -1.0763e-02,  1.1089e-04, -2.2557e-02,  2.2626e-03, -9.3917e-04,
        -8.7978e-02,  9.2347e-02,  1.9292e-02, -4.4159e-01, -1.6016e-02,
         3.0928e-01,  8.0741e-03,  4.2021e-03, -1.4655e-01, -5.0474e-02,
         1.0154e-03, -2.4131e-02, -1.0266e-01, -4.2929e-02,  3.0572e-01,
        -7.1675e-02,  9.5821e-04, -1.3120e-03, -2.7459e-01,  1.0000e+00,
        -5.5257e-02, -4.7422e-02,  7.4287e-02,  2.6331e-02, -6.8696e-02,
        -3.6926e-01, -8.4093e-02,  1.7593e-02,  1.1288e-01,  9.1335e-02,
        -5.2141e-01, -3.0037e-02, -3.6840e-01,  7.4291e-02,  2.4817e-03,
         2.5920e-03,  8.3414e-02,  3.5848e-02,  2.0743e-03,  1.7235e-01,
         6.7366e-02,  1.7739e-03,  1.1405e-01,  9.0249e-04,  3.4645e-02,
         2.0868e-03,  6.4417e-03,  1.5108e-01, -1.6740e-03,  2.6067e-02,
         3.4368e-03,  5.8322e-03, -2.4079e-02,  7.8452e-02, -2.8438e-03,
        -4.0502e-03, -6.6214e-02,  2.1363e-02,  3.3529e-02,  2.8985e-03,
        -3.6496e-02, -6.1801e-01, -7.6918e-01,  1.2974e-03, -5.2257e-01,
        -1.0671e-02,  1.6316e-03,  1.9493e-02, -3.6425e-01, -1.5587e-01,
         6.2078e-02, -1.7501e-01,  3.3651e-04,  3.9345e-01, -1.7460e-01,
        -7.9458e-02,  4.2796e-02,  2.8005e-02,  8.0180e-02,  2.0030e-01,
        -1.2291e-01,  1.2888e-01, -4.1070e-05,  3.5836e-02,  1.2471e-03,
        -3.7142e-01,  1.1615e-01,  3.9788e-02, -2.6664e-04, -2.4320e-02,
         1.3745e-03,  3.2084e-02,  5.2833e-03,  7.0324e-02, -6.3596e-01,
         3.4879e-01, -4.2586e-02,  2.8223e-03,  3.9192e-02,  5.9683e-03,
         5.2176e-01,  6.6358e-03,  7.1311e-01,  8.8636e-04, -6.3952e-03,
         1.9252e-01, -3.4687e-02, -2.0205e-01,  5.0888e-01, -1.0577e-01,
         1.0878e-02,  5.1083e-03, -6.2362e-04, -4.2878e-03,  1.0310e-01,
        -4.0018e-02,  4.1374e-02, -8.7401e-05, -1.1117e-01, -1.3307e-03,
         5.3671e-03,  1.5089e-02,  1.1177e-01, -2.7275e-01,  1.1250e-01,
         1.5175e-03,  9.7672e-02, -5.9771e-02, -4.0344e-03,  1.3928e-01,
        -2.0880e-01,  2.9071e-02, -4.4837e-01,  1.0571e-02,  6.1559e-02,
        -1.4951e-03, -1.4128e-04,  7.4253e-03, -1.5938e-03,  5.8774e-02,
        -6.5616e-02,  4.0555e-02, -3.2288e-04,  7.6843e-02,  1.3435e-02,
        -1.8902e-01, -2.1602e-01, -2.1546e-04, -2.3642e-01, -9.7630e-03,
        -5.9214e-02,  7.4324e-03, -1.8369e-01,  4.0184e-01, -4.6535e-03,
         8.6960e-01,  3.0498e-03, -3.6308e-02,  3.9974e-03, -5.6076e-01,
         3.9397e-02,  8.9245e-04,  1.1667e-03, -6.9010e-03,  8.9793e-04,
        -1.3371e-03, -2.6858e-03,  8.4899e-02,  4.6295e-02, -1.8025e-02,
         1.3984e-02, -1.0430e-01,  3.3785e-02,  1.3143e-03,  2.1726e-04,
        -1.4915e-01,  4.2533e-01,  4.1536e-03,  8.5240e-04,  9.6794e-01,
        -1.1705e-02,  1.2856e-01,  1.0121e-01,  2.3454e-01, -5.1493e-04,
        -1.8100e-02,  2.3500e-01,  3.2596e-01, -1.3836e-04,  1.3583e-02,
         1.2389e-01, -3.1281e-02, -8.2279e-02,  5.0844e-02, -2.9056e-02,
        -1.7224e-02, -3.6567e-01,  7.9866e-02, -9.4121e-03, -2.1411e-02,
        -1.9907e-01, -3.4653e-01, -2.3140e-02,  3.3252e-01,  4.1728e-02,
        -4.4045e-01, -2.6555e-01,  1.8744e-01, -2.7363e-02,  4.8627e-03,
         1.9625e-02,  3.9223e-01,  6.1587e-03,  2.8713e-02, -2.4643e-02,
        -1.5686e-02, -1.7468e-03,  1.4772e-01, -3.9386e-02,  2.3182e-02,
        -1.4488e-03,  2.6238e-03, -1.4749e-02, -4.0266e-03,  2.2692e-03,
        -1.2087e-03,  3.6382e-02,  3.7031e-02, -4.3878e-02, -4.1625e-01,
         7.3488e-02,  3.4450e-01,  1.2732e-01, -3.3519e-01,  4.7700e-01,
        -6.0740e-01,  1.1303e-01, -4.8066e-01,  2.2418e-02,  2.4444e-03,
         4.6742e-02,  6.0572e-03, -2.3893e-04,  1.0216e-01,  1.7152e-02,
         2.2809e-02,  1.6276e-03, -1.9525e-02,  3.5348e-01, -1.3069e-01,
        -3.7981e-02,  2.1455e-02,  7.7260e-01,  4.2011e-02, -3.9833e-01,
         1.3988e-02, -3.1254e-03, -1.5938e-02,  4.8970e-02,  1.3104e-01,
        -3.8414e-01, -4.1675e-03,  2.1431e-02,  2.1515e-01, -1.3565e-02,
         3.1234e-04,  1.7739e-04, -5.4292e-02,  2.2945e-02,  1.6224e-01,
        -2.4394e-01, -1.3098e-03,  1.7650e-01,  1.1274e-03, -3.2804e-03,
         1.4911e-02,  1.3519e-01,  5.7510e-02, -2.9519e-01, -1.8862e-01,
         1.0553e-02, -1.7893e-02,  2.4967e-02,  1.1009e-02,  8.5479e-02,
         6.3560e-02,  1.3389e-01, -3.7455e-02,  6.6843e-03], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0045,  0.0011, -0.0312,  ...,  0.1205, -0.1157, -0.0391],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.3165, -0.0698, -0.0417,  ..., -0.0022, -0.0142,  0.0693],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([0.0173, 0.2845, 0.0094,  ..., 0.0129, 0.0079, 0.0464], device='cuda:0')}
topk_layer:  None
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-0.5105,  0.1496,  0.0392,  0.0903, -0.1568, -0.4092,  0.3688, -0.2736,
        -0.7403,  0.3870,  0.5747,  0.4217, -0.0256,  0.1433,  0.9749, -0.3270,
        -0.4428,  0.0176, -0.6988, -0.6410,  0.4186,  0.6243, -0.1298,  0.0231,
         0.3447, -0.0524, -0.1776, -0.1404,  0.2666, -0.1201,  0.5107,  0.5493,
        -0.6482, -0.5930,  0.6337,  1.0000, -1.0838, -0.0476,  0.7847,  0.0733,
         0.4460,  0.1841,  0.7566, -0.3229,  0.4395, -0.4751, -0.4542,  0.5812,
        -0.0445,  0.4783, -0.7011,  0.4480,  0.2792,  0.7637, -0.0938,  0.6215,
        -0.6184,  0.7859, -0.4463,  0.3975, -0.1486, -0.0076, -0.6155,  0.1102,
        -0.6445, -0.3612, -0.5643, -0.1696,  0.9089,  0.4815,  0.4843,  0.3092,
        -0.0347, -0.5962, -0.0534, -0.7011,  0.1136,  0.2573, -0.9436,  0.5460,
        -0.4472,  0.7750, -0.3512,  0.9307,  0.0703, -0.3815, -0.4113, -0.5002,
        -0.3426, -0.6174,  0.0214, -0.2066, -0.3265,  0.8343, -0.6442,  0.4524],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.9976e-02,  6.1699e-03,  2.4019e-03,  6.0392e-03,  8.4756e-01,
        -7.4254e-01, -1.4550e-03, -6.1978e-02, -7.2665e-03,  1.2007e-01,
        -1.2048e-03,  4.2755e-02,  7.2891e-02,  3.6609e-02,  9.8713e-03,
         4.2635e-02, -1.7606e-02,  1.8680e-02, -1.8105e-01, -2.3585e-01,
         2.5812e-02,  3.5814e-03,  3.2552e-01,  2.9203e-01,  1.5664e-01,
         1.4485e-01, -4.5061e-02, -5.1993e-02, -1.1699e-01,  1.9132e-02,
         1.5974e-02, -2.6301e-01, -2.9754e-01, -2.3803e-02, -2.7128e-02,
        -2.1432e-02,  4.0495e-02,  5.0280e-02, -2.9940e-04,  1.6047e-02,
         1.4387e-01,  5.0591e-03, -4.5106e-03,  6.9360e-04,  4.6246e-02,
        -6.2170e-05, -5.3023e-01,  4.3946e-02, -8.4399e-02, -1.2614e-01,
         1.4879e-02,  4.3965e-03, -9.0257e-03, -6.0968e-03, -1.2920e-01,
         6.5336e-01,  6.4390e-02, -5.9877e-02,  4.4773e-02, -1.1032e-02,
         3.0916e-03, -5.4395e-01,  3.0280e-02, -3.1792e-01, -1.3500e-01,
        -2.9181e-04, -2.6110e-01,  6.3174e-02,  3.8673e-02, -8.6348e-02,
        -1.9835e-01, -3.5634e-02, -2.8475e-01,  2.6840e-01, -1.1208e-02,
         2.0462e-03,  2.4126e-01, -3.3351e-01, -5.8050e-01,  3.4679e-01,
        -1.8159e-02,  7.4245e-03,  1.3566e-02, -1.7060e-06,  3.4349e-01,
        -1.0565e-03,  7.3623e-03, -8.6841e-02, -3.2311e-02,  7.5918e-03,
        -1.0763e-02,  1.1089e-04, -2.2557e-02,  2.2626e-03, -9.3917e-04,
        -8.7978e-02,  9.2347e-02,  1.9292e-02, -4.4159e-01, -1.6016e-02,
         3.0928e-01,  8.0741e-03,  4.2021e-03, -1.4655e-01, -5.0474e-02,
         1.0154e-03, -2.4131e-02, -1.0266e-01, -4.2929e-02,  3.0572e-01,
        -7.1675e-02,  9.5821e-04, -1.3120e-03, -2.7459e-01,  1.0000e+00,
        -5.5257e-02, -4.7422e-02,  7.4287e-02,  2.6331e-02, -6.8696e-02,
        -3.6926e-01, -8.4093e-02,  1.7593e-02,  1.1288e-01,  9.1335e-02,
        -5.2141e-01, -3.0037e-02, -3.6840e-01,  7.4291e-02,  2.4817e-03,
         2.5920e-03,  8.3414e-02,  3.5848e-02,  2.0743e-03,  1.7235e-01,
         6.7366e-02,  1.7739e-03,  1.1405e-01,  9.0249e-04,  3.4645e-02,
         2.0868e-03,  6.4417e-03,  1.5108e-01, -1.6740e-03,  2.6067e-02,
         3.4368e-03,  5.8322e-03, -2.4079e-02,  7.8452e-02, -2.8438e-03,
        -4.0502e-03, -6.6214e-02,  2.1363e-02,  3.3529e-02,  2.8985e-03,
        -3.6496e-02, -6.1801e-01, -7.6918e-01,  1.2974e-03, -5.2257e-01,
        -1.0671e-02,  1.6316e-03,  1.9493e-02, -3.6425e-01, -1.5587e-01,
         6.2078e-02, -1.7501e-01,  3.3651e-04,  3.9345e-01, -1.7460e-01,
        -7.9458e-02,  4.2796e-02,  2.8005e-02,  8.0180e-02,  2.0030e-01,
        -1.2291e-01,  1.2888e-01, -4.1070e-05,  3.5836e-02,  1.2471e-03,
        -3.7142e-01,  1.1615e-01,  3.9788e-02, -2.6664e-04, -2.4320e-02,
         1.3745e-03,  3.2084e-02,  5.2833e-03,  7.0324e-02, -6.3596e-01,
         3.4879e-01, -4.2586e-02,  2.8223e-03,  3.9192e-02,  5.9683e-03,
         5.2176e-01,  6.6358e-03,  7.1311e-01,  8.8636e-04, -6.3952e-03,
         1.9252e-01, -3.4687e-02, -2.0205e-01,  5.0888e-01, -1.0577e-01,
         1.0878e-02,  5.1083e-03, -6.2362e-04, -4.2878e-03,  1.0310e-01,
        -4.0018e-02,  4.1374e-02, -8.7401e-05, -1.1117e-01, -1.3307e-03,
         5.3671e-03,  1.5089e-02,  1.1177e-01, -2.7275e-01,  1.1250e-01,
         1.5175e-03,  9.7672e-02, -5.9771e-02, -4.0344e-03,  1.3928e-01,
        -2.0880e-01,  2.9071e-02, -4.4837e-01,  1.0571e-02,  6.1559e-02,
        -1.4951e-03, -1.4128e-04,  7.4253e-03, -1.5938e-03,  5.8774e-02,
        -6.5616e-02,  4.0555e-02, -3.2288e-04,  7.6843e-02,  1.3435e-02,
        -1.8902e-01, -2.1602e-01, -2.1546e-04, -2.3642e-01, -9.7630e-03,
        -5.9214e-02,  7.4324e-03, -1.8369e-01,  4.0184e-01, -4.6535e-03,
         8.6960e-01,  3.0498e-03, -3.6308e-02,  3.9974e-03, -5.6076e-01,
         3.9397e-02,  8.9245e-04,  1.1667e-03, -6.9010e-03,  8.9793e-04,
        -1.3371e-03, -2.6858e-03,  8.4899e-02,  4.6295e-02, -1.8025e-02,
         1.3984e-02, -1.0430e-01,  3.3785e-02,  1.3143e-03,  2.1726e-04,
        -1.4915e-01,  4.2533e-01,  4.1536e-03,  8.5240e-04,  9.6794e-01,
        -1.1705e-02,  1.2856e-01,  1.0121e-01,  2.3454e-01, -5.1493e-04,
        -1.8100e-02,  2.3500e-01,  3.2596e-01, -1.3836e-04,  1.3583e-02,
         1.2389e-01, -3.1281e-02, -8.2279e-02,  5.0844e-02, -2.9056e-02,
        -1.7224e-02, -3.6567e-01,  7.9866e-02, -9.4121e-03, -2.1411e-02,
        -1.9907e-01, -3.4653e-01, -2.3140e-02,  3.3252e-01,  4.1728e-02,
        -4.4045e-01, -2.6555e-01,  1.8744e-01, -2.7363e-02,  4.8627e-03,
         1.9625e-02,  3.9223e-01,  6.1587e-03,  2.8713e-02, -2.4643e-02,
        -1.5686e-02, -1.7468e-03,  1.4772e-01, -3.9386e-02,  2.3182e-02,
        -1.4488e-03,  2.6238e-03, -1.4749e-02, -4.0266e-03,  2.2692e-03,
        -1.2087e-03,  3.6382e-02,  3.7031e-02, -4.3878e-02, -4.1625e-01,
         7.3488e-02,  3.4450e-01,  1.2732e-01, -3.3519e-01,  4.7700e-01,
        -6.0740e-01,  1.1303e-01, -4.8066e-01,  2.2418e-02,  2.4444e-03,
         4.6742e-02,  6.0572e-03, -2.3893e-04,  1.0216e-01,  1.7152e-02,
         2.2809e-02,  1.6276e-03, -1.9525e-02,  3.5348e-01, -1.3069e-01,
        -3.7981e-02,  2.1455e-02,  7.7260e-01,  4.2011e-02, -3.9833e-01,
         1.3988e-02, -3.1254e-03, -1.5938e-02,  4.8970e-02,  1.3104e-01,
        -3.8414e-01, -4.1675e-03,  2.1431e-02,  2.1515e-01, -1.3565e-02,
         3.1234e-04,  1.7739e-04, -5.4292e-02,  2.2945e-02,  1.6224e-01,
        -2.4394e-01, -1.3098e-03,  1.7650e-01,  1.1274e-03, -3.2804e-03,
         1.4911e-02,  1.3519e-01,  5.7510e-02, -2.9519e-01, -1.8862e-01,
         1.0553e-02, -1.7893e-02,  2.4967e-02,  1.1009e-02,  8.5479e-02,
         6.3560e-02,  1.3389e-01, -3.7455e-02,  6.6843e-03], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0045,  0.0011, -0.0312,  ...,  0.1205, -0.1157, -0.0391],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.3165, -0.0698, -0.0417,  ..., -0.0022, -0.0142,  0.0693],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([0.0173, 0.2845, 0.0094,  ..., 0.0129, 0.0079, 0.0464], device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.0.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([-0.5105,  0.1496,  0.0392,  0.0903, -0.1568, -0.4092,  0.3688, -0.2736,
        -0.7403,  0.3870,  0.5747,  0.4217, -0.0256,  0.1433,  0.9749, -0.3270,
        -0.4428,  0.0176, -0.6988, -0.6410,  0.4186,  0.6243, -0.1298,  0.0231,
         0.3447, -0.0524, -0.1776, -0.1404,  0.2666, -0.1201,  0.5107,  0.5493,
        -0.6482, -0.5930,  0.6337,  1.0000, -1.0838, -0.0476,  0.7847,  0.0733,
         0.4460,  0.1841,  0.7566, -0.3229,  0.4395, -0.4751, -0.4542,  0.5812,
        -0.0445,  0.4783, -0.7011,  0.4480,  0.2792,  0.7637, -0.0938,  0.6215,
        -0.6184,  0.7859, -0.4463,  0.3975, -0.1486, -0.0076, -0.6155,  0.1102,
        -0.6445, -0.3612, -0.5643, -0.1696,  0.9089,  0.4815,  0.4843,  0.3092,
        -0.0347, -0.5962, -0.0534, -0.7011,  0.1136,  0.2573, -0.9436,  0.5460,
        -0.4472,  0.7750, -0.3512,  0.9307,  0.0703, -0.3815, -0.4113, -0.5002,
        -0.3426, -0.6174,  0.0214, -0.2066, -0.3265,  0.8343, -0.6442,  0.4524],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-0.5105,  0.1496,  0.0392,  0.0903, -0.1568, -0.4092,  0.3688, -0.2736,
        -0.7403,  0.3870,  0.5747,  0.4217, -0.0256,  0.1433,  0.9749, -0.3270,
        -0.4428,  0.0176, -0.6988, -0.6410,  0.4186,  0.6243, -0.1298,  0.0231,
         0.3447, -0.0524, -0.1776, -0.1404,  0.2666, -0.1201,  0.5107,  0.5493,
        -0.6482, -0.5930,  0.6337,  1.0000, -1.0838, -0.0476,  0.7847,  0.0733,
         0.4460,  0.1841,  0.7566, -0.3229,  0.4395, -0.4751, -0.4542,  0.5812,
        -0.0445,  0.4783, -0.7011,  0.4480,  0.2792,  0.7637, -0.0938,  0.6215,
        -0.6184,  0.7859, -0.4463,  0.3975, -0.1486, -0.0076, -0.6155,  0.1102,
        -0.6445, -0.3612, -0.5643, -0.1696,  0.9089,  0.4815,  0.4843,  0.3092,
        -0.0347, -0.5962, -0.0534, -0.7011,  0.1136,  0.2573, -0.9436,  0.5460,
        -0.4472,  0.7750, -0.3512,  0.9307,  0.0703, -0.3815, -0.4113, -0.5002,
        -0.3426, -0.6174,  0.0214, -0.2066, -0.3265,  0.8343, -0.6442,  0.4524],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.9976e-02,  6.1699e-03,  2.4019e-03,  6.0392e-03,  8.4756e-01,
        -7.4254e-01, -1.4550e-03, -6.1978e-02, -7.2665e-03,  1.2007e-01,
        -1.2048e-03,  4.2755e-02,  7.2891e-02,  3.6609e-02,  9.8713e-03,
         4.2635e-02, -1.7606e-02,  1.8680e-02, -1.8105e-01, -2.3585e-01,
         2.5812e-02,  3.5814e-03,  3.2552e-01,  2.9203e-01,  1.5664e-01,
         1.4485e-01, -4.5061e-02, -5.1993e-02, -1.1699e-01,  1.9132e-02,
         1.5974e-02, -2.6301e-01, -2.9754e-01, -2.3803e-02, -2.7128e-02,
        -2.1432e-02,  4.0495e-02,  5.0280e-02, -2.9940e-04,  1.6047e-02,
         1.4387e-01,  5.0591e-03, -4.5106e-03,  6.9360e-04,  4.6246e-02,
        -6.2170e-05, -5.3023e-01,  4.3946e-02, -8.4399e-02, -1.2614e-01,
         1.4879e-02,  4.3965e-03, -9.0257e-03, -6.0968e-03, -1.2920e-01,
         6.5336e-01,  6.4390e-02, -5.9877e-02,  4.4773e-02, -1.1032e-02,
         3.0916e-03, -5.4395e-01,  3.0280e-02, -3.1792e-01, -1.3500e-01,
        -2.9181e-04, -2.6110e-01,  6.3174e-02,  3.8673e-02, -8.6348e-02,
        -1.9835e-01, -3.5634e-02, -2.8475e-01,  2.6840e-01, -1.1208e-02,
         2.0462e-03,  2.4126e-01, -3.3351e-01, -5.8050e-01,  3.4679e-01,
        -1.8159e-02,  7.4245e-03,  1.3566e-02, -1.7060e-06,  3.4349e-01,
        -1.0565e-03,  7.3623e-03, -8.6841e-02, -3.2311e-02,  7.5918e-03,
        -1.0763e-02,  1.1089e-04, -2.2557e-02,  2.2626e-03, -9.3917e-04,
        -8.7978e-02,  9.2347e-02,  1.9292e-02, -4.4159e-01, -1.6016e-02,
         3.0928e-01,  8.0741e-03,  4.2021e-03, -1.4655e-01, -5.0474e-02,
         1.0154e-03, -2.4131e-02, -1.0266e-01, -4.2929e-02,  3.0572e-01,
        -7.1675e-02,  9.5821e-04, -1.3120e-03, -2.7459e-01,  1.0000e+00,
        -5.5257e-02, -4.7422e-02,  7.4287e-02,  2.6331e-02, -6.8696e-02,
        -3.6926e-01, -8.4093e-02,  1.7593e-02,  1.1288e-01,  9.1335e-02,
        -5.2141e-01, -3.0037e-02, -3.6840e-01,  7.4291e-02,  2.4817e-03,
         2.5920e-03,  8.3414e-02,  3.5848e-02,  2.0743e-03,  1.7235e-01,
         6.7366e-02,  1.7739e-03,  1.1405e-01,  9.0249e-04,  3.4645e-02,
         2.0868e-03,  6.4417e-03,  1.5108e-01, -1.6740e-03,  2.6067e-02,
         3.4368e-03,  5.8322e-03, -2.4079e-02,  7.8452e-02, -2.8438e-03,
        -4.0502e-03, -6.6214e-02,  2.1363e-02,  3.3529e-02,  2.8985e-03,
        -3.6496e-02, -6.1801e-01, -7.6918e-01,  1.2974e-03, -5.2257e-01,
        -1.0671e-02,  1.6316e-03,  1.9493e-02, -3.6425e-01, -1.5587e-01,
         6.2078e-02, -1.7501e-01,  3.3651e-04,  3.9345e-01, -1.7460e-01,
        -7.9458e-02,  4.2796e-02,  2.8005e-02,  8.0180e-02,  2.0030e-01,
        -1.2291e-01,  1.2888e-01, -4.1070e-05,  3.5836e-02,  1.2471e-03,
        -3.7142e-01,  1.1615e-01,  3.9788e-02, -2.6664e-04, -2.4320e-02,
         1.3745e-03,  3.2084e-02,  5.2833e-03,  7.0324e-02, -6.3596e-01,
         3.4879e-01, -4.2586e-02,  2.8223e-03,  3.9192e-02,  5.9683e-03,
         5.2176e-01,  6.6358e-03,  7.1311e-01,  8.8636e-04, -6.3952e-03,
         1.9252e-01, -3.4687e-02, -2.0205e-01,  5.0888e-01, -1.0577e-01,
         1.0878e-02,  5.1083e-03, -6.2362e-04, -4.2878e-03,  1.0310e-01,
        -4.0018e-02,  4.1374e-02, -8.7401e-05, -1.1117e-01, -1.3307e-03,
         5.3671e-03,  1.5089e-02,  1.1177e-01, -2.7275e-01,  1.1250e-01,
         1.5175e-03,  9.7672e-02, -5.9771e-02, -4.0344e-03,  1.3928e-01,
        -2.0880e-01,  2.9071e-02, -4.4837e-01,  1.0571e-02,  6.1559e-02,
        -1.4951e-03, -1.4128e-04,  7.4253e-03, -1.5938e-03,  5.8774e-02,
        -6.5616e-02,  4.0555e-02, -3.2288e-04,  7.6843e-02,  1.3435e-02,
        -1.8902e-01, -2.1602e-01, -2.1546e-04, -2.3642e-01, -9.7630e-03,
        -5.9214e-02,  7.4324e-03, -1.8369e-01,  4.0184e-01, -4.6535e-03,
         8.6960e-01,  3.0498e-03, -3.6308e-02,  3.9974e-03, -5.6076e-01,
         3.9397e-02,  8.9245e-04,  1.1667e-03, -6.9010e-03,  8.9793e-04,
        -1.3371e-03, -2.6858e-03,  8.4899e-02,  4.6295e-02, -1.8025e-02,
         1.3984e-02, -1.0430e-01,  3.3785e-02,  1.3143e-03,  2.1726e-04,
        -1.4915e-01,  4.2533e-01,  4.1536e-03,  8.5240e-04,  9.6794e-01,
        -1.1705e-02,  1.2856e-01,  1.0121e-01,  2.3454e-01, -5.1493e-04,
        -1.8100e-02,  2.3500e-01,  3.2596e-01, -1.3836e-04,  1.3583e-02,
         1.2389e-01, -3.1281e-02, -8.2279e-02,  5.0844e-02, -2.9056e-02,
        -1.7224e-02, -3.6567e-01,  7.9866e-02, -9.4121e-03, -2.1411e-02,
        -1.9907e-01, -3.4653e-01, -2.3140e-02,  3.3252e-01,  4.1728e-02,
        -4.4045e-01, -2.6555e-01,  1.8744e-01, -2.7363e-02,  4.8627e-03,
         1.9625e-02,  3.9223e-01,  6.1587e-03,  2.8713e-02, -2.4643e-02,
        -1.5686e-02, -1.7468e-03,  1.4772e-01, -3.9386e-02,  2.3182e-02,
        -1.4488e-03,  2.6238e-03, -1.4749e-02, -4.0266e-03,  2.2692e-03,
        -1.2087e-03,  3.6382e-02,  3.7031e-02, -4.3878e-02, -4.1625e-01,
         7.3488e-02,  3.4450e-01,  1.2732e-01, -3.3519e-01,  4.7700e-01,
        -6.0740e-01,  1.1303e-01, -4.8066e-01,  2.2418e-02,  2.4444e-03,
         4.6742e-02,  6.0572e-03, -2.3893e-04,  1.0216e-01,  1.7152e-02,
         2.2809e-02,  1.6276e-03, -1.9525e-02,  3.5348e-01, -1.3069e-01,
        -3.7981e-02,  2.1455e-02,  7.7260e-01,  4.2011e-02, -3.9833e-01,
         1.3988e-02, -3.1254e-03, -1.5938e-02,  4.8970e-02,  1.3104e-01,
        -3.8414e-01, -4.1675e-03,  2.1431e-02,  2.1515e-01, -1.3565e-02,
         3.1234e-04,  1.7739e-04, -5.4292e-02,  2.2945e-02,  1.6224e-01,
        -2.4394e-01, -1.3098e-03,  1.7650e-01,  1.1274e-03, -3.2804e-03,
         1.4911e-02,  1.3519e-01,  5.7510e-02, -2.9519e-01, -1.8862e-01,
         1.0553e-02, -1.7893e-02,  2.4967e-02,  1.1009e-02,  8.5479e-02,
         6.3560e-02,  1.3389e-01, -3.7455e-02,  6.6843e-03], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0045,  0.0011, -0.0312,  ...,  0.1205, -0.1157, -0.0391],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.3165, -0.0698, -0.0417,  ..., -0.0022, -0.0142,  0.0693],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([0.0173, 0.2845, 0.0094,  ..., 0.0129, 0.0079, 0.0464], device='cuda:0')}
topk_layer inside model after retrieval:  [25, 59, 30, 55, 42, 69, 5, 68, 93, 16, 57, 14, 36, 38, 77]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-0.5105,  0.1496,  0.0392,  0.0903, -0.1568, -0.4092,  0.3688, -0.2736,
        -0.7403,  0.3870,  0.5747,  0.4217, -0.0256,  0.1433,  0.9749, -0.3270,
        -0.4428,  0.0176, -0.6988, -0.6410,  0.4186,  0.6243, -0.1298,  0.0231,
         0.3447, -0.0524, -0.1776, -0.1404,  0.2666, -0.1201,  0.5107,  0.5493,
        -0.6482, -0.5930,  0.6337,  1.0000, -1.0838, -0.0476,  0.7847,  0.0733,
         0.4460,  0.1841,  0.7566, -0.3229,  0.4395, -0.4751, -0.4542,  0.5812,
        -0.0445,  0.4783, -0.7011,  0.4480,  0.2792,  0.7637, -0.0938,  0.6215,
        -0.6184,  0.7859, -0.4463,  0.3975, -0.1486, -0.0076, -0.6155,  0.1102,
        -0.6445, -0.3612, -0.5643, -0.1696,  0.9089,  0.4815,  0.4843,  0.3092,
        -0.0347, -0.5962, -0.0534, -0.7011,  0.1136,  0.2573, -0.9436,  0.5460,
        -0.4472,  0.7750, -0.3512,  0.9307,  0.0703, -0.3815, -0.4113, -0.5002,
        -0.3426, -0.6174,  0.0214, -0.2066, -0.3265,  0.8343, -0.6442,  0.4524],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.9976e-02,  6.1699e-03,  2.4019e-03,  6.0392e-03,  8.4756e-01,
        -7.4254e-01, -1.4550e-03, -6.1978e-02, -7.2665e-03,  1.2007e-01,
        -1.2048e-03,  4.2755e-02,  7.2891e-02,  3.6609e-02,  9.8713e-03,
         4.2635e-02, -1.7606e-02,  1.8680e-02, -1.8105e-01, -2.3585e-01,
         2.5812e-02,  3.5814e-03,  3.2552e-01,  2.9203e-01,  1.5664e-01,
         1.4485e-01, -4.5061e-02, -5.1993e-02, -1.1699e-01,  1.9132e-02,
         1.5974e-02, -2.6301e-01, -2.9754e-01, -2.3803e-02, -2.7128e-02,
        -2.1432e-02,  4.0495e-02,  5.0280e-02, -2.9940e-04,  1.6047e-02,
         1.4387e-01,  5.0591e-03, -4.5106e-03,  6.9360e-04,  4.6246e-02,
        -6.2170e-05, -5.3023e-01,  4.3946e-02, -8.4399e-02, -1.2614e-01,
         1.4879e-02,  4.3965e-03, -9.0257e-03, -6.0968e-03, -1.2920e-01,
         6.5336e-01,  6.4390e-02, -5.9877e-02,  4.4773e-02, -1.1032e-02,
         3.0916e-03, -5.4395e-01,  3.0280e-02, -3.1792e-01, -1.3500e-01,
        -2.9181e-04, -2.6110e-01,  6.3174e-02,  3.8673e-02, -8.6348e-02,
        -1.9835e-01, -3.5634e-02, -2.8475e-01,  2.6840e-01, -1.1208e-02,
         2.0462e-03,  2.4126e-01, -3.3351e-01, -5.8050e-01,  3.4679e-01,
        -1.8159e-02,  7.4245e-03,  1.3566e-02, -1.7060e-06,  3.4349e-01,
        -1.0565e-03,  7.3623e-03, -8.6841e-02, -3.2311e-02,  7.5918e-03,
        -1.0763e-02,  1.1089e-04, -2.2557e-02,  2.2626e-03, -9.3917e-04,
        -8.7978e-02,  9.2347e-02,  1.9292e-02, -4.4159e-01, -1.6016e-02,
         3.0928e-01,  8.0741e-03,  4.2021e-03, -1.4655e-01, -5.0474e-02,
         1.0154e-03, -2.4131e-02, -1.0266e-01, -4.2929e-02,  3.0572e-01,
        -7.1675e-02,  9.5821e-04, -1.3120e-03, -2.7459e-01,  1.0000e+00,
        -5.5257e-02, -4.7422e-02,  7.4287e-02,  2.6331e-02, -6.8696e-02,
        -3.6926e-01, -8.4093e-02,  1.7593e-02,  1.1288e-01,  9.1335e-02,
        -5.2141e-01, -3.0037e-02, -3.6840e-01,  7.4291e-02,  2.4817e-03,
         2.5920e-03,  8.3414e-02,  3.5848e-02,  2.0743e-03,  1.7235e-01,
         6.7366e-02,  1.7739e-03,  1.1405e-01,  9.0249e-04,  3.4645e-02,
         2.0868e-03,  6.4417e-03,  1.5108e-01, -1.6740e-03,  2.6067e-02,
         3.4368e-03,  5.8322e-03, -2.4079e-02,  7.8452e-02, -2.8438e-03,
        -4.0502e-03, -6.6214e-02,  2.1363e-02,  3.3529e-02,  2.8985e-03,
        -3.6496e-02, -6.1801e-01, -7.6918e-01,  1.2974e-03, -5.2257e-01,
        -1.0671e-02,  1.6316e-03,  1.9493e-02, -3.6425e-01, -1.5587e-01,
         6.2078e-02, -1.7501e-01,  3.3651e-04,  3.9345e-01, -1.7460e-01,
        -7.9458e-02,  4.2796e-02,  2.8005e-02,  8.0180e-02,  2.0030e-01,
        -1.2291e-01,  1.2888e-01, -4.1070e-05,  3.5836e-02,  1.2471e-03,
        -3.7142e-01,  1.1615e-01,  3.9788e-02, -2.6664e-04, -2.4320e-02,
         1.3745e-03,  3.2084e-02,  5.2833e-03,  7.0324e-02, -6.3596e-01,
         3.4879e-01, -4.2586e-02,  2.8223e-03,  3.9192e-02,  5.9683e-03,
         5.2176e-01,  6.6358e-03,  7.1311e-01,  8.8636e-04, -6.3952e-03,
         1.9252e-01, -3.4687e-02, -2.0205e-01,  5.0888e-01, -1.0577e-01,
         1.0878e-02,  5.1083e-03, -6.2362e-04, -4.2878e-03,  1.0310e-01,
        -4.0018e-02,  4.1374e-02, -8.7401e-05, -1.1117e-01, -1.3307e-03,
         5.3671e-03,  1.5089e-02,  1.1177e-01, -2.7275e-01,  1.1250e-01,
         1.5175e-03,  9.7672e-02, -5.9771e-02, -4.0344e-03,  1.3928e-01,
        -2.0880e-01,  2.9071e-02, -4.4837e-01,  1.0571e-02,  6.1559e-02,
        -1.4951e-03, -1.4128e-04,  7.4253e-03, -1.5938e-03,  5.8774e-02,
        -6.5616e-02,  4.0555e-02, -3.2288e-04,  7.6843e-02,  1.3435e-02,
        -1.8902e-01, -2.1602e-01, -2.1546e-04, -2.3642e-01, -9.7630e-03,
        -5.9214e-02,  7.4324e-03, -1.8369e-01,  4.0184e-01, -4.6535e-03,
         8.6960e-01,  3.0498e-03, -3.6308e-02,  3.9974e-03, -5.6076e-01,
         3.9397e-02,  8.9245e-04,  1.1667e-03, -6.9010e-03,  8.9793e-04,
        -1.3371e-03, -2.6858e-03,  8.4899e-02,  4.6295e-02, -1.8025e-02,
         1.3984e-02, -1.0430e-01,  3.3785e-02,  1.3143e-03,  2.1726e-04,
        -1.4915e-01,  4.2533e-01,  4.1536e-03,  8.5240e-04,  9.6794e-01,
        -1.1705e-02,  1.2856e-01,  1.0121e-01,  2.3454e-01, -5.1493e-04,
        -1.8100e-02,  2.3500e-01,  3.2596e-01, -1.3836e-04,  1.3583e-02,
         1.2389e-01, -3.1281e-02, -8.2279e-02,  5.0844e-02, -2.9056e-02,
        -1.7224e-02, -3.6567e-01,  7.9866e-02, -9.4121e-03, -2.1411e-02,
        -1.9907e-01, -3.4653e-01, -2.3140e-02,  3.3252e-01,  4.1728e-02,
        -4.4045e-01, -2.6555e-01,  1.8744e-01, -2.7363e-02,  4.8627e-03,
         1.9625e-02,  3.9223e-01,  6.1587e-03,  2.8713e-02, -2.4643e-02,
        -1.5686e-02, -1.7468e-03,  1.4772e-01, -3.9386e-02,  2.3182e-02,
        -1.4488e-03,  2.6238e-03, -1.4749e-02, -4.0266e-03,  2.2692e-03,
        -1.2087e-03,  3.6382e-02,  3.7031e-02, -4.3878e-02, -4.1625e-01,
         7.3488e-02,  3.4450e-01,  1.2732e-01, -3.3519e-01,  4.7700e-01,
        -6.0740e-01,  1.1303e-01, -4.8066e-01,  2.2418e-02,  2.4444e-03,
         4.6742e-02,  6.0572e-03, -2.3893e-04,  1.0216e-01,  1.7152e-02,
         2.2809e-02,  1.6276e-03, -1.9525e-02,  3.5348e-01, -1.3069e-01,
        -3.7981e-02,  2.1455e-02,  7.7260e-01,  4.2011e-02, -3.9833e-01,
         1.3988e-02, -3.1254e-03, -1.5938e-02,  4.8970e-02,  1.3104e-01,
        -3.8414e-01, -4.1675e-03,  2.1431e-02,  2.1515e-01, -1.3565e-02,
         3.1234e-04,  1.7739e-04, -5.4292e-02,  2.2945e-02,  1.6224e-01,
        -2.4394e-01, -1.3098e-03,  1.7650e-01,  1.1274e-03, -3.2804e-03,
         1.4911e-02,  1.3519e-01,  5.7510e-02, -2.9519e-01, -1.8862e-01,
         1.0553e-02, -1.7893e-02,  2.4967e-02,  1.1009e-02,  8.5479e-02,
         6.3560e-02,  1.3389e-01, -3.7455e-02,  6.6843e-03], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0045,  0.0011, -0.0312,  ...,  0.1205, -0.1157, -0.0391],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.3165, -0.0698, -0.0417,  ..., -0.0022, -0.0142,  0.0693],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([0.0173, 0.2845, 0.0094,  ..., 0.0129, 0.0079, 0.0464], device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.1.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 2.9976e-02,  6.1699e-03,  2.4019e-03,  6.0392e-03,  8.4756e-01,
        -7.4254e-01, -1.4550e-03, -6.1978e-02, -7.2665e-03,  1.2007e-01,
        -1.2048e-03,  4.2755e-02,  7.2891e-02,  3.6609e-02,  9.8713e-03,
         4.2635e-02, -1.7606e-02,  1.8680e-02, -1.8105e-01, -2.3585e-01,
         2.5812e-02,  3.5814e-03,  3.2552e-01,  2.9203e-01,  1.5664e-01,
         1.4485e-01, -4.5061e-02, -5.1993e-02, -1.1699e-01,  1.9132e-02,
         1.5974e-02, -2.6301e-01, -2.9754e-01, -2.3803e-02, -2.7128e-02,
        -2.1432e-02,  4.0495e-02,  5.0280e-02, -2.9940e-04,  1.6047e-02,
         1.4387e-01,  5.0591e-03, -4.5106e-03,  6.9360e-04,  4.6246e-02,
        -6.2170e-05, -5.3023e-01,  4.3946e-02, -8.4399e-02, -1.2614e-01,
         1.4879e-02,  4.3965e-03, -9.0257e-03, -6.0968e-03, -1.2920e-01,
         6.5336e-01,  6.4390e-02, -5.9877e-02,  4.4773e-02, -1.1032e-02,
         3.0916e-03, -5.4395e-01,  3.0280e-02, -3.1792e-01, -1.3500e-01,
        -2.9181e-04, -2.6110e-01,  6.3174e-02,  3.8673e-02, -8.6348e-02,
        -1.9835e-01, -3.5634e-02, -2.8475e-01,  2.6840e-01, -1.1208e-02,
         2.0462e-03,  2.4126e-01, -3.3351e-01, -5.8050e-01,  3.4679e-01,
        -1.8159e-02,  7.4245e-03,  1.3566e-02, -1.7060e-06,  3.4349e-01,
        -1.0565e-03,  7.3623e-03, -8.6841e-02, -3.2311e-02,  7.5918e-03,
        -1.0763e-02,  1.1089e-04, -2.2557e-02,  2.2626e-03, -9.3917e-04,
        -8.7978e-02,  9.2347e-02,  1.9292e-02, -4.4159e-01, -1.6016e-02,
         3.0928e-01,  8.0741e-03,  4.2021e-03, -1.4655e-01, -5.0474e-02,
         1.0154e-03, -2.4131e-02, -1.0266e-01, -4.2929e-02,  3.0572e-01,
        -7.1675e-02,  9.5821e-04, -1.3120e-03, -2.7459e-01,  1.0000e+00,
        -5.5257e-02, -4.7422e-02,  7.4287e-02,  2.6331e-02, -6.8696e-02,
        -3.6926e-01, -8.4093e-02,  1.7593e-02,  1.1288e-01,  9.1335e-02,
        -5.2141e-01, -3.0037e-02, -3.6840e-01,  7.4291e-02,  2.4817e-03,
         2.5920e-03,  8.3414e-02,  3.5848e-02,  2.0743e-03,  1.7235e-01,
         6.7366e-02,  1.7739e-03,  1.1405e-01,  9.0249e-04,  3.4645e-02,
         2.0868e-03,  6.4417e-03,  1.5108e-01, -1.6740e-03,  2.6067e-02,
         3.4368e-03,  5.8322e-03, -2.4079e-02,  7.8452e-02, -2.8438e-03,
        -4.0502e-03, -6.6214e-02,  2.1363e-02,  3.3529e-02,  2.8985e-03,
        -3.6496e-02, -6.1801e-01, -7.6918e-01,  1.2974e-03, -5.2257e-01,
        -1.0671e-02,  1.6316e-03,  1.9493e-02, -3.6425e-01, -1.5587e-01,
         6.2078e-02, -1.7501e-01,  3.3651e-04,  3.9345e-01, -1.7460e-01,
        -7.9458e-02,  4.2796e-02,  2.8005e-02,  8.0180e-02,  2.0030e-01,
        -1.2291e-01,  1.2888e-01, -4.1070e-05,  3.5836e-02,  1.2471e-03,
        -3.7142e-01,  1.1615e-01,  3.9788e-02, -2.6664e-04, -2.4320e-02,
         1.3745e-03,  3.2084e-02,  5.2833e-03,  7.0324e-02, -6.3596e-01,
         3.4879e-01, -4.2586e-02,  2.8223e-03,  3.9192e-02,  5.9683e-03,
         5.2176e-01,  6.6358e-03,  7.1311e-01,  8.8636e-04, -6.3952e-03,
         1.9252e-01, -3.4687e-02, -2.0205e-01,  5.0888e-01, -1.0577e-01,
         1.0878e-02,  5.1083e-03, -6.2362e-04, -4.2878e-03,  1.0310e-01,
        -4.0018e-02,  4.1374e-02, -8.7401e-05, -1.1117e-01, -1.3307e-03,
         5.3671e-03,  1.5089e-02,  1.1177e-01, -2.7275e-01,  1.1250e-01,
         1.5175e-03,  9.7672e-02, -5.9771e-02, -4.0344e-03,  1.3928e-01,
        -2.0880e-01,  2.9071e-02, -4.4837e-01,  1.0571e-02,  6.1559e-02,
        -1.4951e-03, -1.4128e-04,  7.4253e-03, -1.5938e-03,  5.8774e-02,
        -6.5616e-02,  4.0555e-02, -3.2288e-04,  7.6843e-02,  1.3435e-02,
        -1.8902e-01, -2.1602e-01, -2.1546e-04, -2.3642e-01, -9.7630e-03,
        -5.9214e-02,  7.4324e-03, -1.8369e-01,  4.0184e-01, -4.6535e-03,
         8.6960e-01,  3.0498e-03, -3.6308e-02,  3.9974e-03, -5.6076e-01,
         3.9397e-02,  8.9245e-04,  1.1667e-03, -6.9010e-03,  8.9793e-04,
        -1.3371e-03, -2.6858e-03,  8.4899e-02,  4.6295e-02, -1.8025e-02,
         1.3984e-02, -1.0430e-01,  3.3785e-02,  1.3143e-03,  2.1726e-04,
        -1.4915e-01,  4.2533e-01,  4.1536e-03,  8.5240e-04,  9.6794e-01,
        -1.1705e-02,  1.2856e-01,  1.0121e-01,  2.3454e-01, -5.1493e-04,
        -1.8100e-02,  2.3500e-01,  3.2596e-01, -1.3836e-04,  1.3583e-02,
         1.2389e-01, -3.1281e-02, -8.2279e-02,  5.0844e-02, -2.9056e-02,
        -1.7224e-02, -3.6567e-01,  7.9866e-02, -9.4121e-03, -2.1411e-02,
        -1.9907e-01, -3.4653e-01, -2.3140e-02,  3.3252e-01,  4.1728e-02,
        -4.4045e-01, -2.6555e-01,  1.8744e-01, -2.7363e-02,  4.8627e-03,
         1.9625e-02,  3.9223e-01,  6.1587e-03,  2.8713e-02, -2.4643e-02,
        -1.5686e-02, -1.7468e-03,  1.4772e-01, -3.9386e-02,  2.3182e-02,
        -1.4488e-03,  2.6238e-03, -1.4749e-02, -4.0266e-03,  2.2692e-03,
        -1.2087e-03,  3.6382e-02,  3.7031e-02, -4.3878e-02, -4.1625e-01,
         7.3488e-02,  3.4450e-01,  1.2732e-01, -3.3519e-01,  4.7700e-01,
        -6.0740e-01,  1.1303e-01, -4.8066e-01,  2.2418e-02,  2.4444e-03,
         4.6742e-02,  6.0572e-03, -2.3893e-04,  1.0216e-01,  1.7152e-02,
         2.2809e-02,  1.6276e-03, -1.9525e-02,  3.5348e-01, -1.3069e-01,
        -3.7981e-02,  2.1455e-02,  7.7260e-01,  4.2011e-02, -3.9833e-01,
         1.3988e-02, -3.1254e-03, -1.5938e-02,  4.8970e-02,  1.3104e-01,
        -3.8414e-01, -4.1675e-03,  2.1431e-02,  2.1515e-01, -1.3565e-02,
         3.1234e-04,  1.7739e-04, -5.4292e-02,  2.2945e-02,  1.6224e-01,
        -2.4394e-01, -1.3098e-03,  1.7650e-01,  1.1274e-03, -3.2804e-03,
         1.4911e-02,  1.3519e-01,  5.7510e-02, -2.9519e-01, -1.8862e-01,
         1.0553e-02, -1.7893e-02,  2.4967e-02,  1.1009e-02,  8.5479e-02,
         6.3560e-02,  1.3389e-01, -3.7455e-02,  6.6843e-03], device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-0.5105,  0.1496,  0.0392,  0.0903, -0.1568, -0.4092,  0.3688, -0.2736,
        -0.7403,  0.3870,  0.5747,  0.4217, -0.0256,  0.1433,  0.9749, -0.3270,
        -0.4428,  0.0176, -0.6988, -0.6410,  0.4186,  0.6243, -0.1298,  0.0231,
         0.3447, -0.0524, -0.1776, -0.1404,  0.2666, -0.1201,  0.5107,  0.5493,
        -0.6482, -0.5930,  0.6337,  1.0000, -1.0838, -0.0476,  0.7847,  0.0733,
         0.4460,  0.1841,  0.7566, -0.3229,  0.4395, -0.4751, -0.4542,  0.5812,
        -0.0445,  0.4783, -0.7011,  0.4480,  0.2792,  0.7637, -0.0938,  0.6215,
        -0.6184,  0.7859, -0.4463,  0.3975, -0.1486, -0.0076, -0.6155,  0.1102,
        -0.6445, -0.3612, -0.5643, -0.1696,  0.9089,  0.4815,  0.4843,  0.3092,
        -0.0347, -0.5962, -0.0534, -0.7011,  0.1136,  0.2573, -0.9436,  0.5460,
        -0.4472,  0.7750, -0.3512,  0.9307,  0.0703, -0.3815, -0.4113, -0.5002,
        -0.3426, -0.6174,  0.0214, -0.2066, -0.3265,  0.8343, -0.6442,  0.4524],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.9976e-02,  6.1699e-03,  2.4019e-03,  6.0392e-03,  8.4756e-01,
        -7.4254e-01, -1.4550e-03, -6.1978e-02, -7.2665e-03,  1.2007e-01,
        -1.2048e-03,  4.2755e-02,  7.2891e-02,  3.6609e-02,  9.8713e-03,
         4.2635e-02, -1.7606e-02,  1.8680e-02, -1.8105e-01, -2.3585e-01,
         2.5812e-02,  3.5814e-03,  3.2552e-01,  2.9203e-01,  1.5664e-01,
         1.4485e-01, -4.5061e-02, -5.1993e-02, -1.1699e-01,  1.9132e-02,
         1.5974e-02, -2.6301e-01, -2.9754e-01, -2.3803e-02, -2.7128e-02,
        -2.1432e-02,  4.0495e-02,  5.0280e-02, -2.9940e-04,  1.6047e-02,
         1.4387e-01,  5.0591e-03, -4.5106e-03,  6.9360e-04,  4.6246e-02,
        -6.2170e-05, -5.3023e-01,  4.3946e-02, -8.4399e-02, -1.2614e-01,
         1.4879e-02,  4.3965e-03, -9.0257e-03, -6.0968e-03, -1.2920e-01,
         6.5336e-01,  6.4390e-02, -5.9877e-02,  4.4773e-02, -1.1032e-02,
         3.0916e-03, -5.4395e-01,  3.0280e-02, -3.1792e-01, -1.3500e-01,
        -2.9181e-04, -2.6110e-01,  6.3174e-02,  3.8673e-02, -8.6348e-02,
        -1.9835e-01, -3.5634e-02, -2.8475e-01,  2.6840e-01, -1.1208e-02,
         2.0462e-03,  2.4126e-01, -3.3351e-01, -5.8050e-01,  3.4679e-01,
        -1.8159e-02,  7.4245e-03,  1.3566e-02, -1.7060e-06,  3.4349e-01,
        -1.0565e-03,  7.3623e-03, -8.6841e-02, -3.2311e-02,  7.5918e-03,
        -1.0763e-02,  1.1089e-04, -2.2557e-02,  2.2626e-03, -9.3917e-04,
        -8.7978e-02,  9.2347e-02,  1.9292e-02, -4.4159e-01, -1.6016e-02,
         3.0928e-01,  8.0741e-03,  4.2021e-03, -1.4655e-01, -5.0474e-02,
         1.0154e-03, -2.4131e-02, -1.0266e-01, -4.2929e-02,  3.0572e-01,
        -7.1675e-02,  9.5821e-04, -1.3120e-03, -2.7459e-01,  1.0000e+00,
        -5.5257e-02, -4.7422e-02,  7.4287e-02,  2.6331e-02, -6.8696e-02,
        -3.6926e-01, -8.4093e-02,  1.7593e-02,  1.1288e-01,  9.1335e-02,
        -5.2141e-01, -3.0037e-02, -3.6840e-01,  7.4291e-02,  2.4817e-03,
         2.5920e-03,  8.3414e-02,  3.5848e-02,  2.0743e-03,  1.7235e-01,
         6.7366e-02,  1.7739e-03,  1.1405e-01,  9.0249e-04,  3.4645e-02,
         2.0868e-03,  6.4417e-03,  1.5108e-01, -1.6740e-03,  2.6067e-02,
         3.4368e-03,  5.8322e-03, -2.4079e-02,  7.8452e-02, -2.8438e-03,
        -4.0502e-03, -6.6214e-02,  2.1363e-02,  3.3529e-02,  2.8985e-03,
        -3.6496e-02, -6.1801e-01, -7.6918e-01,  1.2974e-03, -5.2257e-01,
        -1.0671e-02,  1.6316e-03,  1.9493e-02, -3.6425e-01, -1.5587e-01,
         6.2078e-02, -1.7501e-01,  3.3651e-04,  3.9345e-01, -1.7460e-01,
        -7.9458e-02,  4.2796e-02,  2.8005e-02,  8.0180e-02,  2.0030e-01,
        -1.2291e-01,  1.2888e-01, -4.1070e-05,  3.5836e-02,  1.2471e-03,
        -3.7142e-01,  1.1615e-01,  3.9788e-02, -2.6664e-04, -2.4320e-02,
         1.3745e-03,  3.2084e-02,  5.2833e-03,  7.0324e-02, -6.3596e-01,
         3.4879e-01, -4.2586e-02,  2.8223e-03,  3.9192e-02,  5.9683e-03,
         5.2176e-01,  6.6358e-03,  7.1311e-01,  8.8636e-04, -6.3952e-03,
         1.9252e-01, -3.4687e-02, -2.0205e-01,  5.0888e-01, -1.0577e-01,
         1.0878e-02,  5.1083e-03, -6.2362e-04, -4.2878e-03,  1.0310e-01,
        -4.0018e-02,  4.1374e-02, -8.7401e-05, -1.1117e-01, -1.3307e-03,
         5.3671e-03,  1.5089e-02,  1.1177e-01, -2.7275e-01,  1.1250e-01,
         1.5175e-03,  9.7672e-02, -5.9771e-02, -4.0344e-03,  1.3928e-01,
        -2.0880e-01,  2.9071e-02, -4.4837e-01,  1.0571e-02,  6.1559e-02,
        -1.4951e-03, -1.4128e-04,  7.4253e-03, -1.5938e-03,  5.8774e-02,
        -6.5616e-02,  4.0555e-02, -3.2288e-04,  7.6843e-02,  1.3435e-02,
        -1.8902e-01, -2.1602e-01, -2.1546e-04, -2.3642e-01, -9.7630e-03,
        -5.9214e-02,  7.4324e-03, -1.8369e-01,  4.0184e-01, -4.6535e-03,
         8.6960e-01,  3.0498e-03, -3.6308e-02,  3.9974e-03, -5.6076e-01,
         3.9397e-02,  8.9245e-04,  1.1667e-03, -6.9010e-03,  8.9793e-04,
        -1.3371e-03, -2.6858e-03,  8.4899e-02,  4.6295e-02, -1.8025e-02,
         1.3984e-02, -1.0430e-01,  3.3785e-02,  1.3143e-03,  2.1726e-04,
        -1.4915e-01,  4.2533e-01,  4.1536e-03,  8.5240e-04,  9.6794e-01,
        -1.1705e-02,  1.2856e-01,  1.0121e-01,  2.3454e-01, -5.1493e-04,
        -1.8100e-02,  2.3500e-01,  3.2596e-01, -1.3836e-04,  1.3583e-02,
         1.2389e-01, -3.1281e-02, -8.2279e-02,  5.0844e-02, -2.9056e-02,
        -1.7224e-02, -3.6567e-01,  7.9866e-02, -9.4121e-03, -2.1411e-02,
        -1.9907e-01, -3.4653e-01, -2.3140e-02,  3.3252e-01,  4.1728e-02,
        -4.4045e-01, -2.6555e-01,  1.8744e-01, -2.7363e-02,  4.8627e-03,
         1.9625e-02,  3.9223e-01,  6.1587e-03,  2.8713e-02, -2.4643e-02,
        -1.5686e-02, -1.7468e-03,  1.4772e-01, -3.9386e-02,  2.3182e-02,
        -1.4488e-03,  2.6238e-03, -1.4749e-02, -4.0266e-03,  2.2692e-03,
        -1.2087e-03,  3.6382e-02,  3.7031e-02, -4.3878e-02, -4.1625e-01,
         7.3488e-02,  3.4450e-01,  1.2732e-01, -3.3519e-01,  4.7700e-01,
        -6.0740e-01,  1.1303e-01, -4.8066e-01,  2.2418e-02,  2.4444e-03,
         4.6742e-02,  6.0572e-03, -2.3893e-04,  1.0216e-01,  1.7152e-02,
         2.2809e-02,  1.6276e-03, -1.9525e-02,  3.5348e-01, -1.3069e-01,
        -3.7981e-02,  2.1455e-02,  7.7260e-01,  4.2011e-02, -3.9833e-01,
         1.3988e-02, -3.1254e-03, -1.5938e-02,  4.8970e-02,  1.3104e-01,
        -3.8414e-01, -4.1675e-03,  2.1431e-02,  2.1515e-01, -1.3565e-02,
         3.1234e-04,  1.7739e-04, -5.4292e-02,  2.2945e-02,  1.6224e-01,
        -2.4394e-01, -1.3098e-03,  1.7650e-01,  1.1274e-03, -3.2804e-03,
         1.4911e-02,  1.3519e-01,  5.7510e-02, -2.9519e-01, -1.8862e-01,
         1.0553e-02, -1.7893e-02,  2.4967e-02,  1.1009e-02,  8.5479e-02,
         6.3560e-02,  1.3389e-01, -3.7455e-02,  6.6843e-03], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0045,  0.0011, -0.0312,  ...,  0.1205, -0.1157, -0.0391],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.3165, -0.0698, -0.0417,  ..., -0.0022, -0.0142,  0.0693],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([0.0173, 0.2845, 0.0094,  ..., 0.0129, 0.0079, 0.0464], device='cuda:0')}
topk_layer inside model after retrieval:  [197, 349, 72, 345, 121, 76, 124, 193, 109, 365, 274, 296, 354, 92, 73, 56, 75, 105, 36, 188, 202, 343, 336, 304, 162, 41, 266, 159, 100, 104, 163, 148, 22, 9, 186, 229, 190, 379, 4, 174, 358, 114, 118, 373, 146, 217, 278, 347, 70, 191, 169, 99, 50, 180, 137, 26, 94, 383, 324]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-0.5105,  0.1496,  0.0392,  0.0903, -0.1568, -0.4092,  0.3688, -0.2736,
        -0.7403,  0.3870,  0.5747,  0.4217, -0.0256,  0.1433,  0.9749, -0.3270,
        -0.4428,  0.0176, -0.6988, -0.6410,  0.4186,  0.6243, -0.1298,  0.0231,
         0.3447, -0.0524, -0.1776, -0.1404,  0.2666, -0.1201,  0.5107,  0.5493,
        -0.6482, -0.5930,  0.6337,  1.0000, -1.0838, -0.0476,  0.7847,  0.0733,
         0.4460,  0.1841,  0.7566, -0.3229,  0.4395, -0.4751, -0.4542,  0.5812,
        -0.0445,  0.4783, -0.7011,  0.4480,  0.2792,  0.7637, -0.0938,  0.6215,
        -0.6184,  0.7859, -0.4463,  0.3975, -0.1486, -0.0076, -0.6155,  0.1102,
        -0.6445, -0.3612, -0.5643, -0.1696,  0.9089,  0.4815,  0.4843,  0.3092,
        -0.0347, -0.5962, -0.0534, -0.7011,  0.1136,  0.2573, -0.9436,  0.5460,
        -0.4472,  0.7750, -0.3512,  0.9307,  0.0703, -0.3815, -0.4113, -0.5002,
        -0.3426, -0.6174,  0.0214, -0.2066, -0.3265,  0.8343, -0.6442,  0.4524],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.9976e-02,  6.1699e-03,  2.4019e-03,  6.0392e-03,  8.4756e-01,
        -7.4254e-01, -1.4550e-03, -6.1978e-02, -7.2665e-03,  1.2007e-01,
        -1.2048e-03,  4.2755e-02,  7.2891e-02,  3.6609e-02,  9.8713e-03,
         4.2635e-02, -1.7606e-02,  1.8680e-02, -1.8105e-01, -2.3585e-01,
         2.5812e-02,  3.5814e-03,  3.2552e-01,  2.9203e-01,  1.5664e-01,
         1.4485e-01, -4.5061e-02, -5.1993e-02, -1.1699e-01,  1.9132e-02,
         1.5974e-02, -2.6301e-01, -2.9754e-01, -2.3803e-02, -2.7128e-02,
        -2.1432e-02,  4.0495e-02,  5.0280e-02, -2.9940e-04,  1.6047e-02,
         1.4387e-01,  5.0591e-03, -4.5106e-03,  6.9360e-04,  4.6246e-02,
        -6.2170e-05, -5.3023e-01,  4.3946e-02, -8.4399e-02, -1.2614e-01,
         1.4879e-02,  4.3965e-03, -9.0257e-03, -6.0968e-03, -1.2920e-01,
         6.5336e-01,  6.4390e-02, -5.9877e-02,  4.4773e-02, -1.1032e-02,
         3.0916e-03, -5.4395e-01,  3.0280e-02, -3.1792e-01, -1.3500e-01,
        -2.9181e-04, -2.6110e-01,  6.3174e-02,  3.8673e-02, -8.6348e-02,
        -1.9835e-01, -3.5634e-02, -2.8475e-01,  2.6840e-01, -1.1208e-02,
         2.0462e-03,  2.4126e-01, -3.3351e-01, -5.8050e-01,  3.4679e-01,
        -1.8159e-02,  7.4245e-03,  1.3566e-02, -1.7060e-06,  3.4349e-01,
        -1.0565e-03,  7.3623e-03, -8.6841e-02, -3.2311e-02,  7.5918e-03,
        -1.0763e-02,  1.1089e-04, -2.2557e-02,  2.2626e-03, -9.3917e-04,
        -8.7978e-02,  9.2347e-02,  1.9292e-02, -4.4159e-01, -1.6016e-02,
         3.0928e-01,  8.0741e-03,  4.2021e-03, -1.4655e-01, -5.0474e-02,
         1.0154e-03, -2.4131e-02, -1.0266e-01, -4.2929e-02,  3.0572e-01,
        -7.1675e-02,  9.5821e-04, -1.3120e-03, -2.7459e-01,  1.0000e+00,
        -5.5257e-02, -4.7422e-02,  7.4287e-02,  2.6331e-02, -6.8696e-02,
        -3.6926e-01, -8.4093e-02,  1.7593e-02,  1.1288e-01,  9.1335e-02,
        -5.2141e-01, -3.0037e-02, -3.6840e-01,  7.4291e-02,  2.4817e-03,
         2.5920e-03,  8.3414e-02,  3.5848e-02,  2.0743e-03,  1.7235e-01,
         6.7366e-02,  1.7739e-03,  1.1405e-01,  9.0249e-04,  3.4645e-02,
         2.0868e-03,  6.4417e-03,  1.5108e-01, -1.6740e-03,  2.6067e-02,
         3.4368e-03,  5.8322e-03, -2.4079e-02,  7.8452e-02, -2.8438e-03,
        -4.0502e-03, -6.6214e-02,  2.1363e-02,  3.3529e-02,  2.8985e-03,
        -3.6496e-02, -6.1801e-01, -7.6918e-01,  1.2974e-03, -5.2257e-01,
        -1.0671e-02,  1.6316e-03,  1.9493e-02, -3.6425e-01, -1.5587e-01,
         6.2078e-02, -1.7501e-01,  3.3651e-04,  3.9345e-01, -1.7460e-01,
        -7.9458e-02,  4.2796e-02,  2.8005e-02,  8.0180e-02,  2.0030e-01,
        -1.2291e-01,  1.2888e-01, -4.1070e-05,  3.5836e-02,  1.2471e-03,
        -3.7142e-01,  1.1615e-01,  3.9788e-02, -2.6664e-04, -2.4320e-02,
         1.3745e-03,  3.2084e-02,  5.2833e-03,  7.0324e-02, -6.3596e-01,
         3.4879e-01, -4.2586e-02,  2.8223e-03,  3.9192e-02,  5.9683e-03,
         5.2176e-01,  6.6358e-03,  7.1311e-01,  8.8636e-04, -6.3952e-03,
         1.9252e-01, -3.4687e-02, -2.0205e-01,  5.0888e-01, -1.0577e-01,
         1.0878e-02,  5.1083e-03, -6.2362e-04, -4.2878e-03,  1.0310e-01,
        -4.0018e-02,  4.1374e-02, -8.7401e-05, -1.1117e-01, -1.3307e-03,
         5.3671e-03,  1.5089e-02,  1.1177e-01, -2.7275e-01,  1.1250e-01,
         1.5175e-03,  9.7672e-02, -5.9771e-02, -4.0344e-03,  1.3928e-01,
        -2.0880e-01,  2.9071e-02, -4.4837e-01,  1.0571e-02,  6.1559e-02,
        -1.4951e-03, -1.4128e-04,  7.4253e-03, -1.5938e-03,  5.8774e-02,
        -6.5616e-02,  4.0555e-02, -3.2288e-04,  7.6843e-02,  1.3435e-02,
        -1.8902e-01, -2.1602e-01, -2.1546e-04, -2.3642e-01, -9.7630e-03,
        -5.9214e-02,  7.4324e-03, -1.8369e-01,  4.0184e-01, -4.6535e-03,
         8.6960e-01,  3.0498e-03, -3.6308e-02,  3.9974e-03, -5.6076e-01,
         3.9397e-02,  8.9245e-04,  1.1667e-03, -6.9010e-03,  8.9793e-04,
        -1.3371e-03, -2.6858e-03,  8.4899e-02,  4.6295e-02, -1.8025e-02,
         1.3984e-02, -1.0430e-01,  3.3785e-02,  1.3143e-03,  2.1726e-04,
        -1.4915e-01,  4.2533e-01,  4.1536e-03,  8.5240e-04,  9.6794e-01,
        -1.1705e-02,  1.2856e-01,  1.0121e-01,  2.3454e-01, -5.1493e-04,
        -1.8100e-02,  2.3500e-01,  3.2596e-01, -1.3836e-04,  1.3583e-02,
         1.2389e-01, -3.1281e-02, -8.2279e-02,  5.0844e-02, -2.9056e-02,
        -1.7224e-02, -3.6567e-01,  7.9866e-02, -9.4121e-03, -2.1411e-02,
        -1.9907e-01, -3.4653e-01, -2.3140e-02,  3.3252e-01,  4.1728e-02,
        -4.4045e-01, -2.6555e-01,  1.8744e-01, -2.7363e-02,  4.8627e-03,
         1.9625e-02,  3.9223e-01,  6.1587e-03,  2.8713e-02, -2.4643e-02,
        -1.5686e-02, -1.7468e-03,  1.4772e-01, -3.9386e-02,  2.3182e-02,
        -1.4488e-03,  2.6238e-03, -1.4749e-02, -4.0266e-03,  2.2692e-03,
        -1.2087e-03,  3.6382e-02,  3.7031e-02, -4.3878e-02, -4.1625e-01,
         7.3488e-02,  3.4450e-01,  1.2732e-01, -3.3519e-01,  4.7700e-01,
        -6.0740e-01,  1.1303e-01, -4.8066e-01,  2.2418e-02,  2.4444e-03,
         4.6742e-02,  6.0572e-03, -2.3893e-04,  1.0216e-01,  1.7152e-02,
         2.2809e-02,  1.6276e-03, -1.9525e-02,  3.5348e-01, -1.3069e-01,
        -3.7981e-02,  2.1455e-02,  7.7260e-01,  4.2011e-02, -3.9833e-01,
         1.3988e-02, -3.1254e-03, -1.5938e-02,  4.8970e-02,  1.3104e-01,
        -3.8414e-01, -4.1675e-03,  2.1431e-02,  2.1515e-01, -1.3565e-02,
         3.1234e-04,  1.7739e-04, -5.4292e-02,  2.2945e-02,  1.6224e-01,
        -2.4394e-01, -1.3098e-03,  1.7650e-01,  1.1274e-03, -3.2804e-03,
         1.4911e-02,  1.3519e-01,  5.7510e-02, -2.9519e-01, -1.8862e-01,
         1.0553e-02, -1.7893e-02,  2.4967e-02,  1.1009e-02,  8.5479e-02,
         6.3560e-02,  1.3389e-01, -3.7455e-02,  6.6843e-03], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0045,  0.0011, -0.0312,  ...,  0.1205, -0.1157, -0.0391],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.3165, -0.0698, -0.0417,  ..., -0.0022, -0.0142,  0.0693],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([0.0173, 0.2845, 0.0094,  ..., 0.0129, 0.0079, 0.0464], device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.2.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 0.0045,  0.0011, -0.0312,  ...,  0.1205, -0.1157, -0.0391],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-0.5105,  0.1496,  0.0392,  0.0903, -0.1568, -0.4092,  0.3688, -0.2736,
        -0.7403,  0.3870,  0.5747,  0.4217, -0.0256,  0.1433,  0.9749, -0.3270,
        -0.4428,  0.0176, -0.6988, -0.6410,  0.4186,  0.6243, -0.1298,  0.0231,
         0.3447, -0.0524, -0.1776, -0.1404,  0.2666, -0.1201,  0.5107,  0.5493,
        -0.6482, -0.5930,  0.6337,  1.0000, -1.0838, -0.0476,  0.7847,  0.0733,
         0.4460,  0.1841,  0.7566, -0.3229,  0.4395, -0.4751, -0.4542,  0.5812,
        -0.0445,  0.4783, -0.7011,  0.4480,  0.2792,  0.7637, -0.0938,  0.6215,
        -0.6184,  0.7859, -0.4463,  0.3975, -0.1486, -0.0076, -0.6155,  0.1102,
        -0.6445, -0.3612, -0.5643, -0.1696,  0.9089,  0.4815,  0.4843,  0.3092,
        -0.0347, -0.5962, -0.0534, -0.7011,  0.1136,  0.2573, -0.9436,  0.5460,
        -0.4472,  0.7750, -0.3512,  0.9307,  0.0703, -0.3815, -0.4113, -0.5002,
        -0.3426, -0.6174,  0.0214, -0.2066, -0.3265,  0.8343, -0.6442,  0.4524],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.9976e-02,  6.1699e-03,  2.4019e-03,  6.0392e-03,  8.4756e-01,
        -7.4254e-01, -1.4550e-03, -6.1978e-02, -7.2665e-03,  1.2007e-01,
        -1.2048e-03,  4.2755e-02,  7.2891e-02,  3.6609e-02,  9.8713e-03,
         4.2635e-02, -1.7606e-02,  1.8680e-02, -1.8105e-01, -2.3585e-01,
         2.5812e-02,  3.5814e-03,  3.2552e-01,  2.9203e-01,  1.5664e-01,
         1.4485e-01, -4.5061e-02, -5.1993e-02, -1.1699e-01,  1.9132e-02,
         1.5974e-02, -2.6301e-01, -2.9754e-01, -2.3803e-02, -2.7128e-02,
        -2.1432e-02,  4.0495e-02,  5.0280e-02, -2.9940e-04,  1.6047e-02,
         1.4387e-01,  5.0591e-03, -4.5106e-03,  6.9360e-04,  4.6246e-02,
        -6.2170e-05, -5.3023e-01,  4.3946e-02, -8.4399e-02, -1.2614e-01,
         1.4879e-02,  4.3965e-03, -9.0257e-03, -6.0968e-03, -1.2920e-01,
         6.5336e-01,  6.4390e-02, -5.9877e-02,  4.4773e-02, -1.1032e-02,
         3.0916e-03, -5.4395e-01,  3.0280e-02, -3.1792e-01, -1.3500e-01,
        -2.9181e-04, -2.6110e-01,  6.3174e-02,  3.8673e-02, -8.6348e-02,
        -1.9835e-01, -3.5634e-02, -2.8475e-01,  2.6840e-01, -1.1208e-02,
         2.0462e-03,  2.4126e-01, -3.3351e-01, -5.8050e-01,  3.4679e-01,
        -1.8159e-02,  7.4245e-03,  1.3566e-02, -1.7060e-06,  3.4349e-01,
        -1.0565e-03,  7.3623e-03, -8.6841e-02, -3.2311e-02,  7.5918e-03,
        -1.0763e-02,  1.1089e-04, -2.2557e-02,  2.2626e-03, -9.3917e-04,
        -8.7978e-02,  9.2347e-02,  1.9292e-02, -4.4159e-01, -1.6016e-02,
         3.0928e-01,  8.0741e-03,  4.2021e-03, -1.4655e-01, -5.0474e-02,
         1.0154e-03, -2.4131e-02, -1.0266e-01, -4.2929e-02,  3.0572e-01,
        -7.1675e-02,  9.5821e-04, -1.3120e-03, -2.7459e-01,  1.0000e+00,
        -5.5257e-02, -4.7422e-02,  7.4287e-02,  2.6331e-02, -6.8696e-02,
        -3.6926e-01, -8.4093e-02,  1.7593e-02,  1.1288e-01,  9.1335e-02,
        -5.2141e-01, -3.0037e-02, -3.6840e-01,  7.4291e-02,  2.4817e-03,
         2.5920e-03,  8.3414e-02,  3.5848e-02,  2.0743e-03,  1.7235e-01,
         6.7366e-02,  1.7739e-03,  1.1405e-01,  9.0249e-04,  3.4645e-02,
         2.0868e-03,  6.4417e-03,  1.5108e-01, -1.6740e-03,  2.6067e-02,
         3.4368e-03,  5.8322e-03, -2.4079e-02,  7.8452e-02, -2.8438e-03,
        -4.0502e-03, -6.6214e-02,  2.1363e-02,  3.3529e-02,  2.8985e-03,
        -3.6496e-02, -6.1801e-01, -7.6918e-01,  1.2974e-03, -5.2257e-01,
        -1.0671e-02,  1.6316e-03,  1.9493e-02, -3.6425e-01, -1.5587e-01,
         6.2078e-02, -1.7501e-01,  3.3651e-04,  3.9345e-01, -1.7460e-01,
        -7.9458e-02,  4.2796e-02,  2.8005e-02,  8.0180e-02,  2.0030e-01,
        -1.2291e-01,  1.2888e-01, -4.1070e-05,  3.5836e-02,  1.2471e-03,
        -3.7142e-01,  1.1615e-01,  3.9788e-02, -2.6664e-04, -2.4320e-02,
         1.3745e-03,  3.2084e-02,  5.2833e-03,  7.0324e-02, -6.3596e-01,
         3.4879e-01, -4.2586e-02,  2.8223e-03,  3.9192e-02,  5.9683e-03,
         5.2176e-01,  6.6358e-03,  7.1311e-01,  8.8636e-04, -6.3952e-03,
         1.9252e-01, -3.4687e-02, -2.0205e-01,  5.0888e-01, -1.0577e-01,
         1.0878e-02,  5.1083e-03, -6.2362e-04, -4.2878e-03,  1.0310e-01,
        -4.0018e-02,  4.1374e-02, -8.7401e-05, -1.1117e-01, -1.3307e-03,
         5.3671e-03,  1.5089e-02,  1.1177e-01, -2.7275e-01,  1.1250e-01,
         1.5175e-03,  9.7672e-02, -5.9771e-02, -4.0344e-03,  1.3928e-01,
        -2.0880e-01,  2.9071e-02, -4.4837e-01,  1.0571e-02,  6.1559e-02,
        -1.4951e-03, -1.4128e-04,  7.4253e-03, -1.5938e-03,  5.8774e-02,
        -6.5616e-02,  4.0555e-02, -3.2288e-04,  7.6843e-02,  1.3435e-02,
        -1.8902e-01, -2.1602e-01, -2.1546e-04, -2.3642e-01, -9.7630e-03,
        -5.9214e-02,  7.4324e-03, -1.8369e-01,  4.0184e-01, -4.6535e-03,
         8.6960e-01,  3.0498e-03, -3.6308e-02,  3.9974e-03, -5.6076e-01,
         3.9397e-02,  8.9245e-04,  1.1667e-03, -6.9010e-03,  8.9793e-04,
        -1.3371e-03, -2.6858e-03,  8.4899e-02,  4.6295e-02, -1.8025e-02,
         1.3984e-02, -1.0430e-01,  3.3785e-02,  1.3143e-03,  2.1726e-04,
        -1.4915e-01,  4.2533e-01,  4.1536e-03,  8.5240e-04,  9.6794e-01,
        -1.1705e-02,  1.2856e-01,  1.0121e-01,  2.3454e-01, -5.1493e-04,
        -1.8100e-02,  2.3500e-01,  3.2596e-01, -1.3836e-04,  1.3583e-02,
         1.2389e-01, -3.1281e-02, -8.2279e-02,  5.0844e-02, -2.9056e-02,
        -1.7224e-02, -3.6567e-01,  7.9866e-02, -9.4121e-03, -2.1411e-02,
        -1.9907e-01, -3.4653e-01, -2.3140e-02,  3.3252e-01,  4.1728e-02,
        -4.4045e-01, -2.6555e-01,  1.8744e-01, -2.7363e-02,  4.8627e-03,
         1.9625e-02,  3.9223e-01,  6.1587e-03,  2.8713e-02, -2.4643e-02,
        -1.5686e-02, -1.7468e-03,  1.4772e-01, -3.9386e-02,  2.3182e-02,
        -1.4488e-03,  2.6238e-03, -1.4749e-02, -4.0266e-03,  2.2692e-03,
        -1.2087e-03,  3.6382e-02,  3.7031e-02, -4.3878e-02, -4.1625e-01,
         7.3488e-02,  3.4450e-01,  1.2732e-01, -3.3519e-01,  4.7700e-01,
        -6.0740e-01,  1.1303e-01, -4.8066e-01,  2.2418e-02,  2.4444e-03,
         4.6742e-02,  6.0572e-03, -2.3893e-04,  1.0216e-01,  1.7152e-02,
         2.2809e-02,  1.6276e-03, -1.9525e-02,  3.5348e-01, -1.3069e-01,
        -3.7981e-02,  2.1455e-02,  7.7260e-01,  4.2011e-02, -3.9833e-01,
         1.3988e-02, -3.1254e-03, -1.5938e-02,  4.8970e-02,  1.3104e-01,
        -3.8414e-01, -4.1675e-03,  2.1431e-02,  2.1515e-01, -1.3565e-02,
         3.1234e-04,  1.7739e-04, -5.4292e-02,  2.2945e-02,  1.6224e-01,
        -2.4394e-01, -1.3098e-03,  1.7650e-01,  1.1274e-03, -3.2804e-03,
         1.4911e-02,  1.3519e-01,  5.7510e-02, -2.9519e-01, -1.8862e-01,
         1.0553e-02, -1.7893e-02,  2.4967e-02,  1.1009e-02,  8.5479e-02,
         6.3560e-02,  1.3389e-01, -3.7455e-02,  6.6843e-03], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0045,  0.0011, -0.0312,  ...,  0.1205, -0.1157, -0.0391],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.3165, -0.0698, -0.0417,  ..., -0.0022, -0.0142,  0.0693],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([0.0173, 0.2845, 0.0094,  ..., 0.0129, 0.0079, 0.0464], device='cuda:0')}
topk_layer inside model after retrieval:  [958, 733, 761, 250, 1436, 856, 14, 209, 1163, 176, 399, 801, 942, 95, 1532, 118, 622, 959, 1452, 1274, 324, 326, 835, 632, 639, 1172, 852, 1113, 35, 850, 730, 1486, 636, 869, 508, 155, 597, 173, 15, 423, 22, 312, 1278, 281, 12, 408, 783, 1159, 1345, 799, 746, 499, 933, 1126, 1178, 373, 395, 915, 868, 356, 773, 810, 1002, 923, 588, 580, 426, 519, 1238, 164, 1089, 42, 980, 441, 575, 1001, 612, 743, 353, 649, 779, 154, 523, 964, 905, 507, 461, 1046, 626, 294, 981, 1025, 937, 752, 1186, 1032, 405, 953, 313, 58, 542, 1206, 544, 667, 335, 754, 1155, 1508, 1317, 900, 311, 70, 1292, 1184, 707, 439, 682, 1522, 77, 93, 1203, 174, 115, 854, 1125, 1149, 960, 1494, 74, 952, 866, 134, 902, 921, 653, 590, 1038, 238, 872, 1282, 1340, 650, 1297, 1357, 1455, 307, 788, 265, 378, 75, 614, 1265, 243, 454, 406, 1396, 1260, 264, 212, 790, 119, 1461, 1520, 911, 961, 195, 994, 359, 16, 602, 760, 692, 1045, 457, 670, 1302, 732, 1281, 802, 1385, 1460, 728, 714, 889, 1324, 61, 208, 1261, 878, 562, 629, 936, 1132, 574, 257, 1463, 217, 1171, 851, 490, 175, 736, 1462, 1294, 1432, 295, 1242, 1031, 1450, 745, 1192, 41, 1349, 411, 1248, 1227, 1298, 132, 1426, 1379, 348, 105, 705, 1156, 846, 674, 1291, 171, 1442, 381, 188]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-0.5105,  0.1496,  0.0392,  0.0903, -0.1568, -0.4092,  0.3688, -0.2736,
        -0.7403,  0.3870,  0.5747,  0.4217, -0.0256,  0.1433,  0.9749, -0.3270,
        -0.4428,  0.0176, -0.6988, -0.6410,  0.4186,  0.6243, -0.1298,  0.0231,
         0.3447, -0.0524, -0.1776, -0.1404,  0.2666, -0.1201,  0.5107,  0.5493,
        -0.6482, -0.5930,  0.6337,  1.0000, -1.0838, -0.0476,  0.7847,  0.0733,
         0.4460,  0.1841,  0.7566, -0.3229,  0.4395, -0.4751, -0.4542,  0.5812,
        -0.0445,  0.4783, -0.7011,  0.4480,  0.2792,  0.7637, -0.0938,  0.6215,
        -0.6184,  0.7859, -0.4463,  0.3975, -0.1486, -0.0076, -0.6155,  0.1102,
        -0.6445, -0.3612, -0.5643, -0.1696,  0.9089,  0.4815,  0.4843,  0.3092,
        -0.0347, -0.5962, -0.0534, -0.7011,  0.1136,  0.2573, -0.9436,  0.5460,
        -0.4472,  0.7750, -0.3512,  0.9307,  0.0703, -0.3815, -0.4113, -0.5002,
        -0.3426, -0.6174,  0.0214, -0.2066, -0.3265,  0.8343, -0.6442,  0.4524],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.9976e-02,  6.1699e-03,  2.4019e-03,  6.0392e-03,  8.4756e-01,
        -7.4254e-01, -1.4550e-03, -6.1978e-02, -7.2665e-03,  1.2007e-01,
        -1.2048e-03,  4.2755e-02,  7.2891e-02,  3.6609e-02,  9.8713e-03,
         4.2635e-02, -1.7606e-02,  1.8680e-02, -1.8105e-01, -2.3585e-01,
         2.5812e-02,  3.5814e-03,  3.2552e-01,  2.9203e-01,  1.5664e-01,
         1.4485e-01, -4.5061e-02, -5.1993e-02, -1.1699e-01,  1.9132e-02,
         1.5974e-02, -2.6301e-01, -2.9754e-01, -2.3803e-02, -2.7128e-02,
        -2.1432e-02,  4.0495e-02,  5.0280e-02, -2.9940e-04,  1.6047e-02,
         1.4387e-01,  5.0591e-03, -4.5106e-03,  6.9360e-04,  4.6246e-02,
        -6.2170e-05, -5.3023e-01,  4.3946e-02, -8.4399e-02, -1.2614e-01,
         1.4879e-02,  4.3965e-03, -9.0257e-03, -6.0968e-03, -1.2920e-01,
         6.5336e-01,  6.4390e-02, -5.9877e-02,  4.4773e-02, -1.1032e-02,
         3.0916e-03, -5.4395e-01,  3.0280e-02, -3.1792e-01, -1.3500e-01,
        -2.9181e-04, -2.6110e-01,  6.3174e-02,  3.8673e-02, -8.6348e-02,
        -1.9835e-01, -3.5634e-02, -2.8475e-01,  2.6840e-01, -1.1208e-02,
         2.0462e-03,  2.4126e-01, -3.3351e-01, -5.8050e-01,  3.4679e-01,
        -1.8159e-02,  7.4245e-03,  1.3566e-02, -1.7060e-06,  3.4349e-01,
        -1.0565e-03,  7.3623e-03, -8.6841e-02, -3.2311e-02,  7.5918e-03,
        -1.0763e-02,  1.1089e-04, -2.2557e-02,  2.2626e-03, -9.3917e-04,
        -8.7978e-02,  9.2347e-02,  1.9292e-02, -4.4159e-01, -1.6016e-02,
         3.0928e-01,  8.0741e-03,  4.2021e-03, -1.4655e-01, -5.0474e-02,
         1.0154e-03, -2.4131e-02, -1.0266e-01, -4.2929e-02,  3.0572e-01,
        -7.1675e-02,  9.5821e-04, -1.3120e-03, -2.7459e-01,  1.0000e+00,
        -5.5257e-02, -4.7422e-02,  7.4287e-02,  2.6331e-02, -6.8696e-02,
        -3.6926e-01, -8.4093e-02,  1.7593e-02,  1.1288e-01,  9.1335e-02,
        -5.2141e-01, -3.0037e-02, -3.6840e-01,  7.4291e-02,  2.4817e-03,
         2.5920e-03,  8.3414e-02,  3.5848e-02,  2.0743e-03,  1.7235e-01,
         6.7366e-02,  1.7739e-03,  1.1405e-01,  9.0249e-04,  3.4645e-02,
         2.0868e-03,  6.4417e-03,  1.5108e-01, -1.6740e-03,  2.6067e-02,
         3.4368e-03,  5.8322e-03, -2.4079e-02,  7.8452e-02, -2.8438e-03,
        -4.0502e-03, -6.6214e-02,  2.1363e-02,  3.3529e-02,  2.8985e-03,
        -3.6496e-02, -6.1801e-01, -7.6918e-01,  1.2974e-03, -5.2257e-01,
        -1.0671e-02,  1.6316e-03,  1.9493e-02, -3.6425e-01, -1.5587e-01,
         6.2078e-02, -1.7501e-01,  3.3651e-04,  3.9345e-01, -1.7460e-01,
        -7.9458e-02,  4.2796e-02,  2.8005e-02,  8.0180e-02,  2.0030e-01,
        -1.2291e-01,  1.2888e-01, -4.1070e-05,  3.5836e-02,  1.2471e-03,
        -3.7142e-01,  1.1615e-01,  3.9788e-02, -2.6664e-04, -2.4320e-02,
         1.3745e-03,  3.2084e-02,  5.2833e-03,  7.0324e-02, -6.3596e-01,
         3.4879e-01, -4.2586e-02,  2.8223e-03,  3.9192e-02,  5.9683e-03,
         5.2176e-01,  6.6358e-03,  7.1311e-01,  8.8636e-04, -6.3952e-03,
         1.9252e-01, -3.4687e-02, -2.0205e-01,  5.0888e-01, -1.0577e-01,
         1.0878e-02,  5.1083e-03, -6.2362e-04, -4.2878e-03,  1.0310e-01,
        -4.0018e-02,  4.1374e-02, -8.7401e-05, -1.1117e-01, -1.3307e-03,
         5.3671e-03,  1.5089e-02,  1.1177e-01, -2.7275e-01,  1.1250e-01,
         1.5175e-03,  9.7672e-02, -5.9771e-02, -4.0344e-03,  1.3928e-01,
        -2.0880e-01,  2.9071e-02, -4.4837e-01,  1.0571e-02,  6.1559e-02,
        -1.4951e-03, -1.4128e-04,  7.4253e-03, -1.5938e-03,  5.8774e-02,
        -6.5616e-02,  4.0555e-02, -3.2288e-04,  7.6843e-02,  1.3435e-02,
        -1.8902e-01, -2.1602e-01, -2.1546e-04, -2.3642e-01, -9.7630e-03,
        -5.9214e-02,  7.4324e-03, -1.8369e-01,  4.0184e-01, -4.6535e-03,
         8.6960e-01,  3.0498e-03, -3.6308e-02,  3.9974e-03, -5.6076e-01,
         3.9397e-02,  8.9245e-04,  1.1667e-03, -6.9010e-03,  8.9793e-04,
        -1.3371e-03, -2.6858e-03,  8.4899e-02,  4.6295e-02, -1.8025e-02,
         1.3984e-02, -1.0430e-01,  3.3785e-02,  1.3143e-03,  2.1726e-04,
        -1.4915e-01,  4.2533e-01,  4.1536e-03,  8.5240e-04,  9.6794e-01,
        -1.1705e-02,  1.2856e-01,  1.0121e-01,  2.3454e-01, -5.1493e-04,
        -1.8100e-02,  2.3500e-01,  3.2596e-01, -1.3836e-04,  1.3583e-02,
         1.2389e-01, -3.1281e-02, -8.2279e-02,  5.0844e-02, -2.9056e-02,
        -1.7224e-02, -3.6567e-01,  7.9866e-02, -9.4121e-03, -2.1411e-02,
        -1.9907e-01, -3.4653e-01, -2.3140e-02,  3.3252e-01,  4.1728e-02,
        -4.4045e-01, -2.6555e-01,  1.8744e-01, -2.7363e-02,  4.8627e-03,
         1.9625e-02,  3.9223e-01,  6.1587e-03,  2.8713e-02, -2.4643e-02,
        -1.5686e-02, -1.7468e-03,  1.4772e-01, -3.9386e-02,  2.3182e-02,
        -1.4488e-03,  2.6238e-03, -1.4749e-02, -4.0266e-03,  2.2692e-03,
        -1.2087e-03,  3.6382e-02,  3.7031e-02, -4.3878e-02, -4.1625e-01,
         7.3488e-02,  3.4450e-01,  1.2732e-01, -3.3519e-01,  4.7700e-01,
        -6.0740e-01,  1.1303e-01, -4.8066e-01,  2.2418e-02,  2.4444e-03,
         4.6742e-02,  6.0572e-03, -2.3893e-04,  1.0216e-01,  1.7152e-02,
         2.2809e-02,  1.6276e-03, -1.9525e-02,  3.5348e-01, -1.3069e-01,
        -3.7981e-02,  2.1455e-02,  7.7260e-01,  4.2011e-02, -3.9833e-01,
         1.3988e-02, -3.1254e-03, -1.5938e-02,  4.8970e-02,  1.3104e-01,
        -3.8414e-01, -4.1675e-03,  2.1431e-02,  2.1515e-01, -1.3565e-02,
         3.1234e-04,  1.7739e-04, -5.4292e-02,  2.2945e-02,  1.6224e-01,
        -2.4394e-01, -1.3098e-03,  1.7650e-01,  1.1274e-03, -3.2804e-03,
         1.4911e-02,  1.3519e-01,  5.7510e-02, -2.9519e-01, -1.8862e-01,
         1.0553e-02, -1.7893e-02,  2.4967e-02,  1.1009e-02,  8.5479e-02,
         6.3560e-02,  1.3389e-01, -3.7455e-02,  6.6843e-03], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0045,  0.0011, -0.0312,  ...,  0.1205, -0.1157, -0.0391],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.3165, -0.0698, -0.0417,  ..., -0.0022, -0.0142,  0.0693],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([0.0173, 0.2845, 0.0094,  ..., 0.0129, 0.0079, 0.0464], device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.3.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([-0.3165, -0.0698, -0.0417,  ..., -0.0022, -0.0142,  0.0693],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-0.5105,  0.1496,  0.0392,  0.0903, -0.1568, -0.4092,  0.3688, -0.2736,
        -0.7403,  0.3870,  0.5747,  0.4217, -0.0256,  0.1433,  0.9749, -0.3270,
        -0.4428,  0.0176, -0.6988, -0.6410,  0.4186,  0.6243, -0.1298,  0.0231,
         0.3447, -0.0524, -0.1776, -0.1404,  0.2666, -0.1201,  0.5107,  0.5493,
        -0.6482, -0.5930,  0.6337,  1.0000, -1.0838, -0.0476,  0.7847,  0.0733,
         0.4460,  0.1841,  0.7566, -0.3229,  0.4395, -0.4751, -0.4542,  0.5812,
        -0.0445,  0.4783, -0.7011,  0.4480,  0.2792,  0.7637, -0.0938,  0.6215,
        -0.6184,  0.7859, -0.4463,  0.3975, -0.1486, -0.0076, -0.6155,  0.1102,
        -0.6445, -0.3612, -0.5643, -0.1696,  0.9089,  0.4815,  0.4843,  0.3092,
        -0.0347, -0.5962, -0.0534, -0.7011,  0.1136,  0.2573, -0.9436,  0.5460,
        -0.4472,  0.7750, -0.3512,  0.9307,  0.0703, -0.3815, -0.4113, -0.5002,
        -0.3426, -0.6174,  0.0214, -0.2066, -0.3265,  0.8343, -0.6442,  0.4524],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.9976e-02,  6.1699e-03,  2.4019e-03,  6.0392e-03,  8.4756e-01,
        -7.4254e-01, -1.4550e-03, -6.1978e-02, -7.2665e-03,  1.2007e-01,
        -1.2048e-03,  4.2755e-02,  7.2891e-02,  3.6609e-02,  9.8713e-03,
         4.2635e-02, -1.7606e-02,  1.8680e-02, -1.8105e-01, -2.3585e-01,
         2.5812e-02,  3.5814e-03,  3.2552e-01,  2.9203e-01,  1.5664e-01,
         1.4485e-01, -4.5061e-02, -5.1993e-02, -1.1699e-01,  1.9132e-02,
         1.5974e-02, -2.6301e-01, -2.9754e-01, -2.3803e-02, -2.7128e-02,
        -2.1432e-02,  4.0495e-02,  5.0280e-02, -2.9940e-04,  1.6047e-02,
         1.4387e-01,  5.0591e-03, -4.5106e-03,  6.9360e-04,  4.6246e-02,
        -6.2170e-05, -5.3023e-01,  4.3946e-02, -8.4399e-02, -1.2614e-01,
         1.4879e-02,  4.3965e-03, -9.0257e-03, -6.0968e-03, -1.2920e-01,
         6.5336e-01,  6.4390e-02, -5.9877e-02,  4.4773e-02, -1.1032e-02,
         3.0916e-03, -5.4395e-01,  3.0280e-02, -3.1792e-01, -1.3500e-01,
        -2.9181e-04, -2.6110e-01,  6.3174e-02,  3.8673e-02, -8.6348e-02,
        -1.9835e-01, -3.5634e-02, -2.8475e-01,  2.6840e-01, -1.1208e-02,
         2.0462e-03,  2.4126e-01, -3.3351e-01, -5.8050e-01,  3.4679e-01,
        -1.8159e-02,  7.4245e-03,  1.3566e-02, -1.7060e-06,  3.4349e-01,
        -1.0565e-03,  7.3623e-03, -8.6841e-02, -3.2311e-02,  7.5918e-03,
        -1.0763e-02,  1.1089e-04, -2.2557e-02,  2.2626e-03, -9.3917e-04,
        -8.7978e-02,  9.2347e-02,  1.9292e-02, -4.4159e-01, -1.6016e-02,
         3.0928e-01,  8.0741e-03,  4.2021e-03, -1.4655e-01, -5.0474e-02,
         1.0154e-03, -2.4131e-02, -1.0266e-01, -4.2929e-02,  3.0572e-01,
        -7.1675e-02,  9.5821e-04, -1.3120e-03, -2.7459e-01,  1.0000e+00,
        -5.5257e-02, -4.7422e-02,  7.4287e-02,  2.6331e-02, -6.8696e-02,
        -3.6926e-01, -8.4093e-02,  1.7593e-02,  1.1288e-01,  9.1335e-02,
        -5.2141e-01, -3.0037e-02, -3.6840e-01,  7.4291e-02,  2.4817e-03,
         2.5920e-03,  8.3414e-02,  3.5848e-02,  2.0743e-03,  1.7235e-01,
         6.7366e-02,  1.7739e-03,  1.1405e-01,  9.0249e-04,  3.4645e-02,
         2.0868e-03,  6.4417e-03,  1.5108e-01, -1.6740e-03,  2.6067e-02,
         3.4368e-03,  5.8322e-03, -2.4079e-02,  7.8452e-02, -2.8438e-03,
        -4.0502e-03, -6.6214e-02,  2.1363e-02,  3.3529e-02,  2.8985e-03,
        -3.6496e-02, -6.1801e-01, -7.6918e-01,  1.2974e-03, -5.2257e-01,
        -1.0671e-02,  1.6316e-03,  1.9493e-02, -3.6425e-01, -1.5587e-01,
         6.2078e-02, -1.7501e-01,  3.3651e-04,  3.9345e-01, -1.7460e-01,
        -7.9458e-02,  4.2796e-02,  2.8005e-02,  8.0180e-02,  2.0030e-01,
        -1.2291e-01,  1.2888e-01, -4.1070e-05,  3.5836e-02,  1.2471e-03,
        -3.7142e-01,  1.1615e-01,  3.9788e-02, -2.6664e-04, -2.4320e-02,
         1.3745e-03,  3.2084e-02,  5.2833e-03,  7.0324e-02, -6.3596e-01,
         3.4879e-01, -4.2586e-02,  2.8223e-03,  3.9192e-02,  5.9683e-03,
         5.2176e-01,  6.6358e-03,  7.1311e-01,  8.8636e-04, -6.3952e-03,
         1.9252e-01, -3.4687e-02, -2.0205e-01,  5.0888e-01, -1.0577e-01,
         1.0878e-02,  5.1083e-03, -6.2362e-04, -4.2878e-03,  1.0310e-01,
        -4.0018e-02,  4.1374e-02, -8.7401e-05, -1.1117e-01, -1.3307e-03,
         5.3671e-03,  1.5089e-02,  1.1177e-01, -2.7275e-01,  1.1250e-01,
         1.5175e-03,  9.7672e-02, -5.9771e-02, -4.0344e-03,  1.3928e-01,
        -2.0880e-01,  2.9071e-02, -4.4837e-01,  1.0571e-02,  6.1559e-02,
        -1.4951e-03, -1.4128e-04,  7.4253e-03, -1.5938e-03,  5.8774e-02,
        -6.5616e-02,  4.0555e-02, -3.2288e-04,  7.6843e-02,  1.3435e-02,
        -1.8902e-01, -2.1602e-01, -2.1546e-04, -2.3642e-01, -9.7630e-03,
        -5.9214e-02,  7.4324e-03, -1.8369e-01,  4.0184e-01, -4.6535e-03,
         8.6960e-01,  3.0498e-03, -3.6308e-02,  3.9974e-03, -5.6076e-01,
         3.9397e-02,  8.9245e-04,  1.1667e-03, -6.9010e-03,  8.9793e-04,
        -1.3371e-03, -2.6858e-03,  8.4899e-02,  4.6295e-02, -1.8025e-02,
         1.3984e-02, -1.0430e-01,  3.3785e-02,  1.3143e-03,  2.1726e-04,
        -1.4915e-01,  4.2533e-01,  4.1536e-03,  8.5240e-04,  9.6794e-01,
        -1.1705e-02,  1.2856e-01,  1.0121e-01,  2.3454e-01, -5.1493e-04,
        -1.8100e-02,  2.3500e-01,  3.2596e-01, -1.3836e-04,  1.3583e-02,
         1.2389e-01, -3.1281e-02, -8.2279e-02,  5.0844e-02, -2.9056e-02,
        -1.7224e-02, -3.6567e-01,  7.9866e-02, -9.4121e-03, -2.1411e-02,
        -1.9907e-01, -3.4653e-01, -2.3140e-02,  3.3252e-01,  4.1728e-02,
        -4.4045e-01, -2.6555e-01,  1.8744e-01, -2.7363e-02,  4.8627e-03,
         1.9625e-02,  3.9223e-01,  6.1587e-03,  2.8713e-02, -2.4643e-02,
        -1.5686e-02, -1.7468e-03,  1.4772e-01, -3.9386e-02,  2.3182e-02,
        -1.4488e-03,  2.6238e-03, -1.4749e-02, -4.0266e-03,  2.2692e-03,
        -1.2087e-03,  3.6382e-02,  3.7031e-02, -4.3878e-02, -4.1625e-01,
         7.3488e-02,  3.4450e-01,  1.2732e-01, -3.3519e-01,  4.7700e-01,
        -6.0740e-01,  1.1303e-01, -4.8066e-01,  2.2418e-02,  2.4444e-03,
         4.6742e-02,  6.0572e-03, -2.3893e-04,  1.0216e-01,  1.7152e-02,
         2.2809e-02,  1.6276e-03, -1.9525e-02,  3.5348e-01, -1.3069e-01,
        -3.7981e-02,  2.1455e-02,  7.7260e-01,  4.2011e-02, -3.9833e-01,
         1.3988e-02, -3.1254e-03, -1.5938e-02,  4.8970e-02,  1.3104e-01,
        -3.8414e-01, -4.1675e-03,  2.1431e-02,  2.1515e-01, -1.3565e-02,
         3.1234e-04,  1.7739e-04, -5.4292e-02,  2.2945e-02,  1.6224e-01,
        -2.4394e-01, -1.3098e-03,  1.7650e-01,  1.1274e-03, -3.2804e-03,
         1.4911e-02,  1.3519e-01,  5.7510e-02, -2.9519e-01, -1.8862e-01,
         1.0553e-02, -1.7893e-02,  2.4967e-02,  1.1009e-02,  8.5479e-02,
         6.3560e-02,  1.3389e-01, -3.7455e-02,  6.6843e-03], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0045,  0.0011, -0.0312,  ...,  0.1205, -0.1157, -0.0391],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.3165, -0.0698, -0.0417,  ..., -0.0022, -0.0142,  0.0693],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([0.0173, 0.2845, 0.0094,  ..., 0.0129, 0.0079, 0.0464], device='cuda:0')}
topk_layer inside model after retrieval:  [1447, 129, 2500, 2420, 2056, 2414, 2181, 564, 370, 746, 398, 646, 35, 1720, 922, 2523, 199, 419, 696, 1843, 1755, 1025, 2317, 305, 1424, 2193, 1794, 1993, 583, 472, 1926, 534, 640, 1380, 1394, 708, 68, 1292, 1304, 533, 1198, 837, 2331, 1958, 653, 1409, 2085, 162, 1830, 658, 1175, 2447, 2000, 964, 1558, 1280, 436, 1992, 2259, 2237, 888, 1746, 1953, 1637, 1051, 2070, 1506, 1141, 2, 2093, 732, 299, 2069, 1608, 33, 1666, 2390, 828, 2151, 2536, 72, 1003, 2342, 2179, 1887, 1550, 454, 1873, 2547, 1909, 2022, 1001, 1123, 1248, 2057, 810, 34, 1403, 2176, 1443, 1824, 2453, 421, 2185, 1753, 1938, 638, 796, 1385, 1650, 2094, 1045, 561, 507, 720, 1831, 146, 1536, 210, 1457, 1139, 1689, 2460, 2095, 1162, 1524, 302, 800, 1451, 1449, 669, 1783, 365, 1245, 1406, 2174, 1931, 509, 674, 2545, 1056, 425, 1153, 1776, 63, 2283, 935, 586, 2300, 1376, 2162, 2060, 2305, 437, 1555, 1567, 2168, 283, 1283, 1415, 838, 80, 1332, 2112, 481, 1659, 1049, 1638, 43, 1221, 291, 340, 2322, 2272, 404, 450, 575, 1789, 1809, 867, 2394, 1572, 1986, 2515, 1970, 122, 1886, 2462, 491, 1929, 2320, 557, 2227, 1160, 1266, 225, 1960, 1845, 1548, 336, 1157, 1781, 1439, 1074, 2187, 2197, 792, 2285, 2511, 687, 167, 1021, 632, 788, 1864, 1799, 1435, 1591, 1458, 2208, 267, 13, 1088, 2008, 2252, 2387, 2161, 304, 895, 1512, 465, 968, 1199, 1271, 306, 1742, 505, 130, 220, 926, 781, 662, 1554, 855, 1733, 698, 1839, 2062, 499, 1052, 2081, 258, 2308, 357, 468, 1062, 24, 521, 1645, 1375, 768, 2121, 411, 2042, 616, 96, 1890, 242, 2418, 1196, 276, 2028, 2378, 467, 2173, 1066, 772, 366, 2079, 761, 1874, 1446, 362, 284, 1699, 884, 1226, 252, 1054, 543, 597, 1108, 2527, 2310, 1966, 1325, 2456, 203, 27, 959, 2192, 962, 992, 2050, 1253, 1531, 1227, 2518, 1961, 192, 124, 259, 6, 1827, 944, 239, 2157, 885, 2167, 1527, 576, 706, 399, 2484, 1971, 555, 943, 82, 1194, 1694, 1029, 2130, 1633, 1267, 107, 2293, 2508, 2532, 2256, 2011, 316, 1112, 240, 151, 2436, 634, 1206, 1549, 54, 2242, 300, 2309, 689, 1782, 899, 1448, 2113, 2030, 2033, 1107, 1647, 1894, 1968, 1646, 1670, 1326, 2164, 937, 298, 2441, 1138, 1578, 1811, 1622, 980, 1055, 1351, 1837, 2284, 331, 2446, 939, 1697, 1908, 2018]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-0.5105,  0.1496,  0.0392,  0.0903, -0.1568, -0.4092,  0.3688, -0.2736,
        -0.7403,  0.3870,  0.5747,  0.4217, -0.0256,  0.1433,  0.9749, -0.3270,
        -0.4428,  0.0176, -0.6988, -0.6410,  0.4186,  0.6243, -0.1298,  0.0231,
         0.3447, -0.0524, -0.1776, -0.1404,  0.2666, -0.1201,  0.5107,  0.5493,
        -0.6482, -0.5930,  0.6337,  1.0000, -1.0838, -0.0476,  0.7847,  0.0733,
         0.4460,  0.1841,  0.7566, -0.3229,  0.4395, -0.4751, -0.4542,  0.5812,
        -0.0445,  0.4783, -0.7011,  0.4480,  0.2792,  0.7637, -0.0938,  0.6215,
        -0.6184,  0.7859, -0.4463,  0.3975, -0.1486, -0.0076, -0.6155,  0.1102,
        -0.6445, -0.3612, -0.5643, -0.1696,  0.9089,  0.4815,  0.4843,  0.3092,
        -0.0347, -0.5962, -0.0534, -0.7011,  0.1136,  0.2573, -0.9436,  0.5460,
        -0.4472,  0.7750, -0.3512,  0.9307,  0.0703, -0.3815, -0.4113, -0.5002,
        -0.3426, -0.6174,  0.0214, -0.2066, -0.3265,  0.8343, -0.6442,  0.4524],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.9976e-02,  6.1699e-03,  2.4019e-03,  6.0392e-03,  8.4756e-01,
        -7.4254e-01, -1.4550e-03, -6.1978e-02, -7.2665e-03,  1.2007e-01,
        -1.2048e-03,  4.2755e-02,  7.2891e-02,  3.6609e-02,  9.8713e-03,
         4.2635e-02, -1.7606e-02,  1.8680e-02, -1.8105e-01, -2.3585e-01,
         2.5812e-02,  3.5814e-03,  3.2552e-01,  2.9203e-01,  1.5664e-01,
         1.4485e-01, -4.5061e-02, -5.1993e-02, -1.1699e-01,  1.9132e-02,
         1.5974e-02, -2.6301e-01, -2.9754e-01, -2.3803e-02, -2.7128e-02,
        -2.1432e-02,  4.0495e-02,  5.0280e-02, -2.9940e-04,  1.6047e-02,
         1.4387e-01,  5.0591e-03, -4.5106e-03,  6.9360e-04,  4.6246e-02,
        -6.2170e-05, -5.3023e-01,  4.3946e-02, -8.4399e-02, -1.2614e-01,
         1.4879e-02,  4.3965e-03, -9.0257e-03, -6.0968e-03, -1.2920e-01,
         6.5336e-01,  6.4390e-02, -5.9877e-02,  4.4773e-02, -1.1032e-02,
         3.0916e-03, -5.4395e-01,  3.0280e-02, -3.1792e-01, -1.3500e-01,
        -2.9181e-04, -2.6110e-01,  6.3174e-02,  3.8673e-02, -8.6348e-02,
        -1.9835e-01, -3.5634e-02, -2.8475e-01,  2.6840e-01, -1.1208e-02,
         2.0462e-03,  2.4126e-01, -3.3351e-01, -5.8050e-01,  3.4679e-01,
        -1.8159e-02,  7.4245e-03,  1.3566e-02, -1.7060e-06,  3.4349e-01,
        -1.0565e-03,  7.3623e-03, -8.6841e-02, -3.2311e-02,  7.5918e-03,
        -1.0763e-02,  1.1089e-04, -2.2557e-02,  2.2626e-03, -9.3917e-04,
        -8.7978e-02,  9.2347e-02,  1.9292e-02, -4.4159e-01, -1.6016e-02,
         3.0928e-01,  8.0741e-03,  4.2021e-03, -1.4655e-01, -5.0474e-02,
         1.0154e-03, -2.4131e-02, -1.0266e-01, -4.2929e-02,  3.0572e-01,
        -7.1675e-02,  9.5821e-04, -1.3120e-03, -2.7459e-01,  1.0000e+00,
        -5.5257e-02, -4.7422e-02,  7.4287e-02,  2.6331e-02, -6.8696e-02,
        -3.6926e-01, -8.4093e-02,  1.7593e-02,  1.1288e-01,  9.1335e-02,
        -5.2141e-01, -3.0037e-02, -3.6840e-01,  7.4291e-02,  2.4817e-03,
         2.5920e-03,  8.3414e-02,  3.5848e-02,  2.0743e-03,  1.7235e-01,
         6.7366e-02,  1.7739e-03,  1.1405e-01,  9.0249e-04,  3.4645e-02,
         2.0868e-03,  6.4417e-03,  1.5108e-01, -1.6740e-03,  2.6067e-02,
         3.4368e-03,  5.8322e-03, -2.4079e-02,  7.8452e-02, -2.8438e-03,
        -4.0502e-03, -6.6214e-02,  2.1363e-02,  3.3529e-02,  2.8985e-03,
        -3.6496e-02, -6.1801e-01, -7.6918e-01,  1.2974e-03, -5.2257e-01,
        -1.0671e-02,  1.6316e-03,  1.9493e-02, -3.6425e-01, -1.5587e-01,
         6.2078e-02, -1.7501e-01,  3.3651e-04,  3.9345e-01, -1.7460e-01,
        -7.9458e-02,  4.2796e-02,  2.8005e-02,  8.0180e-02,  2.0030e-01,
        -1.2291e-01,  1.2888e-01, -4.1070e-05,  3.5836e-02,  1.2471e-03,
        -3.7142e-01,  1.1615e-01,  3.9788e-02, -2.6664e-04, -2.4320e-02,
         1.3745e-03,  3.2084e-02,  5.2833e-03,  7.0324e-02, -6.3596e-01,
         3.4879e-01, -4.2586e-02,  2.8223e-03,  3.9192e-02,  5.9683e-03,
         5.2176e-01,  6.6358e-03,  7.1311e-01,  8.8636e-04, -6.3952e-03,
         1.9252e-01, -3.4687e-02, -2.0205e-01,  5.0888e-01, -1.0577e-01,
         1.0878e-02,  5.1083e-03, -6.2362e-04, -4.2878e-03,  1.0310e-01,
        -4.0018e-02,  4.1374e-02, -8.7401e-05, -1.1117e-01, -1.3307e-03,
         5.3671e-03,  1.5089e-02,  1.1177e-01, -2.7275e-01,  1.1250e-01,
         1.5175e-03,  9.7672e-02, -5.9771e-02, -4.0344e-03,  1.3928e-01,
        -2.0880e-01,  2.9071e-02, -4.4837e-01,  1.0571e-02,  6.1559e-02,
        -1.4951e-03, -1.4128e-04,  7.4253e-03, -1.5938e-03,  5.8774e-02,
        -6.5616e-02,  4.0555e-02, -3.2288e-04,  7.6843e-02,  1.3435e-02,
        -1.8902e-01, -2.1602e-01, -2.1546e-04, -2.3642e-01, -9.7630e-03,
        -5.9214e-02,  7.4324e-03, -1.8369e-01,  4.0184e-01, -4.6535e-03,
         8.6960e-01,  3.0498e-03, -3.6308e-02,  3.9974e-03, -5.6076e-01,
         3.9397e-02,  8.9245e-04,  1.1667e-03, -6.9010e-03,  8.9793e-04,
        -1.3371e-03, -2.6858e-03,  8.4899e-02,  4.6295e-02, -1.8025e-02,
         1.3984e-02, -1.0430e-01,  3.3785e-02,  1.3143e-03,  2.1726e-04,
        -1.4915e-01,  4.2533e-01,  4.1536e-03,  8.5240e-04,  9.6794e-01,
        -1.1705e-02,  1.2856e-01,  1.0121e-01,  2.3454e-01, -5.1493e-04,
        -1.8100e-02,  2.3500e-01,  3.2596e-01, -1.3836e-04,  1.3583e-02,
         1.2389e-01, -3.1281e-02, -8.2279e-02,  5.0844e-02, -2.9056e-02,
        -1.7224e-02, -3.6567e-01,  7.9866e-02, -9.4121e-03, -2.1411e-02,
        -1.9907e-01, -3.4653e-01, -2.3140e-02,  3.3252e-01,  4.1728e-02,
        -4.4045e-01, -2.6555e-01,  1.8744e-01, -2.7363e-02,  4.8627e-03,
         1.9625e-02,  3.9223e-01,  6.1587e-03,  2.8713e-02, -2.4643e-02,
        -1.5686e-02, -1.7468e-03,  1.4772e-01, -3.9386e-02,  2.3182e-02,
        -1.4488e-03,  2.6238e-03, -1.4749e-02, -4.0266e-03,  2.2692e-03,
        -1.2087e-03,  3.6382e-02,  3.7031e-02, -4.3878e-02, -4.1625e-01,
         7.3488e-02,  3.4450e-01,  1.2732e-01, -3.3519e-01,  4.7700e-01,
        -6.0740e-01,  1.1303e-01, -4.8066e-01,  2.2418e-02,  2.4444e-03,
         4.6742e-02,  6.0572e-03, -2.3893e-04,  1.0216e-01,  1.7152e-02,
         2.2809e-02,  1.6276e-03, -1.9525e-02,  3.5348e-01, -1.3069e-01,
        -3.7981e-02,  2.1455e-02,  7.7260e-01,  4.2011e-02, -3.9833e-01,
         1.3988e-02, -3.1254e-03, -1.5938e-02,  4.8970e-02,  1.3104e-01,
        -3.8414e-01, -4.1675e-03,  2.1431e-02,  2.1515e-01, -1.3565e-02,
         3.1234e-04,  1.7739e-04, -5.4292e-02,  2.2945e-02,  1.6224e-01,
        -2.4394e-01, -1.3098e-03,  1.7650e-01,  1.1274e-03, -3.2804e-03,
         1.4911e-02,  1.3519e-01,  5.7510e-02, -2.9519e-01, -1.8862e-01,
         1.0553e-02, -1.7893e-02,  2.4967e-02,  1.1009e-02,  8.5479e-02,
         6.3560e-02,  1.3389e-01, -3.7455e-02,  6.6843e-03], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0045,  0.0011, -0.0312,  ...,  0.1205, -0.1157, -0.0391],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.3165, -0.0698, -0.0417,  ..., -0.0022, -0.0142,  0.0693],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([0.0173, 0.2845, 0.0094,  ..., 0.0129, 0.0079, 0.0464], device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.4.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([0.0173, 0.2845, 0.0094,  ..., 0.0129, 0.0079, 0.0464], device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-0.5105,  0.1496,  0.0392,  0.0903, -0.1568, -0.4092,  0.3688, -0.2736,
        -0.7403,  0.3870,  0.5747,  0.4217, -0.0256,  0.1433,  0.9749, -0.3270,
        -0.4428,  0.0176, -0.6988, -0.6410,  0.4186,  0.6243, -0.1298,  0.0231,
         0.3447, -0.0524, -0.1776, -0.1404,  0.2666, -0.1201,  0.5107,  0.5493,
        -0.6482, -0.5930,  0.6337,  1.0000, -1.0838, -0.0476,  0.7847,  0.0733,
         0.4460,  0.1841,  0.7566, -0.3229,  0.4395, -0.4751, -0.4542,  0.5812,
        -0.0445,  0.4783, -0.7011,  0.4480,  0.2792,  0.7637, -0.0938,  0.6215,
        -0.6184,  0.7859, -0.4463,  0.3975, -0.1486, -0.0076, -0.6155,  0.1102,
        -0.6445, -0.3612, -0.5643, -0.1696,  0.9089,  0.4815,  0.4843,  0.3092,
        -0.0347, -0.5962, -0.0534, -0.7011,  0.1136,  0.2573, -0.9436,  0.5460,
        -0.4472,  0.7750, -0.3512,  0.9307,  0.0703, -0.3815, -0.4113, -0.5002,
        -0.3426, -0.6174,  0.0214, -0.2066, -0.3265,  0.8343, -0.6442,  0.4524],
       device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.9976e-02,  6.1699e-03,  2.4019e-03,  6.0392e-03,  8.4756e-01,
        -7.4254e-01, -1.4550e-03, -6.1978e-02, -7.2665e-03,  1.2007e-01,
        -1.2048e-03,  4.2755e-02,  7.2891e-02,  3.6609e-02,  9.8713e-03,
         4.2635e-02, -1.7606e-02,  1.8680e-02, -1.8105e-01, -2.3585e-01,
         2.5812e-02,  3.5814e-03,  3.2552e-01,  2.9203e-01,  1.5664e-01,
         1.4485e-01, -4.5061e-02, -5.1993e-02, -1.1699e-01,  1.9132e-02,
         1.5974e-02, -2.6301e-01, -2.9754e-01, -2.3803e-02, -2.7128e-02,
        -2.1432e-02,  4.0495e-02,  5.0280e-02, -2.9940e-04,  1.6047e-02,
         1.4387e-01,  5.0591e-03, -4.5106e-03,  6.9360e-04,  4.6246e-02,
        -6.2170e-05, -5.3023e-01,  4.3946e-02, -8.4399e-02, -1.2614e-01,
         1.4879e-02,  4.3965e-03, -9.0257e-03, -6.0968e-03, -1.2920e-01,
         6.5336e-01,  6.4390e-02, -5.9877e-02,  4.4773e-02, -1.1032e-02,
         3.0916e-03, -5.4395e-01,  3.0280e-02, -3.1792e-01, -1.3500e-01,
        -2.9181e-04, -2.6110e-01,  6.3174e-02,  3.8673e-02, -8.6348e-02,
        -1.9835e-01, -3.5634e-02, -2.8475e-01,  2.6840e-01, -1.1208e-02,
         2.0462e-03,  2.4126e-01, -3.3351e-01, -5.8050e-01,  3.4679e-01,
        -1.8159e-02,  7.4245e-03,  1.3566e-02, -1.7060e-06,  3.4349e-01,
        -1.0565e-03,  7.3623e-03, -8.6841e-02, -3.2311e-02,  7.5918e-03,
        -1.0763e-02,  1.1089e-04, -2.2557e-02,  2.2626e-03, -9.3917e-04,
        -8.7978e-02,  9.2347e-02,  1.9292e-02, -4.4159e-01, -1.6016e-02,
         3.0928e-01,  8.0741e-03,  4.2021e-03, -1.4655e-01, -5.0474e-02,
         1.0154e-03, -2.4131e-02, -1.0266e-01, -4.2929e-02,  3.0572e-01,
        -7.1675e-02,  9.5821e-04, -1.3120e-03, -2.7459e-01,  1.0000e+00,
        -5.5257e-02, -4.7422e-02,  7.4287e-02,  2.6331e-02, -6.8696e-02,
        -3.6926e-01, -8.4093e-02,  1.7593e-02,  1.1288e-01,  9.1335e-02,
        -5.2141e-01, -3.0037e-02, -3.6840e-01,  7.4291e-02,  2.4817e-03,
         2.5920e-03,  8.3414e-02,  3.5848e-02,  2.0743e-03,  1.7235e-01,
         6.7366e-02,  1.7739e-03,  1.1405e-01,  9.0249e-04,  3.4645e-02,
         2.0868e-03,  6.4417e-03,  1.5108e-01, -1.6740e-03,  2.6067e-02,
         3.4368e-03,  5.8322e-03, -2.4079e-02,  7.8452e-02, -2.8438e-03,
        -4.0502e-03, -6.6214e-02,  2.1363e-02,  3.3529e-02,  2.8985e-03,
        -3.6496e-02, -6.1801e-01, -7.6918e-01,  1.2974e-03, -5.2257e-01,
        -1.0671e-02,  1.6316e-03,  1.9493e-02, -3.6425e-01, -1.5587e-01,
         6.2078e-02, -1.7501e-01,  3.3651e-04,  3.9345e-01, -1.7460e-01,
        -7.9458e-02,  4.2796e-02,  2.8005e-02,  8.0180e-02,  2.0030e-01,
        -1.2291e-01,  1.2888e-01, -4.1070e-05,  3.5836e-02,  1.2471e-03,
        -3.7142e-01,  1.1615e-01,  3.9788e-02, -2.6664e-04, -2.4320e-02,
         1.3745e-03,  3.2084e-02,  5.2833e-03,  7.0324e-02, -6.3596e-01,
         3.4879e-01, -4.2586e-02,  2.8223e-03,  3.9192e-02,  5.9683e-03,
         5.2176e-01,  6.6358e-03,  7.1311e-01,  8.8636e-04, -6.3952e-03,
         1.9252e-01, -3.4687e-02, -2.0205e-01,  5.0888e-01, -1.0577e-01,
         1.0878e-02,  5.1083e-03, -6.2362e-04, -4.2878e-03,  1.0310e-01,
        -4.0018e-02,  4.1374e-02, -8.7401e-05, -1.1117e-01, -1.3307e-03,
         5.3671e-03,  1.5089e-02,  1.1177e-01, -2.7275e-01,  1.1250e-01,
         1.5175e-03,  9.7672e-02, -5.9771e-02, -4.0344e-03,  1.3928e-01,
        -2.0880e-01,  2.9071e-02, -4.4837e-01,  1.0571e-02,  6.1559e-02,
        -1.4951e-03, -1.4128e-04,  7.4253e-03, -1.5938e-03,  5.8774e-02,
        -6.5616e-02,  4.0555e-02, -3.2288e-04,  7.6843e-02,  1.3435e-02,
        -1.8902e-01, -2.1602e-01, -2.1546e-04, -2.3642e-01, -9.7630e-03,
        -5.9214e-02,  7.4324e-03, -1.8369e-01,  4.0184e-01, -4.6535e-03,
         8.6960e-01,  3.0498e-03, -3.6308e-02,  3.9974e-03, -5.6076e-01,
         3.9397e-02,  8.9245e-04,  1.1667e-03, -6.9010e-03,  8.9793e-04,
        -1.3371e-03, -2.6858e-03,  8.4899e-02,  4.6295e-02, -1.8025e-02,
         1.3984e-02, -1.0430e-01,  3.3785e-02,  1.3143e-03,  2.1726e-04,
        -1.4915e-01,  4.2533e-01,  4.1536e-03,  8.5240e-04,  9.6794e-01,
        -1.1705e-02,  1.2856e-01,  1.0121e-01,  2.3454e-01, -5.1493e-04,
        -1.8100e-02,  2.3500e-01,  3.2596e-01, -1.3836e-04,  1.3583e-02,
         1.2389e-01, -3.1281e-02, -8.2279e-02,  5.0844e-02, -2.9056e-02,
        -1.7224e-02, -3.6567e-01,  7.9866e-02, -9.4121e-03, -2.1411e-02,
        -1.9907e-01, -3.4653e-01, -2.3140e-02,  3.3252e-01,  4.1728e-02,
        -4.4045e-01, -2.6555e-01,  1.8744e-01, -2.7363e-02,  4.8627e-03,
         1.9625e-02,  3.9223e-01,  6.1587e-03,  2.8713e-02, -2.4643e-02,
        -1.5686e-02, -1.7468e-03,  1.4772e-01, -3.9386e-02,  2.3182e-02,
        -1.4488e-03,  2.6238e-03, -1.4749e-02, -4.0266e-03,  2.2692e-03,
        -1.2087e-03,  3.6382e-02,  3.7031e-02, -4.3878e-02, -4.1625e-01,
         7.3488e-02,  3.4450e-01,  1.2732e-01, -3.3519e-01,  4.7700e-01,
        -6.0740e-01,  1.1303e-01, -4.8066e-01,  2.2418e-02,  2.4444e-03,
         4.6742e-02,  6.0572e-03, -2.3893e-04,  1.0216e-01,  1.7152e-02,
         2.2809e-02,  1.6276e-03, -1.9525e-02,  3.5348e-01, -1.3069e-01,
        -3.7981e-02,  2.1455e-02,  7.7260e-01,  4.2011e-02, -3.9833e-01,
         1.3988e-02, -3.1254e-03, -1.5938e-02,  4.8970e-02,  1.3104e-01,
        -3.8414e-01, -4.1675e-03,  2.1431e-02,  2.1515e-01, -1.3565e-02,
         3.1234e-04,  1.7739e-04, -5.4292e-02,  2.2945e-02,  1.6224e-01,
        -2.4394e-01, -1.3098e-03,  1.7650e-01,  1.1274e-03, -3.2804e-03,
         1.4911e-02,  1.3519e-01,  5.7510e-02, -2.9519e-01, -1.8862e-01,
         1.0553e-02, -1.7893e-02,  2.4967e-02,  1.1009e-02,  8.5479e-02,
         6.3560e-02,  1.3389e-01, -3.7455e-02,  6.6843e-03], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0045,  0.0011, -0.0312,  ...,  0.1205, -0.1157, -0.0391],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.3165, -0.0698, -0.0417,  ..., -0.0022, -0.0142,  0.0693],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([0.0173, 0.2845, 0.0094,  ..., 0.0129, 0.0079, 0.0464], device='cuda:0')}
topk_layer inside model after retrieval:  [139, 2517, 93, 686, 1471, 530, 1657, 1640, 2523, 1571, 1262, 888, 2015, 402, 494, 400, 150, 825, 797, 1877, 653, 2265, 1504, 2188, 625, 1375, 1565, 2326, 1556, 768, 1489, 1250, 654, 1306, 439, 1016, 282, 782, 120, 233, 1597, 2078, 1134, 2507, 959, 1406, 15, 2415, 126, 1387, 2335, 864, 1676, 30, 1548, 172, 1100, 388, 1092, 1010, 1285, 1730, 455, 1987, 1215, 792, 826, 2035, 557, 1719, 607, 660, 1390, 2385, 1269, 1995, 1118, 424, 382, 1477, 412, 2404, 2118, 1152, 627, 798, 2315, 612, 1869, 1397, 2368, 74, 2285, 257, 941, 1167, 350, 616, 2223, 571, 2226, 2224, 838, 929, 2171, 1937, 591, 2334, 725, 735, 2531, 1854, 975, 1892, 4, 1908, 1852, 486, 1639, 1431, 1169, 889, 1140, 287, 2559, 731, 2101, 161, 1867, 1555, 1912, 606, 565, 1156, 1572, 1080, 2141, 605, 966, 883, 1424, 1700, 757, 296, 1138, 703, 674, 2186, 2271, 1707, 1806, 2038, 1518, 1698, 1587, 2558, 6, 145, 2121, 401, 1759, 5, 748, 851, 1338, 1075, 246, 2457, 63, 1583, 1223, 80, 581, 1989, 3, 770, 1524, 1831, 476, 24, 1362, 910, 2311, 460, 2006, 1013, 1513, 114, 1582, 2509, 859, 1230, 1819, 531, 1625, 347, 1957, 2207, 426, 1883, 1113, 1718, 539, 1415, 1359, 593, 520, 2328, 1241, 1946, 808, 383, 2211, 1723, 2485, 844, 863, 1441, 2492, 881, 1366, 1226, 214, 2337, 2268, 2261, 562, 1844, 2248, 361, 564, 1034, 270, 1512, 1323, 1795, 1836, 2149, 1272, 1015, 578, 2218, 1970, 1887, 381, 1645, 271, 743, 718, 1840, 1658, 990, 69, 2554, 2355, 789, 949, 922, 1263, 309, 2172, 1040, 2049, 1696, 1019, 1620, 1898, 1574, 2133, 1143, 40, 1847, 34, 2475, 1198, 2487, 2506, 1439, 54, 36, 2199, 2299, 1289, 2257, 1117, 2418, 1025, 1893, 499, 778, 364, 1654, 82, 945, 2524, 1713, 372, 239, 1545, 2264, 928, 1779, 1811, 704, 689, 2044, 470, 1981, 1172, 137, 2045, 1246, 1705, 1820, 2012, 2495, 1634, 1522, 2322, 1239, 746, 978, 992, 525, 26, 2425, 646, 1420, 1740, 2258, 2538, 832, 1526, 648, 1641, 284, 1660, 1931, 1445, 2246, 1787, 335, 2138, 777, 1558, 1868, 29, 895, 344, 33, 2053, 1460, 2276, 741, 1951, 1098, 2459, 790, 997, 1499, 722, 2455, 2518, 515, 1998, 1474, 2134, 874, 2510, 536, 2163, 1224, 1007, 2117, 664, 1686, 552, 411, 2184, 1914, 784, 2323, 1491, 1035, 712]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
The device used will be: 
True
cuda:0
The device used will be: 
True
cuda:0
chosen_head:  {'blocks.5.layer.bias': tensor([ 0.0040, -0.0205], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 1.5765e-02,  1.0660e-02,  4.3432e-03,  ...,  1.0930e-02,
          1.1431e-02, -3.5607e-03],
        [ 1.2527e-02,  1.1907e-02,  8.8815e-05,  ..., -8.2031e-03,
          9.7858e-03,  1.1328e-02]], device='cuda:0')}

 Model C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c loaded successfuly with best perf



 ----- Architecture Block BatchNorm2dSK3962(5, 5)1.0reflect, number 0 -----
- BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK963842(3, 3)0.6499999761581421reflect, number 1 -----
- BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.4)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK38415362(3, 3)0.25reflect, number 2 -----
- BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block BatchNorm2dSK153625602(3, 3)0.25reflect, number 3 -----
- BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block BatchNorm2dSK256025602(3, 3)0.25reflect, number 4 -----
- BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 5 -----
- Flatten(start_dim=1, end_dim=-1)
- Dropout(p=0.5, inplace=False)
- Linear(in_features=2560, out_features=2, bias=True)
model.heads:  [{'blocks.5.layer.bias': tensor([ 1.5791e-05, -1.6506e-02], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0120,  0.0150,  0.0043,  ...,  0.0072,  0.0111, -0.0045],
        [ 0.0163,  0.0075,  0.0001,  ..., -0.0045,  0.0101,  0.0122]],
       device='cuda:0')}, {'blocks.5.layer.bias': tensor([ 0.0040, -0.0205], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 1.5765e-02,  1.0660e-02,  4.3432e-03,  ...,  1.0930e-02,
          1.1431e-02, -3.5607e-03],
        [ 1.2527e-02,  1.1907e-02,  8.8815e-05,  ..., -8.2031e-03,
          9.7858e-03,  1.1328e-02]], device='cuda:0')}]
[Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (5): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=2560, out_features=2, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)]
LAYER NAME:  Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (5): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=2560, out_features=2, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)
LAYER CHILDREN:  [BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=1.4)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
  )
  (layer): Linear(in_features=2560, out_features=2, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)]
subsubl NAME:  Sequential(
  (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=1.4)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): Flatten(start_dim=1, end_dim=-1)
)
subsubl NAME:  Linear(in_features=2560, out_features=2, bias=True)
subsubl NAME:  Dropout(p=0.5, inplace=False)
CONFIG MODE:  unsupervised

 ********** Hebbian Unsupervised learning of blocks [0, 1, 2, 3, 4] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([49, 33, 72, 51, 71, 92, 15, 14, 23,  0, 71, 75, 81, 69, 40, 43, 92, 97,
        70, 53], device='cuda:0')
TARGETS AFTER SUB:  tensor([16, 41, 41, 16, 16, 16, 16, 41, 41, 16, 16, 41, 41, 41, 41, 41, 16, 16,
        41, 16])
[16, 41]
TARGETS AFTER CLEANER:  tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0])
200
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 200
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               None
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([19, 29,  0, 11,  1, 86, 90, 28, 23, 31, 39, 96, 82, 17, 71, 39,  8, 97,
        80, 71], device='cuda:0')
TARGETS AFTER SUB:  tensor([41, 41, 41, 41, 16, 16, 16, 41, 41, 16, 41, 41, 41, 16, 41, 41, 41, 41,
        16, 41])
[16, 41]
TARGETS AFTER CLEANER:  tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1])
1000
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 1000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               None
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([19, 29,  0, 11,  1, 86, 90, 28, 23, 31, 39, 96, 82, 17, 71, 39,  8, 97,
        80, 71], device='cuda:0')
TARGETS AFTER SUB:  tensor([41, 41, 41, 41, 16, 16, 16, 41, 41, 16, 41, 41, 41, 16, 41, 41, 41, 41,
        16, 41])
[16, 41]
TARGETS AFTER CLEANER:  tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1])
1000
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 1000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
------------------------
INDICES:  1000
IMAGE SIZE: torch.Size([10, 3, 32, 32])
SAVING FOLDER FOR UNSUP:  C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c
prev_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight', 'blocks.4.layer.weight'])
activations_sum.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight', 'blocks.4.layer.weight'])
DEPTH:  5
ACTIVATIONS SHAPE:  tensor([[  601.9374,   703.9833,   310.3566,    42.0509,   -69.9268,   253.2599,
          -262.7180,  -733.5260,  -691.6948,  -494.0582,  -355.1078,  -388.3888,
           -74.5393,   189.2211,   232.1629,   370.7653],
        [  523.4799,   461.6072,    35.2587,  -229.6232,  -246.2045,    25.3626,
          -245.2082,  -610.7347,  -749.2782,  -153.5718,    58.5016,   263.9305,
           589.8027,   650.7285,   746.4763,   738.4242],
        [  666.8883,   473.8051,    25.9993,  -409.2913,  -471.9934,  -175.5719,
          -589.7922,  -696.1540,  -877.1442,  -367.8704,  -140.0936,   278.5020,
           607.5043,   844.1338,   951.8695,   938.7161],
        [  329.2512,    18.3183,  -142.7759,  -624.4454,  -854.0721,  -466.6980,
          -704.2008, -1026.7844,  -922.9582,  -638.7839,  -333.6320,    99.1574,
           562.2313,   836.0555,   727.6843,   572.2960],
        [  244.0575,  -398.7340,  -355.6219,  -731.5029,  -952.3378, -1100.9688,
         -1165.9126, -1470.1650, -1474.2117, -1384.7489,  -782.1042,   -43.3624,
           478.2083,   614.1456,   680.0154,   504.6337],
        [  196.4207,  -433.6513,  -581.2654,  -837.0007, -1173.6094, -1492.1738,
         -1762.8490, -2012.4401, -2213.0942, -1811.2767,  -867.4135,    18.9135,
           462.0242,   613.4412,   848.3236,   508.9171],
        [  158.6474,  -497.8264,  -582.4251,  -940.8123, -1181.6096, -1632.1245,
         -1951.4241, -2246.2966, -2177.4194, -1801.9683, -1180.9446,  -274.8283,
           295.4092,   392.6507,   753.0251,   320.7054],
        [  288.8265,  -572.9028,  -807.2034,  -958.6739, -1257.1117, -1808.4915,
         -2154.7356, -2186.3201, -2143.5386, -1823.1274, -1439.0779,  -724.2006,
          -117.7740,   119.4618,   561.3745,   132.5015],
        [  546.5691,  -473.9626,  -761.1257, -1046.2507, -1255.9542, -1790.2938,
         -1959.6644, -1831.7837, -1747.3900, -1517.7307, -1322.8757,  -870.5596,
           -91.5531,    10.1940,   522.4019,   307.5735],
        [  744.9803,   -94.3421,  -512.1791, -1246.1328, -1203.6733, -1536.9736,
         -1764.1909, -1471.5916, -1571.4803, -1581.1890, -1180.9913,  -649.8683,
           -30.6252,    37.3366,   626.9365,   290.5949],
        [  875.6442,   -20.9369,  -543.5687, -1176.0872,  -916.0437, -1030.7933,
         -1475.6538, -1571.4929, -1530.9735, -1359.5525, -1051.2760,  -254.7745,
           179.0812,   309.7760,   741.7465,   390.8788],
        [  977.4367,   287.6968,  -213.7172,  -683.5319,  -732.0380,  -853.4045,
         -1081.9987, -1223.7992, -1149.9348,  -905.9933,  -710.4205,   123.7400,
           460.1671,   317.4636,   502.9827,   538.5402],
        [ 1028.6965,   382.3928,    27.4228,  -466.6570,  -655.8980,  -689.5293,
          -871.0438,  -812.9849,  -949.1368,  -732.9662,  -316.5059,   329.4455,
           268.8069,    26.9610,   337.7576,   522.9403],
        [  871.2661,   467.1781,    41.0516,  -381.3416,  -542.8304,  -533.3107,
          -691.8385,  -784.8411,  -633.0176,  -315.5892,  -121.2229,   141.1054,
           376.9205,   188.7994,   544.0225,   759.2363],
        [  853.8345,   505.2065,   233.4637,   -81.6318,   -50.5289,   -54.8940,
          -277.4170,  -357.9746,   -95.0382,   178.8050,   288.1489,   487.4308,
           696.6367,   525.4929,   679.5511,  1086.4215],
        [  591.3203,   310.4695,   -46.7553,  -332.6403,  -395.5433,  -361.4823,
          -353.2033,  -443.7885,  -296.9577,  -120.4286,    60.0673,   403.3790,
           441.4655,   263.9799,   376.0109,   620.0803]], device='cuda:0')
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.topk_layer:  <class 'list'>
CHECK_THRESHOLD:  tensor(1, device='cuda:0', dtype=torch.uint8) tensor(-0.0022) tensor(-0.5105)
topk_mask  tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0.])
threshold_mask  tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0],
       dtype=torch.uint8)
final_mask tensor([1., 0., 0., 0., 1., 2., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 2., 0.,
        1., 1., 0., 0., 1., 0., 0., 2., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0.,
        2., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1.,
        0., 1., 1., 1., 1., 0.])
lower_lr_mask  tensor([-0., -0., -0., -0., -0., -1., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -1., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.])
higher_lr_mask  tensor([0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.0000, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.0000, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.0000, 0.1500, 0.0000, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.0000, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500])
lr_mask  tensor([[[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]]])
torch.Size([96, 3, 5, 5])
self.lr tensor([[[0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066]],

        [[0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066]],

        [[0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066]]], device='cuda:0')
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.topk_layer:  <class 'list'>
CHECK_THRESHOLD:  tensor(0, device='cuda:0', dtype=torch.uint8) tensor(-0.0024) tensor(0.0300)
topk_mask  tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1.])
threshold_mask  tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0],
       dtype=torch.uint8)
final_mask tensor([0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 0., 1., 1., 0., 0., 0., 2., 0., 1., 0., 0., 1., 1., 1., 1., 1.,
        2., 0., 1., 0., 0., 2., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 0., 2., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 2., 1.,
        2., 2., 1., 2., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0.,
        1., 0., 2., 0., 1., 1., 0., 0., 1., 2., 1., 0., 0., 1., 2., 2., 1., 1.,
        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 2., 1., 1., 2., 0., 0., 1., 1.,
        1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 0., 2., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 2., 1., 1.,
        1., 2., 1., 0., 1., 0., 0., 2., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1.,
        2., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 2., 0., 1., 0., 0., 0., 1.,
        0., 1., 0., 1., 2., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,
        0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0.,
        1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1.,
        0., 0., 1., 1., 0., 1., 1., 1., 2., 1., 0., 0., 1., 1., 0., 1., 2., 0.,
        0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1.,
        2., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 2., 0., 0., 0., 0., 0.,
        1., 1., 1., 1., 0., 1., 0., 2., 0., 1., 0., 0., 2., 0., 1., 0., 1., 1.,
        0., 0., 1., 0., 0., 2., 1., 0., 1., 0., 0., 0., 0., 2., 1., 1., 1., 0.,
        0., 1., 0., 0., 1., 1.])
lower_lr_mask  tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0., -0., -0., -1.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -1., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -1., -0., -1., -1., -0., -1., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0., -0., -0., -0.,
        -0., -1., -0., -0., -0., -0., -1., -1., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -1., -0., -0., -1., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -1., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -1., -0., -0., -0., -1., -0., -0., -0., -0.,
        -0., -1., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -1., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -1., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -1., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -1., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -1.,
        -0., -0., -0., -0., -1., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -1., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0.])
higher_lr_mask  tensor([0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500,
        0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.0000,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500,
        0.0000, 0.0000, 0.1500, 0.0000, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.0000, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500,
        0.0000, 0.0000, 0.1500, 0.1500, 0.1500, 0.0000, 0.0000, 0.1500, 0.1500,
        0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500,
        0.1500, 0.0000, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.0000, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.0000, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500,
        0.0000, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.0000,
        0.1500, 0.0000, 0.0000, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.0000,
        0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.0000,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.0000, 0.1500, 0.0000, 0.1500, 0.0000, 0.1500, 0.0000, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.0000])
lr_mask  tensor([[[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]]])
torch.Size([384, 96, 3, 3])
self.lr tensor([[[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]]], device='cuda:0')
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.topk_layer:  <class 'list'>
CHECK_THRESHOLD:  tensor(0, device='cuda:0', dtype=torch.uint8) tensor(0.0003) tensor(0.0045)
topk_mask  tensor([0., 0., 0.,  ..., 0., 0., 0.])
threshold_mask  tensor([0, 0, 1,  ..., 0, 1, 1], dtype=torch.uint8)
final_mask tensor([0., 0., 1.,  ..., 0., 1., 1.])
lower_lr_mask  tensor([-0., -0., -0.,  ..., -0., -0., -0.])
higher_lr_mask  tensor([0.1500, 0.1500, 0.1500,  ..., 0.1500, 0.1500, 0.1500])
lr_mask  tensor([[[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        ...,


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]]])
torch.Size([1536, 384, 3, 3])
self.lr tensor([[[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]],

        [[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]],

        [[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]],

        ...,

        [[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]],

        [[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]],

        [[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]]], device='cuda:0')
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.topk_layer:  <class 'list'>
CHECK_THRESHOLD:  tensor(1, device='cuda:0', dtype=torch.uint8) tensor(0.0024) tensor(-0.3165)
topk_mask  tensor([0., 0., 1.,  ..., 0., 0., 0.])
threshold_mask  tensor([1, 1, 1,  ..., 1, 1, 0], dtype=torch.uint8)
final_mask tensor([1., 1., 2.,  ..., 1., 1., 0.])
lower_lr_mask  tensor([-0., -0., -1.,  ..., -0., -0., -0.])
higher_lr_mask  tensor([0.1500, 0.1500, 0.0000,  ..., 0.1500, 0.1500, 0.1500])
lr_mask  tensor([[[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        ...,


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]]])
torch.Size([2560, 1536, 3, 3])
self.lr tensor([[[0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488]],

        [[0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488]],

        [[0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488]],

        ...,

        [[0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488]],

        [[0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488]],

        [[0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488]]], device='cuda:0')
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.topk_layer:  <class 'list'>
CHECK_THRESHOLD:  tensor(0, device='cuda:0', dtype=torch.uint8) tensor(4.4889e-06) tensor(0.0173)
topk_mask  tensor([0., 0., 0.,  ..., 0., 1., 1.])
threshold_mask  tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.uint8)
final_mask tensor([0., 0., 0.,  ..., 0., 1., 1.])
lower_lr_mask  tensor([-0., -0., -0.,  ..., -0., -0., -0.])
higher_lr_mask  tensor([0.1500, 0.1500, 0.1500,  ..., 0.1500, 0.0000, 0.0000])
lr_mask  tensor([[[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        ...,


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.0000]]]])
torch.Size([2560, 2560, 3, 3])
self.lr tensor([[[0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491]],

        [[0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491]],

        [[0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491]],

        ...,

        [[0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491]],

        [[0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491]],

        [[0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491]]], device='cuda:0')
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight', 'blocks.4.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight', 'blocks.4.layer.weight'])
activations_sum[k] 96
activations_sum[k]:  [25, 59, 5, 82, 30, 89, 16, 77, 33, 42]
topk_kernels len:  5
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3', 'conv4']
topk_kernels:  {'conv0': [25, 59, 5, 82, 30, 89, 16, 77, 33, 42, 70, 93, 45, 68, 64], 'conv1': [197, 349, 72, 345, 121, 76, 124, 193, 109, 365, 274, 296, 354, 92, 73, 56, 75, 105, 36, 188, 202, 343, 336, 304, 162, 41, 266, 159, 100, 104, 163, 148, 22, 9, 186, 229, 190, 379, 4, 174, 358, 114, 118, 373, 146, 217, 278, 347, 70, 191, 169, 99, 50, 180, 137, 26, 94, 383, 324], 'conv2': [958, 733, 761, 250, 1436, 856, 14, 209, 1163, 176, 399, 801, 942, 95, 1532, 118, 622, 959, 1452, 1274, 324, 326, 835, 632, 639, 1172, 852, 1113, 35, 850, 730, 1486, 636, 869, 508, 155, 597, 173, 15, 423, 22, 312, 1278, 281, 12, 408, 783, 1159, 1345, 799, 746, 499, 933, 1126, 1178, 373, 395, 915, 868, 356, 773, 810, 1002, 923, 588, 580, 426, 519, 1238, 164, 1089, 42, 980, 441, 575, 1001, 612, 743, 353, 649, 779, 154, 523, 964, 905, 507, 461, 1046, 626, 294, 981, 1025, 937, 752, 1186, 1032, 405, 953, 313, 58, 542, 1206, 544, 667, 335, 754, 1155, 1508, 1317, 900, 311, 70, 1292, 1184, 707, 439, 682, 1522, 77, 93, 1203, 174, 115, 854, 1125, 1149, 960, 1494, 74, 952, 866, 134, 902, 921, 653, 590, 1038, 238, 872, 1282, 1340, 650, 1297, 1357, 1455, 307, 788, 265, 378, 75, 614, 1265, 243, 454, 406, 1396, 1260, 264, 212, 790, 119, 1461, 1520, 911, 961, 195, 994, 359, 16, 602, 760, 692, 1045, 457, 670, 1302, 732, 1281, 802, 1385, 1460, 728, 714, 889, 1324, 61, 208, 1261, 878, 562, 629, 936, 1132, 574, 257, 1463, 217, 1171, 851, 490, 175, 736, 1462, 1294, 1432, 295, 1242, 1031, 1450, 745, 1192, 41, 1349, 411, 1248, 1227, 1298, 132, 1426, 1379, 348, 105, 705, 1156, 846, 674, 1291, 171, 1442, 381, 188], 'conv3': [1447, 129, 2500, 2420, 2056, 2414, 2181, 564, 370, 746, 398, 646, 35, 1720, 922, 2523, 199, 419, 696, 1843, 1755, 1025, 2317, 305, 1424, 2193, 1794, 1993, 583, 472, 1926, 534, 640, 1380, 1394, 708, 68, 1292, 1304, 533, 1198, 837, 2331, 1958, 653, 1409, 2085, 162, 1830, 658, 1175, 2447, 2000, 964, 1558, 1280, 436, 1992, 2259, 2237, 888, 1746, 1953, 1637, 1051, 2070, 1506, 1141, 2, 2093, 732, 299, 2069, 1608, 33, 1666, 2390, 828, 2151, 2536, 72, 1003, 2342, 2179, 1887, 1550, 454, 1873, 2547, 1909, 2022, 1001, 1123, 1248, 2057, 810, 34, 1403, 2176, 1443, 1824, 2453, 421, 2185, 1753, 1938, 638, 796, 1385, 1650, 2094, 1045, 561, 507, 720, 1831, 146, 1536, 210, 1457, 1139, 1689, 2460, 2095, 1162, 1524, 302, 800, 1451, 1449, 669, 1783, 365, 1245, 1406, 2174, 1931, 509, 674, 2545, 1056, 425, 1153, 1776, 63, 2283, 935, 586, 2300, 1376, 2162, 2060, 2305, 437, 1555, 1567, 2168, 283, 1283, 1415, 838, 80, 1332, 2112, 481, 1659, 1049, 1638, 43, 1221, 291, 340, 2322, 2272, 404, 450, 575, 1789, 1809, 867, 2394, 1572, 1986, 2515, 1970, 122, 1886, 2462, 491, 1929, 2320, 557, 2227, 1160, 1266, 225, 1960, 1845, 1548, 336, 1157, 1781, 1439, 1074, 2187, 2197, 792, 2285, 2511, 687, 167, 1021, 632, 788, 1864, 1799, 1435, 1591, 1458, 2208, 267, 13, 1088, 2008, 2252, 2387, 2161, 304, 895, 1512, 465, 968, 1199, 1271, 306, 1742, 505, 130, 220, 926, 781, 662, 1554, 855, 1733, 698, 1839, 2062, 499, 1052, 2081, 258, 2308, 357, 468, 1062, 24, 521, 1645, 1375, 768, 2121, 411, 2042, 616, 96, 1890, 242, 2418, 1196, 276, 2028, 2378, 467, 2173, 1066, 772, 366, 2079, 761, 1874, 1446, 362, 284, 1699, 884, 1226, 252, 1054, 543, 597, 1108, 2527, 2310, 1966, 1325, 2456, 203, 27, 959, 2192, 962, 992, 2050, 1253, 1531, 1227, 2518, 1961, 192, 124, 259, 6, 1827, 944, 239, 2157, 885, 2167, 1527, 576, 706, 399, 2484, 1971, 555, 943, 82, 1194, 1694, 1029, 2130, 1633, 1267, 107, 2293, 2508, 2532, 2256, 2011, 316, 1112, 240, 151, 2436, 634, 1206, 1549, 54, 2242, 300, 2309, 689, 1782, 899, 1448, 2113, 2030, 2033, 1107, 1647, 1894, 1968, 1646, 1670, 1326, 2164, 937, 298, 2441, 1138, 1578, 1811, 1622, 980, 1055, 1351, 1837, 2284, 331, 2446, 939, 1697, 1908, 2018], 'conv4': [139, 2517, 93, 686, 1471, 530, 1657, 1640, 2523, 1571, 1262, 888, 2015, 402, 494, 400, 150, 825, 797, 1877, 653, 2265, 1504, 2188, 625, 1375, 1565, 2326, 1556, 768, 1489, 1250, 654, 1306, 439, 1016, 282, 782, 120, 233, 1597, 2078, 1134, 2507, 959, 1406, 15, 2415, 126, 1387, 2335, 864, 1676, 30, 1548, 172, 1100, 388, 1092, 1010, 1285, 1730, 455, 1987, 1215, 792, 826, 2035, 557, 1719, 607, 660, 1390, 2385, 1269, 1995, 1118, 424, 382, 1477, 412, 2404, 2118, 1152, 627, 798, 2315, 612, 1869, 1397, 2368, 74, 2285, 257, 941, 1167, 350, 616, 2223, 571, 2226, 2224, 838, 929, 2171, 1937, 591, 2334, 725, 735, 2531, 1854, 975, 1892, 4, 1908, 1852, 486, 1639, 1431, 1169, 889, 1140, 287, 2559, 731, 2101, 161, 1867, 1555, 1912, 606, 565, 1156, 1572, 1080, 2141, 605, 966, 883, 1424, 1700, 757, 296, 1138, 703, 674, 2186, 2271, 1707, 1806, 2038, 1518, 1698, 1587, 2558, 6, 145, 2121, 401, 1759, 5, 748, 851, 1338, 1075, 246, 2457, 63, 1583, 1223, 80, 581, 1989, 3, 770, 1524, 1831, 476, 24, 1362, 910, 2311, 460, 2006, 1013, 1513, 114, 1582, 2509, 859, 1230, 1819, 531, 1625, 347, 1957, 2207, 426, 1883, 1113, 1718, 539, 1415, 1359, 593, 520, 2328, 1241, 1946, 808, 383, 2211, 1723, 2485, 844, 863, 1441, 2492, 881, 1366, 1226, 214, 2337, 2268, 2261, 562, 1844, 2248, 361, 564, 1034, 270, 1512, 1323, 1795, 1836, 2149, 1272, 1015, 578, 2218, 1970, 1887, 381, 1645, 271, 743, 718, 1840, 1658, 990, 69, 2554, 2355, 789, 949, 922, 1263, 309, 2172, 1040, 2049, 1696, 1019, 1620, 1898, 1574, 2133, 1143, 40, 1847, 34, 2475, 1198, 2487, 2506, 1439, 54, 36, 2199, 2299, 1289, 2257, 1117, 2418, 1025, 1893, 499, 778, 364, 1654, 82, 945, 2524, 1713, 372, 239, 1545, 2264, 928, 1779, 1811, 704, 689, 2044, 470, 1981, 1172, 137, 2045, 1246, 1705, 1820, 2012, 2495, 1634, 1522, 2322, 1239, 746, 978, 992, 525, 26, 2425, 646, 1420, 1740, 2258, 2538, 832, 1526, 648, 1641, 284, 1660, 1931, 1445, 2246, 1787, 335, 2138, 777, 1558, 1868, 29, 895, 344, 33, 2053, 1460, 2276, 741, 1951, 1098, 2459, 790, 997, 1499, 722, 2455, 2518, 515, 1998, 1474, 2134, 874, 2510, 536, 2163, 1224, 1007, 2117, 664, 1686, 552, 411, 2184, 1914, 784, 2323, 1491, 1035, 712]}
activations_sum[k] len:  96
activations_sum[k] 384
activations_sum[k]:  [50, 56, 75, 118, 206, 144, 94, 317, 382, 336]
topk_kernels len:  5
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3', 'conv4']
topk_kernels:  {'conv0': [25, 59, 5, 82, 30, 89, 16, 77, 33, 42, 70, 93, 45, 68, 64], 'conv1': [50, 56, 75, 118, 206, 144, 94, 317, 382, 336, 354, 140, 41, 264, 191, 326, 142, 233, 324, 92, 262, 127, 355, 104, 31, 221, 365, 137, 298, 200, 6, 195, 88, 130, 345, 352, 67, 234, 371, 39, 180, 219, 96, 14, 190, 62, 292, 134, 48, 113, 304, 121, 64, 99, 357, 117, 266, 373, 312], 'conv2': [958, 733, 761, 250, 1436, 856, 14, 209, 1163, 176, 399, 801, 942, 95, 1532, 118, 622, 959, 1452, 1274, 324, 326, 835, 632, 639, 1172, 852, 1113, 35, 850, 730, 1486, 636, 869, 508, 155, 597, 173, 15, 423, 22, 312, 1278, 281, 12, 408, 783, 1159, 1345, 799, 746, 499, 933, 1126, 1178, 373, 395, 915, 868, 356, 773, 810, 1002, 923, 588, 580, 426, 519, 1238, 164, 1089, 42, 980, 441, 575, 1001, 612, 743, 353, 649, 779, 154, 523, 964, 905, 507, 461, 1046, 626, 294, 981, 1025, 937, 752, 1186, 1032, 405, 953, 313, 58, 542, 1206, 544, 667, 335, 754, 1155, 1508, 1317, 900, 311, 70, 1292, 1184, 707, 439, 682, 1522, 77, 93, 1203, 174, 115, 854, 1125, 1149, 960, 1494, 74, 952, 866, 134, 902, 921, 653, 590, 1038, 238, 872, 1282, 1340, 650, 1297, 1357, 1455, 307, 788, 265, 378, 75, 614, 1265, 243, 454, 406, 1396, 1260, 264, 212, 790, 119, 1461, 1520, 911, 961, 195, 994, 359, 16, 602, 760, 692, 1045, 457, 670, 1302, 732, 1281, 802, 1385, 1460, 728, 714, 889, 1324, 61, 208, 1261, 878, 562, 629, 936, 1132, 574, 257, 1463, 217, 1171, 851, 490, 175, 736, 1462, 1294, 1432, 295, 1242, 1031, 1450, 745, 1192, 41, 1349, 411, 1248, 1227, 1298, 132, 1426, 1379, 348, 105, 705, 1156, 846, 674, 1291, 171, 1442, 381, 188], 'conv3': [1447, 129, 2500, 2420, 2056, 2414, 2181, 564, 370, 746, 398, 646, 35, 1720, 922, 2523, 199, 419, 696, 1843, 1755, 1025, 2317, 305, 1424, 2193, 1794, 1993, 583, 472, 1926, 534, 640, 1380, 1394, 708, 68, 1292, 1304, 533, 1198, 837, 2331, 1958, 653, 1409, 2085, 162, 1830, 658, 1175, 2447, 2000, 964, 1558, 1280, 436, 1992, 2259, 2237, 888, 1746, 1953, 1637, 1051, 2070, 1506, 1141, 2, 2093, 732, 299, 2069, 1608, 33, 1666, 2390, 828, 2151, 2536, 72, 1003, 2342, 2179, 1887, 1550, 454, 1873, 2547, 1909, 2022, 1001, 1123, 1248, 2057, 810, 34, 1403, 2176, 1443, 1824, 2453, 421, 2185, 1753, 1938, 638, 796, 1385, 1650, 2094, 1045, 561, 507, 720, 1831, 146, 1536, 210, 1457, 1139, 1689, 2460, 2095, 1162, 1524, 302, 800, 1451, 1449, 669, 1783, 365, 1245, 1406, 2174, 1931, 509, 674, 2545, 1056, 425, 1153, 1776, 63, 2283, 935, 586, 2300, 1376, 2162, 2060, 2305, 437, 1555, 1567, 2168, 283, 1283, 1415, 838, 80, 1332, 2112, 481, 1659, 1049, 1638, 43, 1221, 291, 340, 2322, 2272, 404, 450, 575, 1789, 1809, 867, 2394, 1572, 1986, 2515, 1970, 122, 1886, 2462, 491, 1929, 2320, 557, 2227, 1160, 1266, 225, 1960, 1845, 1548, 336, 1157, 1781, 1439, 1074, 2187, 2197, 792, 2285, 2511, 687, 167, 1021, 632, 788, 1864, 1799, 1435, 1591, 1458, 2208, 267, 13, 1088, 2008, 2252, 2387, 2161, 304, 895, 1512, 465, 968, 1199, 1271, 306, 1742, 505, 130, 220, 926, 781, 662, 1554, 855, 1733, 698, 1839, 2062, 499, 1052, 2081, 258, 2308, 357, 468, 1062, 24, 521, 1645, 1375, 768, 2121, 411, 2042, 616, 96, 1890, 242, 2418, 1196, 276, 2028, 2378, 467, 2173, 1066, 772, 366, 2079, 761, 1874, 1446, 362, 284, 1699, 884, 1226, 252, 1054, 543, 597, 1108, 2527, 2310, 1966, 1325, 2456, 203, 27, 959, 2192, 962, 992, 2050, 1253, 1531, 1227, 2518, 1961, 192, 124, 259, 6, 1827, 944, 239, 2157, 885, 2167, 1527, 576, 706, 399, 2484, 1971, 555, 943, 82, 1194, 1694, 1029, 2130, 1633, 1267, 107, 2293, 2508, 2532, 2256, 2011, 316, 1112, 240, 151, 2436, 634, 1206, 1549, 54, 2242, 300, 2309, 689, 1782, 899, 1448, 2113, 2030, 2033, 1107, 1647, 1894, 1968, 1646, 1670, 1326, 2164, 937, 298, 2441, 1138, 1578, 1811, 1622, 980, 1055, 1351, 1837, 2284, 331, 2446, 939, 1697, 1908, 2018], 'conv4': [139, 2517, 93, 686, 1471, 530, 1657, 1640, 2523, 1571, 1262, 888, 2015, 402, 494, 400, 150, 825, 797, 1877, 653, 2265, 1504, 2188, 625, 1375, 1565, 2326, 1556, 768, 1489, 1250, 654, 1306, 439, 1016, 282, 782, 120, 233, 1597, 2078, 1134, 2507, 959, 1406, 15, 2415, 126, 1387, 2335, 864, 1676, 30, 1548, 172, 1100, 388, 1092, 1010, 1285, 1730, 455, 1987, 1215, 792, 826, 2035, 557, 1719, 607, 660, 1390, 2385, 1269, 1995, 1118, 424, 382, 1477, 412, 2404, 2118, 1152, 627, 798, 2315, 612, 1869, 1397, 2368, 74, 2285, 257, 941, 1167, 350, 616, 2223, 571, 2226, 2224, 838, 929, 2171, 1937, 591, 2334, 725, 735, 2531, 1854, 975, 1892, 4, 1908, 1852, 486, 1639, 1431, 1169, 889, 1140, 287, 2559, 731, 2101, 161, 1867, 1555, 1912, 606, 565, 1156, 1572, 1080, 2141, 605, 966, 883, 1424, 1700, 757, 296, 1138, 703, 674, 2186, 2271, 1707, 1806, 2038, 1518, 1698, 1587, 2558, 6, 145, 2121, 401, 1759, 5, 748, 851, 1338, 1075, 246, 2457, 63, 1583, 1223, 80, 581, 1989, 3, 770, 1524, 1831, 476, 24, 1362, 910, 2311, 460, 2006, 1013, 1513, 114, 1582, 2509, 859, 1230, 1819, 531, 1625, 347, 1957, 2207, 426, 1883, 1113, 1718, 539, 1415, 1359, 593, 520, 2328, 1241, 1946, 808, 383, 2211, 1723, 2485, 844, 863, 1441, 2492, 881, 1366, 1226, 214, 2337, 2268, 2261, 562, 1844, 2248, 361, 564, 1034, 270, 1512, 1323, 1795, 1836, 2149, 1272, 1015, 578, 2218, 1970, 1887, 381, 1645, 271, 743, 718, 1840, 1658, 990, 69, 2554, 2355, 789, 949, 922, 1263, 309, 2172, 1040, 2049, 1696, 1019, 1620, 1898, 1574, 2133, 1143, 40, 1847, 34, 2475, 1198, 2487, 2506, 1439, 54, 36, 2199, 2299, 1289, 2257, 1117, 2418, 1025, 1893, 499, 778, 364, 1654, 82, 945, 2524, 1713, 372, 239, 1545, 2264, 928, 1779, 1811, 704, 689, 2044, 470, 1981, 1172, 137, 2045, 1246, 1705, 1820, 2012, 2495, 1634, 1522, 2322, 1239, 746, 978, 992, 525, 26, 2425, 646, 1420, 1740, 2258, 2538, 832, 1526, 648, 1641, 284, 1660, 1931, 1445, 2246, 1787, 335, 2138, 777, 1558, 1868, 29, 895, 344, 33, 2053, 1460, 2276, 741, 1951, 1098, 2459, 790, 997, 1499, 722, 2455, 2518, 515, 1998, 1474, 2134, 874, 2510, 536, 2163, 1224, 1007, 2117, 664, 1686, 552, 411, 2184, 1914, 784, 2323, 1491, 1035, 712]}
activations_sum[k] len:  384
activations_sum[k] 1536
activations_sum[k]:  [937, 407, 636, 307, 700, 176, 1086, 195, 1385, 1171]
topk_kernels len:  5
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3', 'conv4']
topk_kernels:  {'conv0': [25, 59, 5, 82, 30, 89, 16, 77, 33, 42, 70, 93, 45, 68, 64], 'conv1': [50, 56, 75, 118, 206, 144, 94, 317, 382, 336, 354, 140, 41, 264, 191, 326, 142, 233, 324, 92, 262, 127, 355, 104, 31, 221, 365, 137, 298, 200, 6, 195, 88, 130, 345, 352, 67, 234, 371, 39, 180, 219, 96, 14, 190, 62, 292, 134, 48, 113, 304, 121, 64, 99, 357, 117, 266, 373, 312], 'conv2': [937, 407, 636, 307, 700, 176, 1086, 195, 1385, 1171, 405, 1317, 1278, 921, 673, 427, 799, 1034, 1486, 854, 421, 311, 731, 616, 794, 1307, 35, 706, 1500, 544, 194, 1360, 450, 958, 598, 1291, 883, 66, 560, 1306, 710, 869, 14, 93, 932, 141, 548, 155, 312, 796, 361, 167, 37, 1534, 851, 490, 551, 1000, 1412, 445, 686, 1226, 838, 906, 1030, 100, 13, 812, 1363, 730, 687, 660, 1006, 139, 150, 268, 437, 1281, 889, 380, 856, 635, 77, 1430, 123, 1286, 1109, 346, 482, 1511, 67, 1359, 1386, 1056, 1294, 786, 1239, 683, 28, 499, 276, 95, 574, 761, 914, 818, 924, 287, 12, 1479, 571, 18, 1405, 520, 955, 566, 699, 1508, 247, 56, 916, 11, 337, 1284, 217, 562, 662, 612, 1242, 1496, 1466, 109, 1442, 304, 1230, 881, 853, 1371, 1395, 1061, 1372, 1007, 1238, 1436, 618, 41, 1263, 634, 577, 1069, 1489, 1272, 353, 1123, 803, 1101, 199, 640, 1134, 946, 105, 726, 1297, 1292, 524, 698, 82, 266, 1472, 814, 626, 622, 174, 289, 1521, 1082, 582, 1041, 1383, 667, 743, 348, 461, 1464, 319, 114, 1071, 1480, 987, 1050, 817, 1046, 302, 489, 747, 610, 1085, 243, 61, 1145, 207, 492, 503, 542, 534, 1096, 1130, 1252, 1408, 1424, 923, 1160, 1092, 1274, 621, 1355, 229, 760, 458, 1019, 1332, 462, 900, 451, 1139, 1159, 1100, 732, 1027, 1295, 1515], 'conv3': [1447, 129, 2500, 2420, 2056, 2414, 2181, 564, 370, 746, 398, 646, 35, 1720, 922, 2523, 199, 419, 696, 1843, 1755, 1025, 2317, 305, 1424, 2193, 1794, 1993, 583, 472, 1926, 534, 640, 1380, 1394, 708, 68, 1292, 1304, 533, 1198, 837, 2331, 1958, 653, 1409, 2085, 162, 1830, 658, 1175, 2447, 2000, 964, 1558, 1280, 436, 1992, 2259, 2237, 888, 1746, 1953, 1637, 1051, 2070, 1506, 1141, 2, 2093, 732, 299, 2069, 1608, 33, 1666, 2390, 828, 2151, 2536, 72, 1003, 2342, 2179, 1887, 1550, 454, 1873, 2547, 1909, 2022, 1001, 1123, 1248, 2057, 810, 34, 1403, 2176, 1443, 1824, 2453, 421, 2185, 1753, 1938, 638, 796, 1385, 1650, 2094, 1045, 561, 507, 720, 1831, 146, 1536, 210, 1457, 1139, 1689, 2460, 2095, 1162, 1524, 302, 800, 1451, 1449, 669, 1783, 365, 1245, 1406, 2174, 1931, 509, 674, 2545, 1056, 425, 1153, 1776, 63, 2283, 935, 586, 2300, 1376, 2162, 2060, 2305, 437, 1555, 1567, 2168, 283, 1283, 1415, 838, 80, 1332, 2112, 481, 1659, 1049, 1638, 43, 1221, 291, 340, 2322, 2272, 404, 450, 575, 1789, 1809, 867, 2394, 1572, 1986, 2515, 1970, 122, 1886, 2462, 491, 1929, 2320, 557, 2227, 1160, 1266, 225, 1960, 1845, 1548, 336, 1157, 1781, 1439, 1074, 2187, 2197, 792, 2285, 2511, 687, 167, 1021, 632, 788, 1864, 1799, 1435, 1591, 1458, 2208, 267, 13, 1088, 2008, 2252, 2387, 2161, 304, 895, 1512, 465, 968, 1199, 1271, 306, 1742, 505, 130, 220, 926, 781, 662, 1554, 855, 1733, 698, 1839, 2062, 499, 1052, 2081, 258, 2308, 357, 468, 1062, 24, 521, 1645, 1375, 768, 2121, 411, 2042, 616, 96, 1890, 242, 2418, 1196, 276, 2028, 2378, 467, 2173, 1066, 772, 366, 2079, 761, 1874, 1446, 362, 284, 1699, 884, 1226, 252, 1054, 543, 597, 1108, 2527, 2310, 1966, 1325, 2456, 203, 27, 959, 2192, 962, 992, 2050, 1253, 1531, 1227, 2518, 1961, 192, 124, 259, 6, 1827, 944, 239, 2157, 885, 2167, 1527, 576, 706, 399, 2484, 1971, 555, 943, 82, 1194, 1694, 1029, 2130, 1633, 1267, 107, 2293, 2508, 2532, 2256, 2011, 316, 1112, 240, 151, 2436, 634, 1206, 1549, 54, 2242, 300, 2309, 689, 1782, 899, 1448, 2113, 2030, 2033, 1107, 1647, 1894, 1968, 1646, 1670, 1326, 2164, 937, 298, 2441, 1138, 1578, 1811, 1622, 980, 1055, 1351, 1837, 2284, 331, 2446, 939, 1697, 1908, 2018], 'conv4': [139, 2517, 93, 686, 1471, 530, 1657, 1640, 2523, 1571, 1262, 888, 2015, 402, 494, 400, 150, 825, 797, 1877, 653, 2265, 1504, 2188, 625, 1375, 1565, 2326, 1556, 768, 1489, 1250, 654, 1306, 439, 1016, 282, 782, 120, 233, 1597, 2078, 1134, 2507, 959, 1406, 15, 2415, 126, 1387, 2335, 864, 1676, 30, 1548, 172, 1100, 388, 1092, 1010, 1285, 1730, 455, 1987, 1215, 792, 826, 2035, 557, 1719, 607, 660, 1390, 2385, 1269, 1995, 1118, 424, 382, 1477, 412, 2404, 2118, 1152, 627, 798, 2315, 612, 1869, 1397, 2368, 74, 2285, 257, 941, 1167, 350, 616, 2223, 571, 2226, 2224, 838, 929, 2171, 1937, 591, 2334, 725, 735, 2531, 1854, 975, 1892, 4, 1908, 1852, 486, 1639, 1431, 1169, 889, 1140, 287, 2559, 731, 2101, 161, 1867, 1555, 1912, 606, 565, 1156, 1572, 1080, 2141, 605, 966, 883, 1424, 1700, 757, 296, 1138, 703, 674, 2186, 2271, 1707, 1806, 2038, 1518, 1698, 1587, 2558, 6, 145, 2121, 401, 1759, 5, 748, 851, 1338, 1075, 246, 2457, 63, 1583, 1223, 80, 581, 1989, 3, 770, 1524, 1831, 476, 24, 1362, 910, 2311, 460, 2006, 1013, 1513, 114, 1582, 2509, 859, 1230, 1819, 531, 1625, 347, 1957, 2207, 426, 1883, 1113, 1718, 539, 1415, 1359, 593, 520, 2328, 1241, 1946, 808, 383, 2211, 1723, 2485, 844, 863, 1441, 2492, 881, 1366, 1226, 214, 2337, 2268, 2261, 562, 1844, 2248, 361, 564, 1034, 270, 1512, 1323, 1795, 1836, 2149, 1272, 1015, 578, 2218, 1970, 1887, 381, 1645, 271, 743, 718, 1840, 1658, 990, 69, 2554, 2355, 789, 949, 922, 1263, 309, 2172, 1040, 2049, 1696, 1019, 1620, 1898, 1574, 2133, 1143, 40, 1847, 34, 2475, 1198, 2487, 2506, 1439, 54, 36, 2199, 2299, 1289, 2257, 1117, 2418, 1025, 1893, 499, 778, 364, 1654, 82, 945, 2524, 1713, 372, 239, 1545, 2264, 928, 1779, 1811, 704, 689, 2044, 470, 1981, 1172, 137, 2045, 1246, 1705, 1820, 2012, 2495, 1634, 1522, 2322, 1239, 746, 978, 992, 525, 26, 2425, 646, 1420, 1740, 2258, 2538, 832, 1526, 648, 1641, 284, 1660, 1931, 1445, 2246, 1787, 335, 2138, 777, 1558, 1868, 29, 895, 344, 33, 2053, 1460, 2276, 741, 1951, 1098, 2459, 790, 997, 1499, 722, 2455, 2518, 515, 1998, 1474, 2134, 874, 2510, 536, 2163, 1224, 1007, 2117, 664, 1686, 552, 411, 2184, 1914, 784, 2323, 1491, 1035, 712]}
activations_sum[k] len:  1536
activations_sum[k] 2560
activations_sum[k]:  [499, 507, 1677, 1339, 512, 1958, 653, 1443, 300, 2049]
topk_kernels len:  5
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3', 'conv4']
topk_kernels:  {'conv0': [25, 59, 5, 82, 30, 89, 16, 77, 33, 42, 70, 93, 45, 68, 64], 'conv1': [50, 56, 75, 118, 206, 144, 94, 317, 382, 336, 354, 140, 41, 264, 191, 326, 142, 233, 324, 92, 262, 127, 355, 104, 31, 221, 365, 137, 298, 200, 6, 195, 88, 130, 345, 352, 67, 234, 371, 39, 180, 219, 96, 14, 190, 62, 292, 134, 48, 113, 304, 121, 64, 99, 357, 117, 266, 373, 312], 'conv2': [937, 407, 636, 307, 700, 176, 1086, 195, 1385, 1171, 405, 1317, 1278, 921, 673, 427, 799, 1034, 1486, 854, 421, 311, 731, 616, 794, 1307, 35, 706, 1500, 544, 194, 1360, 450, 958, 598, 1291, 883, 66, 560, 1306, 710, 869, 14, 93, 932, 141, 548, 155, 312, 796, 361, 167, 37, 1534, 851, 490, 551, 1000, 1412, 445, 686, 1226, 838, 906, 1030, 100, 13, 812, 1363, 730, 687, 660, 1006, 139, 150, 268, 437, 1281, 889, 380, 856, 635, 77, 1430, 123, 1286, 1109, 346, 482, 1511, 67, 1359, 1386, 1056, 1294, 786, 1239, 683, 28, 499, 276, 95, 574, 761, 914, 818, 924, 287, 12, 1479, 571, 18, 1405, 520, 955, 566, 699, 1508, 247, 56, 916, 11, 337, 1284, 217, 562, 662, 612, 1242, 1496, 1466, 109, 1442, 304, 1230, 881, 853, 1371, 1395, 1061, 1372, 1007, 1238, 1436, 618, 41, 1263, 634, 577, 1069, 1489, 1272, 353, 1123, 803, 1101, 199, 640, 1134, 946, 105, 726, 1297, 1292, 524, 698, 82, 266, 1472, 814, 626, 622, 174, 289, 1521, 1082, 582, 1041, 1383, 667, 743, 348, 461, 1464, 319, 114, 1071, 1480, 987, 1050, 817, 1046, 302, 489, 747, 610, 1085, 243, 61, 1145, 207, 492, 503, 542, 534, 1096, 1130, 1252, 1408, 1424, 923, 1160, 1092, 1274, 621, 1355, 229, 760, 458, 1019, 1332, 462, 900, 451, 1139, 1159, 1100, 732, 1027, 1295, 1515], 'conv3': [499, 507, 1677, 1339, 512, 1958, 653, 1443, 300, 2049, 1447, 921, 1755, 340, 2315, 1827, 43, 1862, 1359, 652, 2494, 1938, 446, 2095, 2402, 1403, 2157, 505, 2354, 1127, 1303, 1567, 193, 2103, 1109, 784, 2025, 288, 1670, 243, 2379, 520, 1608, 40, 450, 2435, 1783, 2132, 508, 611, 1465, 1710, 481, 1618, 1843, 2100, 1061, 1125, 2115, 699, 1656, 2414, 1602, 1964, 1510, 2021, 421, 2292, 399, 2032, 1352, 1641, 818, 2221, 993, 484, 1500, 2185, 1926, 1834, 2482, 156, 303, 153, 2317, 299, 2067, 1762, 733, 359, 2154, 991, 1750, 2050, 792, 1378, 2268, 2138, 1341, 2145, 277, 2181, 378, 1520, 783, 1894, 2309, 2554, 1789, 878, 1616, 2244, 853, 2367, 328, 2249, 2247, 2405, 1559, 147, 455, 1370, 2218, 2087, 863, 2241, 2300, 2363, 1242, 1163, 624, 2245, 1548, 197, 1402, 757, 1599, 1715, 1422, 606, 2082, 8, 1646, 372, 2460, 1561, 1411, 2276, 1260, 983, 1985, 208, 287, 689, 1597, 883, 1929, 2000, 2170, 1326, 244, 1226, 356, 841, 726, 651, 367, 1425, 148, 1887, 2324, 780, 1275, 1699, 1366, 1063, 2291, 1658, 2081, 2261, 1998, 103, 2465, 724, 1205, 419, 2184, 1554, 36, 2497, 1659, 580, 469, 389, 6, 555, 1867, 1367, 825, 1482, 1312, 1598, 565, 1020, 195, 882, 2302, 1375, 1138, 377, 1327, 1319, 2448, 2007, 1236, 861, 1083, 2110, 2198, 1324, 1638, 1440, 2454, 914, 934, 1010, 534, 1495, 1347, 496, 1988, 114, 1776, 1435, 2349, 615, 1685, 388, 68, 1693, 1808, 471, 1780, 735, 2389, 1038, 1749, 1057, 816, 613, 1111, 549, 1810, 151, 1348, 29, 846, 1105, 339, 813, 296, 660, 1965, 1229, 2213, 1543, 251, 276, 1791, 1821, 1648, 501, 1647, 1744, 2196, 454, 1874, 2427, 2223, 1300, 2534, 1709, 2294, 1394, 1840, 716, 532, 871, 2415, 1187, 2250, 2111, 382, 1073, 42, 2271, 1, 138, 2559, 1816, 1238, 278, 1689, 2323, 2093, 2108, 1920, 940, 2352, 1077, 1050, 1883, 1705, 527, 971, 589, 854, 1837, 753, 533, 674, 2077, 939, 249, 497, 1788, 33, 2549, 1506, 1509, 353, 1266, 320, 700, 2143, 51, 540, 2232, 1943, 1088, 2112, 553, 1253, 2428, 34, 627, 1340, 482, 597, 1720, 2479, 1110, 1530, 1775, 2120, 2264, 468, 546, 231, 1008, 1448, 1660, 1845, 2325, 1759, 1584, 265, 1331, 729, 104, 2106, 472, 1029, 867, 1696, 1169, 1839, 1522, 511, 1065, 2543, 1211, 1264, 429, 926], 'conv4': [139, 2517, 93, 686, 1471, 530, 1657, 1640, 2523, 1571, 1262, 888, 2015, 402, 494, 400, 150, 825, 797, 1877, 653, 2265, 1504, 2188, 625, 1375, 1565, 2326, 1556, 768, 1489, 1250, 654, 1306, 439, 1016, 282, 782, 120, 233, 1597, 2078, 1134, 2507, 959, 1406, 15, 2415, 126, 1387, 2335, 864, 1676, 30, 1548, 172, 1100, 388, 1092, 1010, 1285, 1730, 455, 1987, 1215, 792, 826, 2035, 557, 1719, 607, 660, 1390, 2385, 1269, 1995, 1118, 424, 382, 1477, 412, 2404, 2118, 1152, 627, 798, 2315, 612, 1869, 1397, 2368, 74, 2285, 257, 941, 1167, 350, 616, 2223, 571, 2226, 2224, 838, 929, 2171, 1937, 591, 2334, 725, 735, 2531, 1854, 975, 1892, 4, 1908, 1852, 486, 1639, 1431, 1169, 889, 1140, 287, 2559, 731, 2101, 161, 1867, 1555, 1912, 606, 565, 1156, 1572, 1080, 2141, 605, 966, 883, 1424, 1700, 757, 296, 1138, 703, 674, 2186, 2271, 1707, 1806, 2038, 1518, 1698, 1587, 2558, 6, 145, 2121, 401, 1759, 5, 748, 851, 1338, 1075, 246, 2457, 63, 1583, 1223, 80, 581, 1989, 3, 770, 1524, 1831, 476, 24, 1362, 910, 2311, 460, 2006, 1013, 1513, 114, 1582, 2509, 859, 1230, 1819, 531, 1625, 347, 1957, 2207, 426, 1883, 1113, 1718, 539, 1415, 1359, 593, 520, 2328, 1241, 1946, 808, 383, 2211, 1723, 2485, 844, 863, 1441, 2492, 881, 1366, 1226, 214, 2337, 2268, 2261, 562, 1844, 2248, 361, 564, 1034, 270, 1512, 1323, 1795, 1836, 2149, 1272, 1015, 578, 2218, 1970, 1887, 381, 1645, 271, 743, 718, 1840, 1658, 990, 69, 2554, 2355, 789, 949, 922, 1263, 309, 2172, 1040, 2049, 1696, 1019, 1620, 1898, 1574, 2133, 1143, 40, 1847, 34, 2475, 1198, 2487, 2506, 1439, 54, 36, 2199, 2299, 1289, 2257, 1117, 2418, 1025, 1893, 499, 778, 364, 1654, 82, 945, 2524, 1713, 372, 239, 1545, 2264, 928, 1779, 1811, 704, 689, 2044, 470, 1981, 1172, 137, 2045, 1246, 1705, 1820, 2012, 2495, 1634, 1522, 2322, 1239, 746, 978, 992, 525, 26, 2425, 646, 1420, 1740, 2258, 2538, 832, 1526, 648, 1641, 284, 1660, 1931, 1445, 2246, 1787, 335, 2138, 777, 1558, 1868, 29, 895, 344, 33, 2053, 1460, 2276, 741, 1951, 1098, 2459, 790, 997, 1499, 722, 2455, 2518, 515, 1998, 1474, 2134, 874, 2510, 536, 2163, 1224, 1007, 2117, 664, 1686, 552, 411, 2184, 1914, 784, 2323, 1491, 1035, 712]}
activations_sum[k] len:  2560
activations_sum[k] 2560
activations_sum[k]:  [1067, 99, 1620, 633, 1644, 2356, 558, 929, 348, 881]
topk_kernels len:  5
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3', 'conv4']
topk_kernels:  {'conv0': [25, 59, 5, 82, 30, 89, 16, 77, 33, 42, 70, 93, 45, 68, 64], 'conv1': [50, 56, 75, 118, 206, 144, 94, 317, 382, 336, 354, 140, 41, 264, 191, 326, 142, 233, 324, 92, 262, 127, 355, 104, 31, 221, 365, 137, 298, 200, 6, 195, 88, 130, 345, 352, 67, 234, 371, 39, 180, 219, 96, 14, 190, 62, 292, 134, 48, 113, 304, 121, 64, 99, 357, 117, 266, 373, 312], 'conv2': [937, 407, 636, 307, 700, 176, 1086, 195, 1385, 1171, 405, 1317, 1278, 921, 673, 427, 799, 1034, 1486, 854, 421, 311, 731, 616, 794, 1307, 35, 706, 1500, 544, 194, 1360, 450, 958, 598, 1291, 883, 66, 560, 1306, 710, 869, 14, 93, 932, 141, 548, 155, 312, 796, 361, 167, 37, 1534, 851, 490, 551, 1000, 1412, 445, 686, 1226, 838, 906, 1030, 100, 13, 812, 1363, 730, 687, 660, 1006, 139, 150, 268, 437, 1281, 889, 380, 856, 635, 77, 1430, 123, 1286, 1109, 346, 482, 1511, 67, 1359, 1386, 1056, 1294, 786, 1239, 683, 28, 499, 276, 95, 574, 761, 914, 818, 924, 287, 12, 1479, 571, 18, 1405, 520, 955, 566, 699, 1508, 247, 56, 916, 11, 337, 1284, 217, 562, 662, 612, 1242, 1496, 1466, 109, 1442, 304, 1230, 881, 853, 1371, 1395, 1061, 1372, 1007, 1238, 1436, 618, 41, 1263, 634, 577, 1069, 1489, 1272, 353, 1123, 803, 1101, 199, 640, 1134, 946, 105, 726, 1297, 1292, 524, 698, 82, 266, 1472, 814, 626, 622, 174, 289, 1521, 1082, 582, 1041, 1383, 667, 743, 348, 461, 1464, 319, 114, 1071, 1480, 987, 1050, 817, 1046, 302, 489, 747, 610, 1085, 243, 61, 1145, 207, 492, 503, 542, 534, 1096, 1130, 1252, 1408, 1424, 923, 1160, 1092, 1274, 621, 1355, 229, 760, 458, 1019, 1332, 462, 900, 451, 1139, 1159, 1100, 732, 1027, 1295, 1515], 'conv3': [499, 507, 1677, 1339, 512, 1958, 653, 1443, 300, 2049, 1447, 921, 1755, 340, 2315, 1827, 43, 1862, 1359, 652, 2494, 1938, 446, 2095, 2402, 1403, 2157, 505, 2354, 1127, 1303, 1567, 193, 2103, 1109, 784, 2025, 288, 1670, 243, 2379, 520, 1608, 40, 450, 2435, 1783, 2132, 508, 611, 1465, 1710, 481, 1618, 1843, 2100, 1061, 1125, 2115, 699, 1656, 2414, 1602, 1964, 1510, 2021, 421, 2292, 399, 2032, 1352, 1641, 818, 2221, 993, 484, 1500, 2185, 1926, 1834, 2482, 156, 303, 153, 2317, 299, 2067, 1762, 733, 359, 2154, 991, 1750, 2050, 792, 1378, 2268, 2138, 1341, 2145, 277, 2181, 378, 1520, 783, 1894, 2309, 2554, 1789, 878, 1616, 2244, 853, 2367, 328, 2249, 2247, 2405, 1559, 147, 455, 1370, 2218, 2087, 863, 2241, 2300, 2363, 1242, 1163, 624, 2245, 1548, 197, 1402, 757, 1599, 1715, 1422, 606, 2082, 8, 1646, 372, 2460, 1561, 1411, 2276, 1260, 983, 1985, 208, 287, 689, 1597, 883, 1929, 2000, 2170, 1326, 244, 1226, 356, 841, 726, 651, 367, 1425, 148, 1887, 2324, 780, 1275, 1699, 1366, 1063, 2291, 1658, 2081, 2261, 1998, 103, 2465, 724, 1205, 419, 2184, 1554, 36, 2497, 1659, 580, 469, 389, 6, 555, 1867, 1367, 825, 1482, 1312, 1598, 565, 1020, 195, 882, 2302, 1375, 1138, 377, 1327, 1319, 2448, 2007, 1236, 861, 1083, 2110, 2198, 1324, 1638, 1440, 2454, 914, 934, 1010, 534, 1495, 1347, 496, 1988, 114, 1776, 1435, 2349, 615, 1685, 388, 68, 1693, 1808, 471, 1780, 735, 2389, 1038, 1749, 1057, 816, 613, 1111, 549, 1810, 151, 1348, 29, 846, 1105, 339, 813, 296, 660, 1965, 1229, 2213, 1543, 251, 276, 1791, 1821, 1648, 501, 1647, 1744, 2196, 454, 1874, 2427, 2223, 1300, 2534, 1709, 2294, 1394, 1840, 716, 532, 871, 2415, 1187, 2250, 2111, 382, 1073, 42, 2271, 1, 138, 2559, 1816, 1238, 278, 1689, 2323, 2093, 2108, 1920, 940, 2352, 1077, 1050, 1883, 1705, 527, 971, 589, 854, 1837, 753, 533, 674, 2077, 939, 249, 497, 1788, 33, 2549, 1506, 1509, 353, 1266, 320, 700, 2143, 51, 540, 2232, 1943, 1088, 2112, 553, 1253, 2428, 34, 627, 1340, 482, 597, 1720, 2479, 1110, 1530, 1775, 2120, 2264, 468, 546, 231, 1008, 1448, 1660, 1845, 2325, 1759, 1584, 265, 1331, 729, 104, 2106, 472, 1029, 867, 1696, 1169, 1839, 1522, 511, 1065, 2543, 1211, 1264, 429, 926], 'conv4': [1067, 99, 1620, 633, 1644, 2356, 558, 929, 348, 881, 1921, 468, 1706, 2285, 82, 966, 1975, 1678, 1173, 1741, 2388, 1928, 1658, 1148, 1254, 2471, 1451, 1261, 1444, 203, 2413, 1692, 2346, 484, 1521, 1727, 1656, 1795, 997, 1734, 1575, 675, 2543, 621, 2039, 670, 1622, 2209, 694, 1993, 1990, 1149, 2404, 969, 967, 893, 1660, 1766, 80, 2041, 2399, 862, 1585, 1797, 783, 1498, 2528, 1441, 1764, 808, 1416, 1517, 1539, 1364, 1047, 1957, 1667, 2258, 1859, 1867, 593, 1187, 1654, 1167, 2158, 960, 258, 615, 931, 366, 2359, 1131, 2207, 2353, 2211, 760, 768, 381, 1184, 1612, 806, 473, 1266, 139, 2438, 1710, 499, 1366, 2153, 272, 965, 1189, 772, 774, 339, 1369, 465, 38, 1191, 2524, 1320, 884, 1712, 2202, 1362, 107, 826, 778, 1809, 2463, 1996, 1548, 1659, 1288, 1483, 1580, 1138, 1221, 2015, 1719, 1030, 1472, 1313, 2437, 188, 1746, 2336, 1739, 1063, 1549, 779, 1323, 788, 1908, 1165, 535, 2031, 1485, 2242, 21, 2476, 1317, 1128, 1019, 309, 649, 641, 712, 1034, 891, 239, 691, 1284, 1757, 2449, 544, 2038, 688, 2491, 993, 1800, 1794, 2225, 1567, 264, 591, 795, 145, 845, 1354, 1813, 2395, 2551, 921, 1604, 830, 2021, 1109, 182, 2518, 512, 674, 1822, 1306, 1637, 2145, 2338, 2305, 390, 369, 1811, 368, 818, 592, 1534, 2421, 364, 1085, 2533, 575, 2176, 1326, 2435, 315, 486, 2530, 2482, 673, 704, 1708, 1135, 2522, 151, 13, 2212, 1183, 1437, 20, 2152, 2217, 722, 2268, 1310, 1418, 568, 976, 2537, 253, 412, 1699, 1904, 1657, 1246, 1263, 1738, 89, 1818, 1756, 734, 193, 91, 1798, 2068, 1296, 1791, 1316, 514, 1054, 786, 295, 1536, 1190, 2385, 1559, 432, 1732, 827, 1924, 2445, 2071, 1974, 518, 1415, 2286, 457, 2335, 520, 1429, 1929, 2237, 548, 1772, 2552, 708, 2251, 435, 547, 762, 479, 1569, 637, 1143, 1874, 1084, 183, 1445, 1101, 504, 2307, 586, 1496, 114, 2403, 690, 1302, 1035, 2321, 503, 823, 451, 2271, 243, 1040, 1123, 1962, 2093, 987, 3, 1278, 433, 1327, 2206, 2519, 1540, 903, 2165, 498, 952, 1265, 1652, 2029, 2327, 508, 1617, 331, 406, 721, 2008, 338, 2060, 2406, 113, 1801, 1392, 477, 1922, 2077, 1541, 81, 1271, 464, 607, 2133, 1344, 1473, 1058, 159, 227, 1698, 1610, 527, 1057, 1589, 1228, 2123, 652, 507, 2360, 2472, 598, 605, 1668, 152, 325, 1718]}
activations_sum[k] len:  2560
['conv0', 'conv1', 'conv2', 'conv3', 'conv4']
delta_weights INFO: 
NUM OF TRACKED COV LAYERS:  5
NUM OF TRACKED WEIGHTS CHANGES PER LAYER:  2
avg_deltas INFO:  <class 'dict'>
avg_deltas keys:  ['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight', 'blocks.4.layer.weight']
################################################
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 4.79e-02	time: 00:00:20	Acc_train 0.00	Acc_test 0.00	convergence: 2.31e+01	R1: 0	Info MB:0.000e+00/SB:0.000e+00/MW:3.069e-06/SW:1.594e-01/MR:2.406e+01/SR:2.593e+00/MeD:1.533e+00/MaD:2.296e+01/MW:0.708/MAW:0.292
|        0 |        1 |        2 |        3 |        4 |        5 |        6 |        7 |        8 |        9 |       10 |      11 |     12 |       13 |       14 |       15 |       16 |       17 |       18 |       19 |       20 |       21 |       22 |       23 |       24 |       25 |       26 |      27 |       28 |       29 |
|----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+--------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------|
|   0.0496 |   0.0475 |   0.0499 |   0.0489 |   0.0431 |   0.0441 |   0.0481 |   0.0496 |   0.0485 |   0.0445 |   0.0485 |   0.051 |   0.05 |   0.0509 |   0.0425 |   0.0483 |   0.0492 |   0.0486 |   0.0496 |   0.0492 |   0.0494 |   0.0493 |   0.0475 |   0.0473 |   0.0493 |   0.0488 |   0.0486 |   0.049 |   0.0493 |   0.0492 |
|  25.59   |  23.55   |  25.95   |  24.91   |  19.58   |  20.48   |  24.1    |  25.56   |  24.55   |  20.77   |  24.49   |  27.03  |  26    |  26.91   |  19.05   |  24.33   |  25.24   |  24.57   |  25.62   |  25.2    |  25.44   |  25.34   |  23.6    |  23.41   |  25.3    |  24.81   |  24.6    |  25.02  |  25.35   |  25.16   |
|   0.02   |   0.04   |   0      |   0.04   |   0.22   |   0.06   |   0.02   |   0.02   |   0.01   |   0.07   |   0.01   |   0.02  |   0.05 |   0.06   |   0.12   |   0      |   0.01   |   0.02   |   0.01   |   0      |   0.02   |   0.07   |   0.03   |   0.02   |   0.01   |   0      |   0.04   |   0.04  |   0.02   |   0.01   |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan    | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan    | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan    | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:-3.316e-05/SW:2.024e-01/MR:2.368e+01/SR:2.385e+00/MeD:1.682e+00/MaD:1.711e+01/MW:0.551/MAW:0.449
|        0 |        1 |        2 |        3 |        4 |        5 |        6 |        7 |        8 |        9 |       10 |      11 |       12 |       13 |       14 |       15 |       16 |       17 |       18 |     19 |       20 |      21 |       22 |       23 |       24 |       25 |     26 |       27 |       28 |       29 |
|----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+--------+----------+---------+----------+----------+----------+----------+--------+----------+----------+----------|
|   0.0442 |   0.0496 |   0.0486 |   0.0488 |   0.0482 |   0.0479 |   0.0514 |   0.0481 |   0.0319 |   0.0491 |   0.0473 |   0.046 |   0.0488 |   0.0497 |   0.0502 |   0.0496 |   0.0496 |   0.0487 |   0.0477 |   0.05 |   0.0488 |   0.046 |   0.0487 |   0.0489 |   0.0478 |   0.0438 |   0.05 |   0.0487 |   0.0511 |   0.0477 |
|  20.54   |  25.56   |  24.62   |  24.83   |  24.2    |  23.9    |  27.42   |  24.17   |  11.17   |  25.15   |  23.4    |  22.12  |  24.82   |  25.68   |  26.19   |  25.64   |  25.6    |  24.75   |  23.75   |  26.05 |  24.85   |  22.16  |  24.74   |  24.88   |  23.82   |  20.2    |  25.95 |  24.75   |  27.08   |  23.79   |
|   0.03   |   0.03   |   0.02   |   0.06   |   0.01   |   0.02   |   0.08   |   0.01   |   0.19   |   0      |   0.02   |   0.03  |   0.02   |   0.02   |   0.01   |   0.03   |   0.05   |   0.03   |   0.01   |   0    |   0      |   0.06  |   0      |   0.03   |   0.04   |   0.03   |   0.05 |   0.01   |   0.03   |   0.02   |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan    | nan      | nan     | nan      | nan      | nan      | nan      | nan    | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan    | nan      | nan     | nan      | nan      | nan      | nan      | nan    | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan    | nan      | nan     | nan      | nan      | nan      | nan      | nan    | nan      | nan      | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:7.920e-05/SW:4.099e-01/MR:2.400e+01/SR:2.177e+00/MeD:1.682e+00/MaD:1.012e+01/MW:0.499/MAW:0.501
|        0 |        1 |        2 |        3 |        4 |        5 |        6 |        7 |        8 |        9 |       10 |       11 |       12 |       13 |       14 |       15 |       16 |       17 |       18 |       19 |       20 |       21 |       22 |       23 |       24 |      25 |       26 |       27 |       28 |       29 |
|----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------|
|   0.0493 |   0.0485 |   0.0481 |   0.0462 |   0.0454 |   0.0481 |   0.0498 |   0.0439 |   0.0492 |   0.0492 |   0.0482 |   0.0484 |   0.0501 |   0.0416 |   0.0511 |   0.0478 |   0.0453 |   0.0494 |   0.0469 |   0.0495 |   0.0411 |   0.0494 |   0.0486 |   0.0483 |   0.0476 |   0.049 |   0.0492 |   0.0465 |   0.0427 |   0.0482 |
|  25.28   |  24.48   |  24.13   |  22.37   |  21.64   |  24.13   |  25.78   |  20.27   |  25.17   |  25.19   |  24.24   |  24.46   |  26.09   |  18.28   |  27.09   |  23.84   |  21.48   |  25.43   |  22.95   |  25.54   |  17.88   |  25.36   |  24.59   |  24.29   |  23.66   |  24.98  |  25.21   |  22.63   |  19.24   |  24.28   |
|   0      |   0.01   |   0.11   |   0.09   |   0.08   |   0.04   |   0.01   |   0.05   |   0.06   |   0      |   0.05   |   0.12   |   0.12   |   0.14   |   0.29   |   0.02   |   0.07   |   0.01   |   0.14   |   0.01   |   0.08   |   0.01   |   0.33   |   0.02   |   0.02   |   0.04  |   0.03   |   0.05   |   0.15   |   0.01   |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:-3.219e-03/SW:7.673e-01/MR:2.242e+01/SR:2.438e+00/MeD:1.953e+00/MaD:8.093e+00/MW:0.742/MAW:0.258
|        0 |        1 |        2 |       3 |        4 |        5 |        6 |        7 |        8 |        9 |       10 |       11 |       12 |       13 |       14 |       15 |       16 |       17 |       18 |       19 |       20 |       21 |       22 |       23 |       24 |       25 |       26 |       27 |       28 |       29 |
|----------+----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------|
|   0.0236 |   0.0244 |   0.0242 |   0.024 |   0.0191 |   0.0216 |   0.0241 |   0.0242 |   0.0242 |   0.0228 |   0.0248 |   0.0237 |   0.0242 |   0.0239 |   0.0242 |   0.0239 |   0.0235 |   0.0216 |   0.0231 |   0.0229 |   0.0228 |   0.0248 |   0.0233 |   0.0229 |   0.0235 |   0.0233 |   0.0226 |   0.0234 |   0.0225 |   0.0248 |
|  23.33   |  24.87   |  24.38   |  24.12  |  15.66   |  19.67   |  24.22   |  24.39   |  24.42   |  21.88   |  25.56   |  23.42   |  24.51   |  23.85   |  24.37   |  23.86   |  23.05   |  19.69   |  22.42   |  21.93   |  21.74   |  25.65   |  22.75   |  21.91   |  23.08   |  22.66   |  21.52   |  22.93   |  21.32   |  25.58   |
|   0.18   |   0.04   |   0.01   |   0.03  |   0.43   |   0.19   |   0.2    |   0.16   |   0.06   |   0.16   |   0.01   |   0.11   |   0.09   |   0.23   |   0.01   |   0.07   |   0.06   |   0.9    |   0.15   |   0.27   |   0.25   |   0.02   |   0.25   |   0.27   |   0.22   |   0.25   |   0.56   |   0.16   |   0.13   |   0.11   |
| nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:-4.039e-03/SW:9.437e-01/MR:8.029e+00/SR:1.533e+00/MeD:1.261e+00/MaD:4.197e+00/MW:0.546/MAW:0.454
|       0 |       1 |       2 |       3 |       4 |       5 |       6 |      7 |       8 |       9 |      10 |      11 |      12 |      13 |      14 |      15 |     16 |      17 |      18 |     19 |      20 |      21 |      22 |      23 |      24 |      25 |      26 |      27 |      28 |      29 |
|---------+---------+---------+---------+---------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+--------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------|
|   0.187 |   0.248 |   0.238 |   0.233 |   0.195 |   0.192 |   0.197 |   0.25 |   0.193 |   0.268 |   0.189 |   0.198 |   0.215 |   0.225 |   0.196 |   0.235 |   0.18 |   0.204 |   0.217 |   0.21 |   0.229 |   0.206 |   0.221 |   0.249 |   0.237 |   0.213 |   0.212 |   0.215 |   0.201 |   0.251 |
|   6.47  |  10.6   |   9.88  |   9.46  |   6.95  |   6.78  |   7.05  |  10.8  |   6.84  |  12.23  |   6.55  |   7.1   |   8.21  |   8.92  |   7.01  |   9.66  |   6.05 |   7.48  |   8.37  |   7.88 |   9.16  |   7.66  |   8.62  |  10.67  |   9.75  |   8.06  |   8.04  |   8.2   |   7.28  |  10.86  |
|   0.41  |   0.33  |   0.41  |   0.27  |   1.28  |   6.95  |   0.38  |   0.23 |   0.72  |   0.26  |   0.48  |   0.47  |   0.28  |   0.46  |   0.95  |   0.35  |   4.72 |   0.5   |   0.37  |   0.49 |   0.42  |   0.93  |   0.28  |   0.36  |   0.37  |  17.81  |   0.25  |   0.16  |   0.48  |   0.38  |
| nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     |
| nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     |
| nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c/models
SAVED HEADS THRESHOLD:  0.89
SAVED model.topk_kernels {'conv0': [25, 59, 5, 82, 30, 89, 16, 77, 33, 42, 70, 93, 45, 68, 64], 'conv1': [50, 56, 75, 118, 206, 144, 94, 317, 382, 336, 354, 140, 41, 264, 191, 326, 142, 233, 324, 92, 262, 127, 355, 104, 31, 221, 365, 137, 298, 200, 6, 195, 88, 130, 345, 352, 67, 234, 371, 39, 180, 219, 96, 14, 190, 62, 292, 134, 48, 113, 304, 121, 64, 99, 357, 117, 266, 373, 312], 'conv2': [937, 407, 636, 307, 700, 176, 1086, 195, 1385, 1171, 405, 1317, 1278, 921, 673, 427, 799, 1034, 1486, 854, 421, 311, 731, 616, 794, 1307, 35, 706, 1500, 544, 194, 1360, 450, 958, 598, 1291, 883, 66, 560, 1306, 710, 869, 14, 93, 932, 141, 548, 155, 312, 796, 361, 167, 37, 1534, 851, 490, 551, 1000, 1412, 445, 686, 1226, 838, 906, 1030, 100, 13, 812, 1363, 730, 687, 660, 1006, 139, 150, 268, 437, 1281, 889, 380, 856, 635, 77, 1430, 123, 1286, 1109, 346, 482, 1511, 67, 1359, 1386, 1056, 1294, 786, 1239, 683, 28, 499, 276, 95, 574, 761, 914, 818, 924, 287, 12, 1479, 571, 18, 1405, 520, 955, 566, 699, 1508, 247, 56, 916, 11, 337, 1284, 217, 562, 662, 612, 1242, 1496, 1466, 109, 1442, 304, 1230, 881, 853, 1371, 1395, 1061, 1372, 1007, 1238, 1436, 618, 41, 1263, 634, 577, 1069, 1489, 1272, 353, 1123, 803, 1101, 199, 640, 1134, 946, 105, 726, 1297, 1292, 524, 698, 82, 266, 1472, 814, 626, 622, 174, 289, 1521, 1082, 582, 1041, 1383, 667, 743, 348, 461, 1464, 319, 114, 1071, 1480, 987, 1050, 817, 1046, 302, 489, 747, 610, 1085, 243, 61, 1145, 207, 492, 503, 542, 534, 1096, 1130, 1252, 1408, 1424, 923, 1160, 1092, 1274, 621, 1355, 229, 760, 458, 1019, 1332, 462, 900, 451, 1139, 1159, 1100, 732, 1027, 1295, 1515], 'conv3': [499, 507, 1677, 1339, 512, 1958, 653, 1443, 300, 2049, 1447, 921, 1755, 340, 2315, 1827, 43, 1862, 1359, 652, 2494, 1938, 446, 2095, 2402, 1403, 2157, 505, 2354, 1127, 1303, 1567, 193, 2103, 1109, 784, 2025, 288, 1670, 243, 2379, 520, 1608, 40, 450, 2435, 1783, 2132, 508, 611, 1465, 1710, 481, 1618, 1843, 2100, 1061, 1125, 2115, 699, 1656, 2414, 1602, 1964, 1510, 2021, 421, 2292, 399, 2032, 1352, 1641, 818, 2221, 993, 484, 1500, 2185, 1926, 1834, 2482, 156, 303, 153, 2317, 299, 2067, 1762, 733, 359, 2154, 991, 1750, 2050, 792, 1378, 2268, 2138, 1341, 2145, 277, 2181, 378, 1520, 783, 1894, 2309, 2554, 1789, 878, 1616, 2244, 853, 2367, 328, 2249, 2247, 2405, 1559, 147, 455, 1370, 2218, 2087, 863, 2241, 2300, 2363, 1242, 1163, 624, 2245, 1548, 197, 1402, 757, 1599, 1715, 1422, 606, 2082, 8, 1646, 372, 2460, 1561, 1411, 2276, 1260, 983, 1985, 208, 287, 689, 1597, 883, 1929, 2000, 2170, 1326, 244, 1226, 356, 841, 726, 651, 367, 1425, 148, 1887, 2324, 780, 1275, 1699, 1366, 1063, 2291, 1658, 2081, 2261, 1998, 103, 2465, 724, 1205, 419, 2184, 1554, 36, 2497, 1659, 580, 469, 389, 6, 555, 1867, 1367, 825, 1482, 1312, 1598, 565, 1020, 195, 882, 2302, 1375, 1138, 377, 1327, 1319, 2448, 2007, 1236, 861, 1083, 2110, 2198, 1324, 1638, 1440, 2454, 914, 934, 1010, 534, 1495, 1347, 496, 1988, 114, 1776, 1435, 2349, 615, 1685, 388, 68, 1693, 1808, 471, 1780, 735, 2389, 1038, 1749, 1057, 816, 613, 1111, 549, 1810, 151, 1348, 29, 846, 1105, 339, 813, 296, 660, 1965, 1229, 2213, 1543, 251, 276, 1791, 1821, 1648, 501, 1647, 1744, 2196, 454, 1874, 2427, 2223, 1300, 2534, 1709, 2294, 1394, 1840, 716, 532, 871, 2415, 1187, 2250, 2111, 382, 1073, 42, 2271, 1, 138, 2559, 1816, 1238, 278, 1689, 2323, 2093, 2108, 1920, 940, 2352, 1077, 1050, 1883, 1705, 527, 971, 589, 854, 1837, 753, 533, 674, 2077, 939, 249, 497, 1788, 33, 2549, 1506, 1509, 353, 1266, 320, 700, 2143, 51, 540, 2232, 1943, 1088, 2112, 553, 1253, 2428, 34, 627, 1340, 482, 597, 1720, 2479, 1110, 1530, 1775, 2120, 2264, 468, 546, 231, 1008, 1448, 1660, 1845, 2325, 1759, 1584, 265, 1331, 729, 104, 2106, 472, 1029, 867, 1696, 1169, 1839, 1522, 511, 1065, 2543, 1211, 1264, 429, 926], 'conv4': [1067, 99, 1620, 633, 1644, 2356, 558, 929, 348, 881, 1921, 468, 1706, 2285, 82, 966, 1975, 1678, 1173, 1741, 2388, 1928, 1658, 1148, 1254, 2471, 1451, 1261, 1444, 203, 2413, 1692, 2346, 484, 1521, 1727, 1656, 1795, 997, 1734, 1575, 675, 2543, 621, 2039, 670, 1622, 2209, 694, 1993, 1990, 1149, 2404, 969, 967, 893, 1660, 1766, 80, 2041, 2399, 862, 1585, 1797, 783, 1498, 2528, 1441, 1764, 808, 1416, 1517, 1539, 1364, 1047, 1957, 1667, 2258, 1859, 1867, 593, 1187, 1654, 1167, 2158, 960, 258, 615, 931, 366, 2359, 1131, 2207, 2353, 2211, 760, 768, 381, 1184, 1612, 806, 473, 1266, 139, 2438, 1710, 499, 1366, 2153, 272, 965, 1189, 772, 774, 339, 1369, 465, 38, 1191, 2524, 1320, 884, 1712, 2202, 1362, 107, 826, 778, 1809, 2463, 1996, 1548, 1659, 1288, 1483, 1580, 1138, 1221, 2015, 1719, 1030, 1472, 1313, 2437, 188, 1746, 2336, 1739, 1063, 1549, 779, 1323, 788, 1908, 1165, 535, 2031, 1485, 2242, 21, 2476, 1317, 1128, 1019, 309, 649, 641, 712, 1034, 891, 239, 691, 1284, 1757, 2449, 544, 2038, 688, 2491, 993, 1800, 1794, 2225, 1567, 264, 591, 795, 145, 845, 1354, 1813, 2395, 2551, 921, 1604, 830, 2021, 1109, 182, 2518, 512, 674, 1822, 1306, 1637, 2145, 2338, 2305, 390, 369, 1811, 368, 818, 592, 1534, 2421, 364, 1085, 2533, 575, 2176, 1326, 2435, 315, 486, 2530, 2482, 673, 704, 1708, 1135, 2522, 151, 13, 2212, 1183, 1437, 20, 2152, 2217, 722, 2268, 1310, 1418, 568, 976, 2537, 253, 412, 1699, 1904, 1657, 1246, 1263, 1738, 89, 1818, 1756, 734, 193, 91, 1798, 2068, 1296, 1791, 1316, 514, 1054, 786, 295, 1536, 1190, 2385, 1559, 432, 1732, 827, 1924, 2445, 2071, 1974, 518, 1415, 2286, 457, 2335, 520, 1429, 1929, 2237, 548, 1772, 2552, 708, 2251, 435, 547, 762, 479, 1569, 637, 1143, 1874, 1084, 183, 1445, 1101, 504, 2307, 586, 1496, 114, 2403, 690, 1302, 1035, 2321, 503, 823, 451, 2271, 243, 1040, 1123, 1962, 2093, 987, 3, 1278, 433, 1327, 2206, 2519, 1540, 903, 2165, 498, 952, 1265, 1652, 2029, 2327, 508, 1617, 331, 406, 721, 2008, 338, 2060, 2406, 113, 1801, 1392, 477, 1922, 2077, 1541, 81, 1271, 464, 607, 2133, 1344, 1473, 1058, 159, 227, 1698, 1610, 527, 1057, 1589, 1228, 2123, 652, 507, 2360, 2472, 598, 605, 1668, 152, 325, 1718]}
SAVED model.heads_thresh 0.89
CONFIG MODE:  supervised

 ********** Supervised learning of blocks [5] **********
SAVING FOLDER FOR SUP:  C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([49, 33, 72, 51, 71, 92, 15, 14, 23,  0, 71, 75, 81, 69, 40, 43, 92, 97,
        70, 53], device='cuda:0')
TARGETS AFTER SUB:  tensor([16, 41, 41, 16, 16, 16, 16, 41, 41, 16, 16, 41, 41, 41, 41, 41, 16, 16,
        41, 16])
[16, 41]
TARGETS AFTER CLEANER:  tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0])
200
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 200
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               None
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([19, 29,  0, 11,  1, 86, 90, 28, 23, 31, 39, 96, 82, 17, 71, 39,  8, 97,
        80, 71], device='cuda:0')
TARGETS AFTER SUB:  tensor([41, 41, 41, 41, 16, 16, 16, 41, 41, 16, 41, 41, 41, 16, 41, 41, 41, 41,
        16, 41])
[16, 41]
TARGETS AFTER CLEANER:  tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1])
1000
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 1000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               None
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([19, 29,  0, 11,  1, 86, 90, 28, 23, 31, 39, 96, 82, 17, 71, 39,  8, 97,
        80, 71], device='cuda:0')
TARGETS AFTER SUB:  tensor([41, 41, 41, 41, 16, 16, 16, 41, 41, 16, 41, 41, 41, 16, 41, 41, 41, 41,
        16, 41])
[16, 41]
TARGETS AFTER CLEANER:  tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1])
1000
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 1000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
------------------------
INDICES:  1000
IMAGE SIZE: torch.Size([64, 3, 32, 32])
num_blocks:  6
Epoch: [1/50]	lr: 1.00e-03	time: 00:00:32	Loss_train 0.07975	Acc_train 71.00	/	Loss_test 0.01264	Acc_test 79.00
BLOCKS IN SUP:  [5]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c/models
SAVED HEADS THRESHOLD:  0.89
SAVED model.topk_kernels {'conv0': [25, 59, 5, 82, 30, 89, 16, 77, 33, 42, 70, 93, 45, 68, 64], 'conv1': [50, 56, 75, 118, 206, 144, 94, 317, 382, 336, 354, 140, 41, 264, 191, 326, 142, 233, 324, 92, 262, 127, 355, 104, 31, 221, 365, 137, 298, 200, 6, 195, 88, 130, 345, 352, 67, 234, 371, 39, 180, 219, 96, 14, 190, 62, 292, 134, 48, 113, 304, 121, 64, 99, 357, 117, 266, 373, 312], 'conv2': [937, 407, 636, 307, 700, 176, 1086, 195, 1385, 1171, 405, 1317, 1278, 921, 673, 427, 799, 1034, 1486, 854, 421, 311, 731, 616, 794, 1307, 35, 706, 1500, 544, 194, 1360, 450, 958, 598, 1291, 883, 66, 560, 1306, 710, 869, 14, 93, 932, 141, 548, 155, 312, 796, 361, 167, 37, 1534, 851, 490, 551, 1000, 1412, 445, 686, 1226, 838, 906, 1030, 100, 13, 812, 1363, 730, 687, 660, 1006, 139, 150, 268, 437, 1281, 889, 380, 856, 635, 77, 1430, 123, 1286, 1109, 346, 482, 1511, 67, 1359, 1386, 1056, 1294, 786, 1239, 683, 28, 499, 276, 95, 574, 761, 914, 818, 924, 287, 12, 1479, 571, 18, 1405, 520, 955, 566, 699, 1508, 247, 56, 916, 11, 337, 1284, 217, 562, 662, 612, 1242, 1496, 1466, 109, 1442, 304, 1230, 881, 853, 1371, 1395, 1061, 1372, 1007, 1238, 1436, 618, 41, 1263, 634, 577, 1069, 1489, 1272, 353, 1123, 803, 1101, 199, 640, 1134, 946, 105, 726, 1297, 1292, 524, 698, 82, 266, 1472, 814, 626, 622, 174, 289, 1521, 1082, 582, 1041, 1383, 667, 743, 348, 461, 1464, 319, 114, 1071, 1480, 987, 1050, 817, 1046, 302, 489, 747, 610, 1085, 243, 61, 1145, 207, 492, 503, 542, 534, 1096, 1130, 1252, 1408, 1424, 923, 1160, 1092, 1274, 621, 1355, 229, 760, 458, 1019, 1332, 462, 900, 451, 1139, 1159, 1100, 732, 1027, 1295, 1515], 'conv3': [499, 507, 1677, 1339, 512, 1958, 653, 1443, 300, 2049, 1447, 921, 1755, 340, 2315, 1827, 43, 1862, 1359, 652, 2494, 1938, 446, 2095, 2402, 1403, 2157, 505, 2354, 1127, 1303, 1567, 193, 2103, 1109, 784, 2025, 288, 1670, 243, 2379, 520, 1608, 40, 450, 2435, 1783, 2132, 508, 611, 1465, 1710, 481, 1618, 1843, 2100, 1061, 1125, 2115, 699, 1656, 2414, 1602, 1964, 1510, 2021, 421, 2292, 399, 2032, 1352, 1641, 818, 2221, 993, 484, 1500, 2185, 1926, 1834, 2482, 156, 303, 153, 2317, 299, 2067, 1762, 733, 359, 2154, 991, 1750, 2050, 792, 1378, 2268, 2138, 1341, 2145, 277, 2181, 378, 1520, 783, 1894, 2309, 2554, 1789, 878, 1616, 2244, 853, 2367, 328, 2249, 2247, 2405, 1559, 147, 455, 1370, 2218, 2087, 863, 2241, 2300, 2363, 1242, 1163, 624, 2245, 1548, 197, 1402, 757, 1599, 1715, 1422, 606, 2082, 8, 1646, 372, 2460, 1561, 1411, 2276, 1260, 983, 1985, 208, 287, 689, 1597, 883, 1929, 2000, 2170, 1326, 244, 1226, 356, 841, 726, 651, 367, 1425, 148, 1887, 2324, 780, 1275, 1699, 1366, 1063, 2291, 1658, 2081, 2261, 1998, 103, 2465, 724, 1205, 419, 2184, 1554, 36, 2497, 1659, 580, 469, 389, 6, 555, 1867, 1367, 825, 1482, 1312, 1598, 565, 1020, 195, 882, 2302, 1375, 1138, 377, 1327, 1319, 2448, 2007, 1236, 861, 1083, 2110, 2198, 1324, 1638, 1440, 2454, 914, 934, 1010, 534, 1495, 1347, 496, 1988, 114, 1776, 1435, 2349, 615, 1685, 388, 68, 1693, 1808, 471, 1780, 735, 2389, 1038, 1749, 1057, 816, 613, 1111, 549, 1810, 151, 1348, 29, 846, 1105, 339, 813, 296, 660, 1965, 1229, 2213, 1543, 251, 276, 1791, 1821, 1648, 501, 1647, 1744, 2196, 454, 1874, 2427, 2223, 1300, 2534, 1709, 2294, 1394, 1840, 716, 532, 871, 2415, 1187, 2250, 2111, 382, 1073, 42, 2271, 1, 138, 2559, 1816, 1238, 278, 1689, 2323, 2093, 2108, 1920, 940, 2352, 1077, 1050, 1883, 1705, 527, 971, 589, 854, 1837, 753, 533, 674, 2077, 939, 249, 497, 1788, 33, 2549, 1506, 1509, 353, 1266, 320, 700, 2143, 51, 540, 2232, 1943, 1088, 2112, 553, 1253, 2428, 34, 627, 1340, 482, 597, 1720, 2479, 1110, 1530, 1775, 2120, 2264, 468, 546, 231, 1008, 1448, 1660, 1845, 2325, 1759, 1584, 265, 1331, 729, 104, 2106, 472, 1029, 867, 1696, 1169, 1839, 1522, 511, 1065, 2543, 1211, 1264, 429, 926], 'conv4': [1067, 99, 1620, 633, 1644, 2356, 558, 929, 348, 881, 1921, 468, 1706, 2285, 82, 966, 1975, 1678, 1173, 1741, 2388, 1928, 1658, 1148, 1254, 2471, 1451, 1261, 1444, 203, 2413, 1692, 2346, 484, 1521, 1727, 1656, 1795, 997, 1734, 1575, 675, 2543, 621, 2039, 670, 1622, 2209, 694, 1993, 1990, 1149, 2404, 969, 967, 893, 1660, 1766, 80, 2041, 2399, 862, 1585, 1797, 783, 1498, 2528, 1441, 1764, 808, 1416, 1517, 1539, 1364, 1047, 1957, 1667, 2258, 1859, 1867, 593, 1187, 1654, 1167, 2158, 960, 258, 615, 931, 366, 2359, 1131, 2207, 2353, 2211, 760, 768, 381, 1184, 1612, 806, 473, 1266, 139, 2438, 1710, 499, 1366, 2153, 272, 965, 1189, 772, 774, 339, 1369, 465, 38, 1191, 2524, 1320, 884, 1712, 2202, 1362, 107, 826, 778, 1809, 2463, 1996, 1548, 1659, 1288, 1483, 1580, 1138, 1221, 2015, 1719, 1030, 1472, 1313, 2437, 188, 1746, 2336, 1739, 1063, 1549, 779, 1323, 788, 1908, 1165, 535, 2031, 1485, 2242, 21, 2476, 1317, 1128, 1019, 309, 649, 641, 712, 1034, 891, 239, 691, 1284, 1757, 2449, 544, 2038, 688, 2491, 993, 1800, 1794, 2225, 1567, 264, 591, 795, 145, 845, 1354, 1813, 2395, 2551, 921, 1604, 830, 2021, 1109, 182, 2518, 512, 674, 1822, 1306, 1637, 2145, 2338, 2305, 390, 369, 1811, 368, 818, 592, 1534, 2421, 364, 1085, 2533, 575, 2176, 1326, 2435, 315, 486, 2530, 2482, 673, 704, 1708, 1135, 2522, 151, 13, 2212, 1183, 1437, 20, 2152, 2217, 722, 2268, 1310, 1418, 568, 976, 2537, 253, 412, 1699, 1904, 1657, 1246, 1263, 1738, 89, 1818, 1756, 734, 193, 91, 1798, 2068, 1296, 1791, 1316, 514, 1054, 786, 295, 1536, 1190, 2385, 1559, 432, 1732, 827, 1924, 2445, 2071, 1974, 518, 1415, 2286, 457, 2335, 520, 1429, 1929, 2237, 548, 1772, 2552, 708, 2251, 435, 547, 762, 479, 1569, 637, 1143, 1874, 1084, 183, 1445, 1101, 504, 2307, 586, 1496, 114, 2403, 690, 1302, 1035, 2321, 503, 823, 451, 2271, 243, 1040, 1123, 1962, 2093, 987, 3, 1278, 433, 1327, 2206, 2519, 1540, 903, 2165, 498, 952, 1265, 1652, 2029, 2327, 508, 1617, 331, 406, 721, 2008, 338, 2060, 2406, 113, 1801, 1392, 477, 1922, 2077, 1541, 81, 1271, 464, 607, 2133, 1344, 1473, 1058, 159, 227, 1698, 1610, 527, 1057, 1589, 1228, 2123, 652, 507, 2360, 2472, 598, 605, 1668, 152, 325, 1718]}
SAVED model.heads_thresh 0.89
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
Epoch: [10/50]	lr: 1.00e-03	time: 00:00:33	Loss_train 0.02296	Acc_train 87.49	/	Loss_test 0.00466	Acc_test 91.00
BLOCKS IN SUP:  [5]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c/models
SAVED HEADS THRESHOLD:  0.89
SAVED model.topk_kernels {'conv0': [25, 59, 5, 82, 30, 89, 16, 77, 33, 42, 70, 93, 45, 68, 64], 'conv1': [50, 56, 75, 118, 206, 144, 94, 317, 382, 336, 354, 140, 41, 264, 191, 326, 142, 233, 324, 92, 262, 127, 355, 104, 31, 221, 365, 137, 298, 200, 6, 195, 88, 130, 345, 352, 67, 234, 371, 39, 180, 219, 96, 14, 190, 62, 292, 134, 48, 113, 304, 121, 64, 99, 357, 117, 266, 373, 312], 'conv2': [937, 407, 636, 307, 700, 176, 1086, 195, 1385, 1171, 405, 1317, 1278, 921, 673, 427, 799, 1034, 1486, 854, 421, 311, 731, 616, 794, 1307, 35, 706, 1500, 544, 194, 1360, 450, 958, 598, 1291, 883, 66, 560, 1306, 710, 869, 14, 93, 932, 141, 548, 155, 312, 796, 361, 167, 37, 1534, 851, 490, 551, 1000, 1412, 445, 686, 1226, 838, 906, 1030, 100, 13, 812, 1363, 730, 687, 660, 1006, 139, 150, 268, 437, 1281, 889, 380, 856, 635, 77, 1430, 123, 1286, 1109, 346, 482, 1511, 67, 1359, 1386, 1056, 1294, 786, 1239, 683, 28, 499, 276, 95, 574, 761, 914, 818, 924, 287, 12, 1479, 571, 18, 1405, 520, 955, 566, 699, 1508, 247, 56, 916, 11, 337, 1284, 217, 562, 662, 612, 1242, 1496, 1466, 109, 1442, 304, 1230, 881, 853, 1371, 1395, 1061, 1372, 1007, 1238, 1436, 618, 41, 1263, 634, 577, 1069, 1489, 1272, 353, 1123, 803, 1101, 199, 640, 1134, 946, 105, 726, 1297, 1292, 524, 698, 82, 266, 1472, 814, 626, 622, 174, 289, 1521, 1082, 582, 1041, 1383, 667, 743, 348, 461, 1464, 319, 114, 1071, 1480, 987, 1050, 817, 1046, 302, 489, 747, 610, 1085, 243, 61, 1145, 207, 492, 503, 542, 534, 1096, 1130, 1252, 1408, 1424, 923, 1160, 1092, 1274, 621, 1355, 229, 760, 458, 1019, 1332, 462, 900, 451, 1139, 1159, 1100, 732, 1027, 1295, 1515], 'conv3': [499, 507, 1677, 1339, 512, 1958, 653, 1443, 300, 2049, 1447, 921, 1755, 340, 2315, 1827, 43, 1862, 1359, 652, 2494, 1938, 446, 2095, 2402, 1403, 2157, 505, 2354, 1127, 1303, 1567, 193, 2103, 1109, 784, 2025, 288, 1670, 243, 2379, 520, 1608, 40, 450, 2435, 1783, 2132, 508, 611, 1465, 1710, 481, 1618, 1843, 2100, 1061, 1125, 2115, 699, 1656, 2414, 1602, 1964, 1510, 2021, 421, 2292, 399, 2032, 1352, 1641, 818, 2221, 993, 484, 1500, 2185, 1926, 1834, 2482, 156, 303, 153, 2317, 299, 2067, 1762, 733, 359, 2154, 991, 1750, 2050, 792, 1378, 2268, 2138, 1341, 2145, 277, 2181, 378, 1520, 783, 1894, 2309, 2554, 1789, 878, 1616, 2244, 853, 2367, 328, 2249, 2247, 2405, 1559, 147, 455, 1370, 2218, 2087, 863, 2241, 2300, 2363, 1242, 1163, 624, 2245, 1548, 197, 1402, 757, 1599, 1715, 1422, 606, 2082, 8, 1646, 372, 2460, 1561, 1411, 2276, 1260, 983, 1985, 208, 287, 689, 1597, 883, 1929, 2000, 2170, 1326, 244, 1226, 356, 841, 726, 651, 367, 1425, 148, 1887, 2324, 780, 1275, 1699, 1366, 1063, 2291, 1658, 2081, 2261, 1998, 103, 2465, 724, 1205, 419, 2184, 1554, 36, 2497, 1659, 580, 469, 389, 6, 555, 1867, 1367, 825, 1482, 1312, 1598, 565, 1020, 195, 882, 2302, 1375, 1138, 377, 1327, 1319, 2448, 2007, 1236, 861, 1083, 2110, 2198, 1324, 1638, 1440, 2454, 914, 934, 1010, 534, 1495, 1347, 496, 1988, 114, 1776, 1435, 2349, 615, 1685, 388, 68, 1693, 1808, 471, 1780, 735, 2389, 1038, 1749, 1057, 816, 613, 1111, 549, 1810, 151, 1348, 29, 846, 1105, 339, 813, 296, 660, 1965, 1229, 2213, 1543, 251, 276, 1791, 1821, 1648, 501, 1647, 1744, 2196, 454, 1874, 2427, 2223, 1300, 2534, 1709, 2294, 1394, 1840, 716, 532, 871, 2415, 1187, 2250, 2111, 382, 1073, 42, 2271, 1, 138, 2559, 1816, 1238, 278, 1689, 2323, 2093, 2108, 1920, 940, 2352, 1077, 1050, 1883, 1705, 527, 971, 589, 854, 1837, 753, 533, 674, 2077, 939, 249, 497, 1788, 33, 2549, 1506, 1509, 353, 1266, 320, 700, 2143, 51, 540, 2232, 1943, 1088, 2112, 553, 1253, 2428, 34, 627, 1340, 482, 597, 1720, 2479, 1110, 1530, 1775, 2120, 2264, 468, 546, 231, 1008, 1448, 1660, 1845, 2325, 1759, 1584, 265, 1331, 729, 104, 2106, 472, 1029, 867, 1696, 1169, 1839, 1522, 511, 1065, 2543, 1211, 1264, 429, 926], 'conv4': [1067, 99, 1620, 633, 1644, 2356, 558, 929, 348, 881, 1921, 468, 1706, 2285, 82, 966, 1975, 1678, 1173, 1741, 2388, 1928, 1658, 1148, 1254, 2471, 1451, 1261, 1444, 203, 2413, 1692, 2346, 484, 1521, 1727, 1656, 1795, 997, 1734, 1575, 675, 2543, 621, 2039, 670, 1622, 2209, 694, 1993, 1990, 1149, 2404, 969, 967, 893, 1660, 1766, 80, 2041, 2399, 862, 1585, 1797, 783, 1498, 2528, 1441, 1764, 808, 1416, 1517, 1539, 1364, 1047, 1957, 1667, 2258, 1859, 1867, 593, 1187, 1654, 1167, 2158, 960, 258, 615, 931, 366, 2359, 1131, 2207, 2353, 2211, 760, 768, 381, 1184, 1612, 806, 473, 1266, 139, 2438, 1710, 499, 1366, 2153, 272, 965, 1189, 772, 774, 339, 1369, 465, 38, 1191, 2524, 1320, 884, 1712, 2202, 1362, 107, 826, 778, 1809, 2463, 1996, 1548, 1659, 1288, 1483, 1580, 1138, 1221, 2015, 1719, 1030, 1472, 1313, 2437, 188, 1746, 2336, 1739, 1063, 1549, 779, 1323, 788, 1908, 1165, 535, 2031, 1485, 2242, 21, 2476, 1317, 1128, 1019, 309, 649, 641, 712, 1034, 891, 239, 691, 1284, 1757, 2449, 544, 2038, 688, 2491, 993, 1800, 1794, 2225, 1567, 264, 591, 795, 145, 845, 1354, 1813, 2395, 2551, 921, 1604, 830, 2021, 1109, 182, 2518, 512, 674, 1822, 1306, 1637, 2145, 2338, 2305, 390, 369, 1811, 368, 818, 592, 1534, 2421, 364, 1085, 2533, 575, 2176, 1326, 2435, 315, 486, 2530, 2482, 673, 704, 1708, 1135, 2522, 151, 13, 2212, 1183, 1437, 20, 2152, 2217, 722, 2268, 1310, 1418, 568, 976, 2537, 253, 412, 1699, 1904, 1657, 1246, 1263, 1738, 89, 1818, 1756, 734, 193, 91, 1798, 2068, 1296, 1791, 1316, 514, 1054, 786, 295, 1536, 1190, 2385, 1559, 432, 1732, 827, 1924, 2445, 2071, 1974, 518, 1415, 2286, 457, 2335, 520, 1429, 1929, 2237, 548, 1772, 2552, 708, 2251, 435, 547, 762, 479, 1569, 637, 1143, 1874, 1084, 183, 1445, 1101, 504, 2307, 586, 1496, 114, 2403, 690, 1302, 1035, 2321, 503, 823, 451, 2271, 243, 1040, 1123, 1962, 2093, 987, 3, 1278, 433, 1327, 2206, 2519, 1540, 903, 2165, 498, 952, 1265, 1652, 2029, 2327, 508, 1617, 331, 406, 721, 2008, 338, 2060, 2406, 113, 1801, 1392, 477, 1922, 2077, 1541, 81, 1271, 464, 607, 2133, 1344, 1473, 1058, 159, 227, 1698, 1610, 527, 1057, 1589, 1228, 2123, 652, 507, 2360, 2472, 598, 605, 1668, 152, 325, 1718]}
SAVED model.heads_thresh 0.89
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
Epoch: [20/50]	lr: 2.50e-04	time: 00:00:35	Loss_train 0.01005	Acc_train 93.10	/	Loss_test 0.00425	Acc_test 90.00
BLOCKS IN SUP:  [5]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c/models
SAVED HEADS THRESHOLD:  0.89
SAVED model.topk_kernels {'conv0': [25, 59, 5, 82, 30, 89, 16, 77, 33, 42, 70, 93, 45, 68, 64], 'conv1': [50, 56, 75, 118, 206, 144, 94, 317, 382, 336, 354, 140, 41, 264, 191, 326, 142, 233, 324, 92, 262, 127, 355, 104, 31, 221, 365, 137, 298, 200, 6, 195, 88, 130, 345, 352, 67, 234, 371, 39, 180, 219, 96, 14, 190, 62, 292, 134, 48, 113, 304, 121, 64, 99, 357, 117, 266, 373, 312], 'conv2': [937, 407, 636, 307, 700, 176, 1086, 195, 1385, 1171, 405, 1317, 1278, 921, 673, 427, 799, 1034, 1486, 854, 421, 311, 731, 616, 794, 1307, 35, 706, 1500, 544, 194, 1360, 450, 958, 598, 1291, 883, 66, 560, 1306, 710, 869, 14, 93, 932, 141, 548, 155, 312, 796, 361, 167, 37, 1534, 851, 490, 551, 1000, 1412, 445, 686, 1226, 838, 906, 1030, 100, 13, 812, 1363, 730, 687, 660, 1006, 139, 150, 268, 437, 1281, 889, 380, 856, 635, 77, 1430, 123, 1286, 1109, 346, 482, 1511, 67, 1359, 1386, 1056, 1294, 786, 1239, 683, 28, 499, 276, 95, 574, 761, 914, 818, 924, 287, 12, 1479, 571, 18, 1405, 520, 955, 566, 699, 1508, 247, 56, 916, 11, 337, 1284, 217, 562, 662, 612, 1242, 1496, 1466, 109, 1442, 304, 1230, 881, 853, 1371, 1395, 1061, 1372, 1007, 1238, 1436, 618, 41, 1263, 634, 577, 1069, 1489, 1272, 353, 1123, 803, 1101, 199, 640, 1134, 946, 105, 726, 1297, 1292, 524, 698, 82, 266, 1472, 814, 626, 622, 174, 289, 1521, 1082, 582, 1041, 1383, 667, 743, 348, 461, 1464, 319, 114, 1071, 1480, 987, 1050, 817, 1046, 302, 489, 747, 610, 1085, 243, 61, 1145, 207, 492, 503, 542, 534, 1096, 1130, 1252, 1408, 1424, 923, 1160, 1092, 1274, 621, 1355, 229, 760, 458, 1019, 1332, 462, 900, 451, 1139, 1159, 1100, 732, 1027, 1295, 1515], 'conv3': [499, 507, 1677, 1339, 512, 1958, 653, 1443, 300, 2049, 1447, 921, 1755, 340, 2315, 1827, 43, 1862, 1359, 652, 2494, 1938, 446, 2095, 2402, 1403, 2157, 505, 2354, 1127, 1303, 1567, 193, 2103, 1109, 784, 2025, 288, 1670, 243, 2379, 520, 1608, 40, 450, 2435, 1783, 2132, 508, 611, 1465, 1710, 481, 1618, 1843, 2100, 1061, 1125, 2115, 699, 1656, 2414, 1602, 1964, 1510, 2021, 421, 2292, 399, 2032, 1352, 1641, 818, 2221, 993, 484, 1500, 2185, 1926, 1834, 2482, 156, 303, 153, 2317, 299, 2067, 1762, 733, 359, 2154, 991, 1750, 2050, 792, 1378, 2268, 2138, 1341, 2145, 277, 2181, 378, 1520, 783, 1894, 2309, 2554, 1789, 878, 1616, 2244, 853, 2367, 328, 2249, 2247, 2405, 1559, 147, 455, 1370, 2218, 2087, 863, 2241, 2300, 2363, 1242, 1163, 624, 2245, 1548, 197, 1402, 757, 1599, 1715, 1422, 606, 2082, 8, 1646, 372, 2460, 1561, 1411, 2276, 1260, 983, 1985, 208, 287, 689, 1597, 883, 1929, 2000, 2170, 1326, 244, 1226, 356, 841, 726, 651, 367, 1425, 148, 1887, 2324, 780, 1275, 1699, 1366, 1063, 2291, 1658, 2081, 2261, 1998, 103, 2465, 724, 1205, 419, 2184, 1554, 36, 2497, 1659, 580, 469, 389, 6, 555, 1867, 1367, 825, 1482, 1312, 1598, 565, 1020, 195, 882, 2302, 1375, 1138, 377, 1327, 1319, 2448, 2007, 1236, 861, 1083, 2110, 2198, 1324, 1638, 1440, 2454, 914, 934, 1010, 534, 1495, 1347, 496, 1988, 114, 1776, 1435, 2349, 615, 1685, 388, 68, 1693, 1808, 471, 1780, 735, 2389, 1038, 1749, 1057, 816, 613, 1111, 549, 1810, 151, 1348, 29, 846, 1105, 339, 813, 296, 660, 1965, 1229, 2213, 1543, 251, 276, 1791, 1821, 1648, 501, 1647, 1744, 2196, 454, 1874, 2427, 2223, 1300, 2534, 1709, 2294, 1394, 1840, 716, 532, 871, 2415, 1187, 2250, 2111, 382, 1073, 42, 2271, 1, 138, 2559, 1816, 1238, 278, 1689, 2323, 2093, 2108, 1920, 940, 2352, 1077, 1050, 1883, 1705, 527, 971, 589, 854, 1837, 753, 533, 674, 2077, 939, 249, 497, 1788, 33, 2549, 1506, 1509, 353, 1266, 320, 700, 2143, 51, 540, 2232, 1943, 1088, 2112, 553, 1253, 2428, 34, 627, 1340, 482, 597, 1720, 2479, 1110, 1530, 1775, 2120, 2264, 468, 546, 231, 1008, 1448, 1660, 1845, 2325, 1759, 1584, 265, 1331, 729, 104, 2106, 472, 1029, 867, 1696, 1169, 1839, 1522, 511, 1065, 2543, 1211, 1264, 429, 926], 'conv4': [1067, 99, 1620, 633, 1644, 2356, 558, 929, 348, 881, 1921, 468, 1706, 2285, 82, 966, 1975, 1678, 1173, 1741, 2388, 1928, 1658, 1148, 1254, 2471, 1451, 1261, 1444, 203, 2413, 1692, 2346, 484, 1521, 1727, 1656, 1795, 997, 1734, 1575, 675, 2543, 621, 2039, 670, 1622, 2209, 694, 1993, 1990, 1149, 2404, 969, 967, 893, 1660, 1766, 80, 2041, 2399, 862, 1585, 1797, 783, 1498, 2528, 1441, 1764, 808, 1416, 1517, 1539, 1364, 1047, 1957, 1667, 2258, 1859, 1867, 593, 1187, 1654, 1167, 2158, 960, 258, 615, 931, 366, 2359, 1131, 2207, 2353, 2211, 760, 768, 381, 1184, 1612, 806, 473, 1266, 139, 2438, 1710, 499, 1366, 2153, 272, 965, 1189, 772, 774, 339, 1369, 465, 38, 1191, 2524, 1320, 884, 1712, 2202, 1362, 107, 826, 778, 1809, 2463, 1996, 1548, 1659, 1288, 1483, 1580, 1138, 1221, 2015, 1719, 1030, 1472, 1313, 2437, 188, 1746, 2336, 1739, 1063, 1549, 779, 1323, 788, 1908, 1165, 535, 2031, 1485, 2242, 21, 2476, 1317, 1128, 1019, 309, 649, 641, 712, 1034, 891, 239, 691, 1284, 1757, 2449, 544, 2038, 688, 2491, 993, 1800, 1794, 2225, 1567, 264, 591, 795, 145, 845, 1354, 1813, 2395, 2551, 921, 1604, 830, 2021, 1109, 182, 2518, 512, 674, 1822, 1306, 1637, 2145, 2338, 2305, 390, 369, 1811, 368, 818, 592, 1534, 2421, 364, 1085, 2533, 575, 2176, 1326, 2435, 315, 486, 2530, 2482, 673, 704, 1708, 1135, 2522, 151, 13, 2212, 1183, 1437, 20, 2152, 2217, 722, 2268, 1310, 1418, 568, 976, 2537, 253, 412, 1699, 1904, 1657, 1246, 1263, 1738, 89, 1818, 1756, 734, 193, 91, 1798, 2068, 1296, 1791, 1316, 514, 1054, 786, 295, 1536, 1190, 2385, 1559, 432, 1732, 827, 1924, 2445, 2071, 1974, 518, 1415, 2286, 457, 2335, 520, 1429, 1929, 2237, 548, 1772, 2552, 708, 2251, 435, 547, 762, 479, 1569, 637, 1143, 1874, 1084, 183, 1445, 1101, 504, 2307, 586, 1496, 114, 2403, 690, 1302, 1035, 2321, 503, 823, 451, 2271, 243, 1040, 1123, 1962, 2093, 987, 3, 1278, 433, 1327, 2206, 2519, 1540, 903, 2165, 498, 952, 1265, 1652, 2029, 2327, 508, 1617, 331, 406, 721, 2008, 338, 2060, 2406, 113, 1801, 1392, 477, 1922, 2077, 1541, 81, 1271, 464, 607, 2133, 1344, 1473, 1058, 159, 227, 1698, 1610, 527, 1057, 1589, 1228, 2123, 652, 507, 2360, 2472, 598, 605, 1668, 152, 325, 1718]}
SAVED model.heads_thresh 0.89
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
Epoch: [30/50]	lr: 1.25e-04	time: 00:00:37	Loss_train 0.00761	Acc_train 94.06	/	Loss_test 0.00318	Acc_test 91.00
BLOCKS IN SUP:  [5]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c/models
SAVED HEADS THRESHOLD:  0.89
SAVED model.topk_kernels {'conv0': [25, 59, 5, 82, 30, 89, 16, 77, 33, 42, 70, 93, 45, 68, 64], 'conv1': [50, 56, 75, 118, 206, 144, 94, 317, 382, 336, 354, 140, 41, 264, 191, 326, 142, 233, 324, 92, 262, 127, 355, 104, 31, 221, 365, 137, 298, 200, 6, 195, 88, 130, 345, 352, 67, 234, 371, 39, 180, 219, 96, 14, 190, 62, 292, 134, 48, 113, 304, 121, 64, 99, 357, 117, 266, 373, 312], 'conv2': [937, 407, 636, 307, 700, 176, 1086, 195, 1385, 1171, 405, 1317, 1278, 921, 673, 427, 799, 1034, 1486, 854, 421, 311, 731, 616, 794, 1307, 35, 706, 1500, 544, 194, 1360, 450, 958, 598, 1291, 883, 66, 560, 1306, 710, 869, 14, 93, 932, 141, 548, 155, 312, 796, 361, 167, 37, 1534, 851, 490, 551, 1000, 1412, 445, 686, 1226, 838, 906, 1030, 100, 13, 812, 1363, 730, 687, 660, 1006, 139, 150, 268, 437, 1281, 889, 380, 856, 635, 77, 1430, 123, 1286, 1109, 346, 482, 1511, 67, 1359, 1386, 1056, 1294, 786, 1239, 683, 28, 499, 276, 95, 574, 761, 914, 818, 924, 287, 12, 1479, 571, 18, 1405, 520, 955, 566, 699, 1508, 247, 56, 916, 11, 337, 1284, 217, 562, 662, 612, 1242, 1496, 1466, 109, 1442, 304, 1230, 881, 853, 1371, 1395, 1061, 1372, 1007, 1238, 1436, 618, 41, 1263, 634, 577, 1069, 1489, 1272, 353, 1123, 803, 1101, 199, 640, 1134, 946, 105, 726, 1297, 1292, 524, 698, 82, 266, 1472, 814, 626, 622, 174, 289, 1521, 1082, 582, 1041, 1383, 667, 743, 348, 461, 1464, 319, 114, 1071, 1480, 987, 1050, 817, 1046, 302, 489, 747, 610, 1085, 243, 61, 1145, 207, 492, 503, 542, 534, 1096, 1130, 1252, 1408, 1424, 923, 1160, 1092, 1274, 621, 1355, 229, 760, 458, 1019, 1332, 462, 900, 451, 1139, 1159, 1100, 732, 1027, 1295, 1515], 'conv3': [499, 507, 1677, 1339, 512, 1958, 653, 1443, 300, 2049, 1447, 921, 1755, 340, 2315, 1827, 43, 1862, 1359, 652, 2494, 1938, 446, 2095, 2402, 1403, 2157, 505, 2354, 1127, 1303, 1567, 193, 2103, 1109, 784, 2025, 288, 1670, 243, 2379, 520, 1608, 40, 450, 2435, 1783, 2132, 508, 611, 1465, 1710, 481, 1618, 1843, 2100, 1061, 1125, 2115, 699, 1656, 2414, 1602, 1964, 1510, 2021, 421, 2292, 399, 2032, 1352, 1641, 818, 2221, 993, 484, 1500, 2185, 1926, 1834, 2482, 156, 303, 153, 2317, 299, 2067, 1762, 733, 359, 2154, 991, 1750, 2050, 792, 1378, 2268, 2138, 1341, 2145, 277, 2181, 378, 1520, 783, 1894, 2309, 2554, 1789, 878, 1616, 2244, 853, 2367, 328, 2249, 2247, 2405, 1559, 147, 455, 1370, 2218, 2087, 863, 2241, 2300, 2363, 1242, 1163, 624, 2245, 1548, 197, 1402, 757, 1599, 1715, 1422, 606, 2082, 8, 1646, 372, 2460, 1561, 1411, 2276, 1260, 983, 1985, 208, 287, 689, 1597, 883, 1929, 2000, 2170, 1326, 244, 1226, 356, 841, 726, 651, 367, 1425, 148, 1887, 2324, 780, 1275, 1699, 1366, 1063, 2291, 1658, 2081, 2261, 1998, 103, 2465, 724, 1205, 419, 2184, 1554, 36, 2497, 1659, 580, 469, 389, 6, 555, 1867, 1367, 825, 1482, 1312, 1598, 565, 1020, 195, 882, 2302, 1375, 1138, 377, 1327, 1319, 2448, 2007, 1236, 861, 1083, 2110, 2198, 1324, 1638, 1440, 2454, 914, 934, 1010, 534, 1495, 1347, 496, 1988, 114, 1776, 1435, 2349, 615, 1685, 388, 68, 1693, 1808, 471, 1780, 735, 2389, 1038, 1749, 1057, 816, 613, 1111, 549, 1810, 151, 1348, 29, 846, 1105, 339, 813, 296, 660, 1965, 1229, 2213, 1543, 251, 276, 1791, 1821, 1648, 501, 1647, 1744, 2196, 454, 1874, 2427, 2223, 1300, 2534, 1709, 2294, 1394, 1840, 716, 532, 871, 2415, 1187, 2250, 2111, 382, 1073, 42, 2271, 1, 138, 2559, 1816, 1238, 278, 1689, 2323, 2093, 2108, 1920, 940, 2352, 1077, 1050, 1883, 1705, 527, 971, 589, 854, 1837, 753, 533, 674, 2077, 939, 249, 497, 1788, 33, 2549, 1506, 1509, 353, 1266, 320, 700, 2143, 51, 540, 2232, 1943, 1088, 2112, 553, 1253, 2428, 34, 627, 1340, 482, 597, 1720, 2479, 1110, 1530, 1775, 2120, 2264, 468, 546, 231, 1008, 1448, 1660, 1845, 2325, 1759, 1584, 265, 1331, 729, 104, 2106, 472, 1029, 867, 1696, 1169, 1839, 1522, 511, 1065, 2543, 1211, 1264, 429, 926], 'conv4': [1067, 99, 1620, 633, 1644, 2356, 558, 929, 348, 881, 1921, 468, 1706, 2285, 82, 966, 1975, 1678, 1173, 1741, 2388, 1928, 1658, 1148, 1254, 2471, 1451, 1261, 1444, 203, 2413, 1692, 2346, 484, 1521, 1727, 1656, 1795, 997, 1734, 1575, 675, 2543, 621, 2039, 670, 1622, 2209, 694, 1993, 1990, 1149, 2404, 969, 967, 893, 1660, 1766, 80, 2041, 2399, 862, 1585, 1797, 783, 1498, 2528, 1441, 1764, 808, 1416, 1517, 1539, 1364, 1047, 1957, 1667, 2258, 1859, 1867, 593, 1187, 1654, 1167, 2158, 960, 258, 615, 931, 366, 2359, 1131, 2207, 2353, 2211, 760, 768, 381, 1184, 1612, 806, 473, 1266, 139, 2438, 1710, 499, 1366, 2153, 272, 965, 1189, 772, 774, 339, 1369, 465, 38, 1191, 2524, 1320, 884, 1712, 2202, 1362, 107, 826, 778, 1809, 2463, 1996, 1548, 1659, 1288, 1483, 1580, 1138, 1221, 2015, 1719, 1030, 1472, 1313, 2437, 188, 1746, 2336, 1739, 1063, 1549, 779, 1323, 788, 1908, 1165, 535, 2031, 1485, 2242, 21, 2476, 1317, 1128, 1019, 309, 649, 641, 712, 1034, 891, 239, 691, 1284, 1757, 2449, 544, 2038, 688, 2491, 993, 1800, 1794, 2225, 1567, 264, 591, 795, 145, 845, 1354, 1813, 2395, 2551, 921, 1604, 830, 2021, 1109, 182, 2518, 512, 674, 1822, 1306, 1637, 2145, 2338, 2305, 390, 369, 1811, 368, 818, 592, 1534, 2421, 364, 1085, 2533, 575, 2176, 1326, 2435, 315, 486, 2530, 2482, 673, 704, 1708, 1135, 2522, 151, 13, 2212, 1183, 1437, 20, 2152, 2217, 722, 2268, 1310, 1418, 568, 976, 2537, 253, 412, 1699, 1904, 1657, 1246, 1263, 1738, 89, 1818, 1756, 734, 193, 91, 1798, 2068, 1296, 1791, 1316, 514, 1054, 786, 295, 1536, 1190, 2385, 1559, 432, 1732, 827, 1924, 2445, 2071, 1974, 518, 1415, 2286, 457, 2335, 520, 1429, 1929, 2237, 548, 1772, 2552, 708, 2251, 435, 547, 762, 479, 1569, 637, 1143, 1874, 1084, 183, 1445, 1101, 504, 2307, 586, 1496, 114, 2403, 690, 1302, 1035, 2321, 503, 823, 451, 2271, 243, 1040, 1123, 1962, 2093, 987, 3, 1278, 433, 1327, 2206, 2519, 1540, 903, 2165, 498, 952, 1265, 1652, 2029, 2327, 508, 1617, 331, 406, 721, 2008, 338, 2060, 2406, 113, 1801, 1392, 477, 1922, 2077, 1541, 81, 1271, 464, 607, 2133, 1344, 1473, 1058, 159, 227, 1698, 1610, 527, 1057, 1589, 1228, 2123, 652, 507, 2360, 2472, 598, 605, 1668, 152, 325, 1718]}
SAVED model.heads_thresh 0.89
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
Epoch: [40/50]	lr: 3.13e-05	time: 00:00:39	Loss_train 0.00692	Acc_train 94.48	/	Loss_test 0.00317	Acc_test 92.50
BLOCKS IN SUP:  [5]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c/models
SAVED HEADS THRESHOLD:  0.89
SAVED model.topk_kernels {'conv0': [25, 59, 5, 82, 30, 89, 16, 77, 33, 42, 70, 93, 45, 68, 64], 'conv1': [50, 56, 75, 118, 206, 144, 94, 317, 382, 336, 354, 140, 41, 264, 191, 326, 142, 233, 324, 92, 262, 127, 355, 104, 31, 221, 365, 137, 298, 200, 6, 195, 88, 130, 345, 352, 67, 234, 371, 39, 180, 219, 96, 14, 190, 62, 292, 134, 48, 113, 304, 121, 64, 99, 357, 117, 266, 373, 312], 'conv2': [937, 407, 636, 307, 700, 176, 1086, 195, 1385, 1171, 405, 1317, 1278, 921, 673, 427, 799, 1034, 1486, 854, 421, 311, 731, 616, 794, 1307, 35, 706, 1500, 544, 194, 1360, 450, 958, 598, 1291, 883, 66, 560, 1306, 710, 869, 14, 93, 932, 141, 548, 155, 312, 796, 361, 167, 37, 1534, 851, 490, 551, 1000, 1412, 445, 686, 1226, 838, 906, 1030, 100, 13, 812, 1363, 730, 687, 660, 1006, 139, 150, 268, 437, 1281, 889, 380, 856, 635, 77, 1430, 123, 1286, 1109, 346, 482, 1511, 67, 1359, 1386, 1056, 1294, 786, 1239, 683, 28, 499, 276, 95, 574, 761, 914, 818, 924, 287, 12, 1479, 571, 18, 1405, 520, 955, 566, 699, 1508, 247, 56, 916, 11, 337, 1284, 217, 562, 662, 612, 1242, 1496, 1466, 109, 1442, 304, 1230, 881, 853, 1371, 1395, 1061, 1372, 1007, 1238, 1436, 618, 41, 1263, 634, 577, 1069, 1489, 1272, 353, 1123, 803, 1101, 199, 640, 1134, 946, 105, 726, 1297, 1292, 524, 698, 82, 266, 1472, 814, 626, 622, 174, 289, 1521, 1082, 582, 1041, 1383, 667, 743, 348, 461, 1464, 319, 114, 1071, 1480, 987, 1050, 817, 1046, 302, 489, 747, 610, 1085, 243, 61, 1145, 207, 492, 503, 542, 534, 1096, 1130, 1252, 1408, 1424, 923, 1160, 1092, 1274, 621, 1355, 229, 760, 458, 1019, 1332, 462, 900, 451, 1139, 1159, 1100, 732, 1027, 1295, 1515], 'conv3': [499, 507, 1677, 1339, 512, 1958, 653, 1443, 300, 2049, 1447, 921, 1755, 340, 2315, 1827, 43, 1862, 1359, 652, 2494, 1938, 446, 2095, 2402, 1403, 2157, 505, 2354, 1127, 1303, 1567, 193, 2103, 1109, 784, 2025, 288, 1670, 243, 2379, 520, 1608, 40, 450, 2435, 1783, 2132, 508, 611, 1465, 1710, 481, 1618, 1843, 2100, 1061, 1125, 2115, 699, 1656, 2414, 1602, 1964, 1510, 2021, 421, 2292, 399, 2032, 1352, 1641, 818, 2221, 993, 484, 1500, 2185, 1926, 1834, 2482, 156, 303, 153, 2317, 299, 2067, 1762, 733, 359, 2154, 991, 1750, 2050, 792, 1378, 2268, 2138, 1341, 2145, 277, 2181, 378, 1520, 783, 1894, 2309, 2554, 1789, 878, 1616, 2244, 853, 2367, 328, 2249, 2247, 2405, 1559, 147, 455, 1370, 2218, 2087, 863, 2241, 2300, 2363, 1242, 1163, 624, 2245, 1548, 197, 1402, 757, 1599, 1715, 1422, 606, 2082, 8, 1646, 372, 2460, 1561, 1411, 2276, 1260, 983, 1985, 208, 287, 689, 1597, 883, 1929, 2000, 2170, 1326, 244, 1226, 356, 841, 726, 651, 367, 1425, 148, 1887, 2324, 780, 1275, 1699, 1366, 1063, 2291, 1658, 2081, 2261, 1998, 103, 2465, 724, 1205, 419, 2184, 1554, 36, 2497, 1659, 580, 469, 389, 6, 555, 1867, 1367, 825, 1482, 1312, 1598, 565, 1020, 195, 882, 2302, 1375, 1138, 377, 1327, 1319, 2448, 2007, 1236, 861, 1083, 2110, 2198, 1324, 1638, 1440, 2454, 914, 934, 1010, 534, 1495, 1347, 496, 1988, 114, 1776, 1435, 2349, 615, 1685, 388, 68, 1693, 1808, 471, 1780, 735, 2389, 1038, 1749, 1057, 816, 613, 1111, 549, 1810, 151, 1348, 29, 846, 1105, 339, 813, 296, 660, 1965, 1229, 2213, 1543, 251, 276, 1791, 1821, 1648, 501, 1647, 1744, 2196, 454, 1874, 2427, 2223, 1300, 2534, 1709, 2294, 1394, 1840, 716, 532, 871, 2415, 1187, 2250, 2111, 382, 1073, 42, 2271, 1, 138, 2559, 1816, 1238, 278, 1689, 2323, 2093, 2108, 1920, 940, 2352, 1077, 1050, 1883, 1705, 527, 971, 589, 854, 1837, 753, 533, 674, 2077, 939, 249, 497, 1788, 33, 2549, 1506, 1509, 353, 1266, 320, 700, 2143, 51, 540, 2232, 1943, 1088, 2112, 553, 1253, 2428, 34, 627, 1340, 482, 597, 1720, 2479, 1110, 1530, 1775, 2120, 2264, 468, 546, 231, 1008, 1448, 1660, 1845, 2325, 1759, 1584, 265, 1331, 729, 104, 2106, 472, 1029, 867, 1696, 1169, 1839, 1522, 511, 1065, 2543, 1211, 1264, 429, 926], 'conv4': [1067, 99, 1620, 633, 1644, 2356, 558, 929, 348, 881, 1921, 468, 1706, 2285, 82, 966, 1975, 1678, 1173, 1741, 2388, 1928, 1658, 1148, 1254, 2471, 1451, 1261, 1444, 203, 2413, 1692, 2346, 484, 1521, 1727, 1656, 1795, 997, 1734, 1575, 675, 2543, 621, 2039, 670, 1622, 2209, 694, 1993, 1990, 1149, 2404, 969, 967, 893, 1660, 1766, 80, 2041, 2399, 862, 1585, 1797, 783, 1498, 2528, 1441, 1764, 808, 1416, 1517, 1539, 1364, 1047, 1957, 1667, 2258, 1859, 1867, 593, 1187, 1654, 1167, 2158, 960, 258, 615, 931, 366, 2359, 1131, 2207, 2353, 2211, 760, 768, 381, 1184, 1612, 806, 473, 1266, 139, 2438, 1710, 499, 1366, 2153, 272, 965, 1189, 772, 774, 339, 1369, 465, 38, 1191, 2524, 1320, 884, 1712, 2202, 1362, 107, 826, 778, 1809, 2463, 1996, 1548, 1659, 1288, 1483, 1580, 1138, 1221, 2015, 1719, 1030, 1472, 1313, 2437, 188, 1746, 2336, 1739, 1063, 1549, 779, 1323, 788, 1908, 1165, 535, 2031, 1485, 2242, 21, 2476, 1317, 1128, 1019, 309, 649, 641, 712, 1034, 891, 239, 691, 1284, 1757, 2449, 544, 2038, 688, 2491, 993, 1800, 1794, 2225, 1567, 264, 591, 795, 145, 845, 1354, 1813, 2395, 2551, 921, 1604, 830, 2021, 1109, 182, 2518, 512, 674, 1822, 1306, 1637, 2145, 2338, 2305, 390, 369, 1811, 368, 818, 592, 1534, 2421, 364, 1085, 2533, 575, 2176, 1326, 2435, 315, 486, 2530, 2482, 673, 704, 1708, 1135, 2522, 151, 13, 2212, 1183, 1437, 20, 2152, 2217, 722, 2268, 1310, 1418, 568, 976, 2537, 253, 412, 1699, 1904, 1657, 1246, 1263, 1738, 89, 1818, 1756, 734, 193, 91, 1798, 2068, 1296, 1791, 1316, 514, 1054, 786, 295, 1536, 1190, 2385, 1559, 432, 1732, 827, 1924, 2445, 2071, 1974, 518, 1415, 2286, 457, 2335, 520, 1429, 1929, 2237, 548, 1772, 2552, 708, 2251, 435, 547, 762, 479, 1569, 637, 1143, 1874, 1084, 183, 1445, 1101, 504, 2307, 586, 1496, 114, 2403, 690, 1302, 1035, 2321, 503, 823, 451, 2271, 243, 1040, 1123, 1962, 2093, 987, 3, 1278, 433, 1327, 2206, 2519, 1540, 903, 2165, 498, 952, 1265, 1652, 2029, 2327, 508, 1617, 331, 406, 721, 2008, 338, 2060, 2406, 113, 1801, 1392, 477, 1922, 2077, 1541, 81, 1271, 464, 607, 2133, 1344, 1473, 1058, 159, 227, 1698, 1610, 527, 1057, 1589, 1228, 2123, 652, 507, 2360, 2472, 598, 605, 1668, 152, 325, 1718]}
SAVED model.heads_thresh 0.89
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
Epoch: [50/50]	lr: 7.81e-06	time: 00:00:40	Loss_train 0.00639	Acc_train 94.68	/	Loss_test 0.00323	Acc_test 92.50
new_head:  {'blocks.5.layer.bias': tensor([ 0.0005, -0.0170], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0349,  0.0030,  0.0162,  ..., -0.0045,  0.0023,  0.0014],
        [-0.0066,  0.0195, -0.0117,  ...,  0.0072,  0.0189,  0.0064]],
       device='cuda:0')}
BLOCKS IN SUP:  [5]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c/models
SAVED HEADS THRESHOLD:  0.9016666666666667
SAVED model.topk_kernels {'conv0': [25, 59, 5, 82, 30, 89, 16, 77, 33, 42, 70, 93, 45, 68, 64], 'conv1': [50, 56, 75, 118, 206, 144, 94, 317, 382, 336, 354, 140, 41, 264, 191, 326, 142, 233, 324, 92, 262, 127, 355, 104, 31, 221, 365, 137, 298, 200, 6, 195, 88, 130, 345, 352, 67, 234, 371, 39, 180, 219, 96, 14, 190, 62, 292, 134, 48, 113, 304, 121, 64, 99, 357, 117, 266, 373, 312], 'conv2': [937, 407, 636, 307, 700, 176, 1086, 195, 1385, 1171, 405, 1317, 1278, 921, 673, 427, 799, 1034, 1486, 854, 421, 311, 731, 616, 794, 1307, 35, 706, 1500, 544, 194, 1360, 450, 958, 598, 1291, 883, 66, 560, 1306, 710, 869, 14, 93, 932, 141, 548, 155, 312, 796, 361, 167, 37, 1534, 851, 490, 551, 1000, 1412, 445, 686, 1226, 838, 906, 1030, 100, 13, 812, 1363, 730, 687, 660, 1006, 139, 150, 268, 437, 1281, 889, 380, 856, 635, 77, 1430, 123, 1286, 1109, 346, 482, 1511, 67, 1359, 1386, 1056, 1294, 786, 1239, 683, 28, 499, 276, 95, 574, 761, 914, 818, 924, 287, 12, 1479, 571, 18, 1405, 520, 955, 566, 699, 1508, 247, 56, 916, 11, 337, 1284, 217, 562, 662, 612, 1242, 1496, 1466, 109, 1442, 304, 1230, 881, 853, 1371, 1395, 1061, 1372, 1007, 1238, 1436, 618, 41, 1263, 634, 577, 1069, 1489, 1272, 353, 1123, 803, 1101, 199, 640, 1134, 946, 105, 726, 1297, 1292, 524, 698, 82, 266, 1472, 814, 626, 622, 174, 289, 1521, 1082, 582, 1041, 1383, 667, 743, 348, 461, 1464, 319, 114, 1071, 1480, 987, 1050, 817, 1046, 302, 489, 747, 610, 1085, 243, 61, 1145, 207, 492, 503, 542, 534, 1096, 1130, 1252, 1408, 1424, 923, 1160, 1092, 1274, 621, 1355, 229, 760, 458, 1019, 1332, 462, 900, 451, 1139, 1159, 1100, 732, 1027, 1295, 1515], 'conv3': [499, 507, 1677, 1339, 512, 1958, 653, 1443, 300, 2049, 1447, 921, 1755, 340, 2315, 1827, 43, 1862, 1359, 652, 2494, 1938, 446, 2095, 2402, 1403, 2157, 505, 2354, 1127, 1303, 1567, 193, 2103, 1109, 784, 2025, 288, 1670, 243, 2379, 520, 1608, 40, 450, 2435, 1783, 2132, 508, 611, 1465, 1710, 481, 1618, 1843, 2100, 1061, 1125, 2115, 699, 1656, 2414, 1602, 1964, 1510, 2021, 421, 2292, 399, 2032, 1352, 1641, 818, 2221, 993, 484, 1500, 2185, 1926, 1834, 2482, 156, 303, 153, 2317, 299, 2067, 1762, 733, 359, 2154, 991, 1750, 2050, 792, 1378, 2268, 2138, 1341, 2145, 277, 2181, 378, 1520, 783, 1894, 2309, 2554, 1789, 878, 1616, 2244, 853, 2367, 328, 2249, 2247, 2405, 1559, 147, 455, 1370, 2218, 2087, 863, 2241, 2300, 2363, 1242, 1163, 624, 2245, 1548, 197, 1402, 757, 1599, 1715, 1422, 606, 2082, 8, 1646, 372, 2460, 1561, 1411, 2276, 1260, 983, 1985, 208, 287, 689, 1597, 883, 1929, 2000, 2170, 1326, 244, 1226, 356, 841, 726, 651, 367, 1425, 148, 1887, 2324, 780, 1275, 1699, 1366, 1063, 2291, 1658, 2081, 2261, 1998, 103, 2465, 724, 1205, 419, 2184, 1554, 36, 2497, 1659, 580, 469, 389, 6, 555, 1867, 1367, 825, 1482, 1312, 1598, 565, 1020, 195, 882, 2302, 1375, 1138, 377, 1327, 1319, 2448, 2007, 1236, 861, 1083, 2110, 2198, 1324, 1638, 1440, 2454, 914, 934, 1010, 534, 1495, 1347, 496, 1988, 114, 1776, 1435, 2349, 615, 1685, 388, 68, 1693, 1808, 471, 1780, 735, 2389, 1038, 1749, 1057, 816, 613, 1111, 549, 1810, 151, 1348, 29, 846, 1105, 339, 813, 296, 660, 1965, 1229, 2213, 1543, 251, 276, 1791, 1821, 1648, 501, 1647, 1744, 2196, 454, 1874, 2427, 2223, 1300, 2534, 1709, 2294, 1394, 1840, 716, 532, 871, 2415, 1187, 2250, 2111, 382, 1073, 42, 2271, 1, 138, 2559, 1816, 1238, 278, 1689, 2323, 2093, 2108, 1920, 940, 2352, 1077, 1050, 1883, 1705, 527, 971, 589, 854, 1837, 753, 533, 674, 2077, 939, 249, 497, 1788, 33, 2549, 1506, 1509, 353, 1266, 320, 700, 2143, 51, 540, 2232, 1943, 1088, 2112, 553, 1253, 2428, 34, 627, 1340, 482, 597, 1720, 2479, 1110, 1530, 1775, 2120, 2264, 468, 546, 231, 1008, 1448, 1660, 1845, 2325, 1759, 1584, 265, 1331, 729, 104, 2106, 472, 1029, 867, 1696, 1169, 1839, 1522, 511, 1065, 2543, 1211, 1264, 429, 926], 'conv4': [1067, 99, 1620, 633, 1644, 2356, 558, 929, 348, 881, 1921, 468, 1706, 2285, 82, 966, 1975, 1678, 1173, 1741, 2388, 1928, 1658, 1148, 1254, 2471, 1451, 1261, 1444, 203, 2413, 1692, 2346, 484, 1521, 1727, 1656, 1795, 997, 1734, 1575, 675, 2543, 621, 2039, 670, 1622, 2209, 694, 1993, 1990, 1149, 2404, 969, 967, 893, 1660, 1766, 80, 2041, 2399, 862, 1585, 1797, 783, 1498, 2528, 1441, 1764, 808, 1416, 1517, 1539, 1364, 1047, 1957, 1667, 2258, 1859, 1867, 593, 1187, 1654, 1167, 2158, 960, 258, 615, 931, 366, 2359, 1131, 2207, 2353, 2211, 760, 768, 381, 1184, 1612, 806, 473, 1266, 139, 2438, 1710, 499, 1366, 2153, 272, 965, 1189, 772, 774, 339, 1369, 465, 38, 1191, 2524, 1320, 884, 1712, 2202, 1362, 107, 826, 778, 1809, 2463, 1996, 1548, 1659, 1288, 1483, 1580, 1138, 1221, 2015, 1719, 1030, 1472, 1313, 2437, 188, 1746, 2336, 1739, 1063, 1549, 779, 1323, 788, 1908, 1165, 535, 2031, 1485, 2242, 21, 2476, 1317, 1128, 1019, 309, 649, 641, 712, 1034, 891, 239, 691, 1284, 1757, 2449, 544, 2038, 688, 2491, 993, 1800, 1794, 2225, 1567, 264, 591, 795, 145, 845, 1354, 1813, 2395, 2551, 921, 1604, 830, 2021, 1109, 182, 2518, 512, 674, 1822, 1306, 1637, 2145, 2338, 2305, 390, 369, 1811, 368, 818, 592, 1534, 2421, 364, 1085, 2533, 575, 2176, 1326, 2435, 315, 486, 2530, 2482, 673, 704, 1708, 1135, 2522, 151, 13, 2212, 1183, 1437, 20, 2152, 2217, 722, 2268, 1310, 1418, 568, 976, 2537, 253, 412, 1699, 1904, 1657, 1246, 1263, 1738, 89, 1818, 1756, 734, 193, 91, 1798, 2068, 1296, 1791, 1316, 514, 1054, 786, 295, 1536, 1190, 2385, 1559, 432, 1732, 827, 1924, 2445, 2071, 1974, 518, 1415, 2286, 457, 2335, 520, 1429, 1929, 2237, 548, 1772, 2552, 708, 2251, 435, 547, 762, 479, 1569, 637, 1143, 1874, 1084, 183, 1445, 1101, 504, 2307, 586, 1496, 114, 2403, 690, 1302, 1035, 2321, 503, 823, 451, 2271, 243, 1040, 1123, 1962, 2093, 987, 3, 1278, 433, 1327, 2206, 2519, 1540, 903, 2165, 498, 952, 1265, 1652, 2029, 2327, 508, 1617, 331, 406, 721, 2008, 338, 2060, 2406, 113, 1801, 1392, 477, 1922, 2077, 1541, 81, 1271, 464, 607, 2133, 1344, 1473, 1058, 159, 227, 1698, 1610, 527, 1057, 1589, 1228, 2123, 652, 507, 2360, 2472, 598, 605, 1668, 152, 325, 1718]}
SAVED model.heads_thresh 0.9016666666666667
RESULT:  {'train_loss': 0.006392252631485462, 'train_acc': 94.67999935150146, 'test_loss': 0.003225994296371937, 'test_acc': 92.5, 'convergence': 23.05847930908203, 'R1': 0, 'dataset_sup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [16, 41]}, 'dataset_unsup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 50, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [16, 41]}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3, 4], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 50, 'lr': 0.005}, 't1': {'blocks': [5], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}}
IN R2:  {'count': 2, 'R0': {'train_loss': 0.014464263804256916, 'train_acc': 84.64999794960022, 'test_loss': 0.005055953748524189, 'test_acc': 82.0, 'convergence': 23.65986442565918, 'R1': 0, 'dataset_sup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [32, 93]}, 'dataset_unsup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 50, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [32, 93]}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3, 4], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 50, 'lr': 0.005}, 't1': {'blocks': [5], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}}, 'model_config': {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}, 'R1': {'train_loss': 0.0061482023447752, 'train_acc': 94.37000155448914, 'test_loss': 0.007179258391261101, 'test_acc': 88.0, 'convergence': 23.331533432006836, 'R1': 0, 'dataset_sup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [35, 92]}, 'dataset_unsup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 50, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [35, 92]}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3, 4], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 50, 'lr': 0.005}, 't1': {'blocks': [5], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}}, 'R2': {'train_loss': 0.006392252631485462, 'train_acc': 94.67999935150146, 'test_loss': 0.003225994296371937, 'test_acc': 92.5, 'convergence': 23.05847930908203, 'R1': 0, 'dataset_sup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [16, 41]}, 'dataset_unsup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 50, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [16, 41]}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3, 4], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 50, 'lr': 0.005}, 't1': {'blocks': [5], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}}}
################################## TASK 3 ############################################
[2, 56]
SEED:  0
block 0, size : 96 16 16
96
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 2.886751345948129
block 1, size : 384 8 8
384
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.8505172717997146
block 2, size : 1536 4 4
1536
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.4252586358998573
block 3, size : 2560 2 2
2560
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.21262931794992865
block 4, size : 2560 1 1
2560
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.1647019614671031
NOWWWW
1 1
2
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
blocks['b%s' % 5] 2560 2560
range = 0.3423265984407288
train_layer_order: consecutive {'t0': {'blocks': [0, 1, 2, 3, 4], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 50, 'lr': 0.005}, 't1': {'blocks': [5], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}
The device used will be: 
True
cuda:0
PARAMSSSS IN LOAD:  {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
avg_deltas:  {'blocks.0.layer.weight': tensor([-5.9425e-02,  4.6107e-03,  9.1470e-03,  7.4974e-03, -6.2646e-01,
         0.0000e+00,  4.0987e-02, -3.5310e-03, -1.0088e-01,  4.6983e-02,
         3.8624e-02,  1.4133e-01, -1.5113e-03,  2.5258e-02,  2.1850e-01,
        -6.7568e-02,  0.0000e+00,  8.5767e-04, -1.3143e-02,  3.5766e-03,
         4.3106e-02,  3.2507e-01, -1.4686e-02,  4.4671e-03,  7.6755e-02,
         0.0000e+00, -1.7707e-02,  1.7228e-03,  6.3087e-02, -1.8436e-02,
         5.0142e-01,  1.1083e-01, -4.7718e-02,  2.4838e-01,  3.2223e-01,
         2.5650e-01,  0.0000e+00, -8.9601e-03,  2.1906e-01,  6.0370e-03,
         1.1743e-01,  2.1399e-02,  4.6723e-01, -9.2209e-03,  2.1992e-01,
         1.3336e-01, -9.0326e-04,  6.4140e-02, -9.6751e-03,  6.9338e-02,
        -9.3173e-02,  3.3182e-02,  1.1179e-01,  2.3805e-01, -3.6064e-02,
         1.6889e-01, -5.6384e-02,  7.4572e-02, -2.7240e-02,  5.0943e-01,
        -1.7200e-02, -4.1931e-03,  3.3258e-03,  6.9868e-03,  6.1792e-01,
        -1.4987e-01, -2.6749e-03, -3.9012e-03,  1.5825e-01,  1.7607e-01,
         5.4258e-01,  2.9316e-02, -1.2535e-02,  2.2970e-02, -1.6736e-02,
         6.3403e-02,  2.8989e-02,  6.3306e-01,  1.2741e-01,  1.7049e-01,
        -3.2663e-02,  2.5349e-01,  5.8460e-01,  4.7269e-02, -3.1581e-04,
        -5.3624e-02, -3.1136e-01, -8.1773e-03,  4.2449e-04,  1.0000e+00,
         2.5712e-03, -9.5579e-02, -7.2559e-02,  3.4898e-01, -9.6754e-03,
         1.1002e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4301e-02,  8.6953e-03,  3.0126e-03,  3.9009e-03,  1.9610e-01,
        -9.3929e-02,  4.4306e-02, -6.4836e-02, -1.0916e-02,  3.7816e-02,
        -1.1031e-03,  6.8237e-02,  4.3740e-02,  5.4395e-02,  2.4431e-03,
         2.8083e-02, -5.7628e-03,  5.9249e-01, -1.0980e-01, -1.4231e-01,
         4.0299e-02,  4.1817e-03,  3.5351e-02,  9.1402e-02,  1.4639e-01,
         1.1838e-01,  0.0000e+00, -9.1319e-02, -2.0171e-02,  5.8165e-02,
         6.4255e-03, -3.3080e-01, -2.2764e-01, -9.4063e-03, -2.7072e-03,
        -1.5896e-02,  2.0474e-02,  6.2648e-02,  1.2202e-03,  6.6576e-03,
         9.3559e-03,  3.4120e-04,  2.3298e-02,  2.0778e-04,  1.3519e-02,
        -6.4451e-05, -1.4235e-01,  1.2419e-02, -1.3542e-01, -1.9586e-01,
        -3.8405e-02,  3.9613e-03, -4.4681e-03,  1.8492e-03, -1.1689e-01,
         2.8412e-02, -3.3030e-03, -8.9514e-02,  3.6833e-02, -5.7454e-03,
         2.4587e-03, -3.1659e-01,  3.2693e-02, -1.3385e-01, -1.5205e-01,
        -3.4440e-04, -8.6145e-02,  1.7937e-02,  2.0698e-02, -3.3556e-02,
         0.0000e+00,  6.2877e-04,  0.0000e+00,  2.0426e-01, -4.8064e-03,
         0.0000e+00,  1.7581e-01, -4.2872e-01, -2.9202e-01,  4.0060e-02,
        -3.9938e-03,  7.0208e-03,  2.7767e-02,  2.0256e-03,  9.3949e-02,
         3.4923e-04,  5.0980e-03, -2.8346e-02, -4.7870e-04,  2.5863e-03,
         1.2981e-02,  1.4565e-04, -9.9445e-03,  1.7168e-03, -1.1461e-01,
        -1.1692e-01,  4.0437e-03,  2.0332e-02, -2.7420e-01, -9.2729e-04,
         3.0638e-01,  6.1992e-03,  3.5989e-03, -8.4824e-02,  0.0000e+00,
        -5.0814e-04, -7.4924e-03, -1.1624e-01, -6.9226e-03,  2.5546e-02,
        -3.2915e-02,  3.2277e-04, -1.5370e-03, -3.3274e-01,  1.0845e-01,
        -6.3231e-02, -5.2442e-02,  4.7349e-02, -7.0031e-03, -2.5018e-02,
        -1.9860e-01, -2.2843e-01,  5.0290e-03,  1.8244e-01,  8.8407e-03,
        -2.2282e-01, -1.3848e-02, -2.9021e-01,  3.2427e-02,  4.6442e-03,
         7.7990e-03,  4.3598e-02,  1.5871e-02,  2.1338e-03,  8.0321e-02,
         6.3615e-02,  1.3045e-04,  7.6888e-02,  1.4460e-01,  3.7224e-02,
         6.1758e-01,  1.2456e-04,  8.6010e-02, -1.3991e-02,  6.9488e-01,
         5.6556e-03, -1.0701e-03, -3.2803e-02,  1.7390e-02, -1.8409e-05,
        -3.7781e-02, -5.2198e-02,  1.8820e-02,  1.4578e-02, -1.7325e-04,
        -2.0112e-02, -2.6558e-01, -5.0111e-01,  5.8467e-02, -2.3437e-02,
        -1.0181e-02,  5.1184e-03,  6.6237e-03, -3.3717e-02, -5.4317e-02,
         6.1498e-02, -1.7371e-01,  1.2783e-03,  8.4734e-02,  0.0000e+00,
        -2.2882e-02,  1.0517e-02,  1.1033e-02,  3.0045e-02,  6.0721e-02,
        -9.2232e-02,  1.2838e-01, -1.1452e-04,  3.0181e-02,  1.5663e-01,
         0.0000e+00,  1.5825e-02,  2.5877e-02,  5.7032e-04, -1.1661e-02,
         4.6237e-03,  2.0629e-02,  2.7096e-03,  1.2248e-02, -2.9423e-01,
         2.6418e-01, -2.3124e-03,  2.5419e-03,  1.3352e-02,  8.4081e-03,
         5.4158e-01,  6.0407e-04,  3.0894e-02,  1.1742e-03, -9.2861e-03,
         3.1449e-01, -3.1219e-02,  0.0000e+00,  1.0989e-01, -2.5976e-02,
         1.3480e-02,  8.5768e-02, -9.9720e-05, -1.1723e-02,  1.2131e-01,
        -3.7995e-02,  1.9493e-02,  1.7359e-04, -5.4308e-02, -2.0628e-03,
         1.1838e-02,  1.6199e-02,  3.6041e-02, -1.7212e-01,  7.1680e-02,
         5.9452e-04,  1.6003e-01, -8.4035e-02, -4.3205e-03,  2.2865e-01,
        -1.3606e-01,  1.8698e-02, -2.5199e-01,  7.3340e-03,  3.4516e-02,
        -5.6091e-03,  9.4647e-05,  2.1698e-02,  3.8162e-02,  5.9559e-02,
        -7.8298e-02,  6.1622e-02,  1.0815e-03,  1.0290e-01,  2.2198e-02,
        -1.0804e-01, -3.5264e-02, -9.7168e-05, -2.2692e-01, -1.7221e-03,
        -8.5281e-02,  1.8435e-02, -1.6534e-01,  4.4417e-02,  4.8840e-04,
         1.0261e-02,  8.3513e-03, -2.6471e-02,  9.3889e-03, -7.0732e-02,
         1.7080e-02,  4.8038e-04,  4.7729e-03, -2.8907e-03,  1.7381e-03,
         1.2990e-03, -5.5237e-04,  1.0000e+00,  9.2932e-03, -1.0981e-01,
         1.2186e-02, -9.1816e-01,  9.7915e-03,  4.8597e-04,  4.1370e-04,
        -7.7339e-02,  1.3608e-01,  2.1453e-03, -2.2491e-04,  1.0329e-01,
        -9.4133e-03,  3.6525e-02,  7.2587e-02,  2.0975e-02,  1.1137e-03,
        -2.5153e-02,  8.8089e-02,  7.1914e-02,  5.4411e-04,  1.2415e-02,
         5.4635e-02, -5.3188e-03, -7.6328e-02,  2.9564e-02, -9.3678e-02,
        -2.3656e-02, -4.7282e-01,  5.4493e-02,  3.5857e-02, -1.5161e-02,
        -2.3324e-01,  0.0000e+00, -1.3668e-02,  7.4648e-02,  3.3564e-02,
        -2.2814e-01, -9.1456e-02,  2.9410e-03, -3.3803e-02, -1.0046e-02,
         2.4779e-02,  9.4589e-02,  5.9049e-03,  3.0233e-03, -1.0984e-02,
        -1.1827e-02, -4.1325e-03,  7.1423e-02, -1.0690e-02,  2.0277e-02,
        -5.4784e-04,  2.5317e-03, -2.2823e-03, -2.0938e-02,  2.1727e-03,
        -7.3736e-04,  4.1919e-03,  9.8474e-02, -2.1832e-02,  0.0000e+00,
         7.5138e-02,  5.2770e-02,  1.0778e-01, -3.7484e-02,  3.4896e-01,
        -4.2573e-01,  3.0404e-02, -1.6727e-01,  4.1312e-02,  1.1351e-03,
         5.9629e-02,  0.0000e+00, -3.2564e-03,  4.2253e-02,  1.1126e-02,
         1.1247e-03,  6.5109e-03, -7.4668e-03,  2.2520e-01, -7.2055e-02,
        -9.0514e-02,  1.3468e-02,  1.9159e-01,  6.3641e-02, -1.6730e-02,
         3.4153e-03, -1.1501e-03, -1.1612e-01,  3.5650e-02,  7.0143e-02,
        -4.7214e-01, -8.4162e-03,  2.5387e-01,  3.6004e-02, -9.3415e-03,
         2.7332e-03,  1.3710e-03, -9.2995e-03,  3.0839e-02,  5.6756e-02,
        -7.8309e-02, -7.1371e-05,  8.7107e-02,  2.8666e-02, -4.3266e-03,
         1.8490e-03,  1.6550e-01,  2.0378e-02,  0.0000e+00, -9.7869e-02,
         5.3253e-02, -7.4405e-03,  1.4890e-02,  8.1428e-03,  4.0072e-02,
         5.9626e-02,  2.3852e-02, -1.4129e-02,  9.5861e-04], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0018, -0.0100,  0.1678,  ...,  0.0216, -0.1948,  0.0158],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0243, -0.0126,  0.0000,  ..., -0.0114, -0.0067,  0.0179],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([-0.0112, -0.0154,  0.0029,  ...,  0.0092, -0.0103,  0.0004],
       device='cuda:0')}
topk_layer:  None
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-5.9425e-02,  4.6107e-03,  9.1470e-03,  7.4974e-03, -6.2646e-01,
         0.0000e+00,  4.0987e-02, -3.5310e-03, -1.0088e-01,  4.6983e-02,
         3.8624e-02,  1.4133e-01, -1.5113e-03,  2.5258e-02,  2.1850e-01,
        -6.7568e-02,  0.0000e+00,  8.5767e-04, -1.3143e-02,  3.5766e-03,
         4.3106e-02,  3.2507e-01, -1.4686e-02,  4.4671e-03,  7.6755e-02,
         0.0000e+00, -1.7707e-02,  1.7228e-03,  6.3087e-02, -1.8436e-02,
         5.0142e-01,  1.1083e-01, -4.7718e-02,  2.4838e-01,  3.2223e-01,
         2.5650e-01,  0.0000e+00, -8.9601e-03,  2.1906e-01,  6.0370e-03,
         1.1743e-01,  2.1399e-02,  4.6723e-01, -9.2209e-03,  2.1992e-01,
         1.3336e-01, -9.0326e-04,  6.4140e-02, -9.6751e-03,  6.9338e-02,
        -9.3173e-02,  3.3182e-02,  1.1179e-01,  2.3805e-01, -3.6064e-02,
         1.6889e-01, -5.6384e-02,  7.4572e-02, -2.7240e-02,  5.0943e-01,
        -1.7200e-02, -4.1931e-03,  3.3258e-03,  6.9868e-03,  6.1792e-01,
        -1.4987e-01, -2.6749e-03, -3.9012e-03,  1.5825e-01,  1.7607e-01,
         5.4258e-01,  2.9316e-02, -1.2535e-02,  2.2970e-02, -1.6736e-02,
         6.3403e-02,  2.8989e-02,  6.3306e-01,  1.2741e-01,  1.7049e-01,
        -3.2663e-02,  2.5349e-01,  5.8460e-01,  4.7269e-02, -3.1581e-04,
        -5.3624e-02, -3.1136e-01, -8.1773e-03,  4.2449e-04,  1.0000e+00,
         2.5712e-03, -9.5579e-02, -7.2559e-02,  3.4898e-01, -9.6754e-03,
         1.1002e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4301e-02,  8.6953e-03,  3.0126e-03,  3.9009e-03,  1.9610e-01,
        -9.3929e-02,  4.4306e-02, -6.4836e-02, -1.0916e-02,  3.7816e-02,
        -1.1031e-03,  6.8237e-02,  4.3740e-02,  5.4395e-02,  2.4431e-03,
         2.8083e-02, -5.7628e-03,  5.9249e-01, -1.0980e-01, -1.4231e-01,
         4.0299e-02,  4.1817e-03,  3.5351e-02,  9.1402e-02,  1.4639e-01,
         1.1838e-01,  0.0000e+00, -9.1319e-02, -2.0171e-02,  5.8165e-02,
         6.4255e-03, -3.3080e-01, -2.2764e-01, -9.4063e-03, -2.7072e-03,
        -1.5896e-02,  2.0474e-02,  6.2648e-02,  1.2202e-03,  6.6576e-03,
         9.3559e-03,  3.4120e-04,  2.3298e-02,  2.0778e-04,  1.3519e-02,
        -6.4451e-05, -1.4235e-01,  1.2419e-02, -1.3542e-01, -1.9586e-01,
        -3.8405e-02,  3.9613e-03, -4.4681e-03,  1.8492e-03, -1.1689e-01,
         2.8412e-02, -3.3030e-03, -8.9514e-02,  3.6833e-02, -5.7454e-03,
         2.4587e-03, -3.1659e-01,  3.2693e-02, -1.3385e-01, -1.5205e-01,
        -3.4440e-04, -8.6145e-02,  1.7937e-02,  2.0698e-02, -3.3556e-02,
         0.0000e+00,  6.2877e-04,  0.0000e+00,  2.0426e-01, -4.8064e-03,
         0.0000e+00,  1.7581e-01, -4.2872e-01, -2.9202e-01,  4.0060e-02,
        -3.9938e-03,  7.0208e-03,  2.7767e-02,  2.0256e-03,  9.3949e-02,
         3.4923e-04,  5.0980e-03, -2.8346e-02, -4.7870e-04,  2.5863e-03,
         1.2981e-02,  1.4565e-04, -9.9445e-03,  1.7168e-03, -1.1461e-01,
        -1.1692e-01,  4.0437e-03,  2.0332e-02, -2.7420e-01, -9.2729e-04,
         3.0638e-01,  6.1992e-03,  3.5989e-03, -8.4824e-02,  0.0000e+00,
        -5.0814e-04, -7.4924e-03, -1.1624e-01, -6.9226e-03,  2.5546e-02,
        -3.2915e-02,  3.2277e-04, -1.5370e-03, -3.3274e-01,  1.0845e-01,
        -6.3231e-02, -5.2442e-02,  4.7349e-02, -7.0031e-03, -2.5018e-02,
        -1.9860e-01, -2.2843e-01,  5.0290e-03,  1.8244e-01,  8.8407e-03,
        -2.2282e-01, -1.3848e-02, -2.9021e-01,  3.2427e-02,  4.6442e-03,
         7.7990e-03,  4.3598e-02,  1.5871e-02,  2.1338e-03,  8.0321e-02,
         6.3615e-02,  1.3045e-04,  7.6888e-02,  1.4460e-01,  3.7224e-02,
         6.1758e-01,  1.2456e-04,  8.6010e-02, -1.3991e-02,  6.9488e-01,
         5.6556e-03, -1.0701e-03, -3.2803e-02,  1.7390e-02, -1.8409e-05,
        -3.7781e-02, -5.2198e-02,  1.8820e-02,  1.4578e-02, -1.7325e-04,
        -2.0112e-02, -2.6558e-01, -5.0111e-01,  5.8467e-02, -2.3437e-02,
        -1.0181e-02,  5.1184e-03,  6.6237e-03, -3.3717e-02, -5.4317e-02,
         6.1498e-02, -1.7371e-01,  1.2783e-03,  8.4734e-02,  0.0000e+00,
        -2.2882e-02,  1.0517e-02,  1.1033e-02,  3.0045e-02,  6.0721e-02,
        -9.2232e-02,  1.2838e-01, -1.1452e-04,  3.0181e-02,  1.5663e-01,
         0.0000e+00,  1.5825e-02,  2.5877e-02,  5.7032e-04, -1.1661e-02,
         4.6237e-03,  2.0629e-02,  2.7096e-03,  1.2248e-02, -2.9423e-01,
         2.6418e-01, -2.3124e-03,  2.5419e-03,  1.3352e-02,  8.4081e-03,
         5.4158e-01,  6.0407e-04,  3.0894e-02,  1.1742e-03, -9.2861e-03,
         3.1449e-01, -3.1219e-02,  0.0000e+00,  1.0989e-01, -2.5976e-02,
         1.3480e-02,  8.5768e-02, -9.9720e-05, -1.1723e-02,  1.2131e-01,
        -3.7995e-02,  1.9493e-02,  1.7359e-04, -5.4308e-02, -2.0628e-03,
         1.1838e-02,  1.6199e-02,  3.6041e-02, -1.7212e-01,  7.1680e-02,
         5.9452e-04,  1.6003e-01, -8.4035e-02, -4.3205e-03,  2.2865e-01,
        -1.3606e-01,  1.8698e-02, -2.5199e-01,  7.3340e-03,  3.4516e-02,
        -5.6091e-03,  9.4647e-05,  2.1698e-02,  3.8162e-02,  5.9559e-02,
        -7.8298e-02,  6.1622e-02,  1.0815e-03,  1.0290e-01,  2.2198e-02,
        -1.0804e-01, -3.5264e-02, -9.7168e-05, -2.2692e-01, -1.7221e-03,
        -8.5281e-02,  1.8435e-02, -1.6534e-01,  4.4417e-02,  4.8840e-04,
         1.0261e-02,  8.3513e-03, -2.6471e-02,  9.3889e-03, -7.0732e-02,
         1.7080e-02,  4.8038e-04,  4.7729e-03, -2.8907e-03,  1.7381e-03,
         1.2990e-03, -5.5237e-04,  1.0000e+00,  9.2932e-03, -1.0981e-01,
         1.2186e-02, -9.1816e-01,  9.7915e-03,  4.8597e-04,  4.1370e-04,
        -7.7339e-02,  1.3608e-01,  2.1453e-03, -2.2491e-04,  1.0329e-01,
        -9.4133e-03,  3.6525e-02,  7.2587e-02,  2.0975e-02,  1.1137e-03,
        -2.5153e-02,  8.8089e-02,  7.1914e-02,  5.4411e-04,  1.2415e-02,
         5.4635e-02, -5.3188e-03, -7.6328e-02,  2.9564e-02, -9.3678e-02,
        -2.3656e-02, -4.7282e-01,  5.4493e-02,  3.5857e-02, -1.5161e-02,
        -2.3324e-01,  0.0000e+00, -1.3668e-02,  7.4648e-02,  3.3564e-02,
        -2.2814e-01, -9.1456e-02,  2.9410e-03, -3.3803e-02, -1.0046e-02,
         2.4779e-02,  9.4589e-02,  5.9049e-03,  3.0233e-03, -1.0984e-02,
        -1.1827e-02, -4.1325e-03,  7.1423e-02, -1.0690e-02,  2.0277e-02,
        -5.4784e-04,  2.5317e-03, -2.2823e-03, -2.0938e-02,  2.1727e-03,
        -7.3736e-04,  4.1919e-03,  9.8474e-02, -2.1832e-02,  0.0000e+00,
         7.5138e-02,  5.2770e-02,  1.0778e-01, -3.7484e-02,  3.4896e-01,
        -4.2573e-01,  3.0404e-02, -1.6727e-01,  4.1312e-02,  1.1351e-03,
         5.9629e-02,  0.0000e+00, -3.2564e-03,  4.2253e-02,  1.1126e-02,
         1.1247e-03,  6.5109e-03, -7.4668e-03,  2.2520e-01, -7.2055e-02,
        -9.0514e-02,  1.3468e-02,  1.9159e-01,  6.3641e-02, -1.6730e-02,
         3.4153e-03, -1.1501e-03, -1.1612e-01,  3.5650e-02,  7.0143e-02,
        -4.7214e-01, -8.4162e-03,  2.5387e-01,  3.6004e-02, -9.3415e-03,
         2.7332e-03,  1.3710e-03, -9.2995e-03,  3.0839e-02,  5.6756e-02,
        -7.8309e-02, -7.1371e-05,  8.7107e-02,  2.8666e-02, -4.3266e-03,
         1.8490e-03,  1.6550e-01,  2.0378e-02,  0.0000e+00, -9.7869e-02,
         5.3253e-02, -7.4405e-03,  1.4890e-02,  8.1428e-03,  4.0072e-02,
         5.9626e-02,  2.3852e-02, -1.4129e-02,  9.5861e-04], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0018, -0.0100,  0.1678,  ...,  0.0216, -0.1948,  0.0158],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0243, -0.0126,  0.0000,  ..., -0.0114, -0.0067,  0.0179],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([-0.0112, -0.0154,  0.0029,  ...,  0.0092, -0.0103,  0.0004],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.0.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([-5.9425e-02,  4.6107e-03,  9.1470e-03,  7.4974e-03, -6.2646e-01,
         0.0000e+00,  4.0987e-02, -3.5310e-03, -1.0088e-01,  4.6983e-02,
         3.8624e-02,  1.4133e-01, -1.5113e-03,  2.5258e-02,  2.1850e-01,
        -6.7568e-02,  0.0000e+00,  8.5767e-04, -1.3143e-02,  3.5766e-03,
         4.3106e-02,  3.2507e-01, -1.4686e-02,  4.4671e-03,  7.6755e-02,
         0.0000e+00, -1.7707e-02,  1.7228e-03,  6.3087e-02, -1.8436e-02,
         5.0142e-01,  1.1083e-01, -4.7718e-02,  2.4838e-01,  3.2223e-01,
         2.5650e-01,  0.0000e+00, -8.9601e-03,  2.1906e-01,  6.0370e-03,
         1.1743e-01,  2.1399e-02,  4.6723e-01, -9.2209e-03,  2.1992e-01,
         1.3336e-01, -9.0326e-04,  6.4140e-02, -9.6751e-03,  6.9338e-02,
        -9.3173e-02,  3.3182e-02,  1.1179e-01,  2.3805e-01, -3.6064e-02,
         1.6889e-01, -5.6384e-02,  7.4572e-02, -2.7240e-02,  5.0943e-01,
        -1.7200e-02, -4.1931e-03,  3.3258e-03,  6.9868e-03,  6.1792e-01,
        -1.4987e-01, -2.6749e-03, -3.9012e-03,  1.5825e-01,  1.7607e-01,
         5.4258e-01,  2.9316e-02, -1.2535e-02,  2.2970e-02, -1.6736e-02,
         6.3403e-02,  2.8989e-02,  6.3306e-01,  1.2741e-01,  1.7049e-01,
        -3.2663e-02,  2.5349e-01,  5.8460e-01,  4.7269e-02, -3.1581e-04,
        -5.3624e-02, -3.1136e-01, -8.1773e-03,  4.2449e-04,  1.0000e+00,
         2.5712e-03, -9.5579e-02, -7.2559e-02,  3.4898e-01, -9.6754e-03,
         1.1002e-01], device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-5.9425e-02,  4.6107e-03,  9.1470e-03,  7.4974e-03, -6.2646e-01,
         0.0000e+00,  4.0987e-02, -3.5310e-03, -1.0088e-01,  4.6983e-02,
         3.8624e-02,  1.4133e-01, -1.5113e-03,  2.5258e-02,  2.1850e-01,
        -6.7568e-02,  0.0000e+00,  8.5767e-04, -1.3143e-02,  3.5766e-03,
         4.3106e-02,  3.2507e-01, -1.4686e-02,  4.4671e-03,  7.6755e-02,
         0.0000e+00, -1.7707e-02,  1.7228e-03,  6.3087e-02, -1.8436e-02,
         5.0142e-01,  1.1083e-01, -4.7718e-02,  2.4838e-01,  3.2223e-01,
         2.5650e-01,  0.0000e+00, -8.9601e-03,  2.1906e-01,  6.0370e-03,
         1.1743e-01,  2.1399e-02,  4.6723e-01, -9.2209e-03,  2.1992e-01,
         1.3336e-01, -9.0326e-04,  6.4140e-02, -9.6751e-03,  6.9338e-02,
        -9.3173e-02,  3.3182e-02,  1.1179e-01,  2.3805e-01, -3.6064e-02,
         1.6889e-01, -5.6384e-02,  7.4572e-02, -2.7240e-02,  5.0943e-01,
        -1.7200e-02, -4.1931e-03,  3.3258e-03,  6.9868e-03,  6.1792e-01,
        -1.4987e-01, -2.6749e-03, -3.9012e-03,  1.5825e-01,  1.7607e-01,
         5.4258e-01,  2.9316e-02, -1.2535e-02,  2.2970e-02, -1.6736e-02,
         6.3403e-02,  2.8989e-02,  6.3306e-01,  1.2741e-01,  1.7049e-01,
        -3.2663e-02,  2.5349e-01,  5.8460e-01,  4.7269e-02, -3.1581e-04,
        -5.3624e-02, -3.1136e-01, -8.1773e-03,  4.2449e-04,  1.0000e+00,
         2.5712e-03, -9.5579e-02, -7.2559e-02,  3.4898e-01, -9.6754e-03,
         1.1002e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4301e-02,  8.6953e-03,  3.0126e-03,  3.9009e-03,  1.9610e-01,
        -9.3929e-02,  4.4306e-02, -6.4836e-02, -1.0916e-02,  3.7816e-02,
        -1.1031e-03,  6.8237e-02,  4.3740e-02,  5.4395e-02,  2.4431e-03,
         2.8083e-02, -5.7628e-03,  5.9249e-01, -1.0980e-01, -1.4231e-01,
         4.0299e-02,  4.1817e-03,  3.5351e-02,  9.1402e-02,  1.4639e-01,
         1.1838e-01,  0.0000e+00, -9.1319e-02, -2.0171e-02,  5.8165e-02,
         6.4255e-03, -3.3080e-01, -2.2764e-01, -9.4063e-03, -2.7072e-03,
        -1.5896e-02,  2.0474e-02,  6.2648e-02,  1.2202e-03,  6.6576e-03,
         9.3559e-03,  3.4120e-04,  2.3298e-02,  2.0778e-04,  1.3519e-02,
        -6.4451e-05, -1.4235e-01,  1.2419e-02, -1.3542e-01, -1.9586e-01,
        -3.8405e-02,  3.9613e-03, -4.4681e-03,  1.8492e-03, -1.1689e-01,
         2.8412e-02, -3.3030e-03, -8.9514e-02,  3.6833e-02, -5.7454e-03,
         2.4587e-03, -3.1659e-01,  3.2693e-02, -1.3385e-01, -1.5205e-01,
        -3.4440e-04, -8.6145e-02,  1.7937e-02,  2.0698e-02, -3.3556e-02,
         0.0000e+00,  6.2877e-04,  0.0000e+00,  2.0426e-01, -4.8064e-03,
         0.0000e+00,  1.7581e-01, -4.2872e-01, -2.9202e-01,  4.0060e-02,
        -3.9938e-03,  7.0208e-03,  2.7767e-02,  2.0256e-03,  9.3949e-02,
         3.4923e-04,  5.0980e-03, -2.8346e-02, -4.7870e-04,  2.5863e-03,
         1.2981e-02,  1.4565e-04, -9.9445e-03,  1.7168e-03, -1.1461e-01,
        -1.1692e-01,  4.0437e-03,  2.0332e-02, -2.7420e-01, -9.2729e-04,
         3.0638e-01,  6.1992e-03,  3.5989e-03, -8.4824e-02,  0.0000e+00,
        -5.0814e-04, -7.4924e-03, -1.1624e-01, -6.9226e-03,  2.5546e-02,
        -3.2915e-02,  3.2277e-04, -1.5370e-03, -3.3274e-01,  1.0845e-01,
        -6.3231e-02, -5.2442e-02,  4.7349e-02, -7.0031e-03, -2.5018e-02,
        -1.9860e-01, -2.2843e-01,  5.0290e-03,  1.8244e-01,  8.8407e-03,
        -2.2282e-01, -1.3848e-02, -2.9021e-01,  3.2427e-02,  4.6442e-03,
         7.7990e-03,  4.3598e-02,  1.5871e-02,  2.1338e-03,  8.0321e-02,
         6.3615e-02,  1.3045e-04,  7.6888e-02,  1.4460e-01,  3.7224e-02,
         6.1758e-01,  1.2456e-04,  8.6010e-02, -1.3991e-02,  6.9488e-01,
         5.6556e-03, -1.0701e-03, -3.2803e-02,  1.7390e-02, -1.8409e-05,
        -3.7781e-02, -5.2198e-02,  1.8820e-02,  1.4578e-02, -1.7325e-04,
        -2.0112e-02, -2.6558e-01, -5.0111e-01,  5.8467e-02, -2.3437e-02,
        -1.0181e-02,  5.1184e-03,  6.6237e-03, -3.3717e-02, -5.4317e-02,
         6.1498e-02, -1.7371e-01,  1.2783e-03,  8.4734e-02,  0.0000e+00,
        -2.2882e-02,  1.0517e-02,  1.1033e-02,  3.0045e-02,  6.0721e-02,
        -9.2232e-02,  1.2838e-01, -1.1452e-04,  3.0181e-02,  1.5663e-01,
         0.0000e+00,  1.5825e-02,  2.5877e-02,  5.7032e-04, -1.1661e-02,
         4.6237e-03,  2.0629e-02,  2.7096e-03,  1.2248e-02, -2.9423e-01,
         2.6418e-01, -2.3124e-03,  2.5419e-03,  1.3352e-02,  8.4081e-03,
         5.4158e-01,  6.0407e-04,  3.0894e-02,  1.1742e-03, -9.2861e-03,
         3.1449e-01, -3.1219e-02,  0.0000e+00,  1.0989e-01, -2.5976e-02,
         1.3480e-02,  8.5768e-02, -9.9720e-05, -1.1723e-02,  1.2131e-01,
        -3.7995e-02,  1.9493e-02,  1.7359e-04, -5.4308e-02, -2.0628e-03,
         1.1838e-02,  1.6199e-02,  3.6041e-02, -1.7212e-01,  7.1680e-02,
         5.9452e-04,  1.6003e-01, -8.4035e-02, -4.3205e-03,  2.2865e-01,
        -1.3606e-01,  1.8698e-02, -2.5199e-01,  7.3340e-03,  3.4516e-02,
        -5.6091e-03,  9.4647e-05,  2.1698e-02,  3.8162e-02,  5.9559e-02,
        -7.8298e-02,  6.1622e-02,  1.0815e-03,  1.0290e-01,  2.2198e-02,
        -1.0804e-01, -3.5264e-02, -9.7168e-05, -2.2692e-01, -1.7221e-03,
        -8.5281e-02,  1.8435e-02, -1.6534e-01,  4.4417e-02,  4.8840e-04,
         1.0261e-02,  8.3513e-03, -2.6471e-02,  9.3889e-03, -7.0732e-02,
         1.7080e-02,  4.8038e-04,  4.7729e-03, -2.8907e-03,  1.7381e-03,
         1.2990e-03, -5.5237e-04,  1.0000e+00,  9.2932e-03, -1.0981e-01,
         1.2186e-02, -9.1816e-01,  9.7915e-03,  4.8597e-04,  4.1370e-04,
        -7.7339e-02,  1.3608e-01,  2.1453e-03, -2.2491e-04,  1.0329e-01,
        -9.4133e-03,  3.6525e-02,  7.2587e-02,  2.0975e-02,  1.1137e-03,
        -2.5153e-02,  8.8089e-02,  7.1914e-02,  5.4411e-04,  1.2415e-02,
         5.4635e-02, -5.3188e-03, -7.6328e-02,  2.9564e-02, -9.3678e-02,
        -2.3656e-02, -4.7282e-01,  5.4493e-02,  3.5857e-02, -1.5161e-02,
        -2.3324e-01,  0.0000e+00, -1.3668e-02,  7.4648e-02,  3.3564e-02,
        -2.2814e-01, -9.1456e-02,  2.9410e-03, -3.3803e-02, -1.0046e-02,
         2.4779e-02,  9.4589e-02,  5.9049e-03,  3.0233e-03, -1.0984e-02,
        -1.1827e-02, -4.1325e-03,  7.1423e-02, -1.0690e-02,  2.0277e-02,
        -5.4784e-04,  2.5317e-03, -2.2823e-03, -2.0938e-02,  2.1727e-03,
        -7.3736e-04,  4.1919e-03,  9.8474e-02, -2.1832e-02,  0.0000e+00,
         7.5138e-02,  5.2770e-02,  1.0778e-01, -3.7484e-02,  3.4896e-01,
        -4.2573e-01,  3.0404e-02, -1.6727e-01,  4.1312e-02,  1.1351e-03,
         5.9629e-02,  0.0000e+00, -3.2564e-03,  4.2253e-02,  1.1126e-02,
         1.1247e-03,  6.5109e-03, -7.4668e-03,  2.2520e-01, -7.2055e-02,
        -9.0514e-02,  1.3468e-02,  1.9159e-01,  6.3641e-02, -1.6730e-02,
         3.4153e-03, -1.1501e-03, -1.1612e-01,  3.5650e-02,  7.0143e-02,
        -4.7214e-01, -8.4162e-03,  2.5387e-01,  3.6004e-02, -9.3415e-03,
         2.7332e-03,  1.3710e-03, -9.2995e-03,  3.0839e-02,  5.6756e-02,
        -7.8309e-02, -7.1371e-05,  8.7107e-02,  2.8666e-02, -4.3266e-03,
         1.8490e-03,  1.6550e-01,  2.0378e-02,  0.0000e+00, -9.7869e-02,
         5.3253e-02, -7.4405e-03,  1.4890e-02,  8.1428e-03,  4.0072e-02,
         5.9626e-02,  2.3852e-02, -1.4129e-02,  9.5861e-04], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0018, -0.0100,  0.1678,  ...,  0.0216, -0.1948,  0.0158],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0243, -0.0126,  0.0000,  ..., -0.0114, -0.0067,  0.0179],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([-0.0112, -0.0154,  0.0029,  ...,  0.0092, -0.0103,  0.0004],
       device='cuda:0')}
topk_layer inside model after retrieval:  [25, 59, 5, 82, 30, 89, 16, 77, 33, 42, 70, 93, 45, 68, 64]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-5.9425e-02,  4.6107e-03,  9.1470e-03,  7.4974e-03, -6.2646e-01,
         0.0000e+00,  4.0987e-02, -3.5310e-03, -1.0088e-01,  4.6983e-02,
         3.8624e-02,  1.4133e-01, -1.5113e-03,  2.5258e-02,  2.1850e-01,
        -6.7568e-02,  0.0000e+00,  8.5767e-04, -1.3143e-02,  3.5766e-03,
         4.3106e-02,  3.2507e-01, -1.4686e-02,  4.4671e-03,  7.6755e-02,
         0.0000e+00, -1.7707e-02,  1.7228e-03,  6.3087e-02, -1.8436e-02,
         5.0142e-01,  1.1083e-01, -4.7718e-02,  2.4838e-01,  3.2223e-01,
         2.5650e-01,  0.0000e+00, -8.9601e-03,  2.1906e-01,  6.0370e-03,
         1.1743e-01,  2.1399e-02,  4.6723e-01, -9.2209e-03,  2.1992e-01,
         1.3336e-01, -9.0326e-04,  6.4140e-02, -9.6751e-03,  6.9338e-02,
        -9.3173e-02,  3.3182e-02,  1.1179e-01,  2.3805e-01, -3.6064e-02,
         1.6889e-01, -5.6384e-02,  7.4572e-02, -2.7240e-02,  5.0943e-01,
        -1.7200e-02, -4.1931e-03,  3.3258e-03,  6.9868e-03,  6.1792e-01,
        -1.4987e-01, -2.6749e-03, -3.9012e-03,  1.5825e-01,  1.7607e-01,
         5.4258e-01,  2.9316e-02, -1.2535e-02,  2.2970e-02, -1.6736e-02,
         6.3403e-02,  2.8989e-02,  6.3306e-01,  1.2741e-01,  1.7049e-01,
        -3.2663e-02,  2.5349e-01,  5.8460e-01,  4.7269e-02, -3.1581e-04,
        -5.3624e-02, -3.1136e-01, -8.1773e-03,  4.2449e-04,  1.0000e+00,
         2.5712e-03, -9.5579e-02, -7.2559e-02,  3.4898e-01, -9.6754e-03,
         1.1002e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4301e-02,  8.6953e-03,  3.0126e-03,  3.9009e-03,  1.9610e-01,
        -9.3929e-02,  4.4306e-02, -6.4836e-02, -1.0916e-02,  3.7816e-02,
        -1.1031e-03,  6.8237e-02,  4.3740e-02,  5.4395e-02,  2.4431e-03,
         2.8083e-02, -5.7628e-03,  5.9249e-01, -1.0980e-01, -1.4231e-01,
         4.0299e-02,  4.1817e-03,  3.5351e-02,  9.1402e-02,  1.4639e-01,
         1.1838e-01,  0.0000e+00, -9.1319e-02, -2.0171e-02,  5.8165e-02,
         6.4255e-03, -3.3080e-01, -2.2764e-01, -9.4063e-03, -2.7072e-03,
        -1.5896e-02,  2.0474e-02,  6.2648e-02,  1.2202e-03,  6.6576e-03,
         9.3559e-03,  3.4120e-04,  2.3298e-02,  2.0778e-04,  1.3519e-02,
        -6.4451e-05, -1.4235e-01,  1.2419e-02, -1.3542e-01, -1.9586e-01,
        -3.8405e-02,  3.9613e-03, -4.4681e-03,  1.8492e-03, -1.1689e-01,
         2.8412e-02, -3.3030e-03, -8.9514e-02,  3.6833e-02, -5.7454e-03,
         2.4587e-03, -3.1659e-01,  3.2693e-02, -1.3385e-01, -1.5205e-01,
        -3.4440e-04, -8.6145e-02,  1.7937e-02,  2.0698e-02, -3.3556e-02,
         0.0000e+00,  6.2877e-04,  0.0000e+00,  2.0426e-01, -4.8064e-03,
         0.0000e+00,  1.7581e-01, -4.2872e-01, -2.9202e-01,  4.0060e-02,
        -3.9938e-03,  7.0208e-03,  2.7767e-02,  2.0256e-03,  9.3949e-02,
         3.4923e-04,  5.0980e-03, -2.8346e-02, -4.7870e-04,  2.5863e-03,
         1.2981e-02,  1.4565e-04, -9.9445e-03,  1.7168e-03, -1.1461e-01,
        -1.1692e-01,  4.0437e-03,  2.0332e-02, -2.7420e-01, -9.2729e-04,
         3.0638e-01,  6.1992e-03,  3.5989e-03, -8.4824e-02,  0.0000e+00,
        -5.0814e-04, -7.4924e-03, -1.1624e-01, -6.9226e-03,  2.5546e-02,
        -3.2915e-02,  3.2277e-04, -1.5370e-03, -3.3274e-01,  1.0845e-01,
        -6.3231e-02, -5.2442e-02,  4.7349e-02, -7.0031e-03, -2.5018e-02,
        -1.9860e-01, -2.2843e-01,  5.0290e-03,  1.8244e-01,  8.8407e-03,
        -2.2282e-01, -1.3848e-02, -2.9021e-01,  3.2427e-02,  4.6442e-03,
         7.7990e-03,  4.3598e-02,  1.5871e-02,  2.1338e-03,  8.0321e-02,
         6.3615e-02,  1.3045e-04,  7.6888e-02,  1.4460e-01,  3.7224e-02,
         6.1758e-01,  1.2456e-04,  8.6010e-02, -1.3991e-02,  6.9488e-01,
         5.6556e-03, -1.0701e-03, -3.2803e-02,  1.7390e-02, -1.8409e-05,
        -3.7781e-02, -5.2198e-02,  1.8820e-02,  1.4578e-02, -1.7325e-04,
        -2.0112e-02, -2.6558e-01, -5.0111e-01,  5.8467e-02, -2.3437e-02,
        -1.0181e-02,  5.1184e-03,  6.6237e-03, -3.3717e-02, -5.4317e-02,
         6.1498e-02, -1.7371e-01,  1.2783e-03,  8.4734e-02,  0.0000e+00,
        -2.2882e-02,  1.0517e-02,  1.1033e-02,  3.0045e-02,  6.0721e-02,
        -9.2232e-02,  1.2838e-01, -1.1452e-04,  3.0181e-02,  1.5663e-01,
         0.0000e+00,  1.5825e-02,  2.5877e-02,  5.7032e-04, -1.1661e-02,
         4.6237e-03,  2.0629e-02,  2.7096e-03,  1.2248e-02, -2.9423e-01,
         2.6418e-01, -2.3124e-03,  2.5419e-03,  1.3352e-02,  8.4081e-03,
         5.4158e-01,  6.0407e-04,  3.0894e-02,  1.1742e-03, -9.2861e-03,
         3.1449e-01, -3.1219e-02,  0.0000e+00,  1.0989e-01, -2.5976e-02,
         1.3480e-02,  8.5768e-02, -9.9720e-05, -1.1723e-02,  1.2131e-01,
        -3.7995e-02,  1.9493e-02,  1.7359e-04, -5.4308e-02, -2.0628e-03,
         1.1838e-02,  1.6199e-02,  3.6041e-02, -1.7212e-01,  7.1680e-02,
         5.9452e-04,  1.6003e-01, -8.4035e-02, -4.3205e-03,  2.2865e-01,
        -1.3606e-01,  1.8698e-02, -2.5199e-01,  7.3340e-03,  3.4516e-02,
        -5.6091e-03,  9.4647e-05,  2.1698e-02,  3.8162e-02,  5.9559e-02,
        -7.8298e-02,  6.1622e-02,  1.0815e-03,  1.0290e-01,  2.2198e-02,
        -1.0804e-01, -3.5264e-02, -9.7168e-05, -2.2692e-01, -1.7221e-03,
        -8.5281e-02,  1.8435e-02, -1.6534e-01,  4.4417e-02,  4.8840e-04,
         1.0261e-02,  8.3513e-03, -2.6471e-02,  9.3889e-03, -7.0732e-02,
         1.7080e-02,  4.8038e-04,  4.7729e-03, -2.8907e-03,  1.7381e-03,
         1.2990e-03, -5.5237e-04,  1.0000e+00,  9.2932e-03, -1.0981e-01,
         1.2186e-02, -9.1816e-01,  9.7915e-03,  4.8597e-04,  4.1370e-04,
        -7.7339e-02,  1.3608e-01,  2.1453e-03, -2.2491e-04,  1.0329e-01,
        -9.4133e-03,  3.6525e-02,  7.2587e-02,  2.0975e-02,  1.1137e-03,
        -2.5153e-02,  8.8089e-02,  7.1914e-02,  5.4411e-04,  1.2415e-02,
         5.4635e-02, -5.3188e-03, -7.6328e-02,  2.9564e-02, -9.3678e-02,
        -2.3656e-02, -4.7282e-01,  5.4493e-02,  3.5857e-02, -1.5161e-02,
        -2.3324e-01,  0.0000e+00, -1.3668e-02,  7.4648e-02,  3.3564e-02,
        -2.2814e-01, -9.1456e-02,  2.9410e-03, -3.3803e-02, -1.0046e-02,
         2.4779e-02,  9.4589e-02,  5.9049e-03,  3.0233e-03, -1.0984e-02,
        -1.1827e-02, -4.1325e-03,  7.1423e-02, -1.0690e-02,  2.0277e-02,
        -5.4784e-04,  2.5317e-03, -2.2823e-03, -2.0938e-02,  2.1727e-03,
        -7.3736e-04,  4.1919e-03,  9.8474e-02, -2.1832e-02,  0.0000e+00,
         7.5138e-02,  5.2770e-02,  1.0778e-01, -3.7484e-02,  3.4896e-01,
        -4.2573e-01,  3.0404e-02, -1.6727e-01,  4.1312e-02,  1.1351e-03,
         5.9629e-02,  0.0000e+00, -3.2564e-03,  4.2253e-02,  1.1126e-02,
         1.1247e-03,  6.5109e-03, -7.4668e-03,  2.2520e-01, -7.2055e-02,
        -9.0514e-02,  1.3468e-02,  1.9159e-01,  6.3641e-02, -1.6730e-02,
         3.4153e-03, -1.1501e-03, -1.1612e-01,  3.5650e-02,  7.0143e-02,
        -4.7214e-01, -8.4162e-03,  2.5387e-01,  3.6004e-02, -9.3415e-03,
         2.7332e-03,  1.3710e-03, -9.2995e-03,  3.0839e-02,  5.6756e-02,
        -7.8309e-02, -7.1371e-05,  8.7107e-02,  2.8666e-02, -4.3266e-03,
         1.8490e-03,  1.6550e-01,  2.0378e-02,  0.0000e+00, -9.7869e-02,
         5.3253e-02, -7.4405e-03,  1.4890e-02,  8.1428e-03,  4.0072e-02,
         5.9626e-02,  2.3852e-02, -1.4129e-02,  9.5861e-04], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0018, -0.0100,  0.1678,  ...,  0.0216, -0.1948,  0.0158],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0243, -0.0126,  0.0000,  ..., -0.0114, -0.0067,  0.0179],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([-0.0112, -0.0154,  0.0029,  ...,  0.0092, -0.0103,  0.0004],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.1.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 2.4301e-02,  8.6953e-03,  3.0126e-03,  3.9009e-03,  1.9610e-01,
        -9.3929e-02,  4.4306e-02, -6.4836e-02, -1.0916e-02,  3.7816e-02,
        -1.1031e-03,  6.8237e-02,  4.3740e-02,  5.4395e-02,  2.4431e-03,
         2.8083e-02, -5.7628e-03,  5.9249e-01, -1.0980e-01, -1.4231e-01,
         4.0299e-02,  4.1817e-03,  3.5351e-02,  9.1402e-02,  1.4639e-01,
         1.1838e-01,  0.0000e+00, -9.1319e-02, -2.0171e-02,  5.8165e-02,
         6.4255e-03, -3.3080e-01, -2.2764e-01, -9.4063e-03, -2.7072e-03,
        -1.5896e-02,  2.0474e-02,  6.2648e-02,  1.2202e-03,  6.6576e-03,
         9.3559e-03,  3.4120e-04,  2.3298e-02,  2.0778e-04,  1.3519e-02,
        -6.4451e-05, -1.4235e-01,  1.2419e-02, -1.3542e-01, -1.9586e-01,
        -3.8405e-02,  3.9613e-03, -4.4681e-03,  1.8492e-03, -1.1689e-01,
         2.8412e-02, -3.3030e-03, -8.9514e-02,  3.6833e-02, -5.7454e-03,
         2.4587e-03, -3.1659e-01,  3.2693e-02, -1.3385e-01, -1.5205e-01,
        -3.4440e-04, -8.6145e-02,  1.7937e-02,  2.0698e-02, -3.3556e-02,
         0.0000e+00,  6.2877e-04,  0.0000e+00,  2.0426e-01, -4.8064e-03,
         0.0000e+00,  1.7581e-01, -4.2872e-01, -2.9202e-01,  4.0060e-02,
        -3.9938e-03,  7.0208e-03,  2.7767e-02,  2.0256e-03,  9.3949e-02,
         3.4923e-04,  5.0980e-03, -2.8346e-02, -4.7870e-04,  2.5863e-03,
         1.2981e-02,  1.4565e-04, -9.9445e-03,  1.7168e-03, -1.1461e-01,
        -1.1692e-01,  4.0437e-03,  2.0332e-02, -2.7420e-01, -9.2729e-04,
         3.0638e-01,  6.1992e-03,  3.5989e-03, -8.4824e-02,  0.0000e+00,
        -5.0814e-04, -7.4924e-03, -1.1624e-01, -6.9226e-03,  2.5546e-02,
        -3.2915e-02,  3.2277e-04, -1.5370e-03, -3.3274e-01,  1.0845e-01,
        -6.3231e-02, -5.2442e-02,  4.7349e-02, -7.0031e-03, -2.5018e-02,
        -1.9860e-01, -2.2843e-01,  5.0290e-03,  1.8244e-01,  8.8407e-03,
        -2.2282e-01, -1.3848e-02, -2.9021e-01,  3.2427e-02,  4.6442e-03,
         7.7990e-03,  4.3598e-02,  1.5871e-02,  2.1338e-03,  8.0321e-02,
         6.3615e-02,  1.3045e-04,  7.6888e-02,  1.4460e-01,  3.7224e-02,
         6.1758e-01,  1.2456e-04,  8.6010e-02, -1.3991e-02,  6.9488e-01,
         5.6556e-03, -1.0701e-03, -3.2803e-02,  1.7390e-02, -1.8409e-05,
        -3.7781e-02, -5.2198e-02,  1.8820e-02,  1.4578e-02, -1.7325e-04,
        -2.0112e-02, -2.6558e-01, -5.0111e-01,  5.8467e-02, -2.3437e-02,
        -1.0181e-02,  5.1184e-03,  6.6237e-03, -3.3717e-02, -5.4317e-02,
         6.1498e-02, -1.7371e-01,  1.2783e-03,  8.4734e-02,  0.0000e+00,
        -2.2882e-02,  1.0517e-02,  1.1033e-02,  3.0045e-02,  6.0721e-02,
        -9.2232e-02,  1.2838e-01, -1.1452e-04,  3.0181e-02,  1.5663e-01,
         0.0000e+00,  1.5825e-02,  2.5877e-02,  5.7032e-04, -1.1661e-02,
         4.6237e-03,  2.0629e-02,  2.7096e-03,  1.2248e-02, -2.9423e-01,
         2.6418e-01, -2.3124e-03,  2.5419e-03,  1.3352e-02,  8.4081e-03,
         5.4158e-01,  6.0407e-04,  3.0894e-02,  1.1742e-03, -9.2861e-03,
         3.1449e-01, -3.1219e-02,  0.0000e+00,  1.0989e-01, -2.5976e-02,
         1.3480e-02,  8.5768e-02, -9.9720e-05, -1.1723e-02,  1.2131e-01,
        -3.7995e-02,  1.9493e-02,  1.7359e-04, -5.4308e-02, -2.0628e-03,
         1.1838e-02,  1.6199e-02,  3.6041e-02, -1.7212e-01,  7.1680e-02,
         5.9452e-04,  1.6003e-01, -8.4035e-02, -4.3205e-03,  2.2865e-01,
        -1.3606e-01,  1.8698e-02, -2.5199e-01,  7.3340e-03,  3.4516e-02,
        -5.6091e-03,  9.4647e-05,  2.1698e-02,  3.8162e-02,  5.9559e-02,
        -7.8298e-02,  6.1622e-02,  1.0815e-03,  1.0290e-01,  2.2198e-02,
        -1.0804e-01, -3.5264e-02, -9.7168e-05, -2.2692e-01, -1.7221e-03,
        -8.5281e-02,  1.8435e-02, -1.6534e-01,  4.4417e-02,  4.8840e-04,
         1.0261e-02,  8.3513e-03, -2.6471e-02,  9.3889e-03, -7.0732e-02,
         1.7080e-02,  4.8038e-04,  4.7729e-03, -2.8907e-03,  1.7381e-03,
         1.2990e-03, -5.5237e-04,  1.0000e+00,  9.2932e-03, -1.0981e-01,
         1.2186e-02, -9.1816e-01,  9.7915e-03,  4.8597e-04,  4.1370e-04,
        -7.7339e-02,  1.3608e-01,  2.1453e-03, -2.2491e-04,  1.0329e-01,
        -9.4133e-03,  3.6525e-02,  7.2587e-02,  2.0975e-02,  1.1137e-03,
        -2.5153e-02,  8.8089e-02,  7.1914e-02,  5.4411e-04,  1.2415e-02,
         5.4635e-02, -5.3188e-03, -7.6328e-02,  2.9564e-02, -9.3678e-02,
        -2.3656e-02, -4.7282e-01,  5.4493e-02,  3.5857e-02, -1.5161e-02,
        -2.3324e-01,  0.0000e+00, -1.3668e-02,  7.4648e-02,  3.3564e-02,
        -2.2814e-01, -9.1456e-02,  2.9410e-03, -3.3803e-02, -1.0046e-02,
         2.4779e-02,  9.4589e-02,  5.9049e-03,  3.0233e-03, -1.0984e-02,
        -1.1827e-02, -4.1325e-03,  7.1423e-02, -1.0690e-02,  2.0277e-02,
        -5.4784e-04,  2.5317e-03, -2.2823e-03, -2.0938e-02,  2.1727e-03,
        -7.3736e-04,  4.1919e-03,  9.8474e-02, -2.1832e-02,  0.0000e+00,
         7.5138e-02,  5.2770e-02,  1.0778e-01, -3.7484e-02,  3.4896e-01,
        -4.2573e-01,  3.0404e-02, -1.6727e-01,  4.1312e-02,  1.1351e-03,
         5.9629e-02,  0.0000e+00, -3.2564e-03,  4.2253e-02,  1.1126e-02,
         1.1247e-03,  6.5109e-03, -7.4668e-03,  2.2520e-01, -7.2055e-02,
        -9.0514e-02,  1.3468e-02,  1.9159e-01,  6.3641e-02, -1.6730e-02,
         3.4153e-03, -1.1501e-03, -1.1612e-01,  3.5650e-02,  7.0143e-02,
        -4.7214e-01, -8.4162e-03,  2.5387e-01,  3.6004e-02, -9.3415e-03,
         2.7332e-03,  1.3710e-03, -9.2995e-03,  3.0839e-02,  5.6756e-02,
        -7.8309e-02, -7.1371e-05,  8.7107e-02,  2.8666e-02, -4.3266e-03,
         1.8490e-03,  1.6550e-01,  2.0378e-02,  0.0000e+00, -9.7869e-02,
         5.3253e-02, -7.4405e-03,  1.4890e-02,  8.1428e-03,  4.0072e-02,
         5.9626e-02,  2.3852e-02, -1.4129e-02,  9.5861e-04], device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-5.9425e-02,  4.6107e-03,  9.1470e-03,  7.4974e-03, -6.2646e-01,
         0.0000e+00,  4.0987e-02, -3.5310e-03, -1.0088e-01,  4.6983e-02,
         3.8624e-02,  1.4133e-01, -1.5113e-03,  2.5258e-02,  2.1850e-01,
        -6.7568e-02,  0.0000e+00,  8.5767e-04, -1.3143e-02,  3.5766e-03,
         4.3106e-02,  3.2507e-01, -1.4686e-02,  4.4671e-03,  7.6755e-02,
         0.0000e+00, -1.7707e-02,  1.7228e-03,  6.3087e-02, -1.8436e-02,
         5.0142e-01,  1.1083e-01, -4.7718e-02,  2.4838e-01,  3.2223e-01,
         2.5650e-01,  0.0000e+00, -8.9601e-03,  2.1906e-01,  6.0370e-03,
         1.1743e-01,  2.1399e-02,  4.6723e-01, -9.2209e-03,  2.1992e-01,
         1.3336e-01, -9.0326e-04,  6.4140e-02, -9.6751e-03,  6.9338e-02,
        -9.3173e-02,  3.3182e-02,  1.1179e-01,  2.3805e-01, -3.6064e-02,
         1.6889e-01, -5.6384e-02,  7.4572e-02, -2.7240e-02,  5.0943e-01,
        -1.7200e-02, -4.1931e-03,  3.3258e-03,  6.9868e-03,  6.1792e-01,
        -1.4987e-01, -2.6749e-03, -3.9012e-03,  1.5825e-01,  1.7607e-01,
         5.4258e-01,  2.9316e-02, -1.2535e-02,  2.2970e-02, -1.6736e-02,
         6.3403e-02,  2.8989e-02,  6.3306e-01,  1.2741e-01,  1.7049e-01,
        -3.2663e-02,  2.5349e-01,  5.8460e-01,  4.7269e-02, -3.1581e-04,
        -5.3624e-02, -3.1136e-01, -8.1773e-03,  4.2449e-04,  1.0000e+00,
         2.5712e-03, -9.5579e-02, -7.2559e-02,  3.4898e-01, -9.6754e-03,
         1.1002e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4301e-02,  8.6953e-03,  3.0126e-03,  3.9009e-03,  1.9610e-01,
        -9.3929e-02,  4.4306e-02, -6.4836e-02, -1.0916e-02,  3.7816e-02,
        -1.1031e-03,  6.8237e-02,  4.3740e-02,  5.4395e-02,  2.4431e-03,
         2.8083e-02, -5.7628e-03,  5.9249e-01, -1.0980e-01, -1.4231e-01,
         4.0299e-02,  4.1817e-03,  3.5351e-02,  9.1402e-02,  1.4639e-01,
         1.1838e-01,  0.0000e+00, -9.1319e-02, -2.0171e-02,  5.8165e-02,
         6.4255e-03, -3.3080e-01, -2.2764e-01, -9.4063e-03, -2.7072e-03,
        -1.5896e-02,  2.0474e-02,  6.2648e-02,  1.2202e-03,  6.6576e-03,
         9.3559e-03,  3.4120e-04,  2.3298e-02,  2.0778e-04,  1.3519e-02,
        -6.4451e-05, -1.4235e-01,  1.2419e-02, -1.3542e-01, -1.9586e-01,
        -3.8405e-02,  3.9613e-03, -4.4681e-03,  1.8492e-03, -1.1689e-01,
         2.8412e-02, -3.3030e-03, -8.9514e-02,  3.6833e-02, -5.7454e-03,
         2.4587e-03, -3.1659e-01,  3.2693e-02, -1.3385e-01, -1.5205e-01,
        -3.4440e-04, -8.6145e-02,  1.7937e-02,  2.0698e-02, -3.3556e-02,
         0.0000e+00,  6.2877e-04,  0.0000e+00,  2.0426e-01, -4.8064e-03,
         0.0000e+00,  1.7581e-01, -4.2872e-01, -2.9202e-01,  4.0060e-02,
        -3.9938e-03,  7.0208e-03,  2.7767e-02,  2.0256e-03,  9.3949e-02,
         3.4923e-04,  5.0980e-03, -2.8346e-02, -4.7870e-04,  2.5863e-03,
         1.2981e-02,  1.4565e-04, -9.9445e-03,  1.7168e-03, -1.1461e-01,
        -1.1692e-01,  4.0437e-03,  2.0332e-02, -2.7420e-01, -9.2729e-04,
         3.0638e-01,  6.1992e-03,  3.5989e-03, -8.4824e-02,  0.0000e+00,
        -5.0814e-04, -7.4924e-03, -1.1624e-01, -6.9226e-03,  2.5546e-02,
        -3.2915e-02,  3.2277e-04, -1.5370e-03, -3.3274e-01,  1.0845e-01,
        -6.3231e-02, -5.2442e-02,  4.7349e-02, -7.0031e-03, -2.5018e-02,
        -1.9860e-01, -2.2843e-01,  5.0290e-03,  1.8244e-01,  8.8407e-03,
        -2.2282e-01, -1.3848e-02, -2.9021e-01,  3.2427e-02,  4.6442e-03,
         7.7990e-03,  4.3598e-02,  1.5871e-02,  2.1338e-03,  8.0321e-02,
         6.3615e-02,  1.3045e-04,  7.6888e-02,  1.4460e-01,  3.7224e-02,
         6.1758e-01,  1.2456e-04,  8.6010e-02, -1.3991e-02,  6.9488e-01,
         5.6556e-03, -1.0701e-03, -3.2803e-02,  1.7390e-02, -1.8409e-05,
        -3.7781e-02, -5.2198e-02,  1.8820e-02,  1.4578e-02, -1.7325e-04,
        -2.0112e-02, -2.6558e-01, -5.0111e-01,  5.8467e-02, -2.3437e-02,
        -1.0181e-02,  5.1184e-03,  6.6237e-03, -3.3717e-02, -5.4317e-02,
         6.1498e-02, -1.7371e-01,  1.2783e-03,  8.4734e-02,  0.0000e+00,
        -2.2882e-02,  1.0517e-02,  1.1033e-02,  3.0045e-02,  6.0721e-02,
        -9.2232e-02,  1.2838e-01, -1.1452e-04,  3.0181e-02,  1.5663e-01,
         0.0000e+00,  1.5825e-02,  2.5877e-02,  5.7032e-04, -1.1661e-02,
         4.6237e-03,  2.0629e-02,  2.7096e-03,  1.2248e-02, -2.9423e-01,
         2.6418e-01, -2.3124e-03,  2.5419e-03,  1.3352e-02,  8.4081e-03,
         5.4158e-01,  6.0407e-04,  3.0894e-02,  1.1742e-03, -9.2861e-03,
         3.1449e-01, -3.1219e-02,  0.0000e+00,  1.0989e-01, -2.5976e-02,
         1.3480e-02,  8.5768e-02, -9.9720e-05, -1.1723e-02,  1.2131e-01,
        -3.7995e-02,  1.9493e-02,  1.7359e-04, -5.4308e-02, -2.0628e-03,
         1.1838e-02,  1.6199e-02,  3.6041e-02, -1.7212e-01,  7.1680e-02,
         5.9452e-04,  1.6003e-01, -8.4035e-02, -4.3205e-03,  2.2865e-01,
        -1.3606e-01,  1.8698e-02, -2.5199e-01,  7.3340e-03,  3.4516e-02,
        -5.6091e-03,  9.4647e-05,  2.1698e-02,  3.8162e-02,  5.9559e-02,
        -7.8298e-02,  6.1622e-02,  1.0815e-03,  1.0290e-01,  2.2198e-02,
        -1.0804e-01, -3.5264e-02, -9.7168e-05, -2.2692e-01, -1.7221e-03,
        -8.5281e-02,  1.8435e-02, -1.6534e-01,  4.4417e-02,  4.8840e-04,
         1.0261e-02,  8.3513e-03, -2.6471e-02,  9.3889e-03, -7.0732e-02,
         1.7080e-02,  4.8038e-04,  4.7729e-03, -2.8907e-03,  1.7381e-03,
         1.2990e-03, -5.5237e-04,  1.0000e+00,  9.2932e-03, -1.0981e-01,
         1.2186e-02, -9.1816e-01,  9.7915e-03,  4.8597e-04,  4.1370e-04,
        -7.7339e-02,  1.3608e-01,  2.1453e-03, -2.2491e-04,  1.0329e-01,
        -9.4133e-03,  3.6525e-02,  7.2587e-02,  2.0975e-02,  1.1137e-03,
        -2.5153e-02,  8.8089e-02,  7.1914e-02,  5.4411e-04,  1.2415e-02,
         5.4635e-02, -5.3188e-03, -7.6328e-02,  2.9564e-02, -9.3678e-02,
        -2.3656e-02, -4.7282e-01,  5.4493e-02,  3.5857e-02, -1.5161e-02,
        -2.3324e-01,  0.0000e+00, -1.3668e-02,  7.4648e-02,  3.3564e-02,
        -2.2814e-01, -9.1456e-02,  2.9410e-03, -3.3803e-02, -1.0046e-02,
         2.4779e-02,  9.4589e-02,  5.9049e-03,  3.0233e-03, -1.0984e-02,
        -1.1827e-02, -4.1325e-03,  7.1423e-02, -1.0690e-02,  2.0277e-02,
        -5.4784e-04,  2.5317e-03, -2.2823e-03, -2.0938e-02,  2.1727e-03,
        -7.3736e-04,  4.1919e-03,  9.8474e-02, -2.1832e-02,  0.0000e+00,
         7.5138e-02,  5.2770e-02,  1.0778e-01, -3.7484e-02,  3.4896e-01,
        -4.2573e-01,  3.0404e-02, -1.6727e-01,  4.1312e-02,  1.1351e-03,
         5.9629e-02,  0.0000e+00, -3.2564e-03,  4.2253e-02,  1.1126e-02,
         1.1247e-03,  6.5109e-03, -7.4668e-03,  2.2520e-01, -7.2055e-02,
        -9.0514e-02,  1.3468e-02,  1.9159e-01,  6.3641e-02, -1.6730e-02,
         3.4153e-03, -1.1501e-03, -1.1612e-01,  3.5650e-02,  7.0143e-02,
        -4.7214e-01, -8.4162e-03,  2.5387e-01,  3.6004e-02, -9.3415e-03,
         2.7332e-03,  1.3710e-03, -9.2995e-03,  3.0839e-02,  5.6756e-02,
        -7.8309e-02, -7.1371e-05,  8.7107e-02,  2.8666e-02, -4.3266e-03,
         1.8490e-03,  1.6550e-01,  2.0378e-02,  0.0000e+00, -9.7869e-02,
         5.3253e-02, -7.4405e-03,  1.4890e-02,  8.1428e-03,  4.0072e-02,
         5.9626e-02,  2.3852e-02, -1.4129e-02,  9.5861e-04], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0018, -0.0100,  0.1678,  ...,  0.0216, -0.1948,  0.0158],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0243, -0.0126,  0.0000,  ..., -0.0114, -0.0067,  0.0179],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([-0.0112, -0.0154,  0.0029,  ...,  0.0092, -0.0103,  0.0004],
       device='cuda:0')}
topk_layer inside model after retrieval:  [50, 56, 75, 118, 206, 144, 94, 317, 382, 336, 354, 140, 41, 264, 191, 326, 142, 233, 324, 92, 262, 127, 355, 104, 31, 221, 365, 137, 298, 200, 6, 195, 88, 130, 345, 352, 67, 234, 371, 39, 180, 219, 96, 14, 190, 62, 292, 134, 48, 113, 304, 121, 64, 99, 357, 117, 266, 373, 312]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-5.9425e-02,  4.6107e-03,  9.1470e-03,  7.4974e-03, -6.2646e-01,
         0.0000e+00,  4.0987e-02, -3.5310e-03, -1.0088e-01,  4.6983e-02,
         3.8624e-02,  1.4133e-01, -1.5113e-03,  2.5258e-02,  2.1850e-01,
        -6.7568e-02,  0.0000e+00,  8.5767e-04, -1.3143e-02,  3.5766e-03,
         4.3106e-02,  3.2507e-01, -1.4686e-02,  4.4671e-03,  7.6755e-02,
         0.0000e+00, -1.7707e-02,  1.7228e-03,  6.3087e-02, -1.8436e-02,
         5.0142e-01,  1.1083e-01, -4.7718e-02,  2.4838e-01,  3.2223e-01,
         2.5650e-01,  0.0000e+00, -8.9601e-03,  2.1906e-01,  6.0370e-03,
         1.1743e-01,  2.1399e-02,  4.6723e-01, -9.2209e-03,  2.1992e-01,
         1.3336e-01, -9.0326e-04,  6.4140e-02, -9.6751e-03,  6.9338e-02,
        -9.3173e-02,  3.3182e-02,  1.1179e-01,  2.3805e-01, -3.6064e-02,
         1.6889e-01, -5.6384e-02,  7.4572e-02, -2.7240e-02,  5.0943e-01,
        -1.7200e-02, -4.1931e-03,  3.3258e-03,  6.9868e-03,  6.1792e-01,
        -1.4987e-01, -2.6749e-03, -3.9012e-03,  1.5825e-01,  1.7607e-01,
         5.4258e-01,  2.9316e-02, -1.2535e-02,  2.2970e-02, -1.6736e-02,
         6.3403e-02,  2.8989e-02,  6.3306e-01,  1.2741e-01,  1.7049e-01,
        -3.2663e-02,  2.5349e-01,  5.8460e-01,  4.7269e-02, -3.1581e-04,
        -5.3624e-02, -3.1136e-01, -8.1773e-03,  4.2449e-04,  1.0000e+00,
         2.5712e-03, -9.5579e-02, -7.2559e-02,  3.4898e-01, -9.6754e-03,
         1.1002e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4301e-02,  8.6953e-03,  3.0126e-03,  3.9009e-03,  1.9610e-01,
        -9.3929e-02,  4.4306e-02, -6.4836e-02, -1.0916e-02,  3.7816e-02,
        -1.1031e-03,  6.8237e-02,  4.3740e-02,  5.4395e-02,  2.4431e-03,
         2.8083e-02, -5.7628e-03,  5.9249e-01, -1.0980e-01, -1.4231e-01,
         4.0299e-02,  4.1817e-03,  3.5351e-02,  9.1402e-02,  1.4639e-01,
         1.1838e-01,  0.0000e+00, -9.1319e-02, -2.0171e-02,  5.8165e-02,
         6.4255e-03, -3.3080e-01, -2.2764e-01, -9.4063e-03, -2.7072e-03,
        -1.5896e-02,  2.0474e-02,  6.2648e-02,  1.2202e-03,  6.6576e-03,
         9.3559e-03,  3.4120e-04,  2.3298e-02,  2.0778e-04,  1.3519e-02,
        -6.4451e-05, -1.4235e-01,  1.2419e-02, -1.3542e-01, -1.9586e-01,
        -3.8405e-02,  3.9613e-03, -4.4681e-03,  1.8492e-03, -1.1689e-01,
         2.8412e-02, -3.3030e-03, -8.9514e-02,  3.6833e-02, -5.7454e-03,
         2.4587e-03, -3.1659e-01,  3.2693e-02, -1.3385e-01, -1.5205e-01,
        -3.4440e-04, -8.6145e-02,  1.7937e-02,  2.0698e-02, -3.3556e-02,
         0.0000e+00,  6.2877e-04,  0.0000e+00,  2.0426e-01, -4.8064e-03,
         0.0000e+00,  1.7581e-01, -4.2872e-01, -2.9202e-01,  4.0060e-02,
        -3.9938e-03,  7.0208e-03,  2.7767e-02,  2.0256e-03,  9.3949e-02,
         3.4923e-04,  5.0980e-03, -2.8346e-02, -4.7870e-04,  2.5863e-03,
         1.2981e-02,  1.4565e-04, -9.9445e-03,  1.7168e-03, -1.1461e-01,
        -1.1692e-01,  4.0437e-03,  2.0332e-02, -2.7420e-01, -9.2729e-04,
         3.0638e-01,  6.1992e-03,  3.5989e-03, -8.4824e-02,  0.0000e+00,
        -5.0814e-04, -7.4924e-03, -1.1624e-01, -6.9226e-03,  2.5546e-02,
        -3.2915e-02,  3.2277e-04, -1.5370e-03, -3.3274e-01,  1.0845e-01,
        -6.3231e-02, -5.2442e-02,  4.7349e-02, -7.0031e-03, -2.5018e-02,
        -1.9860e-01, -2.2843e-01,  5.0290e-03,  1.8244e-01,  8.8407e-03,
        -2.2282e-01, -1.3848e-02, -2.9021e-01,  3.2427e-02,  4.6442e-03,
         7.7990e-03,  4.3598e-02,  1.5871e-02,  2.1338e-03,  8.0321e-02,
         6.3615e-02,  1.3045e-04,  7.6888e-02,  1.4460e-01,  3.7224e-02,
         6.1758e-01,  1.2456e-04,  8.6010e-02, -1.3991e-02,  6.9488e-01,
         5.6556e-03, -1.0701e-03, -3.2803e-02,  1.7390e-02, -1.8409e-05,
        -3.7781e-02, -5.2198e-02,  1.8820e-02,  1.4578e-02, -1.7325e-04,
        -2.0112e-02, -2.6558e-01, -5.0111e-01,  5.8467e-02, -2.3437e-02,
        -1.0181e-02,  5.1184e-03,  6.6237e-03, -3.3717e-02, -5.4317e-02,
         6.1498e-02, -1.7371e-01,  1.2783e-03,  8.4734e-02,  0.0000e+00,
        -2.2882e-02,  1.0517e-02,  1.1033e-02,  3.0045e-02,  6.0721e-02,
        -9.2232e-02,  1.2838e-01, -1.1452e-04,  3.0181e-02,  1.5663e-01,
         0.0000e+00,  1.5825e-02,  2.5877e-02,  5.7032e-04, -1.1661e-02,
         4.6237e-03,  2.0629e-02,  2.7096e-03,  1.2248e-02, -2.9423e-01,
         2.6418e-01, -2.3124e-03,  2.5419e-03,  1.3352e-02,  8.4081e-03,
         5.4158e-01,  6.0407e-04,  3.0894e-02,  1.1742e-03, -9.2861e-03,
         3.1449e-01, -3.1219e-02,  0.0000e+00,  1.0989e-01, -2.5976e-02,
         1.3480e-02,  8.5768e-02, -9.9720e-05, -1.1723e-02,  1.2131e-01,
        -3.7995e-02,  1.9493e-02,  1.7359e-04, -5.4308e-02, -2.0628e-03,
         1.1838e-02,  1.6199e-02,  3.6041e-02, -1.7212e-01,  7.1680e-02,
         5.9452e-04,  1.6003e-01, -8.4035e-02, -4.3205e-03,  2.2865e-01,
        -1.3606e-01,  1.8698e-02, -2.5199e-01,  7.3340e-03,  3.4516e-02,
        -5.6091e-03,  9.4647e-05,  2.1698e-02,  3.8162e-02,  5.9559e-02,
        -7.8298e-02,  6.1622e-02,  1.0815e-03,  1.0290e-01,  2.2198e-02,
        -1.0804e-01, -3.5264e-02, -9.7168e-05, -2.2692e-01, -1.7221e-03,
        -8.5281e-02,  1.8435e-02, -1.6534e-01,  4.4417e-02,  4.8840e-04,
         1.0261e-02,  8.3513e-03, -2.6471e-02,  9.3889e-03, -7.0732e-02,
         1.7080e-02,  4.8038e-04,  4.7729e-03, -2.8907e-03,  1.7381e-03,
         1.2990e-03, -5.5237e-04,  1.0000e+00,  9.2932e-03, -1.0981e-01,
         1.2186e-02, -9.1816e-01,  9.7915e-03,  4.8597e-04,  4.1370e-04,
        -7.7339e-02,  1.3608e-01,  2.1453e-03, -2.2491e-04,  1.0329e-01,
        -9.4133e-03,  3.6525e-02,  7.2587e-02,  2.0975e-02,  1.1137e-03,
        -2.5153e-02,  8.8089e-02,  7.1914e-02,  5.4411e-04,  1.2415e-02,
         5.4635e-02, -5.3188e-03, -7.6328e-02,  2.9564e-02, -9.3678e-02,
        -2.3656e-02, -4.7282e-01,  5.4493e-02,  3.5857e-02, -1.5161e-02,
        -2.3324e-01,  0.0000e+00, -1.3668e-02,  7.4648e-02,  3.3564e-02,
        -2.2814e-01, -9.1456e-02,  2.9410e-03, -3.3803e-02, -1.0046e-02,
         2.4779e-02,  9.4589e-02,  5.9049e-03,  3.0233e-03, -1.0984e-02,
        -1.1827e-02, -4.1325e-03,  7.1423e-02, -1.0690e-02,  2.0277e-02,
        -5.4784e-04,  2.5317e-03, -2.2823e-03, -2.0938e-02,  2.1727e-03,
        -7.3736e-04,  4.1919e-03,  9.8474e-02, -2.1832e-02,  0.0000e+00,
         7.5138e-02,  5.2770e-02,  1.0778e-01, -3.7484e-02,  3.4896e-01,
        -4.2573e-01,  3.0404e-02, -1.6727e-01,  4.1312e-02,  1.1351e-03,
         5.9629e-02,  0.0000e+00, -3.2564e-03,  4.2253e-02,  1.1126e-02,
         1.1247e-03,  6.5109e-03, -7.4668e-03,  2.2520e-01, -7.2055e-02,
        -9.0514e-02,  1.3468e-02,  1.9159e-01,  6.3641e-02, -1.6730e-02,
         3.4153e-03, -1.1501e-03, -1.1612e-01,  3.5650e-02,  7.0143e-02,
        -4.7214e-01, -8.4162e-03,  2.5387e-01,  3.6004e-02, -9.3415e-03,
         2.7332e-03,  1.3710e-03, -9.2995e-03,  3.0839e-02,  5.6756e-02,
        -7.8309e-02, -7.1371e-05,  8.7107e-02,  2.8666e-02, -4.3266e-03,
         1.8490e-03,  1.6550e-01,  2.0378e-02,  0.0000e+00, -9.7869e-02,
         5.3253e-02, -7.4405e-03,  1.4890e-02,  8.1428e-03,  4.0072e-02,
         5.9626e-02,  2.3852e-02, -1.4129e-02,  9.5861e-04], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0018, -0.0100,  0.1678,  ...,  0.0216, -0.1948,  0.0158],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0243, -0.0126,  0.0000,  ..., -0.0114, -0.0067,  0.0179],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([-0.0112, -0.0154,  0.0029,  ...,  0.0092, -0.0103,  0.0004],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.2.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 0.0018, -0.0100,  0.1678,  ...,  0.0216, -0.1948,  0.0158],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-5.9425e-02,  4.6107e-03,  9.1470e-03,  7.4974e-03, -6.2646e-01,
         0.0000e+00,  4.0987e-02, -3.5310e-03, -1.0088e-01,  4.6983e-02,
         3.8624e-02,  1.4133e-01, -1.5113e-03,  2.5258e-02,  2.1850e-01,
        -6.7568e-02,  0.0000e+00,  8.5767e-04, -1.3143e-02,  3.5766e-03,
         4.3106e-02,  3.2507e-01, -1.4686e-02,  4.4671e-03,  7.6755e-02,
         0.0000e+00, -1.7707e-02,  1.7228e-03,  6.3087e-02, -1.8436e-02,
         5.0142e-01,  1.1083e-01, -4.7718e-02,  2.4838e-01,  3.2223e-01,
         2.5650e-01,  0.0000e+00, -8.9601e-03,  2.1906e-01,  6.0370e-03,
         1.1743e-01,  2.1399e-02,  4.6723e-01, -9.2209e-03,  2.1992e-01,
         1.3336e-01, -9.0326e-04,  6.4140e-02, -9.6751e-03,  6.9338e-02,
        -9.3173e-02,  3.3182e-02,  1.1179e-01,  2.3805e-01, -3.6064e-02,
         1.6889e-01, -5.6384e-02,  7.4572e-02, -2.7240e-02,  5.0943e-01,
        -1.7200e-02, -4.1931e-03,  3.3258e-03,  6.9868e-03,  6.1792e-01,
        -1.4987e-01, -2.6749e-03, -3.9012e-03,  1.5825e-01,  1.7607e-01,
         5.4258e-01,  2.9316e-02, -1.2535e-02,  2.2970e-02, -1.6736e-02,
         6.3403e-02,  2.8989e-02,  6.3306e-01,  1.2741e-01,  1.7049e-01,
        -3.2663e-02,  2.5349e-01,  5.8460e-01,  4.7269e-02, -3.1581e-04,
        -5.3624e-02, -3.1136e-01, -8.1773e-03,  4.2449e-04,  1.0000e+00,
         2.5712e-03, -9.5579e-02, -7.2559e-02,  3.4898e-01, -9.6754e-03,
         1.1002e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4301e-02,  8.6953e-03,  3.0126e-03,  3.9009e-03,  1.9610e-01,
        -9.3929e-02,  4.4306e-02, -6.4836e-02, -1.0916e-02,  3.7816e-02,
        -1.1031e-03,  6.8237e-02,  4.3740e-02,  5.4395e-02,  2.4431e-03,
         2.8083e-02, -5.7628e-03,  5.9249e-01, -1.0980e-01, -1.4231e-01,
         4.0299e-02,  4.1817e-03,  3.5351e-02,  9.1402e-02,  1.4639e-01,
         1.1838e-01,  0.0000e+00, -9.1319e-02, -2.0171e-02,  5.8165e-02,
         6.4255e-03, -3.3080e-01, -2.2764e-01, -9.4063e-03, -2.7072e-03,
        -1.5896e-02,  2.0474e-02,  6.2648e-02,  1.2202e-03,  6.6576e-03,
         9.3559e-03,  3.4120e-04,  2.3298e-02,  2.0778e-04,  1.3519e-02,
        -6.4451e-05, -1.4235e-01,  1.2419e-02, -1.3542e-01, -1.9586e-01,
        -3.8405e-02,  3.9613e-03, -4.4681e-03,  1.8492e-03, -1.1689e-01,
         2.8412e-02, -3.3030e-03, -8.9514e-02,  3.6833e-02, -5.7454e-03,
         2.4587e-03, -3.1659e-01,  3.2693e-02, -1.3385e-01, -1.5205e-01,
        -3.4440e-04, -8.6145e-02,  1.7937e-02,  2.0698e-02, -3.3556e-02,
         0.0000e+00,  6.2877e-04,  0.0000e+00,  2.0426e-01, -4.8064e-03,
         0.0000e+00,  1.7581e-01, -4.2872e-01, -2.9202e-01,  4.0060e-02,
        -3.9938e-03,  7.0208e-03,  2.7767e-02,  2.0256e-03,  9.3949e-02,
         3.4923e-04,  5.0980e-03, -2.8346e-02, -4.7870e-04,  2.5863e-03,
         1.2981e-02,  1.4565e-04, -9.9445e-03,  1.7168e-03, -1.1461e-01,
        -1.1692e-01,  4.0437e-03,  2.0332e-02, -2.7420e-01, -9.2729e-04,
         3.0638e-01,  6.1992e-03,  3.5989e-03, -8.4824e-02,  0.0000e+00,
        -5.0814e-04, -7.4924e-03, -1.1624e-01, -6.9226e-03,  2.5546e-02,
        -3.2915e-02,  3.2277e-04, -1.5370e-03, -3.3274e-01,  1.0845e-01,
        -6.3231e-02, -5.2442e-02,  4.7349e-02, -7.0031e-03, -2.5018e-02,
        -1.9860e-01, -2.2843e-01,  5.0290e-03,  1.8244e-01,  8.8407e-03,
        -2.2282e-01, -1.3848e-02, -2.9021e-01,  3.2427e-02,  4.6442e-03,
         7.7990e-03,  4.3598e-02,  1.5871e-02,  2.1338e-03,  8.0321e-02,
         6.3615e-02,  1.3045e-04,  7.6888e-02,  1.4460e-01,  3.7224e-02,
         6.1758e-01,  1.2456e-04,  8.6010e-02, -1.3991e-02,  6.9488e-01,
         5.6556e-03, -1.0701e-03, -3.2803e-02,  1.7390e-02, -1.8409e-05,
        -3.7781e-02, -5.2198e-02,  1.8820e-02,  1.4578e-02, -1.7325e-04,
        -2.0112e-02, -2.6558e-01, -5.0111e-01,  5.8467e-02, -2.3437e-02,
        -1.0181e-02,  5.1184e-03,  6.6237e-03, -3.3717e-02, -5.4317e-02,
         6.1498e-02, -1.7371e-01,  1.2783e-03,  8.4734e-02,  0.0000e+00,
        -2.2882e-02,  1.0517e-02,  1.1033e-02,  3.0045e-02,  6.0721e-02,
        -9.2232e-02,  1.2838e-01, -1.1452e-04,  3.0181e-02,  1.5663e-01,
         0.0000e+00,  1.5825e-02,  2.5877e-02,  5.7032e-04, -1.1661e-02,
         4.6237e-03,  2.0629e-02,  2.7096e-03,  1.2248e-02, -2.9423e-01,
         2.6418e-01, -2.3124e-03,  2.5419e-03,  1.3352e-02,  8.4081e-03,
         5.4158e-01,  6.0407e-04,  3.0894e-02,  1.1742e-03, -9.2861e-03,
         3.1449e-01, -3.1219e-02,  0.0000e+00,  1.0989e-01, -2.5976e-02,
         1.3480e-02,  8.5768e-02, -9.9720e-05, -1.1723e-02,  1.2131e-01,
        -3.7995e-02,  1.9493e-02,  1.7359e-04, -5.4308e-02, -2.0628e-03,
         1.1838e-02,  1.6199e-02,  3.6041e-02, -1.7212e-01,  7.1680e-02,
         5.9452e-04,  1.6003e-01, -8.4035e-02, -4.3205e-03,  2.2865e-01,
        -1.3606e-01,  1.8698e-02, -2.5199e-01,  7.3340e-03,  3.4516e-02,
        -5.6091e-03,  9.4647e-05,  2.1698e-02,  3.8162e-02,  5.9559e-02,
        -7.8298e-02,  6.1622e-02,  1.0815e-03,  1.0290e-01,  2.2198e-02,
        -1.0804e-01, -3.5264e-02, -9.7168e-05, -2.2692e-01, -1.7221e-03,
        -8.5281e-02,  1.8435e-02, -1.6534e-01,  4.4417e-02,  4.8840e-04,
         1.0261e-02,  8.3513e-03, -2.6471e-02,  9.3889e-03, -7.0732e-02,
         1.7080e-02,  4.8038e-04,  4.7729e-03, -2.8907e-03,  1.7381e-03,
         1.2990e-03, -5.5237e-04,  1.0000e+00,  9.2932e-03, -1.0981e-01,
         1.2186e-02, -9.1816e-01,  9.7915e-03,  4.8597e-04,  4.1370e-04,
        -7.7339e-02,  1.3608e-01,  2.1453e-03, -2.2491e-04,  1.0329e-01,
        -9.4133e-03,  3.6525e-02,  7.2587e-02,  2.0975e-02,  1.1137e-03,
        -2.5153e-02,  8.8089e-02,  7.1914e-02,  5.4411e-04,  1.2415e-02,
         5.4635e-02, -5.3188e-03, -7.6328e-02,  2.9564e-02, -9.3678e-02,
        -2.3656e-02, -4.7282e-01,  5.4493e-02,  3.5857e-02, -1.5161e-02,
        -2.3324e-01,  0.0000e+00, -1.3668e-02,  7.4648e-02,  3.3564e-02,
        -2.2814e-01, -9.1456e-02,  2.9410e-03, -3.3803e-02, -1.0046e-02,
         2.4779e-02,  9.4589e-02,  5.9049e-03,  3.0233e-03, -1.0984e-02,
        -1.1827e-02, -4.1325e-03,  7.1423e-02, -1.0690e-02,  2.0277e-02,
        -5.4784e-04,  2.5317e-03, -2.2823e-03, -2.0938e-02,  2.1727e-03,
        -7.3736e-04,  4.1919e-03,  9.8474e-02, -2.1832e-02,  0.0000e+00,
         7.5138e-02,  5.2770e-02,  1.0778e-01, -3.7484e-02,  3.4896e-01,
        -4.2573e-01,  3.0404e-02, -1.6727e-01,  4.1312e-02,  1.1351e-03,
         5.9629e-02,  0.0000e+00, -3.2564e-03,  4.2253e-02,  1.1126e-02,
         1.1247e-03,  6.5109e-03, -7.4668e-03,  2.2520e-01, -7.2055e-02,
        -9.0514e-02,  1.3468e-02,  1.9159e-01,  6.3641e-02, -1.6730e-02,
         3.4153e-03, -1.1501e-03, -1.1612e-01,  3.5650e-02,  7.0143e-02,
        -4.7214e-01, -8.4162e-03,  2.5387e-01,  3.6004e-02, -9.3415e-03,
         2.7332e-03,  1.3710e-03, -9.2995e-03,  3.0839e-02,  5.6756e-02,
        -7.8309e-02, -7.1371e-05,  8.7107e-02,  2.8666e-02, -4.3266e-03,
         1.8490e-03,  1.6550e-01,  2.0378e-02,  0.0000e+00, -9.7869e-02,
         5.3253e-02, -7.4405e-03,  1.4890e-02,  8.1428e-03,  4.0072e-02,
         5.9626e-02,  2.3852e-02, -1.4129e-02,  9.5861e-04], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0018, -0.0100,  0.1678,  ...,  0.0216, -0.1948,  0.0158],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0243, -0.0126,  0.0000,  ..., -0.0114, -0.0067,  0.0179],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([-0.0112, -0.0154,  0.0029,  ...,  0.0092, -0.0103,  0.0004],
       device='cuda:0')}
topk_layer inside model after retrieval:  [937, 407, 636, 307, 700, 176, 1086, 195, 1385, 1171, 405, 1317, 1278, 921, 673, 427, 799, 1034, 1486, 854, 421, 311, 731, 616, 794, 1307, 35, 706, 1500, 544, 194, 1360, 450, 958, 598, 1291, 883, 66, 560, 1306, 710, 869, 14, 93, 932, 141, 548, 155, 312, 796, 361, 167, 37, 1534, 851, 490, 551, 1000, 1412, 445, 686, 1226, 838, 906, 1030, 100, 13, 812, 1363, 730, 687, 660, 1006, 139, 150, 268, 437, 1281, 889, 380, 856, 635, 77, 1430, 123, 1286, 1109, 346, 482, 1511, 67, 1359, 1386, 1056, 1294, 786, 1239, 683, 28, 499, 276, 95, 574, 761, 914, 818, 924, 287, 12, 1479, 571, 18, 1405, 520, 955, 566, 699, 1508, 247, 56, 916, 11, 337, 1284, 217, 562, 662, 612, 1242, 1496, 1466, 109, 1442, 304, 1230, 881, 853, 1371, 1395, 1061, 1372, 1007, 1238, 1436, 618, 41, 1263, 634, 577, 1069, 1489, 1272, 353, 1123, 803, 1101, 199, 640, 1134, 946, 105, 726, 1297, 1292, 524, 698, 82, 266, 1472, 814, 626, 622, 174, 289, 1521, 1082, 582, 1041, 1383, 667, 743, 348, 461, 1464, 319, 114, 1071, 1480, 987, 1050, 817, 1046, 302, 489, 747, 610, 1085, 243, 61, 1145, 207, 492, 503, 542, 534, 1096, 1130, 1252, 1408, 1424, 923, 1160, 1092, 1274, 621, 1355, 229, 760, 458, 1019, 1332, 462, 900, 451, 1139, 1159, 1100, 732, 1027, 1295, 1515]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-5.9425e-02,  4.6107e-03,  9.1470e-03,  7.4974e-03, -6.2646e-01,
         0.0000e+00,  4.0987e-02, -3.5310e-03, -1.0088e-01,  4.6983e-02,
         3.8624e-02,  1.4133e-01, -1.5113e-03,  2.5258e-02,  2.1850e-01,
        -6.7568e-02,  0.0000e+00,  8.5767e-04, -1.3143e-02,  3.5766e-03,
         4.3106e-02,  3.2507e-01, -1.4686e-02,  4.4671e-03,  7.6755e-02,
         0.0000e+00, -1.7707e-02,  1.7228e-03,  6.3087e-02, -1.8436e-02,
         5.0142e-01,  1.1083e-01, -4.7718e-02,  2.4838e-01,  3.2223e-01,
         2.5650e-01,  0.0000e+00, -8.9601e-03,  2.1906e-01,  6.0370e-03,
         1.1743e-01,  2.1399e-02,  4.6723e-01, -9.2209e-03,  2.1992e-01,
         1.3336e-01, -9.0326e-04,  6.4140e-02, -9.6751e-03,  6.9338e-02,
        -9.3173e-02,  3.3182e-02,  1.1179e-01,  2.3805e-01, -3.6064e-02,
         1.6889e-01, -5.6384e-02,  7.4572e-02, -2.7240e-02,  5.0943e-01,
        -1.7200e-02, -4.1931e-03,  3.3258e-03,  6.9868e-03,  6.1792e-01,
        -1.4987e-01, -2.6749e-03, -3.9012e-03,  1.5825e-01,  1.7607e-01,
         5.4258e-01,  2.9316e-02, -1.2535e-02,  2.2970e-02, -1.6736e-02,
         6.3403e-02,  2.8989e-02,  6.3306e-01,  1.2741e-01,  1.7049e-01,
        -3.2663e-02,  2.5349e-01,  5.8460e-01,  4.7269e-02, -3.1581e-04,
        -5.3624e-02, -3.1136e-01, -8.1773e-03,  4.2449e-04,  1.0000e+00,
         2.5712e-03, -9.5579e-02, -7.2559e-02,  3.4898e-01, -9.6754e-03,
         1.1002e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4301e-02,  8.6953e-03,  3.0126e-03,  3.9009e-03,  1.9610e-01,
        -9.3929e-02,  4.4306e-02, -6.4836e-02, -1.0916e-02,  3.7816e-02,
        -1.1031e-03,  6.8237e-02,  4.3740e-02,  5.4395e-02,  2.4431e-03,
         2.8083e-02, -5.7628e-03,  5.9249e-01, -1.0980e-01, -1.4231e-01,
         4.0299e-02,  4.1817e-03,  3.5351e-02,  9.1402e-02,  1.4639e-01,
         1.1838e-01,  0.0000e+00, -9.1319e-02, -2.0171e-02,  5.8165e-02,
         6.4255e-03, -3.3080e-01, -2.2764e-01, -9.4063e-03, -2.7072e-03,
        -1.5896e-02,  2.0474e-02,  6.2648e-02,  1.2202e-03,  6.6576e-03,
         9.3559e-03,  3.4120e-04,  2.3298e-02,  2.0778e-04,  1.3519e-02,
        -6.4451e-05, -1.4235e-01,  1.2419e-02, -1.3542e-01, -1.9586e-01,
        -3.8405e-02,  3.9613e-03, -4.4681e-03,  1.8492e-03, -1.1689e-01,
         2.8412e-02, -3.3030e-03, -8.9514e-02,  3.6833e-02, -5.7454e-03,
         2.4587e-03, -3.1659e-01,  3.2693e-02, -1.3385e-01, -1.5205e-01,
        -3.4440e-04, -8.6145e-02,  1.7937e-02,  2.0698e-02, -3.3556e-02,
         0.0000e+00,  6.2877e-04,  0.0000e+00,  2.0426e-01, -4.8064e-03,
         0.0000e+00,  1.7581e-01, -4.2872e-01, -2.9202e-01,  4.0060e-02,
        -3.9938e-03,  7.0208e-03,  2.7767e-02,  2.0256e-03,  9.3949e-02,
         3.4923e-04,  5.0980e-03, -2.8346e-02, -4.7870e-04,  2.5863e-03,
         1.2981e-02,  1.4565e-04, -9.9445e-03,  1.7168e-03, -1.1461e-01,
        -1.1692e-01,  4.0437e-03,  2.0332e-02, -2.7420e-01, -9.2729e-04,
         3.0638e-01,  6.1992e-03,  3.5989e-03, -8.4824e-02,  0.0000e+00,
        -5.0814e-04, -7.4924e-03, -1.1624e-01, -6.9226e-03,  2.5546e-02,
        -3.2915e-02,  3.2277e-04, -1.5370e-03, -3.3274e-01,  1.0845e-01,
        -6.3231e-02, -5.2442e-02,  4.7349e-02, -7.0031e-03, -2.5018e-02,
        -1.9860e-01, -2.2843e-01,  5.0290e-03,  1.8244e-01,  8.8407e-03,
        -2.2282e-01, -1.3848e-02, -2.9021e-01,  3.2427e-02,  4.6442e-03,
         7.7990e-03,  4.3598e-02,  1.5871e-02,  2.1338e-03,  8.0321e-02,
         6.3615e-02,  1.3045e-04,  7.6888e-02,  1.4460e-01,  3.7224e-02,
         6.1758e-01,  1.2456e-04,  8.6010e-02, -1.3991e-02,  6.9488e-01,
         5.6556e-03, -1.0701e-03, -3.2803e-02,  1.7390e-02, -1.8409e-05,
        -3.7781e-02, -5.2198e-02,  1.8820e-02,  1.4578e-02, -1.7325e-04,
        -2.0112e-02, -2.6558e-01, -5.0111e-01,  5.8467e-02, -2.3437e-02,
        -1.0181e-02,  5.1184e-03,  6.6237e-03, -3.3717e-02, -5.4317e-02,
         6.1498e-02, -1.7371e-01,  1.2783e-03,  8.4734e-02,  0.0000e+00,
        -2.2882e-02,  1.0517e-02,  1.1033e-02,  3.0045e-02,  6.0721e-02,
        -9.2232e-02,  1.2838e-01, -1.1452e-04,  3.0181e-02,  1.5663e-01,
         0.0000e+00,  1.5825e-02,  2.5877e-02,  5.7032e-04, -1.1661e-02,
         4.6237e-03,  2.0629e-02,  2.7096e-03,  1.2248e-02, -2.9423e-01,
         2.6418e-01, -2.3124e-03,  2.5419e-03,  1.3352e-02,  8.4081e-03,
         5.4158e-01,  6.0407e-04,  3.0894e-02,  1.1742e-03, -9.2861e-03,
         3.1449e-01, -3.1219e-02,  0.0000e+00,  1.0989e-01, -2.5976e-02,
         1.3480e-02,  8.5768e-02, -9.9720e-05, -1.1723e-02,  1.2131e-01,
        -3.7995e-02,  1.9493e-02,  1.7359e-04, -5.4308e-02, -2.0628e-03,
         1.1838e-02,  1.6199e-02,  3.6041e-02, -1.7212e-01,  7.1680e-02,
         5.9452e-04,  1.6003e-01, -8.4035e-02, -4.3205e-03,  2.2865e-01,
        -1.3606e-01,  1.8698e-02, -2.5199e-01,  7.3340e-03,  3.4516e-02,
        -5.6091e-03,  9.4647e-05,  2.1698e-02,  3.8162e-02,  5.9559e-02,
        -7.8298e-02,  6.1622e-02,  1.0815e-03,  1.0290e-01,  2.2198e-02,
        -1.0804e-01, -3.5264e-02, -9.7168e-05, -2.2692e-01, -1.7221e-03,
        -8.5281e-02,  1.8435e-02, -1.6534e-01,  4.4417e-02,  4.8840e-04,
         1.0261e-02,  8.3513e-03, -2.6471e-02,  9.3889e-03, -7.0732e-02,
         1.7080e-02,  4.8038e-04,  4.7729e-03, -2.8907e-03,  1.7381e-03,
         1.2990e-03, -5.5237e-04,  1.0000e+00,  9.2932e-03, -1.0981e-01,
         1.2186e-02, -9.1816e-01,  9.7915e-03,  4.8597e-04,  4.1370e-04,
        -7.7339e-02,  1.3608e-01,  2.1453e-03, -2.2491e-04,  1.0329e-01,
        -9.4133e-03,  3.6525e-02,  7.2587e-02,  2.0975e-02,  1.1137e-03,
        -2.5153e-02,  8.8089e-02,  7.1914e-02,  5.4411e-04,  1.2415e-02,
         5.4635e-02, -5.3188e-03, -7.6328e-02,  2.9564e-02, -9.3678e-02,
        -2.3656e-02, -4.7282e-01,  5.4493e-02,  3.5857e-02, -1.5161e-02,
        -2.3324e-01,  0.0000e+00, -1.3668e-02,  7.4648e-02,  3.3564e-02,
        -2.2814e-01, -9.1456e-02,  2.9410e-03, -3.3803e-02, -1.0046e-02,
         2.4779e-02,  9.4589e-02,  5.9049e-03,  3.0233e-03, -1.0984e-02,
        -1.1827e-02, -4.1325e-03,  7.1423e-02, -1.0690e-02,  2.0277e-02,
        -5.4784e-04,  2.5317e-03, -2.2823e-03, -2.0938e-02,  2.1727e-03,
        -7.3736e-04,  4.1919e-03,  9.8474e-02, -2.1832e-02,  0.0000e+00,
         7.5138e-02,  5.2770e-02,  1.0778e-01, -3.7484e-02,  3.4896e-01,
        -4.2573e-01,  3.0404e-02, -1.6727e-01,  4.1312e-02,  1.1351e-03,
         5.9629e-02,  0.0000e+00, -3.2564e-03,  4.2253e-02,  1.1126e-02,
         1.1247e-03,  6.5109e-03, -7.4668e-03,  2.2520e-01, -7.2055e-02,
        -9.0514e-02,  1.3468e-02,  1.9159e-01,  6.3641e-02, -1.6730e-02,
         3.4153e-03, -1.1501e-03, -1.1612e-01,  3.5650e-02,  7.0143e-02,
        -4.7214e-01, -8.4162e-03,  2.5387e-01,  3.6004e-02, -9.3415e-03,
         2.7332e-03,  1.3710e-03, -9.2995e-03,  3.0839e-02,  5.6756e-02,
        -7.8309e-02, -7.1371e-05,  8.7107e-02,  2.8666e-02, -4.3266e-03,
         1.8490e-03,  1.6550e-01,  2.0378e-02,  0.0000e+00, -9.7869e-02,
         5.3253e-02, -7.4405e-03,  1.4890e-02,  8.1428e-03,  4.0072e-02,
         5.9626e-02,  2.3852e-02, -1.4129e-02,  9.5861e-04], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0018, -0.0100,  0.1678,  ...,  0.0216, -0.1948,  0.0158],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0243, -0.0126,  0.0000,  ..., -0.0114, -0.0067,  0.0179],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([-0.0112, -0.0154,  0.0029,  ...,  0.0092, -0.0103,  0.0004],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.3.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 0.0243, -0.0126,  0.0000,  ..., -0.0114, -0.0067,  0.0179],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-5.9425e-02,  4.6107e-03,  9.1470e-03,  7.4974e-03, -6.2646e-01,
         0.0000e+00,  4.0987e-02, -3.5310e-03, -1.0088e-01,  4.6983e-02,
         3.8624e-02,  1.4133e-01, -1.5113e-03,  2.5258e-02,  2.1850e-01,
        -6.7568e-02,  0.0000e+00,  8.5767e-04, -1.3143e-02,  3.5766e-03,
         4.3106e-02,  3.2507e-01, -1.4686e-02,  4.4671e-03,  7.6755e-02,
         0.0000e+00, -1.7707e-02,  1.7228e-03,  6.3087e-02, -1.8436e-02,
         5.0142e-01,  1.1083e-01, -4.7718e-02,  2.4838e-01,  3.2223e-01,
         2.5650e-01,  0.0000e+00, -8.9601e-03,  2.1906e-01,  6.0370e-03,
         1.1743e-01,  2.1399e-02,  4.6723e-01, -9.2209e-03,  2.1992e-01,
         1.3336e-01, -9.0326e-04,  6.4140e-02, -9.6751e-03,  6.9338e-02,
        -9.3173e-02,  3.3182e-02,  1.1179e-01,  2.3805e-01, -3.6064e-02,
         1.6889e-01, -5.6384e-02,  7.4572e-02, -2.7240e-02,  5.0943e-01,
        -1.7200e-02, -4.1931e-03,  3.3258e-03,  6.9868e-03,  6.1792e-01,
        -1.4987e-01, -2.6749e-03, -3.9012e-03,  1.5825e-01,  1.7607e-01,
         5.4258e-01,  2.9316e-02, -1.2535e-02,  2.2970e-02, -1.6736e-02,
         6.3403e-02,  2.8989e-02,  6.3306e-01,  1.2741e-01,  1.7049e-01,
        -3.2663e-02,  2.5349e-01,  5.8460e-01,  4.7269e-02, -3.1581e-04,
        -5.3624e-02, -3.1136e-01, -8.1773e-03,  4.2449e-04,  1.0000e+00,
         2.5712e-03, -9.5579e-02, -7.2559e-02,  3.4898e-01, -9.6754e-03,
         1.1002e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4301e-02,  8.6953e-03,  3.0126e-03,  3.9009e-03,  1.9610e-01,
        -9.3929e-02,  4.4306e-02, -6.4836e-02, -1.0916e-02,  3.7816e-02,
        -1.1031e-03,  6.8237e-02,  4.3740e-02,  5.4395e-02,  2.4431e-03,
         2.8083e-02, -5.7628e-03,  5.9249e-01, -1.0980e-01, -1.4231e-01,
         4.0299e-02,  4.1817e-03,  3.5351e-02,  9.1402e-02,  1.4639e-01,
         1.1838e-01,  0.0000e+00, -9.1319e-02, -2.0171e-02,  5.8165e-02,
         6.4255e-03, -3.3080e-01, -2.2764e-01, -9.4063e-03, -2.7072e-03,
        -1.5896e-02,  2.0474e-02,  6.2648e-02,  1.2202e-03,  6.6576e-03,
         9.3559e-03,  3.4120e-04,  2.3298e-02,  2.0778e-04,  1.3519e-02,
        -6.4451e-05, -1.4235e-01,  1.2419e-02, -1.3542e-01, -1.9586e-01,
        -3.8405e-02,  3.9613e-03, -4.4681e-03,  1.8492e-03, -1.1689e-01,
         2.8412e-02, -3.3030e-03, -8.9514e-02,  3.6833e-02, -5.7454e-03,
         2.4587e-03, -3.1659e-01,  3.2693e-02, -1.3385e-01, -1.5205e-01,
        -3.4440e-04, -8.6145e-02,  1.7937e-02,  2.0698e-02, -3.3556e-02,
         0.0000e+00,  6.2877e-04,  0.0000e+00,  2.0426e-01, -4.8064e-03,
         0.0000e+00,  1.7581e-01, -4.2872e-01, -2.9202e-01,  4.0060e-02,
        -3.9938e-03,  7.0208e-03,  2.7767e-02,  2.0256e-03,  9.3949e-02,
         3.4923e-04,  5.0980e-03, -2.8346e-02, -4.7870e-04,  2.5863e-03,
         1.2981e-02,  1.4565e-04, -9.9445e-03,  1.7168e-03, -1.1461e-01,
        -1.1692e-01,  4.0437e-03,  2.0332e-02, -2.7420e-01, -9.2729e-04,
         3.0638e-01,  6.1992e-03,  3.5989e-03, -8.4824e-02,  0.0000e+00,
        -5.0814e-04, -7.4924e-03, -1.1624e-01, -6.9226e-03,  2.5546e-02,
        -3.2915e-02,  3.2277e-04, -1.5370e-03, -3.3274e-01,  1.0845e-01,
        -6.3231e-02, -5.2442e-02,  4.7349e-02, -7.0031e-03, -2.5018e-02,
        -1.9860e-01, -2.2843e-01,  5.0290e-03,  1.8244e-01,  8.8407e-03,
        -2.2282e-01, -1.3848e-02, -2.9021e-01,  3.2427e-02,  4.6442e-03,
         7.7990e-03,  4.3598e-02,  1.5871e-02,  2.1338e-03,  8.0321e-02,
         6.3615e-02,  1.3045e-04,  7.6888e-02,  1.4460e-01,  3.7224e-02,
         6.1758e-01,  1.2456e-04,  8.6010e-02, -1.3991e-02,  6.9488e-01,
         5.6556e-03, -1.0701e-03, -3.2803e-02,  1.7390e-02, -1.8409e-05,
        -3.7781e-02, -5.2198e-02,  1.8820e-02,  1.4578e-02, -1.7325e-04,
        -2.0112e-02, -2.6558e-01, -5.0111e-01,  5.8467e-02, -2.3437e-02,
        -1.0181e-02,  5.1184e-03,  6.6237e-03, -3.3717e-02, -5.4317e-02,
         6.1498e-02, -1.7371e-01,  1.2783e-03,  8.4734e-02,  0.0000e+00,
        -2.2882e-02,  1.0517e-02,  1.1033e-02,  3.0045e-02,  6.0721e-02,
        -9.2232e-02,  1.2838e-01, -1.1452e-04,  3.0181e-02,  1.5663e-01,
         0.0000e+00,  1.5825e-02,  2.5877e-02,  5.7032e-04, -1.1661e-02,
         4.6237e-03,  2.0629e-02,  2.7096e-03,  1.2248e-02, -2.9423e-01,
         2.6418e-01, -2.3124e-03,  2.5419e-03,  1.3352e-02,  8.4081e-03,
         5.4158e-01,  6.0407e-04,  3.0894e-02,  1.1742e-03, -9.2861e-03,
         3.1449e-01, -3.1219e-02,  0.0000e+00,  1.0989e-01, -2.5976e-02,
         1.3480e-02,  8.5768e-02, -9.9720e-05, -1.1723e-02,  1.2131e-01,
        -3.7995e-02,  1.9493e-02,  1.7359e-04, -5.4308e-02, -2.0628e-03,
         1.1838e-02,  1.6199e-02,  3.6041e-02, -1.7212e-01,  7.1680e-02,
         5.9452e-04,  1.6003e-01, -8.4035e-02, -4.3205e-03,  2.2865e-01,
        -1.3606e-01,  1.8698e-02, -2.5199e-01,  7.3340e-03,  3.4516e-02,
        -5.6091e-03,  9.4647e-05,  2.1698e-02,  3.8162e-02,  5.9559e-02,
        -7.8298e-02,  6.1622e-02,  1.0815e-03,  1.0290e-01,  2.2198e-02,
        -1.0804e-01, -3.5264e-02, -9.7168e-05, -2.2692e-01, -1.7221e-03,
        -8.5281e-02,  1.8435e-02, -1.6534e-01,  4.4417e-02,  4.8840e-04,
         1.0261e-02,  8.3513e-03, -2.6471e-02,  9.3889e-03, -7.0732e-02,
         1.7080e-02,  4.8038e-04,  4.7729e-03, -2.8907e-03,  1.7381e-03,
         1.2990e-03, -5.5237e-04,  1.0000e+00,  9.2932e-03, -1.0981e-01,
         1.2186e-02, -9.1816e-01,  9.7915e-03,  4.8597e-04,  4.1370e-04,
        -7.7339e-02,  1.3608e-01,  2.1453e-03, -2.2491e-04,  1.0329e-01,
        -9.4133e-03,  3.6525e-02,  7.2587e-02,  2.0975e-02,  1.1137e-03,
        -2.5153e-02,  8.8089e-02,  7.1914e-02,  5.4411e-04,  1.2415e-02,
         5.4635e-02, -5.3188e-03, -7.6328e-02,  2.9564e-02, -9.3678e-02,
        -2.3656e-02, -4.7282e-01,  5.4493e-02,  3.5857e-02, -1.5161e-02,
        -2.3324e-01,  0.0000e+00, -1.3668e-02,  7.4648e-02,  3.3564e-02,
        -2.2814e-01, -9.1456e-02,  2.9410e-03, -3.3803e-02, -1.0046e-02,
         2.4779e-02,  9.4589e-02,  5.9049e-03,  3.0233e-03, -1.0984e-02,
        -1.1827e-02, -4.1325e-03,  7.1423e-02, -1.0690e-02,  2.0277e-02,
        -5.4784e-04,  2.5317e-03, -2.2823e-03, -2.0938e-02,  2.1727e-03,
        -7.3736e-04,  4.1919e-03,  9.8474e-02, -2.1832e-02,  0.0000e+00,
         7.5138e-02,  5.2770e-02,  1.0778e-01, -3.7484e-02,  3.4896e-01,
        -4.2573e-01,  3.0404e-02, -1.6727e-01,  4.1312e-02,  1.1351e-03,
         5.9629e-02,  0.0000e+00, -3.2564e-03,  4.2253e-02,  1.1126e-02,
         1.1247e-03,  6.5109e-03, -7.4668e-03,  2.2520e-01, -7.2055e-02,
        -9.0514e-02,  1.3468e-02,  1.9159e-01,  6.3641e-02, -1.6730e-02,
         3.4153e-03, -1.1501e-03, -1.1612e-01,  3.5650e-02,  7.0143e-02,
        -4.7214e-01, -8.4162e-03,  2.5387e-01,  3.6004e-02, -9.3415e-03,
         2.7332e-03,  1.3710e-03, -9.2995e-03,  3.0839e-02,  5.6756e-02,
        -7.8309e-02, -7.1371e-05,  8.7107e-02,  2.8666e-02, -4.3266e-03,
         1.8490e-03,  1.6550e-01,  2.0378e-02,  0.0000e+00, -9.7869e-02,
         5.3253e-02, -7.4405e-03,  1.4890e-02,  8.1428e-03,  4.0072e-02,
         5.9626e-02,  2.3852e-02, -1.4129e-02,  9.5861e-04], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0018, -0.0100,  0.1678,  ...,  0.0216, -0.1948,  0.0158],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0243, -0.0126,  0.0000,  ..., -0.0114, -0.0067,  0.0179],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([-0.0112, -0.0154,  0.0029,  ...,  0.0092, -0.0103,  0.0004],
       device='cuda:0')}
topk_layer inside model after retrieval:  [499, 507, 1677, 1339, 512, 1958, 653, 1443, 300, 2049, 1447, 921, 1755, 340, 2315, 1827, 43, 1862, 1359, 652, 2494, 1938, 446, 2095, 2402, 1403, 2157, 505, 2354, 1127, 1303, 1567, 193, 2103, 1109, 784, 2025, 288, 1670, 243, 2379, 520, 1608, 40, 450, 2435, 1783, 2132, 508, 611, 1465, 1710, 481, 1618, 1843, 2100, 1061, 1125, 2115, 699, 1656, 2414, 1602, 1964, 1510, 2021, 421, 2292, 399, 2032, 1352, 1641, 818, 2221, 993, 484, 1500, 2185, 1926, 1834, 2482, 156, 303, 153, 2317, 299, 2067, 1762, 733, 359, 2154, 991, 1750, 2050, 792, 1378, 2268, 2138, 1341, 2145, 277, 2181, 378, 1520, 783, 1894, 2309, 2554, 1789, 878, 1616, 2244, 853, 2367, 328, 2249, 2247, 2405, 1559, 147, 455, 1370, 2218, 2087, 863, 2241, 2300, 2363, 1242, 1163, 624, 2245, 1548, 197, 1402, 757, 1599, 1715, 1422, 606, 2082, 8, 1646, 372, 2460, 1561, 1411, 2276, 1260, 983, 1985, 208, 287, 689, 1597, 883, 1929, 2000, 2170, 1326, 244, 1226, 356, 841, 726, 651, 367, 1425, 148, 1887, 2324, 780, 1275, 1699, 1366, 1063, 2291, 1658, 2081, 2261, 1998, 103, 2465, 724, 1205, 419, 2184, 1554, 36, 2497, 1659, 580, 469, 389, 6, 555, 1867, 1367, 825, 1482, 1312, 1598, 565, 1020, 195, 882, 2302, 1375, 1138, 377, 1327, 1319, 2448, 2007, 1236, 861, 1083, 2110, 2198, 1324, 1638, 1440, 2454, 914, 934, 1010, 534, 1495, 1347, 496, 1988, 114, 1776, 1435, 2349, 615, 1685, 388, 68, 1693, 1808, 471, 1780, 735, 2389, 1038, 1749, 1057, 816, 613, 1111, 549, 1810, 151, 1348, 29, 846, 1105, 339, 813, 296, 660, 1965, 1229, 2213, 1543, 251, 276, 1791, 1821, 1648, 501, 1647, 1744, 2196, 454, 1874, 2427, 2223, 1300, 2534, 1709, 2294, 1394, 1840, 716, 532, 871, 2415, 1187, 2250, 2111, 382, 1073, 42, 2271, 1, 138, 2559, 1816, 1238, 278, 1689, 2323, 2093, 2108, 1920, 940, 2352, 1077, 1050, 1883, 1705, 527, 971, 589, 854, 1837, 753, 533, 674, 2077, 939, 249, 497, 1788, 33, 2549, 1506, 1509, 353, 1266, 320, 700, 2143, 51, 540, 2232, 1943, 1088, 2112, 553, 1253, 2428, 34, 627, 1340, 482, 597, 1720, 2479, 1110, 1530, 1775, 2120, 2264, 468, 546, 231, 1008, 1448, 1660, 1845, 2325, 1759, 1584, 265, 1331, 729, 104, 2106, 472, 1029, 867, 1696, 1169, 1839, 1522, 511, 1065, 2543, 1211, 1264, 429, 926]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-5.9425e-02,  4.6107e-03,  9.1470e-03,  7.4974e-03, -6.2646e-01,
         0.0000e+00,  4.0987e-02, -3.5310e-03, -1.0088e-01,  4.6983e-02,
         3.8624e-02,  1.4133e-01, -1.5113e-03,  2.5258e-02,  2.1850e-01,
        -6.7568e-02,  0.0000e+00,  8.5767e-04, -1.3143e-02,  3.5766e-03,
         4.3106e-02,  3.2507e-01, -1.4686e-02,  4.4671e-03,  7.6755e-02,
         0.0000e+00, -1.7707e-02,  1.7228e-03,  6.3087e-02, -1.8436e-02,
         5.0142e-01,  1.1083e-01, -4.7718e-02,  2.4838e-01,  3.2223e-01,
         2.5650e-01,  0.0000e+00, -8.9601e-03,  2.1906e-01,  6.0370e-03,
         1.1743e-01,  2.1399e-02,  4.6723e-01, -9.2209e-03,  2.1992e-01,
         1.3336e-01, -9.0326e-04,  6.4140e-02, -9.6751e-03,  6.9338e-02,
        -9.3173e-02,  3.3182e-02,  1.1179e-01,  2.3805e-01, -3.6064e-02,
         1.6889e-01, -5.6384e-02,  7.4572e-02, -2.7240e-02,  5.0943e-01,
        -1.7200e-02, -4.1931e-03,  3.3258e-03,  6.9868e-03,  6.1792e-01,
        -1.4987e-01, -2.6749e-03, -3.9012e-03,  1.5825e-01,  1.7607e-01,
         5.4258e-01,  2.9316e-02, -1.2535e-02,  2.2970e-02, -1.6736e-02,
         6.3403e-02,  2.8989e-02,  6.3306e-01,  1.2741e-01,  1.7049e-01,
        -3.2663e-02,  2.5349e-01,  5.8460e-01,  4.7269e-02, -3.1581e-04,
        -5.3624e-02, -3.1136e-01, -8.1773e-03,  4.2449e-04,  1.0000e+00,
         2.5712e-03, -9.5579e-02, -7.2559e-02,  3.4898e-01, -9.6754e-03,
         1.1002e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4301e-02,  8.6953e-03,  3.0126e-03,  3.9009e-03,  1.9610e-01,
        -9.3929e-02,  4.4306e-02, -6.4836e-02, -1.0916e-02,  3.7816e-02,
        -1.1031e-03,  6.8237e-02,  4.3740e-02,  5.4395e-02,  2.4431e-03,
         2.8083e-02, -5.7628e-03,  5.9249e-01, -1.0980e-01, -1.4231e-01,
         4.0299e-02,  4.1817e-03,  3.5351e-02,  9.1402e-02,  1.4639e-01,
         1.1838e-01,  0.0000e+00, -9.1319e-02, -2.0171e-02,  5.8165e-02,
         6.4255e-03, -3.3080e-01, -2.2764e-01, -9.4063e-03, -2.7072e-03,
        -1.5896e-02,  2.0474e-02,  6.2648e-02,  1.2202e-03,  6.6576e-03,
         9.3559e-03,  3.4120e-04,  2.3298e-02,  2.0778e-04,  1.3519e-02,
        -6.4451e-05, -1.4235e-01,  1.2419e-02, -1.3542e-01, -1.9586e-01,
        -3.8405e-02,  3.9613e-03, -4.4681e-03,  1.8492e-03, -1.1689e-01,
         2.8412e-02, -3.3030e-03, -8.9514e-02,  3.6833e-02, -5.7454e-03,
         2.4587e-03, -3.1659e-01,  3.2693e-02, -1.3385e-01, -1.5205e-01,
        -3.4440e-04, -8.6145e-02,  1.7937e-02,  2.0698e-02, -3.3556e-02,
         0.0000e+00,  6.2877e-04,  0.0000e+00,  2.0426e-01, -4.8064e-03,
         0.0000e+00,  1.7581e-01, -4.2872e-01, -2.9202e-01,  4.0060e-02,
        -3.9938e-03,  7.0208e-03,  2.7767e-02,  2.0256e-03,  9.3949e-02,
         3.4923e-04,  5.0980e-03, -2.8346e-02, -4.7870e-04,  2.5863e-03,
         1.2981e-02,  1.4565e-04, -9.9445e-03,  1.7168e-03, -1.1461e-01,
        -1.1692e-01,  4.0437e-03,  2.0332e-02, -2.7420e-01, -9.2729e-04,
         3.0638e-01,  6.1992e-03,  3.5989e-03, -8.4824e-02,  0.0000e+00,
        -5.0814e-04, -7.4924e-03, -1.1624e-01, -6.9226e-03,  2.5546e-02,
        -3.2915e-02,  3.2277e-04, -1.5370e-03, -3.3274e-01,  1.0845e-01,
        -6.3231e-02, -5.2442e-02,  4.7349e-02, -7.0031e-03, -2.5018e-02,
        -1.9860e-01, -2.2843e-01,  5.0290e-03,  1.8244e-01,  8.8407e-03,
        -2.2282e-01, -1.3848e-02, -2.9021e-01,  3.2427e-02,  4.6442e-03,
         7.7990e-03,  4.3598e-02,  1.5871e-02,  2.1338e-03,  8.0321e-02,
         6.3615e-02,  1.3045e-04,  7.6888e-02,  1.4460e-01,  3.7224e-02,
         6.1758e-01,  1.2456e-04,  8.6010e-02, -1.3991e-02,  6.9488e-01,
         5.6556e-03, -1.0701e-03, -3.2803e-02,  1.7390e-02, -1.8409e-05,
        -3.7781e-02, -5.2198e-02,  1.8820e-02,  1.4578e-02, -1.7325e-04,
        -2.0112e-02, -2.6558e-01, -5.0111e-01,  5.8467e-02, -2.3437e-02,
        -1.0181e-02,  5.1184e-03,  6.6237e-03, -3.3717e-02, -5.4317e-02,
         6.1498e-02, -1.7371e-01,  1.2783e-03,  8.4734e-02,  0.0000e+00,
        -2.2882e-02,  1.0517e-02,  1.1033e-02,  3.0045e-02,  6.0721e-02,
        -9.2232e-02,  1.2838e-01, -1.1452e-04,  3.0181e-02,  1.5663e-01,
         0.0000e+00,  1.5825e-02,  2.5877e-02,  5.7032e-04, -1.1661e-02,
         4.6237e-03,  2.0629e-02,  2.7096e-03,  1.2248e-02, -2.9423e-01,
         2.6418e-01, -2.3124e-03,  2.5419e-03,  1.3352e-02,  8.4081e-03,
         5.4158e-01,  6.0407e-04,  3.0894e-02,  1.1742e-03, -9.2861e-03,
         3.1449e-01, -3.1219e-02,  0.0000e+00,  1.0989e-01, -2.5976e-02,
         1.3480e-02,  8.5768e-02, -9.9720e-05, -1.1723e-02,  1.2131e-01,
        -3.7995e-02,  1.9493e-02,  1.7359e-04, -5.4308e-02, -2.0628e-03,
         1.1838e-02,  1.6199e-02,  3.6041e-02, -1.7212e-01,  7.1680e-02,
         5.9452e-04,  1.6003e-01, -8.4035e-02, -4.3205e-03,  2.2865e-01,
        -1.3606e-01,  1.8698e-02, -2.5199e-01,  7.3340e-03,  3.4516e-02,
        -5.6091e-03,  9.4647e-05,  2.1698e-02,  3.8162e-02,  5.9559e-02,
        -7.8298e-02,  6.1622e-02,  1.0815e-03,  1.0290e-01,  2.2198e-02,
        -1.0804e-01, -3.5264e-02, -9.7168e-05, -2.2692e-01, -1.7221e-03,
        -8.5281e-02,  1.8435e-02, -1.6534e-01,  4.4417e-02,  4.8840e-04,
         1.0261e-02,  8.3513e-03, -2.6471e-02,  9.3889e-03, -7.0732e-02,
         1.7080e-02,  4.8038e-04,  4.7729e-03, -2.8907e-03,  1.7381e-03,
         1.2990e-03, -5.5237e-04,  1.0000e+00,  9.2932e-03, -1.0981e-01,
         1.2186e-02, -9.1816e-01,  9.7915e-03,  4.8597e-04,  4.1370e-04,
        -7.7339e-02,  1.3608e-01,  2.1453e-03, -2.2491e-04,  1.0329e-01,
        -9.4133e-03,  3.6525e-02,  7.2587e-02,  2.0975e-02,  1.1137e-03,
        -2.5153e-02,  8.8089e-02,  7.1914e-02,  5.4411e-04,  1.2415e-02,
         5.4635e-02, -5.3188e-03, -7.6328e-02,  2.9564e-02, -9.3678e-02,
        -2.3656e-02, -4.7282e-01,  5.4493e-02,  3.5857e-02, -1.5161e-02,
        -2.3324e-01,  0.0000e+00, -1.3668e-02,  7.4648e-02,  3.3564e-02,
        -2.2814e-01, -9.1456e-02,  2.9410e-03, -3.3803e-02, -1.0046e-02,
         2.4779e-02,  9.4589e-02,  5.9049e-03,  3.0233e-03, -1.0984e-02,
        -1.1827e-02, -4.1325e-03,  7.1423e-02, -1.0690e-02,  2.0277e-02,
        -5.4784e-04,  2.5317e-03, -2.2823e-03, -2.0938e-02,  2.1727e-03,
        -7.3736e-04,  4.1919e-03,  9.8474e-02, -2.1832e-02,  0.0000e+00,
         7.5138e-02,  5.2770e-02,  1.0778e-01, -3.7484e-02,  3.4896e-01,
        -4.2573e-01,  3.0404e-02, -1.6727e-01,  4.1312e-02,  1.1351e-03,
         5.9629e-02,  0.0000e+00, -3.2564e-03,  4.2253e-02,  1.1126e-02,
         1.1247e-03,  6.5109e-03, -7.4668e-03,  2.2520e-01, -7.2055e-02,
        -9.0514e-02,  1.3468e-02,  1.9159e-01,  6.3641e-02, -1.6730e-02,
         3.4153e-03, -1.1501e-03, -1.1612e-01,  3.5650e-02,  7.0143e-02,
        -4.7214e-01, -8.4162e-03,  2.5387e-01,  3.6004e-02, -9.3415e-03,
         2.7332e-03,  1.3710e-03, -9.2995e-03,  3.0839e-02,  5.6756e-02,
        -7.8309e-02, -7.1371e-05,  8.7107e-02,  2.8666e-02, -4.3266e-03,
         1.8490e-03,  1.6550e-01,  2.0378e-02,  0.0000e+00, -9.7869e-02,
         5.3253e-02, -7.4405e-03,  1.4890e-02,  8.1428e-03,  4.0072e-02,
         5.9626e-02,  2.3852e-02, -1.4129e-02,  9.5861e-04], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0018, -0.0100,  0.1678,  ...,  0.0216, -0.1948,  0.0158],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0243, -0.0126,  0.0000,  ..., -0.0114, -0.0067,  0.0179],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([-0.0112, -0.0154,  0.0029,  ...,  0.0092, -0.0103,  0.0004],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.4.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([-0.0112, -0.0154,  0.0029,  ...,  0.0092, -0.0103,  0.0004],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-5.9425e-02,  4.6107e-03,  9.1470e-03,  7.4974e-03, -6.2646e-01,
         0.0000e+00,  4.0987e-02, -3.5310e-03, -1.0088e-01,  4.6983e-02,
         3.8624e-02,  1.4133e-01, -1.5113e-03,  2.5258e-02,  2.1850e-01,
        -6.7568e-02,  0.0000e+00,  8.5767e-04, -1.3143e-02,  3.5766e-03,
         4.3106e-02,  3.2507e-01, -1.4686e-02,  4.4671e-03,  7.6755e-02,
         0.0000e+00, -1.7707e-02,  1.7228e-03,  6.3087e-02, -1.8436e-02,
         5.0142e-01,  1.1083e-01, -4.7718e-02,  2.4838e-01,  3.2223e-01,
         2.5650e-01,  0.0000e+00, -8.9601e-03,  2.1906e-01,  6.0370e-03,
         1.1743e-01,  2.1399e-02,  4.6723e-01, -9.2209e-03,  2.1992e-01,
         1.3336e-01, -9.0326e-04,  6.4140e-02, -9.6751e-03,  6.9338e-02,
        -9.3173e-02,  3.3182e-02,  1.1179e-01,  2.3805e-01, -3.6064e-02,
         1.6889e-01, -5.6384e-02,  7.4572e-02, -2.7240e-02,  5.0943e-01,
        -1.7200e-02, -4.1931e-03,  3.3258e-03,  6.9868e-03,  6.1792e-01,
        -1.4987e-01, -2.6749e-03, -3.9012e-03,  1.5825e-01,  1.7607e-01,
         5.4258e-01,  2.9316e-02, -1.2535e-02,  2.2970e-02, -1.6736e-02,
         6.3403e-02,  2.8989e-02,  6.3306e-01,  1.2741e-01,  1.7049e-01,
        -3.2663e-02,  2.5349e-01,  5.8460e-01,  4.7269e-02, -3.1581e-04,
        -5.3624e-02, -3.1136e-01, -8.1773e-03,  4.2449e-04,  1.0000e+00,
         2.5712e-03, -9.5579e-02, -7.2559e-02,  3.4898e-01, -9.6754e-03,
         1.1002e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4301e-02,  8.6953e-03,  3.0126e-03,  3.9009e-03,  1.9610e-01,
        -9.3929e-02,  4.4306e-02, -6.4836e-02, -1.0916e-02,  3.7816e-02,
        -1.1031e-03,  6.8237e-02,  4.3740e-02,  5.4395e-02,  2.4431e-03,
         2.8083e-02, -5.7628e-03,  5.9249e-01, -1.0980e-01, -1.4231e-01,
         4.0299e-02,  4.1817e-03,  3.5351e-02,  9.1402e-02,  1.4639e-01,
         1.1838e-01,  0.0000e+00, -9.1319e-02, -2.0171e-02,  5.8165e-02,
         6.4255e-03, -3.3080e-01, -2.2764e-01, -9.4063e-03, -2.7072e-03,
        -1.5896e-02,  2.0474e-02,  6.2648e-02,  1.2202e-03,  6.6576e-03,
         9.3559e-03,  3.4120e-04,  2.3298e-02,  2.0778e-04,  1.3519e-02,
        -6.4451e-05, -1.4235e-01,  1.2419e-02, -1.3542e-01, -1.9586e-01,
        -3.8405e-02,  3.9613e-03, -4.4681e-03,  1.8492e-03, -1.1689e-01,
         2.8412e-02, -3.3030e-03, -8.9514e-02,  3.6833e-02, -5.7454e-03,
         2.4587e-03, -3.1659e-01,  3.2693e-02, -1.3385e-01, -1.5205e-01,
        -3.4440e-04, -8.6145e-02,  1.7937e-02,  2.0698e-02, -3.3556e-02,
         0.0000e+00,  6.2877e-04,  0.0000e+00,  2.0426e-01, -4.8064e-03,
         0.0000e+00,  1.7581e-01, -4.2872e-01, -2.9202e-01,  4.0060e-02,
        -3.9938e-03,  7.0208e-03,  2.7767e-02,  2.0256e-03,  9.3949e-02,
         3.4923e-04,  5.0980e-03, -2.8346e-02, -4.7870e-04,  2.5863e-03,
         1.2981e-02,  1.4565e-04, -9.9445e-03,  1.7168e-03, -1.1461e-01,
        -1.1692e-01,  4.0437e-03,  2.0332e-02, -2.7420e-01, -9.2729e-04,
         3.0638e-01,  6.1992e-03,  3.5989e-03, -8.4824e-02,  0.0000e+00,
        -5.0814e-04, -7.4924e-03, -1.1624e-01, -6.9226e-03,  2.5546e-02,
        -3.2915e-02,  3.2277e-04, -1.5370e-03, -3.3274e-01,  1.0845e-01,
        -6.3231e-02, -5.2442e-02,  4.7349e-02, -7.0031e-03, -2.5018e-02,
        -1.9860e-01, -2.2843e-01,  5.0290e-03,  1.8244e-01,  8.8407e-03,
        -2.2282e-01, -1.3848e-02, -2.9021e-01,  3.2427e-02,  4.6442e-03,
         7.7990e-03,  4.3598e-02,  1.5871e-02,  2.1338e-03,  8.0321e-02,
         6.3615e-02,  1.3045e-04,  7.6888e-02,  1.4460e-01,  3.7224e-02,
         6.1758e-01,  1.2456e-04,  8.6010e-02, -1.3991e-02,  6.9488e-01,
         5.6556e-03, -1.0701e-03, -3.2803e-02,  1.7390e-02, -1.8409e-05,
        -3.7781e-02, -5.2198e-02,  1.8820e-02,  1.4578e-02, -1.7325e-04,
        -2.0112e-02, -2.6558e-01, -5.0111e-01,  5.8467e-02, -2.3437e-02,
        -1.0181e-02,  5.1184e-03,  6.6237e-03, -3.3717e-02, -5.4317e-02,
         6.1498e-02, -1.7371e-01,  1.2783e-03,  8.4734e-02,  0.0000e+00,
        -2.2882e-02,  1.0517e-02,  1.1033e-02,  3.0045e-02,  6.0721e-02,
        -9.2232e-02,  1.2838e-01, -1.1452e-04,  3.0181e-02,  1.5663e-01,
         0.0000e+00,  1.5825e-02,  2.5877e-02,  5.7032e-04, -1.1661e-02,
         4.6237e-03,  2.0629e-02,  2.7096e-03,  1.2248e-02, -2.9423e-01,
         2.6418e-01, -2.3124e-03,  2.5419e-03,  1.3352e-02,  8.4081e-03,
         5.4158e-01,  6.0407e-04,  3.0894e-02,  1.1742e-03, -9.2861e-03,
         3.1449e-01, -3.1219e-02,  0.0000e+00,  1.0989e-01, -2.5976e-02,
         1.3480e-02,  8.5768e-02, -9.9720e-05, -1.1723e-02,  1.2131e-01,
        -3.7995e-02,  1.9493e-02,  1.7359e-04, -5.4308e-02, -2.0628e-03,
         1.1838e-02,  1.6199e-02,  3.6041e-02, -1.7212e-01,  7.1680e-02,
         5.9452e-04,  1.6003e-01, -8.4035e-02, -4.3205e-03,  2.2865e-01,
        -1.3606e-01,  1.8698e-02, -2.5199e-01,  7.3340e-03,  3.4516e-02,
        -5.6091e-03,  9.4647e-05,  2.1698e-02,  3.8162e-02,  5.9559e-02,
        -7.8298e-02,  6.1622e-02,  1.0815e-03,  1.0290e-01,  2.2198e-02,
        -1.0804e-01, -3.5264e-02, -9.7168e-05, -2.2692e-01, -1.7221e-03,
        -8.5281e-02,  1.8435e-02, -1.6534e-01,  4.4417e-02,  4.8840e-04,
         1.0261e-02,  8.3513e-03, -2.6471e-02,  9.3889e-03, -7.0732e-02,
         1.7080e-02,  4.8038e-04,  4.7729e-03, -2.8907e-03,  1.7381e-03,
         1.2990e-03, -5.5237e-04,  1.0000e+00,  9.2932e-03, -1.0981e-01,
         1.2186e-02, -9.1816e-01,  9.7915e-03,  4.8597e-04,  4.1370e-04,
        -7.7339e-02,  1.3608e-01,  2.1453e-03, -2.2491e-04,  1.0329e-01,
        -9.4133e-03,  3.6525e-02,  7.2587e-02,  2.0975e-02,  1.1137e-03,
        -2.5153e-02,  8.8089e-02,  7.1914e-02,  5.4411e-04,  1.2415e-02,
         5.4635e-02, -5.3188e-03, -7.6328e-02,  2.9564e-02, -9.3678e-02,
        -2.3656e-02, -4.7282e-01,  5.4493e-02,  3.5857e-02, -1.5161e-02,
        -2.3324e-01,  0.0000e+00, -1.3668e-02,  7.4648e-02,  3.3564e-02,
        -2.2814e-01, -9.1456e-02,  2.9410e-03, -3.3803e-02, -1.0046e-02,
         2.4779e-02,  9.4589e-02,  5.9049e-03,  3.0233e-03, -1.0984e-02,
        -1.1827e-02, -4.1325e-03,  7.1423e-02, -1.0690e-02,  2.0277e-02,
        -5.4784e-04,  2.5317e-03, -2.2823e-03, -2.0938e-02,  2.1727e-03,
        -7.3736e-04,  4.1919e-03,  9.8474e-02, -2.1832e-02,  0.0000e+00,
         7.5138e-02,  5.2770e-02,  1.0778e-01, -3.7484e-02,  3.4896e-01,
        -4.2573e-01,  3.0404e-02, -1.6727e-01,  4.1312e-02,  1.1351e-03,
         5.9629e-02,  0.0000e+00, -3.2564e-03,  4.2253e-02,  1.1126e-02,
         1.1247e-03,  6.5109e-03, -7.4668e-03,  2.2520e-01, -7.2055e-02,
        -9.0514e-02,  1.3468e-02,  1.9159e-01,  6.3641e-02, -1.6730e-02,
         3.4153e-03, -1.1501e-03, -1.1612e-01,  3.5650e-02,  7.0143e-02,
        -4.7214e-01, -8.4162e-03,  2.5387e-01,  3.6004e-02, -9.3415e-03,
         2.7332e-03,  1.3710e-03, -9.2995e-03,  3.0839e-02,  5.6756e-02,
        -7.8309e-02, -7.1371e-05,  8.7107e-02,  2.8666e-02, -4.3266e-03,
         1.8490e-03,  1.6550e-01,  2.0378e-02,  0.0000e+00, -9.7869e-02,
         5.3253e-02, -7.4405e-03,  1.4890e-02,  8.1428e-03,  4.0072e-02,
         5.9626e-02,  2.3852e-02, -1.4129e-02,  9.5861e-04], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0018, -0.0100,  0.1678,  ...,  0.0216, -0.1948,  0.0158],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0243, -0.0126,  0.0000,  ..., -0.0114, -0.0067,  0.0179],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([-0.0112, -0.0154,  0.0029,  ...,  0.0092, -0.0103,  0.0004],
       device='cuda:0')}
topk_layer inside model after retrieval:  [1067, 99, 1620, 633, 1644, 2356, 558, 929, 348, 881, 1921, 468, 1706, 2285, 82, 966, 1975, 1678, 1173, 1741, 2388, 1928, 1658, 1148, 1254, 2471, 1451, 1261, 1444, 203, 2413, 1692, 2346, 484, 1521, 1727, 1656, 1795, 997, 1734, 1575, 675, 2543, 621, 2039, 670, 1622, 2209, 694, 1993, 1990, 1149, 2404, 969, 967, 893, 1660, 1766, 80, 2041, 2399, 862, 1585, 1797, 783, 1498, 2528, 1441, 1764, 808, 1416, 1517, 1539, 1364, 1047, 1957, 1667, 2258, 1859, 1867, 593, 1187, 1654, 1167, 2158, 960, 258, 615, 931, 366, 2359, 1131, 2207, 2353, 2211, 760, 768, 381, 1184, 1612, 806, 473, 1266, 139, 2438, 1710, 499, 1366, 2153, 272, 965, 1189, 772, 774, 339, 1369, 465, 38, 1191, 2524, 1320, 884, 1712, 2202, 1362, 107, 826, 778, 1809, 2463, 1996, 1548, 1659, 1288, 1483, 1580, 1138, 1221, 2015, 1719, 1030, 1472, 1313, 2437, 188, 1746, 2336, 1739, 1063, 1549, 779, 1323, 788, 1908, 1165, 535, 2031, 1485, 2242, 21, 2476, 1317, 1128, 1019, 309, 649, 641, 712, 1034, 891, 239, 691, 1284, 1757, 2449, 544, 2038, 688, 2491, 993, 1800, 1794, 2225, 1567, 264, 591, 795, 145, 845, 1354, 1813, 2395, 2551, 921, 1604, 830, 2021, 1109, 182, 2518, 512, 674, 1822, 1306, 1637, 2145, 2338, 2305, 390, 369, 1811, 368, 818, 592, 1534, 2421, 364, 1085, 2533, 575, 2176, 1326, 2435, 315, 486, 2530, 2482, 673, 704, 1708, 1135, 2522, 151, 13, 2212, 1183, 1437, 20, 2152, 2217, 722, 2268, 1310, 1418, 568, 976, 2537, 253, 412, 1699, 1904, 1657, 1246, 1263, 1738, 89, 1818, 1756, 734, 193, 91, 1798, 2068, 1296, 1791, 1316, 514, 1054, 786, 295, 1536, 1190, 2385, 1559, 432, 1732, 827, 1924, 2445, 2071, 1974, 518, 1415, 2286, 457, 2335, 520, 1429, 1929, 2237, 548, 1772, 2552, 708, 2251, 435, 547, 762, 479, 1569, 637, 1143, 1874, 1084, 183, 1445, 1101, 504, 2307, 586, 1496, 114, 2403, 690, 1302, 1035, 2321, 503, 823, 451, 2271, 243, 1040, 1123, 1962, 2093, 987, 3, 1278, 433, 1327, 2206, 2519, 1540, 903, 2165, 498, 952, 1265, 1652, 2029, 2327, 508, 1617, 331, 406, 721, 2008, 338, 2060, 2406, 113, 1801, 1392, 477, 1922, 2077, 1541, 81, 1271, 464, 607, 2133, 1344, 1473, 1058, 159, 227, 1698, 1610, 527, 1057, 1589, 1228, 2123, 652, 507, 2360, 2472, 598, 605, 1668, 152, 325, 1718]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
The device used will be: 
True
cuda:0
The device used will be: 
True
cuda:0
chosen_head:  {'blocks.5.layer.bias': tensor([ 0.0005, -0.0170], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0349,  0.0030,  0.0162,  ..., -0.0045,  0.0023,  0.0014],
        [-0.0066,  0.0195, -0.0117,  ...,  0.0072,  0.0189,  0.0064]],
       device='cuda:0')}

 Model C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c loaded successfuly with best perf



 ----- Architecture Block BatchNorm2dSK3962(5, 5)1.0reflect, number 0 -----
- BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK963842(3, 3)0.6499999761581421reflect, number 1 -----
- BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.4)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK38415362(3, 3)0.25reflect, number 2 -----
- BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block BatchNorm2dSK153625602(3, 3)0.25reflect, number 3 -----
- BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block BatchNorm2dSK256025602(3, 3)0.25reflect, number 4 -----
- BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 5 -----
- Flatten(start_dim=1, end_dim=-1)
- Dropout(p=0.5, inplace=False)
- Linear(in_features=2560, out_features=2, bias=True)
model.heads:  [{'blocks.5.layer.bias': tensor([ 1.5791e-05, -1.6506e-02], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0120,  0.0150,  0.0043,  ...,  0.0072,  0.0111, -0.0045],
        [ 0.0163,  0.0075,  0.0001,  ..., -0.0045,  0.0101,  0.0122]],
       device='cuda:0')}, {'blocks.5.layer.bias': tensor([ 0.0040, -0.0205], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 1.5765e-02,  1.0660e-02,  4.3432e-03,  ...,  1.0930e-02,
          1.1431e-02, -3.5607e-03],
        [ 1.2527e-02,  1.1907e-02,  8.8815e-05,  ..., -8.2031e-03,
          9.7858e-03,  1.1328e-02]], device='cuda:0')}, {'blocks.5.layer.bias': tensor([ 0.0005, -0.0170], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0349,  0.0030,  0.0162,  ..., -0.0045,  0.0023,  0.0014],
        [-0.0066,  0.0195, -0.0117,  ...,  0.0072,  0.0189,  0.0064]],
       device='cuda:0')}]
[Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (5): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=2560, out_features=2, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)]
LAYER NAME:  Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (5): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=2560, out_features=2, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)
LAYER CHILDREN:  [BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=1.4)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
  )
  (layer): Linear(in_features=2560, out_features=2, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)]
subsubl NAME:  Sequential(
  (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=1.4)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): Flatten(start_dim=1, end_dim=-1)
)
subsubl NAME:  Linear(in_features=2560, out_features=2, bias=True)
subsubl NAME:  Dropout(p=0.5, inplace=False)
CONFIG MODE:  unsupervised

 ********** Hebbian Unsupervised learning of blocks [0, 1, 2, 3, 4] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([49, 33, 72, 51, 71, 92, 15, 14, 23,  0, 71, 75, 81, 69, 40, 43, 92, 97,
        70, 53], device='cuda:0')
TARGETS AFTER SUB:  tensor([56, 56,  2, 56,  2,  2,  2,  2,  2,  2,  2, 56,  2, 56, 56, 56,  2, 56,
        56,  2])
[2, 56]
TARGETS AFTER CLEANER:  tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0])
200
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 200
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               None
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([19, 29,  0, 11,  1, 86, 90, 28, 23, 31, 39, 96, 82, 17, 71, 39,  8, 97,
        80, 71], device='cuda:0')
TARGETS AFTER SUB:  tensor([56, 56,  2,  2, 56, 56,  2,  2, 56, 56, 56, 56, 56,  2,  2, 56, 56, 56,
        56, 56])
[2, 56]
TARGETS AFTER CLEANER:  tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1])
1000
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 1000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               None
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([19, 29,  0, 11,  1, 86, 90, 28, 23, 31, 39, 96, 82, 17, 71, 39,  8, 97,
        80, 71], device='cuda:0')
TARGETS AFTER SUB:  tensor([56, 56,  2,  2, 56, 56,  2,  2, 56, 56, 56, 56, 56,  2,  2, 56, 56, 56,
        56, 56])
[2, 56]
TARGETS AFTER CLEANER:  tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1])
1000
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 1000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
------------------------
INDICES:  1000
IMAGE SIZE: torch.Size([10, 3, 32, 32])
SAVING FOLDER FOR UNSUP:  C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c
prev_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight', 'blocks.4.layer.weight'])
activations_sum.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight', 'blocks.4.layer.weight'])
DEPTH:  5
ACTIVATIONS SHAPE:  tensor([[ -597.6266,  -724.1854,  -595.4902,  -965.3329,  -889.1849, -1082.0585,
         -1030.5042,  -959.7306,  -972.1354,  -535.8741,  -572.3545,  -854.4436,
         -1195.0708, -1053.2418,  -859.4435,  -746.7335],
        [ -241.6357,  -302.4287,   -49.9446,  -276.6114,  -188.8496,  -458.1862,
          -555.2711,  -636.1036,  -447.5674,  -104.7293,  -363.2772,  -774.0098,
          -905.4498,  -771.6196,  -609.9765,  -414.6369],
        [   21.0596,    12.5662,   111.1192,  -155.0081,  -129.9234,  -263.8091,
          -351.0544,  -486.3203,  -376.2020,  -231.7017,  -520.0882,  -842.2440,
          -922.3976,  -666.7134,  -654.5768,  -485.3420],
        [   93.7178,    36.9891,   125.7828,   -15.8652,  -181.8336,  -250.3794,
            -8.0876,  -244.5598,  -344.7102,  -337.8434,  -416.2515,  -662.7451,
          -827.3393,  -812.5061,  -677.7216,  -429.3124],
        [  122.9682,    -3.2472,   272.8977,   337.4455,    39.8771,  -135.7689,
           207.1662,    91.0761,  -207.3924,   -32.6513,  -233.4659,  -431.7566,
          -854.4030,  -836.7723,  -728.9257,  -461.1363],
        [   33.1958,   137.2809,   543.5589,   712.1246,   423.5212,   322.1637,
           741.1708,   569.5610,   309.8749,   355.2053,    78.6195,  -260.4062,
          -776.9320,  -811.9801,  -686.5699,  -395.2437],
        [    3.0824,    41.9475,   622.8086,   825.3304,   731.1753,   853.0963,
          1307.1558,  1272.4080,   897.2378,   674.2965,   302.2094,  -408.2984,
          -903.8644,  -994.5692,  -876.6178,  -447.9548],
        [   36.1655,  -110.1720,   631.9316,  1189.6707,  1268.3320,  1199.1963,
          1765.1985,  1990.7328,  1630.9359,  1293.1824,   566.4016,  -135.9599,
          -928.8855, -1242.9796,  -963.6716,  -441.8500],
        [  -13.4901,  -237.1906,   427.4551,  1302.3688,  1508.3921,  1625.7344,
          1957.1371,  2128.1609,  2135.1096,  1982.0767,   764.4960,    27.2118,
          -855.4423, -1297.9321, -1043.8151,  -570.2892],
        [  137.4700,  -180.2818,   432.5145,  1295.5300,  1526.1313,  1886.8602,
          2053.3469,  2315.6812,  2402.9390,  2575.6311,  1572.9020,   344.4932,
          -749.2139, -1224.1774, -1235.2140,  -746.7131],
        [   55.9937,  -231.8045,   408.7889,  1080.4246,  1253.2515,  1797.9242,
          2039.4480,  2210.9688,  2342.7319,  2525.4761,  1902.8738,   709.8051,
          -481.4493,  -999.1249, -1295.6772,  -506.2500],
        [  -73.2132,  -295.5278,   223.0880,   600.6702,   943.7789,  1520.0884,
          1738.8032,  1966.4094,  2088.9124,  2282.8096,  1713.1820,   591.0330,
          -296.5432,  -589.9827, -1086.4595,  -349.8098],
        [ -393.1735,  -539.3003,  -180.2367,   -95.8166,   245.1963,   633.9977,
           954.0392,  1164.6793,  1150.3079,  1318.0255,   778.3070,   223.5020,
          -340.4320,  -550.5650, -1201.4617,  -461.4890],
        [ -845.9079, -1048.3718,  -880.2805,  -779.4108,  -665.2349,  -192.3716,
           149.0807,    86.7733,   402.5950,   295.9246,  -114.7814,  -208.5713,
          -622.8802,  -632.4702,  -978.0442,  -549.9467],
        [-1056.7712, -1169.3958, -1148.4376,  -910.9138,  -632.1631,  -324.4737,
           -70.2965,    83.9844,   173.0256,   100.2757,  -194.3470,  -175.1660,
          -601.5565,  -600.3018,  -585.7484,  -675.6340],
        [-1308.7975, -1399.4417, -1201.8081, -1009.0054,  -901.2308,  -873.5843,
          -646.2295,  -665.4487,  -398.2794,  -264.5235,  -855.8639, -1361.7914,
         -1843.4973, -1805.3191, -1748.6420, -1535.3025]], device='cuda:0')
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.topk_layer:  <class 'list'>
CHECK_THRESHOLD:  tensor(1, device='cuda:0', dtype=torch.uint8) tensor(-0.0013) tensor(-0.0594)
topk_mask  tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0.])
threshold_mask  tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0],
       dtype=torch.uint8)
final_mask tensor([1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0.,
        1., 0., 0., 0., 1., 0., 0., 2., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0.,
        1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0.,
        1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,
        0., 1., 1., 1., 1., 0.])
lower_lr_mask  tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.])
higher_lr_mask  tensor([0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500,
        0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.0000, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500])
lr_mask  tensor([[[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]]])
torch.Size([96, 3, 5, 5])
self.lr tensor([[[0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066]],

        [[0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066]],

        [[0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066]]], device='cuda:0')
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.topk_layer:  <class 'list'>
CHECK_THRESHOLD:  tensor(0, device='cuda:0', dtype=torch.uint8) tensor(-0.0003) tensor(0.0243)
topk_mask  tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0.])
threshold_mask  tensor([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0],
       dtype=torch.uint8)
final_mask tensor([0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 2., 0., 1., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 2., 1., 1., 1., 1.,
        1., 0., 0., 1., 1., 2., 0., 0., 0., 1., 1., 0., 2., 1., 2., 0., 1., 0.,
        1., 0., 2., 1., 0., 1., 0., 1., 1., 1., 2., 0., 1., 1., 0., 1., 0., 0.,
        0., 0., 1., 2., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 1., 2., 0., 2., 1., 2., 0., 1., 2., 0., 0., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 1., 2., 0., 1., 1., 1., 2., 1., 1., 2., 0., 0., 0., 1.,
        1., 2., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.,
        1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.,
        1., 1., 1., 1., 0., 0., 1., 0., 2., 1., 1., 0., 1., 0., 0., 1., 1., 0.,
        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1.,
        1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0.,
        1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 2., 0., 2., 0., 0., 0.,
        1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 2., 0.,
        0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 2., 1., 0., 1., 0., 0., 1.,
        1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 2., 1., 0., 0., 1., 0.,
        1., 0., 1., 2., 0., 0., 0., 0., 0., 1., 2., 0., 2., 2., 1., 1., 0., 1.,
        0., 0., 1., 0., 0., 2., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0.,
        0., 0., 0., 1., 2., 0.])
lower_lr_mask  tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -1., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -1., -0., -0., -0., -0., -0., -0., -0., -0., -0., -1.,
        -0., -0., -0., -0., -0., -0., -1., -0., -1., -0., -0., -0., -0., -0.,
        -1., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -1., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -1., -0., -1., -0., -1., -0.,
        -0., -1., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -1., -0., -0., -0., -0., -1., -0., -0., -1., -0., -0., -0., -0.,
        -0., -1., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0.,
        -1., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -1., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0., -0., -0.,
        -0., -0., -1., -0., -1., -1., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -1., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -1., -0.])
higher_lr_mask  tensor([0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000,
        0.1500, 0.0000, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500,
        0.1500, 0.1500, 0.0000, 0.1500, 0.0000, 0.1500, 0.0000, 0.1500, 0.1500,
        0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.0000, 0.0000, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.0000, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.0000,
        0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.0000, 0.1500, 0.0000, 0.1500,
        0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.0000, 0.0000, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500,
        0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000,
        0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.0000, 0.1500, 0.0000, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500,
        0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.0000, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.0000, 0.1500, 0.0000, 0.0000, 0.1500, 0.0000, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.0000, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500])
lr_mask  tensor([[[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]]])
torch.Size([384, 96, 3, 3])
self.lr tensor([[[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]]], device='cuda:0')
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.topk_layer:  <class 'list'>
CHECK_THRESHOLD:  tensor(0, device='cuda:0', dtype=torch.uint8) tensor(-7.6617e-05) tensor(0.0018)
topk_mask  tensor([0., 0., 0.,  ..., 0., 1., 0.])
threshold_mask  tensor([0, 1, 0,  ..., 0, 1, 0], dtype=torch.uint8)
final_mask tensor([0., 1., 0.,  ..., 0., 2., 0.])
lower_lr_mask  tensor([-0., -0., -0.,  ..., -0., -1., -0.])
higher_lr_mask  tensor([0.1500, 0.1500, 0.1500,  ..., 0.1500, 0.0000, 0.1500])
lr_mask  tensor([[[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        ...,


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]]])
torch.Size([1536, 384, 3, 3])
self.lr tensor([[[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]],

        [[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]],

        [[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]],

        ...,

        [[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]],

        [[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]],

        [[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]]], device='cuda:0')
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.topk_layer:  <class 'list'>
CHECK_THRESHOLD:  tensor(0, device='cuda:0', dtype=torch.uint8) tensor(-0.0005) tensor(0.0243)
topk_mask  tensor([0., 1., 0.,  ..., 0., 0., 1.])
threshold_mask  tensor([0, 1, 0,  ..., 1, 1, 0], dtype=torch.uint8)
final_mask tensor([0., 2., 0.,  ..., 1., 1., 1.])
lower_lr_mask  tensor([-0., -1., -0.,  ..., -0., -0., -0.])
higher_lr_mask  tensor([0.1500, 0.0000, 0.1500,  ..., 0.1500, 0.1500, 0.0000])
lr_mask  tensor([[[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        ...,


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]]])
torch.Size([2560, 1536, 3, 3])
self.lr tensor([[[0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488]],

        [[0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488]],

        [[0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488]],

        ...,

        [[0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488]],

        [[0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488]],

        [[0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488]]], device='cuda:0')
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.topk_layer:  <class 'list'>
CHECK_THRESHOLD:  tensor(1, device='cuda:0', dtype=torch.uint8) tensor(9.7478e-05) tensor(-0.0112)
topk_mask  tensor([0., 0., 0.,  ..., 0., 0., 0.])
threshold_mask  tensor([1, 1, 0,  ..., 0, 1, 0], dtype=torch.uint8)
final_mask tensor([1., 1., 0.,  ..., 0., 1., 0.])
lower_lr_mask  tensor([-0., -0., -0.,  ..., -0., -0., -0.])
higher_lr_mask  tensor([0.1500, 0.1500, 0.1500,  ..., 0.1500, 0.1500, 0.1500])
lr_mask  tensor([[[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        ...,


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]]])
torch.Size([2560, 2560, 3, 3])
self.lr tensor([[[0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491]],

        [[0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491]],

        [[0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491]],

        ...,

        [[0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491]],

        [[0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491]],

        [[0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491]]], device='cuda:0')
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight', 'blocks.4.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight', 'blocks.4.layer.weight'])
activations_sum[k] 96
activations_sum[k]:  [25, 59, 77, 30, 5, 82, 33, 16, 89, 42]
topk_kernels len:  5
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3', 'conv4']
topk_kernels:  {'conv0': [25, 59, 77, 30, 5, 82, 33, 16, 89, 42, 68, 70, 53, 35, 93], 'conv1': [50, 56, 75, 118, 206, 144, 94, 317, 382, 336, 354, 140, 41, 264, 191, 326, 142, 233, 324, 92, 262, 127, 355, 104, 31, 221, 365, 137, 298, 200, 6, 195, 88, 130, 345, 352, 67, 234, 371, 39, 180, 219, 96, 14, 190, 62, 292, 134, 48, 113, 304, 121, 64, 99, 357, 117, 266, 373, 312], 'conv2': [937, 407, 636, 307, 700, 176, 1086, 195, 1385, 1171, 405, 1317, 1278, 921, 673, 427, 799, 1034, 1486, 854, 421, 311, 731, 616, 794, 1307, 35, 706, 1500, 544, 194, 1360, 450, 958, 598, 1291, 883, 66, 560, 1306, 710, 869, 14, 93, 932, 141, 548, 155, 312, 796, 361, 167, 37, 1534, 851, 490, 551, 1000, 1412, 445, 686, 1226, 838, 906, 1030, 100, 13, 812, 1363, 730, 687, 660, 1006, 139, 150, 268, 437, 1281, 889, 380, 856, 635, 77, 1430, 123, 1286, 1109, 346, 482, 1511, 67, 1359, 1386, 1056, 1294, 786, 1239, 683, 28, 499, 276, 95, 574, 761, 914, 818, 924, 287, 12, 1479, 571, 18, 1405, 520, 955, 566, 699, 1508, 247, 56, 916, 11, 337, 1284, 217, 562, 662, 612, 1242, 1496, 1466, 109, 1442, 304, 1230, 881, 853, 1371, 1395, 1061, 1372, 1007, 1238, 1436, 618, 41, 1263, 634, 577, 1069, 1489, 1272, 353, 1123, 803, 1101, 199, 640, 1134, 946, 105, 726, 1297, 1292, 524, 698, 82, 266, 1472, 814, 626, 622, 174, 289, 1521, 1082, 582, 1041, 1383, 667, 743, 348, 461, 1464, 319, 114, 1071, 1480, 987, 1050, 817, 1046, 302, 489, 747, 610, 1085, 243, 61, 1145, 207, 492, 503, 542, 534, 1096, 1130, 1252, 1408, 1424, 923, 1160, 1092, 1274, 621, 1355, 229, 760, 458, 1019, 1332, 462, 900, 451, 1139, 1159, 1100, 732, 1027, 1295, 1515], 'conv3': [499, 507, 1677, 1339, 512, 1958, 653, 1443, 300, 2049, 1447, 921, 1755, 340, 2315, 1827, 43, 1862, 1359, 652, 2494, 1938, 446, 2095, 2402, 1403, 2157, 505, 2354, 1127, 1303, 1567, 193, 2103, 1109, 784, 2025, 288, 1670, 243, 2379, 520, 1608, 40, 450, 2435, 1783, 2132, 508, 611, 1465, 1710, 481, 1618, 1843, 2100, 1061, 1125, 2115, 699, 1656, 2414, 1602, 1964, 1510, 2021, 421, 2292, 399, 2032, 1352, 1641, 818, 2221, 993, 484, 1500, 2185, 1926, 1834, 2482, 156, 303, 153, 2317, 299, 2067, 1762, 733, 359, 2154, 991, 1750, 2050, 792, 1378, 2268, 2138, 1341, 2145, 277, 2181, 378, 1520, 783, 1894, 2309, 2554, 1789, 878, 1616, 2244, 853, 2367, 328, 2249, 2247, 2405, 1559, 147, 455, 1370, 2218, 2087, 863, 2241, 2300, 2363, 1242, 1163, 624, 2245, 1548, 197, 1402, 757, 1599, 1715, 1422, 606, 2082, 8, 1646, 372, 2460, 1561, 1411, 2276, 1260, 983, 1985, 208, 287, 689, 1597, 883, 1929, 2000, 2170, 1326, 244, 1226, 356, 841, 726, 651, 367, 1425, 148, 1887, 2324, 780, 1275, 1699, 1366, 1063, 2291, 1658, 2081, 2261, 1998, 103, 2465, 724, 1205, 419, 2184, 1554, 36, 2497, 1659, 580, 469, 389, 6, 555, 1867, 1367, 825, 1482, 1312, 1598, 565, 1020, 195, 882, 2302, 1375, 1138, 377, 1327, 1319, 2448, 2007, 1236, 861, 1083, 2110, 2198, 1324, 1638, 1440, 2454, 914, 934, 1010, 534, 1495, 1347, 496, 1988, 114, 1776, 1435, 2349, 615, 1685, 388, 68, 1693, 1808, 471, 1780, 735, 2389, 1038, 1749, 1057, 816, 613, 1111, 549, 1810, 151, 1348, 29, 846, 1105, 339, 813, 296, 660, 1965, 1229, 2213, 1543, 251, 276, 1791, 1821, 1648, 501, 1647, 1744, 2196, 454, 1874, 2427, 2223, 1300, 2534, 1709, 2294, 1394, 1840, 716, 532, 871, 2415, 1187, 2250, 2111, 382, 1073, 42, 2271, 1, 138, 2559, 1816, 1238, 278, 1689, 2323, 2093, 2108, 1920, 940, 2352, 1077, 1050, 1883, 1705, 527, 971, 589, 854, 1837, 753, 533, 674, 2077, 939, 249, 497, 1788, 33, 2549, 1506, 1509, 353, 1266, 320, 700, 2143, 51, 540, 2232, 1943, 1088, 2112, 553, 1253, 2428, 34, 627, 1340, 482, 597, 1720, 2479, 1110, 1530, 1775, 2120, 2264, 468, 546, 231, 1008, 1448, 1660, 1845, 2325, 1759, 1584, 265, 1331, 729, 104, 2106, 472, 1029, 867, 1696, 1169, 1839, 1522, 511, 1065, 2543, 1211, 1264, 429, 926], 'conv4': [1067, 99, 1620, 633, 1644, 2356, 558, 929, 348, 881, 1921, 468, 1706, 2285, 82, 966, 1975, 1678, 1173, 1741, 2388, 1928, 1658, 1148, 1254, 2471, 1451, 1261, 1444, 203, 2413, 1692, 2346, 484, 1521, 1727, 1656, 1795, 997, 1734, 1575, 675, 2543, 621, 2039, 670, 1622, 2209, 694, 1993, 1990, 1149, 2404, 969, 967, 893, 1660, 1766, 80, 2041, 2399, 862, 1585, 1797, 783, 1498, 2528, 1441, 1764, 808, 1416, 1517, 1539, 1364, 1047, 1957, 1667, 2258, 1859, 1867, 593, 1187, 1654, 1167, 2158, 960, 258, 615, 931, 366, 2359, 1131, 2207, 2353, 2211, 760, 768, 381, 1184, 1612, 806, 473, 1266, 139, 2438, 1710, 499, 1366, 2153, 272, 965, 1189, 772, 774, 339, 1369, 465, 38, 1191, 2524, 1320, 884, 1712, 2202, 1362, 107, 826, 778, 1809, 2463, 1996, 1548, 1659, 1288, 1483, 1580, 1138, 1221, 2015, 1719, 1030, 1472, 1313, 2437, 188, 1746, 2336, 1739, 1063, 1549, 779, 1323, 788, 1908, 1165, 535, 2031, 1485, 2242, 21, 2476, 1317, 1128, 1019, 309, 649, 641, 712, 1034, 891, 239, 691, 1284, 1757, 2449, 544, 2038, 688, 2491, 993, 1800, 1794, 2225, 1567, 264, 591, 795, 145, 845, 1354, 1813, 2395, 2551, 921, 1604, 830, 2021, 1109, 182, 2518, 512, 674, 1822, 1306, 1637, 2145, 2338, 2305, 390, 369, 1811, 368, 818, 592, 1534, 2421, 364, 1085, 2533, 575, 2176, 1326, 2435, 315, 486, 2530, 2482, 673, 704, 1708, 1135, 2522, 151, 13, 2212, 1183, 1437, 20, 2152, 2217, 722, 2268, 1310, 1418, 568, 976, 2537, 253, 412, 1699, 1904, 1657, 1246, 1263, 1738, 89, 1818, 1756, 734, 193, 91, 1798, 2068, 1296, 1791, 1316, 514, 1054, 786, 295, 1536, 1190, 2385, 1559, 432, 1732, 827, 1924, 2445, 2071, 1974, 518, 1415, 2286, 457, 2335, 520, 1429, 1929, 2237, 548, 1772, 2552, 708, 2251, 435, 547, 762, 479, 1569, 637, 1143, 1874, 1084, 183, 1445, 1101, 504, 2307, 586, 1496, 114, 2403, 690, 1302, 1035, 2321, 503, 823, 451, 2271, 243, 1040, 1123, 1962, 2093, 987, 3, 1278, 433, 1327, 2206, 2519, 1540, 903, 2165, 498, 952, 1265, 1652, 2029, 2327, 508, 1617, 331, 406, 721, 2008, 338, 2060, 2406, 113, 1801, 1392, 477, 1922, 2077, 1541, 81, 1271, 464, 607, 2133, 1344, 1473, 1058, 159, 227, 1698, 1610, 527, 1057, 1589, 1228, 2123, 652, 507, 2360, 2472, 598, 605, 1668, 152, 325, 1718]}
activations_sum[k] len:  96
activations_sum[k] 384
activations_sum[k]:  [50, 75, 56, 206, 354, 336, 365, 94, 118, 180]
topk_kernels len:  5
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3', 'conv4']
topk_kernels:  {'conv0': [25, 59, 77, 30, 5, 82, 33, 16, 89, 42, 68, 70, 53, 35, 93], 'conv1': [50, 75, 56, 206, 354, 336, 365, 94, 118, 180, 345, 41, 191, 382, 140, 317, 104, 121, 352, 324, 92, 99, 264, 190, 304, 233, 105, 100, 117, 88, 202, 67, 144, 357, 26, 355, 326, 163, 130, 137, 146, 62, 36, 6, 312, 68, 217, 219, 39, 35, 296, 349, 197, 134, 171, 131, 33, 193, 292], 'conv2': [937, 407, 636, 307, 700, 176, 1086, 195, 1385, 1171, 405, 1317, 1278, 921, 673, 427, 799, 1034, 1486, 854, 421, 311, 731, 616, 794, 1307, 35, 706, 1500, 544, 194, 1360, 450, 958, 598, 1291, 883, 66, 560, 1306, 710, 869, 14, 93, 932, 141, 548, 155, 312, 796, 361, 167, 37, 1534, 851, 490, 551, 1000, 1412, 445, 686, 1226, 838, 906, 1030, 100, 13, 812, 1363, 730, 687, 660, 1006, 139, 150, 268, 437, 1281, 889, 380, 856, 635, 77, 1430, 123, 1286, 1109, 346, 482, 1511, 67, 1359, 1386, 1056, 1294, 786, 1239, 683, 28, 499, 276, 95, 574, 761, 914, 818, 924, 287, 12, 1479, 571, 18, 1405, 520, 955, 566, 699, 1508, 247, 56, 916, 11, 337, 1284, 217, 562, 662, 612, 1242, 1496, 1466, 109, 1442, 304, 1230, 881, 853, 1371, 1395, 1061, 1372, 1007, 1238, 1436, 618, 41, 1263, 634, 577, 1069, 1489, 1272, 353, 1123, 803, 1101, 199, 640, 1134, 946, 105, 726, 1297, 1292, 524, 698, 82, 266, 1472, 814, 626, 622, 174, 289, 1521, 1082, 582, 1041, 1383, 667, 743, 348, 461, 1464, 319, 114, 1071, 1480, 987, 1050, 817, 1046, 302, 489, 747, 610, 1085, 243, 61, 1145, 207, 492, 503, 542, 534, 1096, 1130, 1252, 1408, 1424, 923, 1160, 1092, 1274, 621, 1355, 229, 760, 458, 1019, 1332, 462, 900, 451, 1139, 1159, 1100, 732, 1027, 1295, 1515], 'conv3': [499, 507, 1677, 1339, 512, 1958, 653, 1443, 300, 2049, 1447, 921, 1755, 340, 2315, 1827, 43, 1862, 1359, 652, 2494, 1938, 446, 2095, 2402, 1403, 2157, 505, 2354, 1127, 1303, 1567, 193, 2103, 1109, 784, 2025, 288, 1670, 243, 2379, 520, 1608, 40, 450, 2435, 1783, 2132, 508, 611, 1465, 1710, 481, 1618, 1843, 2100, 1061, 1125, 2115, 699, 1656, 2414, 1602, 1964, 1510, 2021, 421, 2292, 399, 2032, 1352, 1641, 818, 2221, 993, 484, 1500, 2185, 1926, 1834, 2482, 156, 303, 153, 2317, 299, 2067, 1762, 733, 359, 2154, 991, 1750, 2050, 792, 1378, 2268, 2138, 1341, 2145, 277, 2181, 378, 1520, 783, 1894, 2309, 2554, 1789, 878, 1616, 2244, 853, 2367, 328, 2249, 2247, 2405, 1559, 147, 455, 1370, 2218, 2087, 863, 2241, 2300, 2363, 1242, 1163, 624, 2245, 1548, 197, 1402, 757, 1599, 1715, 1422, 606, 2082, 8, 1646, 372, 2460, 1561, 1411, 2276, 1260, 983, 1985, 208, 287, 689, 1597, 883, 1929, 2000, 2170, 1326, 244, 1226, 356, 841, 726, 651, 367, 1425, 148, 1887, 2324, 780, 1275, 1699, 1366, 1063, 2291, 1658, 2081, 2261, 1998, 103, 2465, 724, 1205, 419, 2184, 1554, 36, 2497, 1659, 580, 469, 389, 6, 555, 1867, 1367, 825, 1482, 1312, 1598, 565, 1020, 195, 882, 2302, 1375, 1138, 377, 1327, 1319, 2448, 2007, 1236, 861, 1083, 2110, 2198, 1324, 1638, 1440, 2454, 914, 934, 1010, 534, 1495, 1347, 496, 1988, 114, 1776, 1435, 2349, 615, 1685, 388, 68, 1693, 1808, 471, 1780, 735, 2389, 1038, 1749, 1057, 816, 613, 1111, 549, 1810, 151, 1348, 29, 846, 1105, 339, 813, 296, 660, 1965, 1229, 2213, 1543, 251, 276, 1791, 1821, 1648, 501, 1647, 1744, 2196, 454, 1874, 2427, 2223, 1300, 2534, 1709, 2294, 1394, 1840, 716, 532, 871, 2415, 1187, 2250, 2111, 382, 1073, 42, 2271, 1, 138, 2559, 1816, 1238, 278, 1689, 2323, 2093, 2108, 1920, 940, 2352, 1077, 1050, 1883, 1705, 527, 971, 589, 854, 1837, 753, 533, 674, 2077, 939, 249, 497, 1788, 33, 2549, 1506, 1509, 353, 1266, 320, 700, 2143, 51, 540, 2232, 1943, 1088, 2112, 553, 1253, 2428, 34, 627, 1340, 482, 597, 1720, 2479, 1110, 1530, 1775, 2120, 2264, 468, 546, 231, 1008, 1448, 1660, 1845, 2325, 1759, 1584, 265, 1331, 729, 104, 2106, 472, 1029, 867, 1696, 1169, 1839, 1522, 511, 1065, 2543, 1211, 1264, 429, 926], 'conv4': [1067, 99, 1620, 633, 1644, 2356, 558, 929, 348, 881, 1921, 468, 1706, 2285, 82, 966, 1975, 1678, 1173, 1741, 2388, 1928, 1658, 1148, 1254, 2471, 1451, 1261, 1444, 203, 2413, 1692, 2346, 484, 1521, 1727, 1656, 1795, 997, 1734, 1575, 675, 2543, 621, 2039, 670, 1622, 2209, 694, 1993, 1990, 1149, 2404, 969, 967, 893, 1660, 1766, 80, 2041, 2399, 862, 1585, 1797, 783, 1498, 2528, 1441, 1764, 808, 1416, 1517, 1539, 1364, 1047, 1957, 1667, 2258, 1859, 1867, 593, 1187, 1654, 1167, 2158, 960, 258, 615, 931, 366, 2359, 1131, 2207, 2353, 2211, 760, 768, 381, 1184, 1612, 806, 473, 1266, 139, 2438, 1710, 499, 1366, 2153, 272, 965, 1189, 772, 774, 339, 1369, 465, 38, 1191, 2524, 1320, 884, 1712, 2202, 1362, 107, 826, 778, 1809, 2463, 1996, 1548, 1659, 1288, 1483, 1580, 1138, 1221, 2015, 1719, 1030, 1472, 1313, 2437, 188, 1746, 2336, 1739, 1063, 1549, 779, 1323, 788, 1908, 1165, 535, 2031, 1485, 2242, 21, 2476, 1317, 1128, 1019, 309, 649, 641, 712, 1034, 891, 239, 691, 1284, 1757, 2449, 544, 2038, 688, 2491, 993, 1800, 1794, 2225, 1567, 264, 591, 795, 145, 845, 1354, 1813, 2395, 2551, 921, 1604, 830, 2021, 1109, 182, 2518, 512, 674, 1822, 1306, 1637, 2145, 2338, 2305, 390, 369, 1811, 368, 818, 592, 1534, 2421, 364, 1085, 2533, 575, 2176, 1326, 2435, 315, 486, 2530, 2482, 673, 704, 1708, 1135, 2522, 151, 13, 2212, 1183, 1437, 20, 2152, 2217, 722, 2268, 1310, 1418, 568, 976, 2537, 253, 412, 1699, 1904, 1657, 1246, 1263, 1738, 89, 1818, 1756, 734, 193, 91, 1798, 2068, 1296, 1791, 1316, 514, 1054, 786, 295, 1536, 1190, 2385, 1559, 432, 1732, 827, 1924, 2445, 2071, 1974, 518, 1415, 2286, 457, 2335, 520, 1429, 1929, 2237, 548, 1772, 2552, 708, 2251, 435, 547, 762, 479, 1569, 637, 1143, 1874, 1084, 183, 1445, 1101, 504, 2307, 586, 1496, 114, 2403, 690, 1302, 1035, 2321, 503, 823, 451, 2271, 243, 1040, 1123, 1962, 2093, 987, 3, 1278, 433, 1327, 2206, 2519, 1540, 903, 2165, 498, 952, 1265, 1652, 2029, 2327, 508, 1617, 331, 406, 721, 2008, 338, 2060, 2406, 113, 1801, 1392, 477, 1922, 2077, 1541, 81, 1271, 464, 607, 2133, 1344, 1473, 1058, 159, 227, 1698, 1610, 527, 1057, 1589, 1228, 2123, 652, 507, 2360, 2472, 598, 605, 1668, 152, 325, 1718]}
activations_sum[k] len:  384
activations_sum[k] 1536
activations_sum[k]:  [761, 958, 937, 307, 176, 195, 1317, 622, 95, 856]
topk_kernels len:  5
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3', 'conv4']
topk_kernels:  {'conv0': [25, 59, 77, 30, 5, 82, 33, 16, 89, 42, 68, 70, 53, 35, 93], 'conv1': [50, 75, 56, 206, 354, 336, 365, 94, 118, 180, 345, 41, 191, 382, 140, 317, 104, 121, 352, 324, 92, 99, 264, 190, 304, 233, 105, 100, 117, 88, 202, 67, 144, 357, 26, 355, 326, 163, 130, 137, 146, 62, 36, 6, 312, 68, 217, 219, 39, 35, 296, 349, 197, 134, 171, 131, 33, 193, 292], 'conv2': [761, 958, 937, 307, 176, 195, 1317, 622, 95, 856, 1486, 731, 1385, 544, 1436, 1297, 612, 636, 1274, 311, 673, 407, 1294, 490, 348, 700, 1278, 421, 1171, 450, 35, 921, 626, 405, 250, 1281, 733, 353, 14, 499, 93, 1292, 312, 395, 380, 427, 1046, 905, 208, 1235, 1452, 1086, 1006, 61, 799, 457, 932, 869, 56, 1307, 1034, 851, 484, 981, 548, 1123, 855, 12, 1360, 639, 359, 1508, 209, 580, 1159, 1442, 743, 1404, 635, 1236, 1412, 1345, 577, 144, 66, 361, 1291, 1359, 276, 796, 1534, 1450, 1061, 866, 1306, 783, 900, 289, 123, 846, 1203, 730, 838, 818, 164, 1388, 1125, 1193, 1227, 875, 1100, 881, 105, 520, 67, 287, 1049, 1113, 574, 1105, 1083, 1000, 683, 1071, 667, 1521, 1286, 1126, 11, 243, 916, 449, 618, 698, 1466, 735, 1272, 952, 1405, 896, 229, 670, 294, 794, 1232, 542, 460, 198, 924, 850, 141, 746, 614, 1371, 1400, 773, 1242, 810, 268, 1172, 973, 155, 534, 167, 1421, 524, 1480, 711, 754, 1434, 454, 1535, 246, 911, 109, 379, 560, 681, 1500, 434, 906, 341, 692, 261, 994, 726, 942, 801, 302, 728, 598, 1030, 1233, 100, 1441, 1265, 852, 1238, 408, 991, 805, 368, 217, 687, 1008, 1116, 1464, 854, 1386, 313, 1372, 523, 1230, 935, 423, 812, 329, 1033, 199, 1325, 1516, 309, 1180, 1314, 15, 1472, 1361, 1455, 461, 640, 1489], 'conv3': [499, 507, 1677, 1339, 512, 1958, 653, 1443, 300, 2049, 1447, 921, 1755, 340, 2315, 1827, 43, 1862, 1359, 652, 2494, 1938, 446, 2095, 2402, 1403, 2157, 505, 2354, 1127, 1303, 1567, 193, 2103, 1109, 784, 2025, 288, 1670, 243, 2379, 520, 1608, 40, 450, 2435, 1783, 2132, 508, 611, 1465, 1710, 481, 1618, 1843, 2100, 1061, 1125, 2115, 699, 1656, 2414, 1602, 1964, 1510, 2021, 421, 2292, 399, 2032, 1352, 1641, 818, 2221, 993, 484, 1500, 2185, 1926, 1834, 2482, 156, 303, 153, 2317, 299, 2067, 1762, 733, 359, 2154, 991, 1750, 2050, 792, 1378, 2268, 2138, 1341, 2145, 277, 2181, 378, 1520, 783, 1894, 2309, 2554, 1789, 878, 1616, 2244, 853, 2367, 328, 2249, 2247, 2405, 1559, 147, 455, 1370, 2218, 2087, 863, 2241, 2300, 2363, 1242, 1163, 624, 2245, 1548, 197, 1402, 757, 1599, 1715, 1422, 606, 2082, 8, 1646, 372, 2460, 1561, 1411, 2276, 1260, 983, 1985, 208, 287, 689, 1597, 883, 1929, 2000, 2170, 1326, 244, 1226, 356, 841, 726, 651, 367, 1425, 148, 1887, 2324, 780, 1275, 1699, 1366, 1063, 2291, 1658, 2081, 2261, 1998, 103, 2465, 724, 1205, 419, 2184, 1554, 36, 2497, 1659, 580, 469, 389, 6, 555, 1867, 1367, 825, 1482, 1312, 1598, 565, 1020, 195, 882, 2302, 1375, 1138, 377, 1327, 1319, 2448, 2007, 1236, 861, 1083, 2110, 2198, 1324, 1638, 1440, 2454, 914, 934, 1010, 534, 1495, 1347, 496, 1988, 114, 1776, 1435, 2349, 615, 1685, 388, 68, 1693, 1808, 471, 1780, 735, 2389, 1038, 1749, 1057, 816, 613, 1111, 549, 1810, 151, 1348, 29, 846, 1105, 339, 813, 296, 660, 1965, 1229, 2213, 1543, 251, 276, 1791, 1821, 1648, 501, 1647, 1744, 2196, 454, 1874, 2427, 2223, 1300, 2534, 1709, 2294, 1394, 1840, 716, 532, 871, 2415, 1187, 2250, 2111, 382, 1073, 42, 2271, 1, 138, 2559, 1816, 1238, 278, 1689, 2323, 2093, 2108, 1920, 940, 2352, 1077, 1050, 1883, 1705, 527, 971, 589, 854, 1837, 753, 533, 674, 2077, 939, 249, 497, 1788, 33, 2549, 1506, 1509, 353, 1266, 320, 700, 2143, 51, 540, 2232, 1943, 1088, 2112, 553, 1253, 2428, 34, 627, 1340, 482, 597, 1720, 2479, 1110, 1530, 1775, 2120, 2264, 468, 546, 231, 1008, 1448, 1660, 1845, 2325, 1759, 1584, 265, 1331, 729, 104, 2106, 472, 1029, 867, 1696, 1169, 1839, 1522, 511, 1065, 2543, 1211, 1264, 429, 926], 'conv4': [1067, 99, 1620, 633, 1644, 2356, 558, 929, 348, 881, 1921, 468, 1706, 2285, 82, 966, 1975, 1678, 1173, 1741, 2388, 1928, 1658, 1148, 1254, 2471, 1451, 1261, 1444, 203, 2413, 1692, 2346, 484, 1521, 1727, 1656, 1795, 997, 1734, 1575, 675, 2543, 621, 2039, 670, 1622, 2209, 694, 1993, 1990, 1149, 2404, 969, 967, 893, 1660, 1766, 80, 2041, 2399, 862, 1585, 1797, 783, 1498, 2528, 1441, 1764, 808, 1416, 1517, 1539, 1364, 1047, 1957, 1667, 2258, 1859, 1867, 593, 1187, 1654, 1167, 2158, 960, 258, 615, 931, 366, 2359, 1131, 2207, 2353, 2211, 760, 768, 381, 1184, 1612, 806, 473, 1266, 139, 2438, 1710, 499, 1366, 2153, 272, 965, 1189, 772, 774, 339, 1369, 465, 38, 1191, 2524, 1320, 884, 1712, 2202, 1362, 107, 826, 778, 1809, 2463, 1996, 1548, 1659, 1288, 1483, 1580, 1138, 1221, 2015, 1719, 1030, 1472, 1313, 2437, 188, 1746, 2336, 1739, 1063, 1549, 779, 1323, 788, 1908, 1165, 535, 2031, 1485, 2242, 21, 2476, 1317, 1128, 1019, 309, 649, 641, 712, 1034, 891, 239, 691, 1284, 1757, 2449, 544, 2038, 688, 2491, 993, 1800, 1794, 2225, 1567, 264, 591, 795, 145, 845, 1354, 1813, 2395, 2551, 921, 1604, 830, 2021, 1109, 182, 2518, 512, 674, 1822, 1306, 1637, 2145, 2338, 2305, 390, 369, 1811, 368, 818, 592, 1534, 2421, 364, 1085, 2533, 575, 2176, 1326, 2435, 315, 486, 2530, 2482, 673, 704, 1708, 1135, 2522, 151, 13, 2212, 1183, 1437, 20, 2152, 2217, 722, 2268, 1310, 1418, 568, 976, 2537, 253, 412, 1699, 1904, 1657, 1246, 1263, 1738, 89, 1818, 1756, 734, 193, 91, 1798, 2068, 1296, 1791, 1316, 514, 1054, 786, 295, 1536, 1190, 2385, 1559, 432, 1732, 827, 1924, 2445, 2071, 1974, 518, 1415, 2286, 457, 2335, 520, 1429, 1929, 2237, 548, 1772, 2552, 708, 2251, 435, 547, 762, 479, 1569, 637, 1143, 1874, 1084, 183, 1445, 1101, 504, 2307, 586, 1496, 114, 2403, 690, 1302, 1035, 2321, 503, 823, 451, 2271, 243, 1040, 1123, 1962, 2093, 987, 3, 1278, 433, 1327, 2206, 2519, 1540, 903, 2165, 498, 952, 1265, 1652, 2029, 2327, 508, 1617, 331, 406, 721, 2008, 338, 2060, 2406, 113, 1801, 1392, 477, 1922, 2077, 1541, 81, 1271, 464, 607, 2133, 1344, 1473, 1058, 159, 227, 1698, 1610, 527, 1057, 1589, 1228, 2123, 652, 507, 2360, 2472, 598, 605, 1668, 152, 325, 1718]}
activations_sum[k] len:  1536
activations_sum[k] 2560
activations_sum[k]:  [499, 1755, 1447, 505, 1394, 507, 1567, 1720, 653, 2500]
topk_kernels len:  5
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3', 'conv4']
topk_kernels:  {'conv0': [25, 59, 77, 30, 5, 82, 33, 16, 89, 42, 68, 70, 53, 35, 93], 'conv1': [50, 75, 56, 206, 354, 336, 365, 94, 118, 180, 345, 41, 191, 382, 140, 317, 104, 121, 352, 324, 92, 99, 264, 190, 304, 233, 105, 100, 117, 88, 202, 67, 144, 357, 26, 355, 326, 163, 130, 137, 146, 62, 36, 6, 312, 68, 217, 219, 39, 35, 296, 349, 197, 134, 171, 131, 33, 193, 292], 'conv2': [761, 958, 937, 307, 176, 195, 1317, 622, 95, 856, 1486, 731, 1385, 544, 1436, 1297, 612, 636, 1274, 311, 673, 407, 1294, 490, 348, 700, 1278, 421, 1171, 450, 35, 921, 626, 405, 250, 1281, 733, 353, 14, 499, 93, 1292, 312, 395, 380, 427, 1046, 905, 208, 1235, 1452, 1086, 1006, 61, 799, 457, 932, 869, 56, 1307, 1034, 851, 484, 981, 548, 1123, 855, 12, 1360, 639, 359, 1508, 209, 580, 1159, 1442, 743, 1404, 635, 1236, 1412, 1345, 577, 144, 66, 361, 1291, 1359, 276, 796, 1534, 1450, 1061, 866, 1306, 783, 900, 289, 123, 846, 1203, 730, 838, 818, 164, 1388, 1125, 1193, 1227, 875, 1100, 881, 105, 520, 67, 287, 1049, 1113, 574, 1105, 1083, 1000, 683, 1071, 667, 1521, 1286, 1126, 11, 243, 916, 449, 618, 698, 1466, 735, 1272, 952, 1405, 896, 229, 670, 294, 794, 1232, 542, 460, 198, 924, 850, 141, 746, 614, 1371, 1400, 773, 1242, 810, 268, 1172, 973, 155, 534, 167, 1421, 524, 1480, 711, 754, 1434, 454, 1535, 246, 911, 109, 379, 560, 681, 1500, 434, 906, 341, 692, 261, 994, 726, 942, 801, 302, 728, 598, 1030, 1233, 100, 1441, 1265, 852, 1238, 408, 991, 805, 368, 217, 687, 1008, 1116, 1464, 854, 1386, 313, 1372, 523, 1230, 935, 423, 812, 329, 1033, 199, 1325, 1516, 309, 1180, 1314, 15, 1472, 1361, 1455, 461, 640, 1489], 'conv3': [499, 1755, 1447, 505, 1394, 507, 1567, 1720, 653, 2500, 861, 2300, 398, 421, 689, 2309, 1762, 1554, 1137, 2460, 2154, 1348, 1843, 1506, 28, 1403, 1873, 340, 652, 2448, 589, 1783, 1326, 1127, 878, 648, 2106, 468, 2203, 2181, 2285, 2340, 43, 2062, 1943, 1894, 2185, 938, 1831, 828, 1617, 1536, 419, 1500, 1179, 1558, 1226, 512, 2317, 1100, 533, 80, 935, 1723, 1162, 1845, 758, 481, 1887, 2000, 1938, 1440, 792, 2241, 2282, 1867, 646, 2502, 926, 2518, 1599, 1359, 284, 540, 555, 1187, 1375, 2515, 390, 1406, 1433, 1054, 1619, 607, 1457, 1968, 1405, 971, 225, 2008, 746, 1789, 1206, 1438, 96, 593, 2072, 2086, 1164, 2117, 1125, 810, 1339, 2523, 1608, 1713, 2095, 1794, 2494, 1080, 526, 2391, 425, 1638, 1868, 446, 1597, 819, 520, 2, 1561, 2112, 2223, 2078, 1616, 1555, 177, 564, 1088, 194, 1533, 1942, 1253, 103, 2545, 2162, 1109, 354, 1218, 1275, 1448, 2254, 1380, 1698, 1217, 2114, 339, 781, 2441, 1280, 151, 1324, 2037, 1677, 775, 1964, 280, 2081, 1872, 36, 724, 81, 509, 669, 1061, 1759, 1781, 2151, 193, 277, 2456, 1644, 761, 2442, 231, 1341, 2315, 1770, 2453, 1647, 2183, 907, 1248, 2474, 1516, 362, 1492, 1449, 454, 240, 1995, 1874, 393, 559, 508, 2271, 472, 624, 1543, 2170, 470, 687, 2511, 2110, 1319, 2559, 1363, 884, 546, 291, 1424, 1097, 990, 1958, 2367, 2479, 145, 45, 1656, 1548, 2060, 943, 1684, 2084, 129, 1776, 1824, 2050, 613, 1613, 2001, 252, 780, 1153, 655, 818, 867, 1436, 1689, 2270, 1651, 1992, 6, 1385, 1346, 2328, 1170, 1788, 2067, 2134, 450, 1038, 1694, 962, 934, 697, 1132, 2283, 615, 1409, 1439, 37, 1510, 1799, 2525, 484, 29, 480, 611, 1150, 195, 1283, 236, 2305, 1664, 272, 313, 2465, 632, 1303, 683, 1194, 665, 202, 660, 365, 674, 730, 737, 1497, 1465, 2352, 63, 1926, 2028, 1646, 548, 2555, 1378, 172, 720, 1443, 980, 2323, 2482, 299, 1660, 1360, 557, 921, 40, 939, 1862, 2098, 1267, 995, 832, 1, 1393, 423, 1808, 1495, 399, 356, 1446, 662, 487, 1224, 2013, 1469, 663, 2090, 1225, 1924, 409, 1340, 2103, 2302, 1830, 588, 2357, 2093, 2003, 2033, 477, 2046, 394, 1266, 616, 1758, 2188, 1119, 370, 360, 2295, 297, 1415, 1292, 1499, 991, 1381, 2414, 11, 2031, 1659, 1998, 1670, 431, 1844, 1431, 598, 1370, 2532, 2011, 1996], 'conv4': [1067, 99, 1620, 633, 1644, 2356, 558, 929, 348, 881, 1921, 468, 1706, 2285, 82, 966, 1975, 1678, 1173, 1741, 2388, 1928, 1658, 1148, 1254, 2471, 1451, 1261, 1444, 203, 2413, 1692, 2346, 484, 1521, 1727, 1656, 1795, 997, 1734, 1575, 675, 2543, 621, 2039, 670, 1622, 2209, 694, 1993, 1990, 1149, 2404, 969, 967, 893, 1660, 1766, 80, 2041, 2399, 862, 1585, 1797, 783, 1498, 2528, 1441, 1764, 808, 1416, 1517, 1539, 1364, 1047, 1957, 1667, 2258, 1859, 1867, 593, 1187, 1654, 1167, 2158, 960, 258, 615, 931, 366, 2359, 1131, 2207, 2353, 2211, 760, 768, 381, 1184, 1612, 806, 473, 1266, 139, 2438, 1710, 499, 1366, 2153, 272, 965, 1189, 772, 774, 339, 1369, 465, 38, 1191, 2524, 1320, 884, 1712, 2202, 1362, 107, 826, 778, 1809, 2463, 1996, 1548, 1659, 1288, 1483, 1580, 1138, 1221, 2015, 1719, 1030, 1472, 1313, 2437, 188, 1746, 2336, 1739, 1063, 1549, 779, 1323, 788, 1908, 1165, 535, 2031, 1485, 2242, 21, 2476, 1317, 1128, 1019, 309, 649, 641, 712, 1034, 891, 239, 691, 1284, 1757, 2449, 544, 2038, 688, 2491, 993, 1800, 1794, 2225, 1567, 264, 591, 795, 145, 845, 1354, 1813, 2395, 2551, 921, 1604, 830, 2021, 1109, 182, 2518, 512, 674, 1822, 1306, 1637, 2145, 2338, 2305, 390, 369, 1811, 368, 818, 592, 1534, 2421, 364, 1085, 2533, 575, 2176, 1326, 2435, 315, 486, 2530, 2482, 673, 704, 1708, 1135, 2522, 151, 13, 2212, 1183, 1437, 20, 2152, 2217, 722, 2268, 1310, 1418, 568, 976, 2537, 253, 412, 1699, 1904, 1657, 1246, 1263, 1738, 89, 1818, 1756, 734, 193, 91, 1798, 2068, 1296, 1791, 1316, 514, 1054, 786, 295, 1536, 1190, 2385, 1559, 432, 1732, 827, 1924, 2445, 2071, 1974, 518, 1415, 2286, 457, 2335, 520, 1429, 1929, 2237, 548, 1772, 2552, 708, 2251, 435, 547, 762, 479, 1569, 637, 1143, 1874, 1084, 183, 1445, 1101, 504, 2307, 586, 1496, 114, 2403, 690, 1302, 1035, 2321, 503, 823, 451, 2271, 243, 1040, 1123, 1962, 2093, 987, 3, 1278, 433, 1327, 2206, 2519, 1540, 903, 2165, 498, 952, 1265, 1652, 2029, 2327, 508, 1617, 331, 406, 721, 2008, 338, 2060, 2406, 113, 1801, 1392, 477, 1922, 2077, 1541, 81, 1271, 464, 607, 2133, 1344, 1473, 1058, 159, 227, 1698, 1610, 527, 1057, 1589, 1228, 2123, 652, 507, 2360, 2472, 598, 605, 1668, 152, 325, 1718]}
activations_sum[k] len:  2560
activations_sum[k] 2560
activations_sum[k]:  [1323, 433, 929, 593, 448, 671, 2211, 2271, 2518, 2285]
topk_kernels len:  5
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3', 'conv4']
topk_kernels:  {'conv0': [25, 59, 77, 30, 5, 82, 33, 16, 89, 42, 68, 70, 53, 35, 93], 'conv1': [50, 75, 56, 206, 354, 336, 365, 94, 118, 180, 345, 41, 191, 382, 140, 317, 104, 121, 352, 324, 92, 99, 264, 190, 304, 233, 105, 100, 117, 88, 202, 67, 144, 357, 26, 355, 326, 163, 130, 137, 146, 62, 36, 6, 312, 68, 217, 219, 39, 35, 296, 349, 197, 134, 171, 131, 33, 193, 292], 'conv2': [761, 958, 937, 307, 176, 195, 1317, 622, 95, 856, 1486, 731, 1385, 544, 1436, 1297, 612, 636, 1274, 311, 673, 407, 1294, 490, 348, 700, 1278, 421, 1171, 450, 35, 921, 626, 405, 250, 1281, 733, 353, 14, 499, 93, 1292, 312, 395, 380, 427, 1046, 905, 208, 1235, 1452, 1086, 1006, 61, 799, 457, 932, 869, 56, 1307, 1034, 851, 484, 981, 548, 1123, 855, 12, 1360, 639, 359, 1508, 209, 580, 1159, 1442, 743, 1404, 635, 1236, 1412, 1345, 577, 144, 66, 361, 1291, 1359, 276, 796, 1534, 1450, 1061, 866, 1306, 783, 900, 289, 123, 846, 1203, 730, 838, 818, 164, 1388, 1125, 1193, 1227, 875, 1100, 881, 105, 520, 67, 287, 1049, 1113, 574, 1105, 1083, 1000, 683, 1071, 667, 1521, 1286, 1126, 11, 243, 916, 449, 618, 698, 1466, 735, 1272, 952, 1405, 896, 229, 670, 294, 794, 1232, 542, 460, 198, 924, 850, 141, 746, 614, 1371, 1400, 773, 1242, 810, 268, 1172, 973, 155, 534, 167, 1421, 524, 1480, 711, 754, 1434, 454, 1535, 246, 911, 109, 379, 560, 681, 1500, 434, 906, 341, 692, 261, 994, 726, 942, 801, 302, 728, 598, 1030, 1233, 100, 1441, 1265, 852, 1238, 408, 991, 805, 368, 217, 687, 1008, 1116, 1464, 854, 1386, 313, 1372, 523, 1230, 935, 423, 812, 329, 1033, 199, 1325, 1516, 309, 1180, 1314, 15, 1472, 1361, 1455, 461, 640, 1489], 'conv3': [499, 1755, 1447, 505, 1394, 507, 1567, 1720, 653, 2500, 861, 2300, 398, 421, 689, 2309, 1762, 1554, 1137, 2460, 2154, 1348, 1843, 1506, 28, 1403, 1873, 340, 652, 2448, 589, 1783, 1326, 1127, 878, 648, 2106, 468, 2203, 2181, 2285, 2340, 43, 2062, 1943, 1894, 2185, 938, 1831, 828, 1617, 1536, 419, 1500, 1179, 1558, 1226, 512, 2317, 1100, 533, 80, 935, 1723, 1162, 1845, 758, 481, 1887, 2000, 1938, 1440, 792, 2241, 2282, 1867, 646, 2502, 926, 2518, 1599, 1359, 284, 540, 555, 1187, 1375, 2515, 390, 1406, 1433, 1054, 1619, 607, 1457, 1968, 1405, 971, 225, 2008, 746, 1789, 1206, 1438, 96, 593, 2072, 2086, 1164, 2117, 1125, 810, 1339, 2523, 1608, 1713, 2095, 1794, 2494, 1080, 526, 2391, 425, 1638, 1868, 446, 1597, 819, 520, 2, 1561, 2112, 2223, 2078, 1616, 1555, 177, 564, 1088, 194, 1533, 1942, 1253, 103, 2545, 2162, 1109, 354, 1218, 1275, 1448, 2254, 1380, 1698, 1217, 2114, 339, 781, 2441, 1280, 151, 1324, 2037, 1677, 775, 1964, 280, 2081, 1872, 36, 724, 81, 509, 669, 1061, 1759, 1781, 2151, 193, 277, 2456, 1644, 761, 2442, 231, 1341, 2315, 1770, 2453, 1647, 2183, 907, 1248, 2474, 1516, 362, 1492, 1449, 454, 240, 1995, 1874, 393, 559, 508, 2271, 472, 624, 1543, 2170, 470, 687, 2511, 2110, 1319, 2559, 1363, 884, 546, 291, 1424, 1097, 990, 1958, 2367, 2479, 145, 45, 1656, 1548, 2060, 943, 1684, 2084, 129, 1776, 1824, 2050, 613, 1613, 2001, 252, 780, 1153, 655, 818, 867, 1436, 1689, 2270, 1651, 1992, 6, 1385, 1346, 2328, 1170, 1788, 2067, 2134, 450, 1038, 1694, 962, 934, 697, 1132, 2283, 615, 1409, 1439, 37, 1510, 1799, 2525, 484, 29, 480, 611, 1150, 195, 1283, 236, 2305, 1664, 272, 313, 2465, 632, 1303, 683, 1194, 665, 202, 660, 365, 674, 730, 737, 1497, 1465, 2352, 63, 1926, 2028, 1646, 548, 2555, 1378, 172, 720, 1443, 980, 2323, 2482, 299, 1660, 1360, 557, 921, 40, 939, 1862, 2098, 1267, 995, 832, 1, 1393, 423, 1808, 1495, 399, 356, 1446, 662, 487, 1224, 2013, 1469, 663, 2090, 1225, 1924, 409, 1340, 2103, 2302, 1830, 588, 2357, 2093, 2003, 2033, 477, 2046, 394, 1266, 616, 1758, 2188, 1119, 370, 360, 2295, 297, 1415, 1292, 1499, 991, 1381, 2414, 11, 2031, 1659, 1998, 1670, 431, 1844, 1431, 598, 1370, 2532, 2011, 1996], 'conv4': [1323, 433, 929, 593, 448, 671, 2211, 2271, 2518, 2285, 2385, 1620, 2268, 397, 1029, 1250, 1887, 2121, 1473, 966, 881, 1898, 499, 99, 1026, 1867, 2214, 91, 783, 546, 1572, 1629, 997, 514, 188, 784, 1784, 2038, 13, 1148, 2158, 624, 1996, 2046, 1989, 615, 969, 1167, 2039, 2356, 1908, 1149, 2395, 2519, 1654, 1376, 2528, 1741, 754, 603, 910, 673, 1519, 161, 1364, 139, 674, 608, 507, 372, 808, 1657, 1610, 2258, 1019, 1925, 2308, 1285, 1498, 2388, 1090, 2548, 1764, 721, 6, 2537, 137, 2015, 1356, 1822, 1131, 2461, 1791, 1708, 2276, 1617, 1067, 1730, 1267, 411, 2224, 348, 2253, 1712, 21, 696, 1261, 2006, 1522, 1942, 1485, 1706, 1064, 610, 1772, 1100, 218, 1710, 1007, 551, 1739, 1153, 1348, 1622, 567, 437, 3, 82, 1797, 1539, 633, 1990, 2012, 1366, 1184, 605, 1284, 1524, 978, 725, 183, 1830, 468, 2423, 1412, 67, 398, 607, 193, 2022, 1727, 34, 113, 774, 258, 1575, 788, 2307, 1471, 2551, 2320, 1218, 2326, 1360, 1777, 1424, 2476, 489, 1832, 1418, 1490, 1135, 2265, 843, 412, 1237, 2209, 1425, 1492, 2366, 1223, 391, 1929, 2145, 2210, 1169, 2557, 530, 2212, 29, 1789, 535, 761, 772, 147, 1095, 381, 426, 1583, 627, 1483, 1746, 2064, 865, 1246, 2327, 1513, 1724, 1897, 1819, 2227, 935, 2438, 990, 2558, 1193, 687, 1139, 1221, 926, 589, 945, 2355, 2544, 2321, 586, 1280, 1817, 2176, 494, 2471, 959, 1310, 340, 2135, 1774, 704, 413, 1080, 347, 181, 415, 702, 712, 1521, 2120, 366, 993, 1511, 1058, 1025, 2543, 1390, 1641, 24, 152, 312, 1801, 1795, 2552, 2415, 880, 1719, 2345, 1416, 28, 1375, 1904, 1354, 270, 2482, 2237, 1692, 1686, 1656, 2436, 1387, 806, 635, 1969, 1536, 918, 225, 142, 2510, 379, 1011, 395, 675, 963, 1063, 2256, 2066, 1214, 1691, 1846, 641, 1550, 1342, 1152, 2361, 329, 493, 2202, 1127, 339, 241, 2251, 666, 70, 126, 1966, 813, 2484, 830, 708, 85, 429, 637, 648, 315, 1400, 470, 656, 864, 2445, 2333, 1868, 1071, 2505, 2133, 1403, 2207, 2393, 1509, 16, 95, 716, 1036, 796, 975, 778, 1847, 2305, 1748, 1813, 2228, 949, 263, 1255, 2208, 1420, 1085, 1472, 802, 26, 2070, 2435, 558, 1053, 1434, 2138, 2238, 1216, 1144, 1683, 2538, 544, 2406, 477, 1119, 497, 1099, 625, 240, 455, 2334, 276, 863, 1892, 1234, 1596, 1578, 592, 1547]}
activations_sum[k] len:  2560
['conv0', 'conv1', 'conv2', 'conv3', 'conv4']
delta_weights INFO: 
NUM OF TRACKED COV LAYERS:  5
NUM OF TRACKED WEIGHTS CHANGES PER LAYER:  2
avg_deltas INFO:  <class 'dict'>
avg_deltas keys:  ['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight', 'blocks.4.layer.weight']
################################################
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 4.77e-02	time: 00:00:20	Acc_train 0.00	Acc_test 0.00	convergence: 2.28e+01	R1: 0	Info MB:0.000e+00/SB:0.000e+00/MW:3.091e-06/SW:1.579e-01/MR:2.379e+01/SR:2.855e+00/MeD:1.779e+00/MaD:2.274e+01/MW:0.615/MAW:0.385
|        0 |        1 |        2 |        3 |       4 |        5 |       6 |        7 |        8 |        9 |      10 |       11 |       12 |       13 |      14 |       15 |       16 |       17 |       18 |       19 |       20 |       21 |       22 |       23 |       24 |       25 |       26 |       27 |       28 |       29 |
|----------+----------+----------+----------+---------+----------+---------+----------+----------+----------+---------+----------+----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------|
|   0.0503 |   0.0483 |   0.0501 |   0.0489 |   0.041 |   0.0421 |   0.048 |   0.0475 |   0.0469 |   0.0421 |   0.048 |   0.0479 |   0.0481 |   0.0511 |   0.043 |   0.0486 |   0.0495 |   0.0496 |   0.0486 |   0.0492 |   0.0494 |   0.0478 |   0.0486 |   0.0472 |   0.0496 |   0.0489 |   0.0479 |   0.0495 |   0.0482 |   0.0501 |
|  26.32   |  24.36   |  26.07   |  24.91   |  17.81  |  18.76   |  24.04  |  23.58   |  22.99   |  18.75   |  24.02  |  23.97   |  24.14   |  27.1    |  19.47  |  24.58   |  25.47   |  25.57   |  24.63   |  25.24   |  25.44   |  23.83   |  24.61   |  23.26   |  25.56   |  24.88   |  23.91   |  25.46   |  24.27   |  26.07   |
|   0.01   |   0.06   |   0.02   |   0.01   |   0.1   |   0.08   |   0.03  |   0.11   |   0.09   |   0.09   |   0.05  |   0.05   |   0.04   |   0.09   |   0.06  |   0      |   0.01   |   0.02   |   0.02   |   0      |   0.02   |   0.15   |   0.02   |   0.02   |   0.01   |   0      |   0.02   |   0.01   |   0.04   |   0.02   |
| nan      | nan      | nan      | nan      | nan     | nan      | nan     | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan     | nan      | nan     | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan     | nan      | nan     | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:-2.498e-05/SW:2.001e-01/MR:2.338e+01/SR:2.654e+00/MeD:1.903e+00/MaD:1.898e+01/MW:0.543/MAW:0.457
|        0 |        1 |        2 |        3 |        4 |        5 |        6 |        7 |        8 |        9 |       10 |       11 |      12 |       13 |       14 |       15 |       16 |       17 |       18 |       19 |       20 |       21 |       22 |       23 |       24 |      25 |       26 |       27 |       28 |       29 |
|----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------|
|   0.0443 |   0.0497 |   0.0485 |   0.0478 |   0.0487 |   0.0456 |   0.0501 |   0.0476 |   0.0319 |   0.0494 |   0.0464 |   0.0476 |   0.049 |   0.0496 |   0.0493 |   0.0495 |   0.0499 |   0.0484 |   0.0477 |   0.0504 |   0.0483 |   0.0468 |   0.0489 |   0.0474 |   0.0468 |   0.043 |   0.0481 |   0.0488 |   0.0486 |   0.0478 |
|  20.6    |  25.7    |  24.55   |  23.86   |  24.67   |  21.78   |  26.08   |  23.68   |  11.18   |  25.43   |  22.55   |  23.68   |  25.02  |  25.56   |  25.3    |  25.47   |  25.85   |  24.45   |  23.75   |  26.45   |  24.34   |  22.92   |  24.95   |  23.49   |  22.86   |  19.53  |  24.11   |  24.86   |  24.64   |  23.82   |
|   0.02   |   0.07   |   0.04   |   0.05   |   0.01   |   0.07   |   0.08   |   0.02   |   0.04   |   0      |   0.03   |   0.03   |   0.01  |   0.02   |   0.04   |   0.05   |   0.02   |   0.03   |   0.01   |   0.01   |   0.01   |   0.03   |   0.01   |   0.03   |   0.02   |   0.01  |   0.08   |   0.01   |   0.08   |   0.02   |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:8.446e-05/SW:4.079e-01/MR:2.386e+01/SR:2.423e+00/MeD:1.902e+00/MaD:1.002e+01/MW:0.503/MAW:0.497
|        0 |        1 |        2 |        3 |        4 |        5 |        6 |        7 |        8 |        9 |       10 |       11 |      12 |       13 |      14 |       15 |       16 |       17 |       18 |       19 |       20 |       21 |       22 |       23 |       24 |       25 |       26 |       27 |       28 |       29 |
|----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------|
|   0.0494 |   0.0486 |   0.0479 |   0.0457 |   0.0453 |   0.0485 |   0.0499 |   0.0434 |   0.0492 |   0.0493 |   0.0483 |   0.0482 |   0.051 |   0.0414 |   0.053 |   0.0482 |   0.0432 |   0.0496 |   0.0458 |   0.0495 |   0.0416 |   0.0495 |   0.0481 |   0.0482 |   0.0475 |   0.0488 |   0.0491 |   0.0451 |   0.0425 |   0.0481 |
|  25.39   |  24.59   |  23.93   |  21.89   |  21.55   |  24.49   |  25.91   |  19.83   |  25.18   |  25.33   |  24.37   |  24.27   |  27.03  |  18.13   |  29.1   |  24.2    |  19.69   |  25.61   |  21.99   |  25.46   |  18.29   |  25.52   |  24.11   |  24.25   |  23.59   |  24.82   |  25.11   |  21.38   |  19.05   |  24.16   |
|   0      |   0.02   |   0.04   |   0.09   |   0.03   |   0.02   |   0.01   |   0.04   |   0.06   |   0.01   |   0.02   |   0.14   |   0.09  |   0.06   |   0.34  |   0.05   |   0.24   |   0.01   |   0.09   |   0.02   |   0.08   |   0.04   |   0.13   |   0.03   |   0.02   |   0.03   |   0.03   |   0.05   |   0.06   |   0.02   |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:-3.155e-03/SW:7.493e-01/MR:2.186e+01/SR:2.693e+00/MeD:2.209e+00/MaD:8.149e+00/MW:0.765/MAW:0.235
|        0 |        1 |        2 |       3 |        4 |        5 |        6 |        7 |       8 |        9 |       10 |       11 |       12 |       13 |       14 |       15 |       16 |       17 |       18 |       19 |       20 |       21 |       22 |       23 |       24 |      25 |       26 |       27 |       28 |       29 |
|----------+----------+----------+---------+----------+----------+----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------|
|   0.0231 |   0.0244 |   0.0241 |   0.024 |   0.0185 |   0.0213 |   0.0239 |   0.0238 |   0.024 |   0.0225 |   0.0247 |   0.0234 |   0.0241 |   0.0237 |   0.0242 |   0.0237 |   0.0233 |   0.0209 |   0.0227 |   0.0224 |   0.0223 |   0.0248 |   0.0228 |   0.0223 |   0.0231 |   0.023 |   0.0212 |   0.0232 |   0.0221 |   0.0247 |
|  22.27   |  24.75   |  24.27   |  23.96  |  14.68   |  19.21   |  23.87   |  23.64   |  24.02  |  21.26   |  25.45   |  22.92   |  24.15   |  23.4    |  24.39   |  23.45   |  22.7    |  18.39   |  21.62   |  21.1    |  20.9    |  25.58   |  21.83   |  20.94   |  22.37   |  22.15  |  19.01   |  22.46   |  20.61   |  25.37   |
|   0.27   |   0.04   |   0.03   |   0.04  |   0.57   |   0.13   |   0.12   |   0.18   |   0.08  |   0.17   |   0.02   |   0.12   |   0.11   |   0.18   |   0.04   |   0.11   |   0.1    |   0.52   |   0.17   |   0.23   |   0.24   |   0.04   |   0.42   |   0.41   |   0.18   |   0.18  |   0.77   |   0.13   |   0.19   |   0.08   |
| nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:-5.875e-03/SW:9.063e-01/MR:7.714e+00/SR:1.454e+00/MeD:1.192e+00/MaD:4.299e+00/MW:0.570/MAW:0.430
|       0 |       1 |       2 |       3 |      4 |       5 |       6 |       7 |       8 |       9 |      10 |      11 |      12 |     13 |      14 |      15 |      16 |      17 |      18 |      19 |      20 |      21 |      22 |      23 |      24 |      25 |      26 |      27 |      28 |      29 |
|---------+---------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------|
|   0.184 |   0.244 |   0.232 |   0.228 |   0.19 |   0.173 |   0.196 |   0.249 |   0.189 |   0.265 |   0.187 |   0.194 |   0.211 |   0.22 |   0.195 |   0.232 |   0.164 |   0.198 |   0.213 |   0.204 |   0.223 |   0.201 |   0.214 |   0.243 |   0.231 |   0.213 |   0.213 |   0.213 |   0.196 |   0.246 |
|   6.3   |  10.28  |   9.39  |   9.14  |   6.63 |   5.68  |   6.99  |  10.67  |   6.6   |  12.01  |   6.48  |   6.85  |   7.98  |   8.58 |   6.96  |   9.39  |   5.23  |   7.13  |   8.09  |   7.53  |   8.78  |   7.31  |   8.15  |  10.22  |   9.32  |   8.06  |   8.11  |   8.12  |   7.03  |  10.43  |
|   0.4   |   0.32  |   0.34  |   0.37  |   1.46 |   3.77  |   0.46  |   0.24  |   0.76  |   0.22  |   0.54  |   0.44  |   0.43  |   0.39 |   0.78  |   0.34  |   3.42  |   0.54  |   0.53  |   1.15  |   0.58  |   0.79  |   0.46  |   0.41  |   0.4   |  18.39  |   0.35  |   0.56  |   0.38  |   0.39  |
| nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     |
| nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     |
| nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c/models
SAVED HEADS THRESHOLD:  0.9016666666666667
SAVED model.topk_kernels {'conv0': [25, 59, 77, 30, 5, 82, 33, 16, 89, 42, 68, 70, 53, 35, 93], 'conv1': [50, 75, 56, 206, 354, 336, 365, 94, 118, 180, 345, 41, 191, 382, 140, 317, 104, 121, 352, 324, 92, 99, 264, 190, 304, 233, 105, 100, 117, 88, 202, 67, 144, 357, 26, 355, 326, 163, 130, 137, 146, 62, 36, 6, 312, 68, 217, 219, 39, 35, 296, 349, 197, 134, 171, 131, 33, 193, 292], 'conv2': [761, 958, 937, 307, 176, 195, 1317, 622, 95, 856, 1486, 731, 1385, 544, 1436, 1297, 612, 636, 1274, 311, 673, 407, 1294, 490, 348, 700, 1278, 421, 1171, 450, 35, 921, 626, 405, 250, 1281, 733, 353, 14, 499, 93, 1292, 312, 395, 380, 427, 1046, 905, 208, 1235, 1452, 1086, 1006, 61, 799, 457, 932, 869, 56, 1307, 1034, 851, 484, 981, 548, 1123, 855, 12, 1360, 639, 359, 1508, 209, 580, 1159, 1442, 743, 1404, 635, 1236, 1412, 1345, 577, 144, 66, 361, 1291, 1359, 276, 796, 1534, 1450, 1061, 866, 1306, 783, 900, 289, 123, 846, 1203, 730, 838, 818, 164, 1388, 1125, 1193, 1227, 875, 1100, 881, 105, 520, 67, 287, 1049, 1113, 574, 1105, 1083, 1000, 683, 1071, 667, 1521, 1286, 1126, 11, 243, 916, 449, 618, 698, 1466, 735, 1272, 952, 1405, 896, 229, 670, 294, 794, 1232, 542, 460, 198, 924, 850, 141, 746, 614, 1371, 1400, 773, 1242, 810, 268, 1172, 973, 155, 534, 167, 1421, 524, 1480, 711, 754, 1434, 454, 1535, 246, 911, 109, 379, 560, 681, 1500, 434, 906, 341, 692, 261, 994, 726, 942, 801, 302, 728, 598, 1030, 1233, 100, 1441, 1265, 852, 1238, 408, 991, 805, 368, 217, 687, 1008, 1116, 1464, 854, 1386, 313, 1372, 523, 1230, 935, 423, 812, 329, 1033, 199, 1325, 1516, 309, 1180, 1314, 15, 1472, 1361, 1455, 461, 640, 1489], 'conv3': [499, 1755, 1447, 505, 1394, 507, 1567, 1720, 653, 2500, 861, 2300, 398, 421, 689, 2309, 1762, 1554, 1137, 2460, 2154, 1348, 1843, 1506, 28, 1403, 1873, 340, 652, 2448, 589, 1783, 1326, 1127, 878, 648, 2106, 468, 2203, 2181, 2285, 2340, 43, 2062, 1943, 1894, 2185, 938, 1831, 828, 1617, 1536, 419, 1500, 1179, 1558, 1226, 512, 2317, 1100, 533, 80, 935, 1723, 1162, 1845, 758, 481, 1887, 2000, 1938, 1440, 792, 2241, 2282, 1867, 646, 2502, 926, 2518, 1599, 1359, 284, 540, 555, 1187, 1375, 2515, 390, 1406, 1433, 1054, 1619, 607, 1457, 1968, 1405, 971, 225, 2008, 746, 1789, 1206, 1438, 96, 593, 2072, 2086, 1164, 2117, 1125, 810, 1339, 2523, 1608, 1713, 2095, 1794, 2494, 1080, 526, 2391, 425, 1638, 1868, 446, 1597, 819, 520, 2, 1561, 2112, 2223, 2078, 1616, 1555, 177, 564, 1088, 194, 1533, 1942, 1253, 103, 2545, 2162, 1109, 354, 1218, 1275, 1448, 2254, 1380, 1698, 1217, 2114, 339, 781, 2441, 1280, 151, 1324, 2037, 1677, 775, 1964, 280, 2081, 1872, 36, 724, 81, 509, 669, 1061, 1759, 1781, 2151, 193, 277, 2456, 1644, 761, 2442, 231, 1341, 2315, 1770, 2453, 1647, 2183, 907, 1248, 2474, 1516, 362, 1492, 1449, 454, 240, 1995, 1874, 393, 559, 508, 2271, 472, 624, 1543, 2170, 470, 687, 2511, 2110, 1319, 2559, 1363, 884, 546, 291, 1424, 1097, 990, 1958, 2367, 2479, 145, 45, 1656, 1548, 2060, 943, 1684, 2084, 129, 1776, 1824, 2050, 613, 1613, 2001, 252, 780, 1153, 655, 818, 867, 1436, 1689, 2270, 1651, 1992, 6, 1385, 1346, 2328, 1170, 1788, 2067, 2134, 450, 1038, 1694, 962, 934, 697, 1132, 2283, 615, 1409, 1439, 37, 1510, 1799, 2525, 484, 29, 480, 611, 1150, 195, 1283, 236, 2305, 1664, 272, 313, 2465, 632, 1303, 683, 1194, 665, 202, 660, 365, 674, 730, 737, 1497, 1465, 2352, 63, 1926, 2028, 1646, 548, 2555, 1378, 172, 720, 1443, 980, 2323, 2482, 299, 1660, 1360, 557, 921, 40, 939, 1862, 2098, 1267, 995, 832, 1, 1393, 423, 1808, 1495, 399, 356, 1446, 662, 487, 1224, 2013, 1469, 663, 2090, 1225, 1924, 409, 1340, 2103, 2302, 1830, 588, 2357, 2093, 2003, 2033, 477, 2046, 394, 1266, 616, 1758, 2188, 1119, 370, 360, 2295, 297, 1415, 1292, 1499, 991, 1381, 2414, 11, 2031, 1659, 1998, 1670, 431, 1844, 1431, 598, 1370, 2532, 2011, 1996], 'conv4': [1323, 433, 929, 593, 448, 671, 2211, 2271, 2518, 2285, 2385, 1620, 2268, 397, 1029, 1250, 1887, 2121, 1473, 966, 881, 1898, 499, 99, 1026, 1867, 2214, 91, 783, 546, 1572, 1629, 997, 514, 188, 784, 1784, 2038, 13, 1148, 2158, 624, 1996, 2046, 1989, 615, 969, 1167, 2039, 2356, 1908, 1149, 2395, 2519, 1654, 1376, 2528, 1741, 754, 603, 910, 673, 1519, 161, 1364, 139, 674, 608, 507, 372, 808, 1657, 1610, 2258, 1019, 1925, 2308, 1285, 1498, 2388, 1090, 2548, 1764, 721, 6, 2537, 137, 2015, 1356, 1822, 1131, 2461, 1791, 1708, 2276, 1617, 1067, 1730, 1267, 411, 2224, 348, 2253, 1712, 21, 696, 1261, 2006, 1522, 1942, 1485, 1706, 1064, 610, 1772, 1100, 218, 1710, 1007, 551, 1739, 1153, 1348, 1622, 567, 437, 3, 82, 1797, 1539, 633, 1990, 2012, 1366, 1184, 605, 1284, 1524, 978, 725, 183, 1830, 468, 2423, 1412, 67, 398, 607, 193, 2022, 1727, 34, 113, 774, 258, 1575, 788, 2307, 1471, 2551, 2320, 1218, 2326, 1360, 1777, 1424, 2476, 489, 1832, 1418, 1490, 1135, 2265, 843, 412, 1237, 2209, 1425, 1492, 2366, 1223, 391, 1929, 2145, 2210, 1169, 2557, 530, 2212, 29, 1789, 535, 761, 772, 147, 1095, 381, 426, 1583, 627, 1483, 1746, 2064, 865, 1246, 2327, 1513, 1724, 1897, 1819, 2227, 935, 2438, 990, 2558, 1193, 687, 1139, 1221, 926, 589, 945, 2355, 2544, 2321, 586, 1280, 1817, 2176, 494, 2471, 959, 1310, 340, 2135, 1774, 704, 413, 1080, 347, 181, 415, 702, 712, 1521, 2120, 366, 993, 1511, 1058, 1025, 2543, 1390, 1641, 24, 152, 312, 1801, 1795, 2552, 2415, 880, 1719, 2345, 1416, 28, 1375, 1904, 1354, 270, 2482, 2237, 1692, 1686, 1656, 2436, 1387, 806, 635, 1969, 1536, 918, 225, 142, 2510, 379, 1011, 395, 675, 963, 1063, 2256, 2066, 1214, 1691, 1846, 641, 1550, 1342, 1152, 2361, 329, 493, 2202, 1127, 339, 241, 2251, 666, 70, 126, 1966, 813, 2484, 830, 708, 85, 429, 637, 648, 315, 1400, 470, 656, 864, 2445, 2333, 1868, 1071, 2505, 2133, 1403, 2207, 2393, 1509, 16, 95, 716, 1036, 796, 975, 778, 1847, 2305, 1748, 1813, 2228, 949, 263, 1255, 2208, 1420, 1085, 1472, 802, 26, 2070, 2435, 558, 1053, 1434, 2138, 2238, 1216, 1144, 1683, 2538, 544, 2406, 477, 1119, 497, 1099, 625, 240, 455, 2334, 276, 863, 1892, 1234, 1596, 1578, 592, 1547]}
SAVED model.heads_thresh 0.9016666666666667
CONFIG MODE:  supervised

 ********** Supervised learning of blocks [5] **********
SAVING FOLDER FOR SUP:  C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([49, 33, 72, 51, 71, 92, 15, 14, 23,  0, 71, 75, 81, 69, 40, 43, 92, 97,
        70, 53], device='cuda:0')
TARGETS AFTER SUB:  tensor([56, 56,  2, 56,  2,  2,  2,  2,  2,  2,  2, 56,  2, 56, 56, 56,  2, 56,
        56,  2])
[2, 56]
TARGETS AFTER CLEANER:  tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0])
200
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 200
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               None
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([19, 29,  0, 11,  1, 86, 90, 28, 23, 31, 39, 96, 82, 17, 71, 39,  8, 97,
        80, 71], device='cuda:0')
TARGETS AFTER SUB:  tensor([56, 56,  2,  2, 56, 56,  2,  2, 56, 56, 56, 56, 56,  2,  2, 56, 56, 56,
        56, 56])
[2, 56]
TARGETS AFTER CLEANER:  tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1])
1000
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 1000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               None
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([19, 29,  0, 11,  1, 86, 90, 28, 23, 31, 39, 96, 82, 17, 71, 39,  8, 97,
        80, 71], device='cuda:0')
TARGETS AFTER SUB:  tensor([56, 56,  2,  2, 56, 56,  2,  2, 56, 56, 56, 56, 56,  2,  2, 56, 56, 56,
        56, 56])
[2, 56]
TARGETS AFTER CLEANER:  tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1])
1000
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 1000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
------------------------
INDICES:  1000
IMAGE SIZE: torch.Size([64, 3, 32, 32])
num_blocks:  6
Epoch: [1/50]	lr: 1.00e-03	time: 00:00:31	Loss_train 0.05997	Acc_train 79.10	/	Loss_test 0.00887	Acc_test 91.00
BLOCKS IN SUP:  [5]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c/models
SAVED HEADS THRESHOLD:  0.9016666666666667
SAVED model.topk_kernels {'conv0': [25, 59, 77, 30, 5, 82, 33, 16, 89, 42, 68, 70, 53, 35, 93], 'conv1': [50, 75, 56, 206, 354, 336, 365, 94, 118, 180, 345, 41, 191, 382, 140, 317, 104, 121, 352, 324, 92, 99, 264, 190, 304, 233, 105, 100, 117, 88, 202, 67, 144, 357, 26, 355, 326, 163, 130, 137, 146, 62, 36, 6, 312, 68, 217, 219, 39, 35, 296, 349, 197, 134, 171, 131, 33, 193, 292], 'conv2': [761, 958, 937, 307, 176, 195, 1317, 622, 95, 856, 1486, 731, 1385, 544, 1436, 1297, 612, 636, 1274, 311, 673, 407, 1294, 490, 348, 700, 1278, 421, 1171, 450, 35, 921, 626, 405, 250, 1281, 733, 353, 14, 499, 93, 1292, 312, 395, 380, 427, 1046, 905, 208, 1235, 1452, 1086, 1006, 61, 799, 457, 932, 869, 56, 1307, 1034, 851, 484, 981, 548, 1123, 855, 12, 1360, 639, 359, 1508, 209, 580, 1159, 1442, 743, 1404, 635, 1236, 1412, 1345, 577, 144, 66, 361, 1291, 1359, 276, 796, 1534, 1450, 1061, 866, 1306, 783, 900, 289, 123, 846, 1203, 730, 838, 818, 164, 1388, 1125, 1193, 1227, 875, 1100, 881, 105, 520, 67, 287, 1049, 1113, 574, 1105, 1083, 1000, 683, 1071, 667, 1521, 1286, 1126, 11, 243, 916, 449, 618, 698, 1466, 735, 1272, 952, 1405, 896, 229, 670, 294, 794, 1232, 542, 460, 198, 924, 850, 141, 746, 614, 1371, 1400, 773, 1242, 810, 268, 1172, 973, 155, 534, 167, 1421, 524, 1480, 711, 754, 1434, 454, 1535, 246, 911, 109, 379, 560, 681, 1500, 434, 906, 341, 692, 261, 994, 726, 942, 801, 302, 728, 598, 1030, 1233, 100, 1441, 1265, 852, 1238, 408, 991, 805, 368, 217, 687, 1008, 1116, 1464, 854, 1386, 313, 1372, 523, 1230, 935, 423, 812, 329, 1033, 199, 1325, 1516, 309, 1180, 1314, 15, 1472, 1361, 1455, 461, 640, 1489], 'conv3': [499, 1755, 1447, 505, 1394, 507, 1567, 1720, 653, 2500, 861, 2300, 398, 421, 689, 2309, 1762, 1554, 1137, 2460, 2154, 1348, 1843, 1506, 28, 1403, 1873, 340, 652, 2448, 589, 1783, 1326, 1127, 878, 648, 2106, 468, 2203, 2181, 2285, 2340, 43, 2062, 1943, 1894, 2185, 938, 1831, 828, 1617, 1536, 419, 1500, 1179, 1558, 1226, 512, 2317, 1100, 533, 80, 935, 1723, 1162, 1845, 758, 481, 1887, 2000, 1938, 1440, 792, 2241, 2282, 1867, 646, 2502, 926, 2518, 1599, 1359, 284, 540, 555, 1187, 1375, 2515, 390, 1406, 1433, 1054, 1619, 607, 1457, 1968, 1405, 971, 225, 2008, 746, 1789, 1206, 1438, 96, 593, 2072, 2086, 1164, 2117, 1125, 810, 1339, 2523, 1608, 1713, 2095, 1794, 2494, 1080, 526, 2391, 425, 1638, 1868, 446, 1597, 819, 520, 2, 1561, 2112, 2223, 2078, 1616, 1555, 177, 564, 1088, 194, 1533, 1942, 1253, 103, 2545, 2162, 1109, 354, 1218, 1275, 1448, 2254, 1380, 1698, 1217, 2114, 339, 781, 2441, 1280, 151, 1324, 2037, 1677, 775, 1964, 280, 2081, 1872, 36, 724, 81, 509, 669, 1061, 1759, 1781, 2151, 193, 277, 2456, 1644, 761, 2442, 231, 1341, 2315, 1770, 2453, 1647, 2183, 907, 1248, 2474, 1516, 362, 1492, 1449, 454, 240, 1995, 1874, 393, 559, 508, 2271, 472, 624, 1543, 2170, 470, 687, 2511, 2110, 1319, 2559, 1363, 884, 546, 291, 1424, 1097, 990, 1958, 2367, 2479, 145, 45, 1656, 1548, 2060, 943, 1684, 2084, 129, 1776, 1824, 2050, 613, 1613, 2001, 252, 780, 1153, 655, 818, 867, 1436, 1689, 2270, 1651, 1992, 6, 1385, 1346, 2328, 1170, 1788, 2067, 2134, 450, 1038, 1694, 962, 934, 697, 1132, 2283, 615, 1409, 1439, 37, 1510, 1799, 2525, 484, 29, 480, 611, 1150, 195, 1283, 236, 2305, 1664, 272, 313, 2465, 632, 1303, 683, 1194, 665, 202, 660, 365, 674, 730, 737, 1497, 1465, 2352, 63, 1926, 2028, 1646, 548, 2555, 1378, 172, 720, 1443, 980, 2323, 2482, 299, 1660, 1360, 557, 921, 40, 939, 1862, 2098, 1267, 995, 832, 1, 1393, 423, 1808, 1495, 399, 356, 1446, 662, 487, 1224, 2013, 1469, 663, 2090, 1225, 1924, 409, 1340, 2103, 2302, 1830, 588, 2357, 2093, 2003, 2033, 477, 2046, 394, 1266, 616, 1758, 2188, 1119, 370, 360, 2295, 297, 1415, 1292, 1499, 991, 1381, 2414, 11, 2031, 1659, 1998, 1670, 431, 1844, 1431, 598, 1370, 2532, 2011, 1996], 'conv4': [1323, 433, 929, 593, 448, 671, 2211, 2271, 2518, 2285, 2385, 1620, 2268, 397, 1029, 1250, 1887, 2121, 1473, 966, 881, 1898, 499, 99, 1026, 1867, 2214, 91, 783, 546, 1572, 1629, 997, 514, 188, 784, 1784, 2038, 13, 1148, 2158, 624, 1996, 2046, 1989, 615, 969, 1167, 2039, 2356, 1908, 1149, 2395, 2519, 1654, 1376, 2528, 1741, 754, 603, 910, 673, 1519, 161, 1364, 139, 674, 608, 507, 372, 808, 1657, 1610, 2258, 1019, 1925, 2308, 1285, 1498, 2388, 1090, 2548, 1764, 721, 6, 2537, 137, 2015, 1356, 1822, 1131, 2461, 1791, 1708, 2276, 1617, 1067, 1730, 1267, 411, 2224, 348, 2253, 1712, 21, 696, 1261, 2006, 1522, 1942, 1485, 1706, 1064, 610, 1772, 1100, 218, 1710, 1007, 551, 1739, 1153, 1348, 1622, 567, 437, 3, 82, 1797, 1539, 633, 1990, 2012, 1366, 1184, 605, 1284, 1524, 978, 725, 183, 1830, 468, 2423, 1412, 67, 398, 607, 193, 2022, 1727, 34, 113, 774, 258, 1575, 788, 2307, 1471, 2551, 2320, 1218, 2326, 1360, 1777, 1424, 2476, 489, 1832, 1418, 1490, 1135, 2265, 843, 412, 1237, 2209, 1425, 1492, 2366, 1223, 391, 1929, 2145, 2210, 1169, 2557, 530, 2212, 29, 1789, 535, 761, 772, 147, 1095, 381, 426, 1583, 627, 1483, 1746, 2064, 865, 1246, 2327, 1513, 1724, 1897, 1819, 2227, 935, 2438, 990, 2558, 1193, 687, 1139, 1221, 926, 589, 945, 2355, 2544, 2321, 586, 1280, 1817, 2176, 494, 2471, 959, 1310, 340, 2135, 1774, 704, 413, 1080, 347, 181, 415, 702, 712, 1521, 2120, 366, 993, 1511, 1058, 1025, 2543, 1390, 1641, 24, 152, 312, 1801, 1795, 2552, 2415, 880, 1719, 2345, 1416, 28, 1375, 1904, 1354, 270, 2482, 2237, 1692, 1686, 1656, 2436, 1387, 806, 635, 1969, 1536, 918, 225, 142, 2510, 379, 1011, 395, 675, 963, 1063, 2256, 2066, 1214, 1691, 1846, 641, 1550, 1342, 1152, 2361, 329, 493, 2202, 1127, 339, 241, 2251, 666, 70, 126, 1966, 813, 2484, 830, 708, 85, 429, 637, 648, 315, 1400, 470, 656, 864, 2445, 2333, 1868, 1071, 2505, 2133, 1403, 2207, 2393, 1509, 16, 95, 716, 1036, 796, 975, 778, 1847, 2305, 1748, 1813, 2228, 949, 263, 1255, 2208, 1420, 1085, 1472, 802, 26, 2070, 2435, 558, 1053, 1434, 2138, 2238, 1216, 1144, 1683, 2538, 544, 2406, 477, 1119, 497, 1099, 625, 240, 455, 2334, 276, 863, 1892, 1234, 1596, 1578, 592, 1547]}
SAVED model.heads_thresh 0.9016666666666667
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
Epoch: [10/50]	lr: 1.00e-03	time: 00:00:32	Loss_train 0.01792	Acc_train 91.24	/	Loss_test 0.00237	Acc_test 96.00
BLOCKS IN SUP:  [5]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c/models
SAVED HEADS THRESHOLD:  0.9016666666666667
SAVED model.topk_kernels {'conv0': [25, 59, 77, 30, 5, 82, 33, 16, 89, 42, 68, 70, 53, 35, 93], 'conv1': [50, 75, 56, 206, 354, 336, 365, 94, 118, 180, 345, 41, 191, 382, 140, 317, 104, 121, 352, 324, 92, 99, 264, 190, 304, 233, 105, 100, 117, 88, 202, 67, 144, 357, 26, 355, 326, 163, 130, 137, 146, 62, 36, 6, 312, 68, 217, 219, 39, 35, 296, 349, 197, 134, 171, 131, 33, 193, 292], 'conv2': [761, 958, 937, 307, 176, 195, 1317, 622, 95, 856, 1486, 731, 1385, 544, 1436, 1297, 612, 636, 1274, 311, 673, 407, 1294, 490, 348, 700, 1278, 421, 1171, 450, 35, 921, 626, 405, 250, 1281, 733, 353, 14, 499, 93, 1292, 312, 395, 380, 427, 1046, 905, 208, 1235, 1452, 1086, 1006, 61, 799, 457, 932, 869, 56, 1307, 1034, 851, 484, 981, 548, 1123, 855, 12, 1360, 639, 359, 1508, 209, 580, 1159, 1442, 743, 1404, 635, 1236, 1412, 1345, 577, 144, 66, 361, 1291, 1359, 276, 796, 1534, 1450, 1061, 866, 1306, 783, 900, 289, 123, 846, 1203, 730, 838, 818, 164, 1388, 1125, 1193, 1227, 875, 1100, 881, 105, 520, 67, 287, 1049, 1113, 574, 1105, 1083, 1000, 683, 1071, 667, 1521, 1286, 1126, 11, 243, 916, 449, 618, 698, 1466, 735, 1272, 952, 1405, 896, 229, 670, 294, 794, 1232, 542, 460, 198, 924, 850, 141, 746, 614, 1371, 1400, 773, 1242, 810, 268, 1172, 973, 155, 534, 167, 1421, 524, 1480, 711, 754, 1434, 454, 1535, 246, 911, 109, 379, 560, 681, 1500, 434, 906, 341, 692, 261, 994, 726, 942, 801, 302, 728, 598, 1030, 1233, 100, 1441, 1265, 852, 1238, 408, 991, 805, 368, 217, 687, 1008, 1116, 1464, 854, 1386, 313, 1372, 523, 1230, 935, 423, 812, 329, 1033, 199, 1325, 1516, 309, 1180, 1314, 15, 1472, 1361, 1455, 461, 640, 1489], 'conv3': [499, 1755, 1447, 505, 1394, 507, 1567, 1720, 653, 2500, 861, 2300, 398, 421, 689, 2309, 1762, 1554, 1137, 2460, 2154, 1348, 1843, 1506, 28, 1403, 1873, 340, 652, 2448, 589, 1783, 1326, 1127, 878, 648, 2106, 468, 2203, 2181, 2285, 2340, 43, 2062, 1943, 1894, 2185, 938, 1831, 828, 1617, 1536, 419, 1500, 1179, 1558, 1226, 512, 2317, 1100, 533, 80, 935, 1723, 1162, 1845, 758, 481, 1887, 2000, 1938, 1440, 792, 2241, 2282, 1867, 646, 2502, 926, 2518, 1599, 1359, 284, 540, 555, 1187, 1375, 2515, 390, 1406, 1433, 1054, 1619, 607, 1457, 1968, 1405, 971, 225, 2008, 746, 1789, 1206, 1438, 96, 593, 2072, 2086, 1164, 2117, 1125, 810, 1339, 2523, 1608, 1713, 2095, 1794, 2494, 1080, 526, 2391, 425, 1638, 1868, 446, 1597, 819, 520, 2, 1561, 2112, 2223, 2078, 1616, 1555, 177, 564, 1088, 194, 1533, 1942, 1253, 103, 2545, 2162, 1109, 354, 1218, 1275, 1448, 2254, 1380, 1698, 1217, 2114, 339, 781, 2441, 1280, 151, 1324, 2037, 1677, 775, 1964, 280, 2081, 1872, 36, 724, 81, 509, 669, 1061, 1759, 1781, 2151, 193, 277, 2456, 1644, 761, 2442, 231, 1341, 2315, 1770, 2453, 1647, 2183, 907, 1248, 2474, 1516, 362, 1492, 1449, 454, 240, 1995, 1874, 393, 559, 508, 2271, 472, 624, 1543, 2170, 470, 687, 2511, 2110, 1319, 2559, 1363, 884, 546, 291, 1424, 1097, 990, 1958, 2367, 2479, 145, 45, 1656, 1548, 2060, 943, 1684, 2084, 129, 1776, 1824, 2050, 613, 1613, 2001, 252, 780, 1153, 655, 818, 867, 1436, 1689, 2270, 1651, 1992, 6, 1385, 1346, 2328, 1170, 1788, 2067, 2134, 450, 1038, 1694, 962, 934, 697, 1132, 2283, 615, 1409, 1439, 37, 1510, 1799, 2525, 484, 29, 480, 611, 1150, 195, 1283, 236, 2305, 1664, 272, 313, 2465, 632, 1303, 683, 1194, 665, 202, 660, 365, 674, 730, 737, 1497, 1465, 2352, 63, 1926, 2028, 1646, 548, 2555, 1378, 172, 720, 1443, 980, 2323, 2482, 299, 1660, 1360, 557, 921, 40, 939, 1862, 2098, 1267, 995, 832, 1, 1393, 423, 1808, 1495, 399, 356, 1446, 662, 487, 1224, 2013, 1469, 663, 2090, 1225, 1924, 409, 1340, 2103, 2302, 1830, 588, 2357, 2093, 2003, 2033, 477, 2046, 394, 1266, 616, 1758, 2188, 1119, 370, 360, 2295, 297, 1415, 1292, 1499, 991, 1381, 2414, 11, 2031, 1659, 1998, 1670, 431, 1844, 1431, 598, 1370, 2532, 2011, 1996], 'conv4': [1323, 433, 929, 593, 448, 671, 2211, 2271, 2518, 2285, 2385, 1620, 2268, 397, 1029, 1250, 1887, 2121, 1473, 966, 881, 1898, 499, 99, 1026, 1867, 2214, 91, 783, 546, 1572, 1629, 997, 514, 188, 784, 1784, 2038, 13, 1148, 2158, 624, 1996, 2046, 1989, 615, 969, 1167, 2039, 2356, 1908, 1149, 2395, 2519, 1654, 1376, 2528, 1741, 754, 603, 910, 673, 1519, 161, 1364, 139, 674, 608, 507, 372, 808, 1657, 1610, 2258, 1019, 1925, 2308, 1285, 1498, 2388, 1090, 2548, 1764, 721, 6, 2537, 137, 2015, 1356, 1822, 1131, 2461, 1791, 1708, 2276, 1617, 1067, 1730, 1267, 411, 2224, 348, 2253, 1712, 21, 696, 1261, 2006, 1522, 1942, 1485, 1706, 1064, 610, 1772, 1100, 218, 1710, 1007, 551, 1739, 1153, 1348, 1622, 567, 437, 3, 82, 1797, 1539, 633, 1990, 2012, 1366, 1184, 605, 1284, 1524, 978, 725, 183, 1830, 468, 2423, 1412, 67, 398, 607, 193, 2022, 1727, 34, 113, 774, 258, 1575, 788, 2307, 1471, 2551, 2320, 1218, 2326, 1360, 1777, 1424, 2476, 489, 1832, 1418, 1490, 1135, 2265, 843, 412, 1237, 2209, 1425, 1492, 2366, 1223, 391, 1929, 2145, 2210, 1169, 2557, 530, 2212, 29, 1789, 535, 761, 772, 147, 1095, 381, 426, 1583, 627, 1483, 1746, 2064, 865, 1246, 2327, 1513, 1724, 1897, 1819, 2227, 935, 2438, 990, 2558, 1193, 687, 1139, 1221, 926, 589, 945, 2355, 2544, 2321, 586, 1280, 1817, 2176, 494, 2471, 959, 1310, 340, 2135, 1774, 704, 413, 1080, 347, 181, 415, 702, 712, 1521, 2120, 366, 993, 1511, 1058, 1025, 2543, 1390, 1641, 24, 152, 312, 1801, 1795, 2552, 2415, 880, 1719, 2345, 1416, 28, 1375, 1904, 1354, 270, 2482, 2237, 1692, 1686, 1656, 2436, 1387, 806, 635, 1969, 1536, 918, 225, 142, 2510, 379, 1011, 395, 675, 963, 1063, 2256, 2066, 1214, 1691, 1846, 641, 1550, 1342, 1152, 2361, 329, 493, 2202, 1127, 339, 241, 2251, 666, 70, 126, 1966, 813, 2484, 830, 708, 85, 429, 637, 648, 315, 1400, 470, 656, 864, 2445, 2333, 1868, 1071, 2505, 2133, 1403, 2207, 2393, 1509, 16, 95, 716, 1036, 796, 975, 778, 1847, 2305, 1748, 1813, 2228, 949, 263, 1255, 2208, 1420, 1085, 1472, 802, 26, 2070, 2435, 558, 1053, 1434, 2138, 2238, 1216, 1144, 1683, 2538, 544, 2406, 477, 1119, 497, 1099, 625, 240, 455, 2334, 276, 863, 1892, 1234, 1596, 1578, 592, 1547]}
SAVED model.heads_thresh 0.9016666666666667
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
Epoch: [20/50]	lr: 2.50e-04	time: 00:00:34	Loss_train 0.00898	Acc_train 94.43	/	Loss_test 0.00259	Acc_test 95.00
BLOCKS IN SUP:  [5]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c/models
SAVED HEADS THRESHOLD:  0.9016666666666667
SAVED model.topk_kernels {'conv0': [25, 59, 77, 30, 5, 82, 33, 16, 89, 42, 68, 70, 53, 35, 93], 'conv1': [50, 75, 56, 206, 354, 336, 365, 94, 118, 180, 345, 41, 191, 382, 140, 317, 104, 121, 352, 324, 92, 99, 264, 190, 304, 233, 105, 100, 117, 88, 202, 67, 144, 357, 26, 355, 326, 163, 130, 137, 146, 62, 36, 6, 312, 68, 217, 219, 39, 35, 296, 349, 197, 134, 171, 131, 33, 193, 292], 'conv2': [761, 958, 937, 307, 176, 195, 1317, 622, 95, 856, 1486, 731, 1385, 544, 1436, 1297, 612, 636, 1274, 311, 673, 407, 1294, 490, 348, 700, 1278, 421, 1171, 450, 35, 921, 626, 405, 250, 1281, 733, 353, 14, 499, 93, 1292, 312, 395, 380, 427, 1046, 905, 208, 1235, 1452, 1086, 1006, 61, 799, 457, 932, 869, 56, 1307, 1034, 851, 484, 981, 548, 1123, 855, 12, 1360, 639, 359, 1508, 209, 580, 1159, 1442, 743, 1404, 635, 1236, 1412, 1345, 577, 144, 66, 361, 1291, 1359, 276, 796, 1534, 1450, 1061, 866, 1306, 783, 900, 289, 123, 846, 1203, 730, 838, 818, 164, 1388, 1125, 1193, 1227, 875, 1100, 881, 105, 520, 67, 287, 1049, 1113, 574, 1105, 1083, 1000, 683, 1071, 667, 1521, 1286, 1126, 11, 243, 916, 449, 618, 698, 1466, 735, 1272, 952, 1405, 896, 229, 670, 294, 794, 1232, 542, 460, 198, 924, 850, 141, 746, 614, 1371, 1400, 773, 1242, 810, 268, 1172, 973, 155, 534, 167, 1421, 524, 1480, 711, 754, 1434, 454, 1535, 246, 911, 109, 379, 560, 681, 1500, 434, 906, 341, 692, 261, 994, 726, 942, 801, 302, 728, 598, 1030, 1233, 100, 1441, 1265, 852, 1238, 408, 991, 805, 368, 217, 687, 1008, 1116, 1464, 854, 1386, 313, 1372, 523, 1230, 935, 423, 812, 329, 1033, 199, 1325, 1516, 309, 1180, 1314, 15, 1472, 1361, 1455, 461, 640, 1489], 'conv3': [499, 1755, 1447, 505, 1394, 507, 1567, 1720, 653, 2500, 861, 2300, 398, 421, 689, 2309, 1762, 1554, 1137, 2460, 2154, 1348, 1843, 1506, 28, 1403, 1873, 340, 652, 2448, 589, 1783, 1326, 1127, 878, 648, 2106, 468, 2203, 2181, 2285, 2340, 43, 2062, 1943, 1894, 2185, 938, 1831, 828, 1617, 1536, 419, 1500, 1179, 1558, 1226, 512, 2317, 1100, 533, 80, 935, 1723, 1162, 1845, 758, 481, 1887, 2000, 1938, 1440, 792, 2241, 2282, 1867, 646, 2502, 926, 2518, 1599, 1359, 284, 540, 555, 1187, 1375, 2515, 390, 1406, 1433, 1054, 1619, 607, 1457, 1968, 1405, 971, 225, 2008, 746, 1789, 1206, 1438, 96, 593, 2072, 2086, 1164, 2117, 1125, 810, 1339, 2523, 1608, 1713, 2095, 1794, 2494, 1080, 526, 2391, 425, 1638, 1868, 446, 1597, 819, 520, 2, 1561, 2112, 2223, 2078, 1616, 1555, 177, 564, 1088, 194, 1533, 1942, 1253, 103, 2545, 2162, 1109, 354, 1218, 1275, 1448, 2254, 1380, 1698, 1217, 2114, 339, 781, 2441, 1280, 151, 1324, 2037, 1677, 775, 1964, 280, 2081, 1872, 36, 724, 81, 509, 669, 1061, 1759, 1781, 2151, 193, 277, 2456, 1644, 761, 2442, 231, 1341, 2315, 1770, 2453, 1647, 2183, 907, 1248, 2474, 1516, 362, 1492, 1449, 454, 240, 1995, 1874, 393, 559, 508, 2271, 472, 624, 1543, 2170, 470, 687, 2511, 2110, 1319, 2559, 1363, 884, 546, 291, 1424, 1097, 990, 1958, 2367, 2479, 145, 45, 1656, 1548, 2060, 943, 1684, 2084, 129, 1776, 1824, 2050, 613, 1613, 2001, 252, 780, 1153, 655, 818, 867, 1436, 1689, 2270, 1651, 1992, 6, 1385, 1346, 2328, 1170, 1788, 2067, 2134, 450, 1038, 1694, 962, 934, 697, 1132, 2283, 615, 1409, 1439, 37, 1510, 1799, 2525, 484, 29, 480, 611, 1150, 195, 1283, 236, 2305, 1664, 272, 313, 2465, 632, 1303, 683, 1194, 665, 202, 660, 365, 674, 730, 737, 1497, 1465, 2352, 63, 1926, 2028, 1646, 548, 2555, 1378, 172, 720, 1443, 980, 2323, 2482, 299, 1660, 1360, 557, 921, 40, 939, 1862, 2098, 1267, 995, 832, 1, 1393, 423, 1808, 1495, 399, 356, 1446, 662, 487, 1224, 2013, 1469, 663, 2090, 1225, 1924, 409, 1340, 2103, 2302, 1830, 588, 2357, 2093, 2003, 2033, 477, 2046, 394, 1266, 616, 1758, 2188, 1119, 370, 360, 2295, 297, 1415, 1292, 1499, 991, 1381, 2414, 11, 2031, 1659, 1998, 1670, 431, 1844, 1431, 598, 1370, 2532, 2011, 1996], 'conv4': [1323, 433, 929, 593, 448, 671, 2211, 2271, 2518, 2285, 2385, 1620, 2268, 397, 1029, 1250, 1887, 2121, 1473, 966, 881, 1898, 499, 99, 1026, 1867, 2214, 91, 783, 546, 1572, 1629, 997, 514, 188, 784, 1784, 2038, 13, 1148, 2158, 624, 1996, 2046, 1989, 615, 969, 1167, 2039, 2356, 1908, 1149, 2395, 2519, 1654, 1376, 2528, 1741, 754, 603, 910, 673, 1519, 161, 1364, 139, 674, 608, 507, 372, 808, 1657, 1610, 2258, 1019, 1925, 2308, 1285, 1498, 2388, 1090, 2548, 1764, 721, 6, 2537, 137, 2015, 1356, 1822, 1131, 2461, 1791, 1708, 2276, 1617, 1067, 1730, 1267, 411, 2224, 348, 2253, 1712, 21, 696, 1261, 2006, 1522, 1942, 1485, 1706, 1064, 610, 1772, 1100, 218, 1710, 1007, 551, 1739, 1153, 1348, 1622, 567, 437, 3, 82, 1797, 1539, 633, 1990, 2012, 1366, 1184, 605, 1284, 1524, 978, 725, 183, 1830, 468, 2423, 1412, 67, 398, 607, 193, 2022, 1727, 34, 113, 774, 258, 1575, 788, 2307, 1471, 2551, 2320, 1218, 2326, 1360, 1777, 1424, 2476, 489, 1832, 1418, 1490, 1135, 2265, 843, 412, 1237, 2209, 1425, 1492, 2366, 1223, 391, 1929, 2145, 2210, 1169, 2557, 530, 2212, 29, 1789, 535, 761, 772, 147, 1095, 381, 426, 1583, 627, 1483, 1746, 2064, 865, 1246, 2327, 1513, 1724, 1897, 1819, 2227, 935, 2438, 990, 2558, 1193, 687, 1139, 1221, 926, 589, 945, 2355, 2544, 2321, 586, 1280, 1817, 2176, 494, 2471, 959, 1310, 340, 2135, 1774, 704, 413, 1080, 347, 181, 415, 702, 712, 1521, 2120, 366, 993, 1511, 1058, 1025, 2543, 1390, 1641, 24, 152, 312, 1801, 1795, 2552, 2415, 880, 1719, 2345, 1416, 28, 1375, 1904, 1354, 270, 2482, 2237, 1692, 1686, 1656, 2436, 1387, 806, 635, 1969, 1536, 918, 225, 142, 2510, 379, 1011, 395, 675, 963, 1063, 2256, 2066, 1214, 1691, 1846, 641, 1550, 1342, 1152, 2361, 329, 493, 2202, 1127, 339, 241, 2251, 666, 70, 126, 1966, 813, 2484, 830, 708, 85, 429, 637, 648, 315, 1400, 470, 656, 864, 2445, 2333, 1868, 1071, 2505, 2133, 1403, 2207, 2393, 1509, 16, 95, 716, 1036, 796, 975, 778, 1847, 2305, 1748, 1813, 2228, 949, 263, 1255, 2208, 1420, 1085, 1472, 802, 26, 2070, 2435, 558, 1053, 1434, 2138, 2238, 1216, 1144, 1683, 2538, 544, 2406, 477, 1119, 497, 1099, 625, 240, 455, 2334, 276, 863, 1892, 1234, 1596, 1578, 592, 1547]}
SAVED model.heads_thresh 0.9016666666666667
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
Epoch: [30/50]	lr: 1.25e-04	time: 00:00:36	Loss_train 0.00671	Acc_train 95.42	/	Loss_test 0.00228	Acc_test 97.50
BLOCKS IN SUP:  [5]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c/models
SAVED HEADS THRESHOLD:  0.9016666666666667
SAVED model.topk_kernels {'conv0': [25, 59, 77, 30, 5, 82, 33, 16, 89, 42, 68, 70, 53, 35, 93], 'conv1': [50, 75, 56, 206, 354, 336, 365, 94, 118, 180, 345, 41, 191, 382, 140, 317, 104, 121, 352, 324, 92, 99, 264, 190, 304, 233, 105, 100, 117, 88, 202, 67, 144, 357, 26, 355, 326, 163, 130, 137, 146, 62, 36, 6, 312, 68, 217, 219, 39, 35, 296, 349, 197, 134, 171, 131, 33, 193, 292], 'conv2': [761, 958, 937, 307, 176, 195, 1317, 622, 95, 856, 1486, 731, 1385, 544, 1436, 1297, 612, 636, 1274, 311, 673, 407, 1294, 490, 348, 700, 1278, 421, 1171, 450, 35, 921, 626, 405, 250, 1281, 733, 353, 14, 499, 93, 1292, 312, 395, 380, 427, 1046, 905, 208, 1235, 1452, 1086, 1006, 61, 799, 457, 932, 869, 56, 1307, 1034, 851, 484, 981, 548, 1123, 855, 12, 1360, 639, 359, 1508, 209, 580, 1159, 1442, 743, 1404, 635, 1236, 1412, 1345, 577, 144, 66, 361, 1291, 1359, 276, 796, 1534, 1450, 1061, 866, 1306, 783, 900, 289, 123, 846, 1203, 730, 838, 818, 164, 1388, 1125, 1193, 1227, 875, 1100, 881, 105, 520, 67, 287, 1049, 1113, 574, 1105, 1083, 1000, 683, 1071, 667, 1521, 1286, 1126, 11, 243, 916, 449, 618, 698, 1466, 735, 1272, 952, 1405, 896, 229, 670, 294, 794, 1232, 542, 460, 198, 924, 850, 141, 746, 614, 1371, 1400, 773, 1242, 810, 268, 1172, 973, 155, 534, 167, 1421, 524, 1480, 711, 754, 1434, 454, 1535, 246, 911, 109, 379, 560, 681, 1500, 434, 906, 341, 692, 261, 994, 726, 942, 801, 302, 728, 598, 1030, 1233, 100, 1441, 1265, 852, 1238, 408, 991, 805, 368, 217, 687, 1008, 1116, 1464, 854, 1386, 313, 1372, 523, 1230, 935, 423, 812, 329, 1033, 199, 1325, 1516, 309, 1180, 1314, 15, 1472, 1361, 1455, 461, 640, 1489], 'conv3': [499, 1755, 1447, 505, 1394, 507, 1567, 1720, 653, 2500, 861, 2300, 398, 421, 689, 2309, 1762, 1554, 1137, 2460, 2154, 1348, 1843, 1506, 28, 1403, 1873, 340, 652, 2448, 589, 1783, 1326, 1127, 878, 648, 2106, 468, 2203, 2181, 2285, 2340, 43, 2062, 1943, 1894, 2185, 938, 1831, 828, 1617, 1536, 419, 1500, 1179, 1558, 1226, 512, 2317, 1100, 533, 80, 935, 1723, 1162, 1845, 758, 481, 1887, 2000, 1938, 1440, 792, 2241, 2282, 1867, 646, 2502, 926, 2518, 1599, 1359, 284, 540, 555, 1187, 1375, 2515, 390, 1406, 1433, 1054, 1619, 607, 1457, 1968, 1405, 971, 225, 2008, 746, 1789, 1206, 1438, 96, 593, 2072, 2086, 1164, 2117, 1125, 810, 1339, 2523, 1608, 1713, 2095, 1794, 2494, 1080, 526, 2391, 425, 1638, 1868, 446, 1597, 819, 520, 2, 1561, 2112, 2223, 2078, 1616, 1555, 177, 564, 1088, 194, 1533, 1942, 1253, 103, 2545, 2162, 1109, 354, 1218, 1275, 1448, 2254, 1380, 1698, 1217, 2114, 339, 781, 2441, 1280, 151, 1324, 2037, 1677, 775, 1964, 280, 2081, 1872, 36, 724, 81, 509, 669, 1061, 1759, 1781, 2151, 193, 277, 2456, 1644, 761, 2442, 231, 1341, 2315, 1770, 2453, 1647, 2183, 907, 1248, 2474, 1516, 362, 1492, 1449, 454, 240, 1995, 1874, 393, 559, 508, 2271, 472, 624, 1543, 2170, 470, 687, 2511, 2110, 1319, 2559, 1363, 884, 546, 291, 1424, 1097, 990, 1958, 2367, 2479, 145, 45, 1656, 1548, 2060, 943, 1684, 2084, 129, 1776, 1824, 2050, 613, 1613, 2001, 252, 780, 1153, 655, 818, 867, 1436, 1689, 2270, 1651, 1992, 6, 1385, 1346, 2328, 1170, 1788, 2067, 2134, 450, 1038, 1694, 962, 934, 697, 1132, 2283, 615, 1409, 1439, 37, 1510, 1799, 2525, 484, 29, 480, 611, 1150, 195, 1283, 236, 2305, 1664, 272, 313, 2465, 632, 1303, 683, 1194, 665, 202, 660, 365, 674, 730, 737, 1497, 1465, 2352, 63, 1926, 2028, 1646, 548, 2555, 1378, 172, 720, 1443, 980, 2323, 2482, 299, 1660, 1360, 557, 921, 40, 939, 1862, 2098, 1267, 995, 832, 1, 1393, 423, 1808, 1495, 399, 356, 1446, 662, 487, 1224, 2013, 1469, 663, 2090, 1225, 1924, 409, 1340, 2103, 2302, 1830, 588, 2357, 2093, 2003, 2033, 477, 2046, 394, 1266, 616, 1758, 2188, 1119, 370, 360, 2295, 297, 1415, 1292, 1499, 991, 1381, 2414, 11, 2031, 1659, 1998, 1670, 431, 1844, 1431, 598, 1370, 2532, 2011, 1996], 'conv4': [1323, 433, 929, 593, 448, 671, 2211, 2271, 2518, 2285, 2385, 1620, 2268, 397, 1029, 1250, 1887, 2121, 1473, 966, 881, 1898, 499, 99, 1026, 1867, 2214, 91, 783, 546, 1572, 1629, 997, 514, 188, 784, 1784, 2038, 13, 1148, 2158, 624, 1996, 2046, 1989, 615, 969, 1167, 2039, 2356, 1908, 1149, 2395, 2519, 1654, 1376, 2528, 1741, 754, 603, 910, 673, 1519, 161, 1364, 139, 674, 608, 507, 372, 808, 1657, 1610, 2258, 1019, 1925, 2308, 1285, 1498, 2388, 1090, 2548, 1764, 721, 6, 2537, 137, 2015, 1356, 1822, 1131, 2461, 1791, 1708, 2276, 1617, 1067, 1730, 1267, 411, 2224, 348, 2253, 1712, 21, 696, 1261, 2006, 1522, 1942, 1485, 1706, 1064, 610, 1772, 1100, 218, 1710, 1007, 551, 1739, 1153, 1348, 1622, 567, 437, 3, 82, 1797, 1539, 633, 1990, 2012, 1366, 1184, 605, 1284, 1524, 978, 725, 183, 1830, 468, 2423, 1412, 67, 398, 607, 193, 2022, 1727, 34, 113, 774, 258, 1575, 788, 2307, 1471, 2551, 2320, 1218, 2326, 1360, 1777, 1424, 2476, 489, 1832, 1418, 1490, 1135, 2265, 843, 412, 1237, 2209, 1425, 1492, 2366, 1223, 391, 1929, 2145, 2210, 1169, 2557, 530, 2212, 29, 1789, 535, 761, 772, 147, 1095, 381, 426, 1583, 627, 1483, 1746, 2064, 865, 1246, 2327, 1513, 1724, 1897, 1819, 2227, 935, 2438, 990, 2558, 1193, 687, 1139, 1221, 926, 589, 945, 2355, 2544, 2321, 586, 1280, 1817, 2176, 494, 2471, 959, 1310, 340, 2135, 1774, 704, 413, 1080, 347, 181, 415, 702, 712, 1521, 2120, 366, 993, 1511, 1058, 1025, 2543, 1390, 1641, 24, 152, 312, 1801, 1795, 2552, 2415, 880, 1719, 2345, 1416, 28, 1375, 1904, 1354, 270, 2482, 2237, 1692, 1686, 1656, 2436, 1387, 806, 635, 1969, 1536, 918, 225, 142, 2510, 379, 1011, 395, 675, 963, 1063, 2256, 2066, 1214, 1691, 1846, 641, 1550, 1342, 1152, 2361, 329, 493, 2202, 1127, 339, 241, 2251, 666, 70, 126, 1966, 813, 2484, 830, 708, 85, 429, 637, 648, 315, 1400, 470, 656, 864, 2445, 2333, 1868, 1071, 2505, 2133, 1403, 2207, 2393, 1509, 16, 95, 716, 1036, 796, 975, 778, 1847, 2305, 1748, 1813, 2228, 949, 263, 1255, 2208, 1420, 1085, 1472, 802, 26, 2070, 2435, 558, 1053, 1434, 2138, 2238, 1216, 1144, 1683, 2538, 544, 2406, 477, 1119, 497, 1099, 625, 240, 455, 2334, 276, 863, 1892, 1234, 1596, 1578, 592, 1547]}
SAVED model.heads_thresh 0.9016666666666667
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
Epoch: [40/50]	lr: 3.13e-05	time: 00:00:38	Loss_train 0.00450	Acc_train 96.19	/	Loss_test 0.00216	Acc_test 96.50
BLOCKS IN SUP:  [5]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c/models
SAVED HEADS THRESHOLD:  0.9016666666666667
SAVED model.topk_kernels {'conv0': [25, 59, 77, 30, 5, 82, 33, 16, 89, 42, 68, 70, 53, 35, 93], 'conv1': [50, 75, 56, 206, 354, 336, 365, 94, 118, 180, 345, 41, 191, 382, 140, 317, 104, 121, 352, 324, 92, 99, 264, 190, 304, 233, 105, 100, 117, 88, 202, 67, 144, 357, 26, 355, 326, 163, 130, 137, 146, 62, 36, 6, 312, 68, 217, 219, 39, 35, 296, 349, 197, 134, 171, 131, 33, 193, 292], 'conv2': [761, 958, 937, 307, 176, 195, 1317, 622, 95, 856, 1486, 731, 1385, 544, 1436, 1297, 612, 636, 1274, 311, 673, 407, 1294, 490, 348, 700, 1278, 421, 1171, 450, 35, 921, 626, 405, 250, 1281, 733, 353, 14, 499, 93, 1292, 312, 395, 380, 427, 1046, 905, 208, 1235, 1452, 1086, 1006, 61, 799, 457, 932, 869, 56, 1307, 1034, 851, 484, 981, 548, 1123, 855, 12, 1360, 639, 359, 1508, 209, 580, 1159, 1442, 743, 1404, 635, 1236, 1412, 1345, 577, 144, 66, 361, 1291, 1359, 276, 796, 1534, 1450, 1061, 866, 1306, 783, 900, 289, 123, 846, 1203, 730, 838, 818, 164, 1388, 1125, 1193, 1227, 875, 1100, 881, 105, 520, 67, 287, 1049, 1113, 574, 1105, 1083, 1000, 683, 1071, 667, 1521, 1286, 1126, 11, 243, 916, 449, 618, 698, 1466, 735, 1272, 952, 1405, 896, 229, 670, 294, 794, 1232, 542, 460, 198, 924, 850, 141, 746, 614, 1371, 1400, 773, 1242, 810, 268, 1172, 973, 155, 534, 167, 1421, 524, 1480, 711, 754, 1434, 454, 1535, 246, 911, 109, 379, 560, 681, 1500, 434, 906, 341, 692, 261, 994, 726, 942, 801, 302, 728, 598, 1030, 1233, 100, 1441, 1265, 852, 1238, 408, 991, 805, 368, 217, 687, 1008, 1116, 1464, 854, 1386, 313, 1372, 523, 1230, 935, 423, 812, 329, 1033, 199, 1325, 1516, 309, 1180, 1314, 15, 1472, 1361, 1455, 461, 640, 1489], 'conv3': [499, 1755, 1447, 505, 1394, 507, 1567, 1720, 653, 2500, 861, 2300, 398, 421, 689, 2309, 1762, 1554, 1137, 2460, 2154, 1348, 1843, 1506, 28, 1403, 1873, 340, 652, 2448, 589, 1783, 1326, 1127, 878, 648, 2106, 468, 2203, 2181, 2285, 2340, 43, 2062, 1943, 1894, 2185, 938, 1831, 828, 1617, 1536, 419, 1500, 1179, 1558, 1226, 512, 2317, 1100, 533, 80, 935, 1723, 1162, 1845, 758, 481, 1887, 2000, 1938, 1440, 792, 2241, 2282, 1867, 646, 2502, 926, 2518, 1599, 1359, 284, 540, 555, 1187, 1375, 2515, 390, 1406, 1433, 1054, 1619, 607, 1457, 1968, 1405, 971, 225, 2008, 746, 1789, 1206, 1438, 96, 593, 2072, 2086, 1164, 2117, 1125, 810, 1339, 2523, 1608, 1713, 2095, 1794, 2494, 1080, 526, 2391, 425, 1638, 1868, 446, 1597, 819, 520, 2, 1561, 2112, 2223, 2078, 1616, 1555, 177, 564, 1088, 194, 1533, 1942, 1253, 103, 2545, 2162, 1109, 354, 1218, 1275, 1448, 2254, 1380, 1698, 1217, 2114, 339, 781, 2441, 1280, 151, 1324, 2037, 1677, 775, 1964, 280, 2081, 1872, 36, 724, 81, 509, 669, 1061, 1759, 1781, 2151, 193, 277, 2456, 1644, 761, 2442, 231, 1341, 2315, 1770, 2453, 1647, 2183, 907, 1248, 2474, 1516, 362, 1492, 1449, 454, 240, 1995, 1874, 393, 559, 508, 2271, 472, 624, 1543, 2170, 470, 687, 2511, 2110, 1319, 2559, 1363, 884, 546, 291, 1424, 1097, 990, 1958, 2367, 2479, 145, 45, 1656, 1548, 2060, 943, 1684, 2084, 129, 1776, 1824, 2050, 613, 1613, 2001, 252, 780, 1153, 655, 818, 867, 1436, 1689, 2270, 1651, 1992, 6, 1385, 1346, 2328, 1170, 1788, 2067, 2134, 450, 1038, 1694, 962, 934, 697, 1132, 2283, 615, 1409, 1439, 37, 1510, 1799, 2525, 484, 29, 480, 611, 1150, 195, 1283, 236, 2305, 1664, 272, 313, 2465, 632, 1303, 683, 1194, 665, 202, 660, 365, 674, 730, 737, 1497, 1465, 2352, 63, 1926, 2028, 1646, 548, 2555, 1378, 172, 720, 1443, 980, 2323, 2482, 299, 1660, 1360, 557, 921, 40, 939, 1862, 2098, 1267, 995, 832, 1, 1393, 423, 1808, 1495, 399, 356, 1446, 662, 487, 1224, 2013, 1469, 663, 2090, 1225, 1924, 409, 1340, 2103, 2302, 1830, 588, 2357, 2093, 2003, 2033, 477, 2046, 394, 1266, 616, 1758, 2188, 1119, 370, 360, 2295, 297, 1415, 1292, 1499, 991, 1381, 2414, 11, 2031, 1659, 1998, 1670, 431, 1844, 1431, 598, 1370, 2532, 2011, 1996], 'conv4': [1323, 433, 929, 593, 448, 671, 2211, 2271, 2518, 2285, 2385, 1620, 2268, 397, 1029, 1250, 1887, 2121, 1473, 966, 881, 1898, 499, 99, 1026, 1867, 2214, 91, 783, 546, 1572, 1629, 997, 514, 188, 784, 1784, 2038, 13, 1148, 2158, 624, 1996, 2046, 1989, 615, 969, 1167, 2039, 2356, 1908, 1149, 2395, 2519, 1654, 1376, 2528, 1741, 754, 603, 910, 673, 1519, 161, 1364, 139, 674, 608, 507, 372, 808, 1657, 1610, 2258, 1019, 1925, 2308, 1285, 1498, 2388, 1090, 2548, 1764, 721, 6, 2537, 137, 2015, 1356, 1822, 1131, 2461, 1791, 1708, 2276, 1617, 1067, 1730, 1267, 411, 2224, 348, 2253, 1712, 21, 696, 1261, 2006, 1522, 1942, 1485, 1706, 1064, 610, 1772, 1100, 218, 1710, 1007, 551, 1739, 1153, 1348, 1622, 567, 437, 3, 82, 1797, 1539, 633, 1990, 2012, 1366, 1184, 605, 1284, 1524, 978, 725, 183, 1830, 468, 2423, 1412, 67, 398, 607, 193, 2022, 1727, 34, 113, 774, 258, 1575, 788, 2307, 1471, 2551, 2320, 1218, 2326, 1360, 1777, 1424, 2476, 489, 1832, 1418, 1490, 1135, 2265, 843, 412, 1237, 2209, 1425, 1492, 2366, 1223, 391, 1929, 2145, 2210, 1169, 2557, 530, 2212, 29, 1789, 535, 761, 772, 147, 1095, 381, 426, 1583, 627, 1483, 1746, 2064, 865, 1246, 2327, 1513, 1724, 1897, 1819, 2227, 935, 2438, 990, 2558, 1193, 687, 1139, 1221, 926, 589, 945, 2355, 2544, 2321, 586, 1280, 1817, 2176, 494, 2471, 959, 1310, 340, 2135, 1774, 704, 413, 1080, 347, 181, 415, 702, 712, 1521, 2120, 366, 993, 1511, 1058, 1025, 2543, 1390, 1641, 24, 152, 312, 1801, 1795, 2552, 2415, 880, 1719, 2345, 1416, 28, 1375, 1904, 1354, 270, 2482, 2237, 1692, 1686, 1656, 2436, 1387, 806, 635, 1969, 1536, 918, 225, 142, 2510, 379, 1011, 395, 675, 963, 1063, 2256, 2066, 1214, 1691, 1846, 641, 1550, 1342, 1152, 2361, 329, 493, 2202, 1127, 339, 241, 2251, 666, 70, 126, 1966, 813, 2484, 830, 708, 85, 429, 637, 648, 315, 1400, 470, 656, 864, 2445, 2333, 1868, 1071, 2505, 2133, 1403, 2207, 2393, 1509, 16, 95, 716, 1036, 796, 975, 778, 1847, 2305, 1748, 1813, 2228, 949, 263, 1255, 2208, 1420, 1085, 1472, 802, 26, 2070, 2435, 558, 1053, 1434, 2138, 2238, 1216, 1144, 1683, 2538, 544, 2406, 477, 1119, 497, 1099, 625, 240, 455, 2334, 276, 863, 1892, 1234, 1596, 1578, 592, 1547]}
SAVED model.heads_thresh 0.9016666666666667
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
Epoch: [50/50]	lr: 7.81e-06	time: 00:00:39	Loss_train 0.00500	Acc_train 96.08	/	Loss_test 0.00206	Acc_test 96.00
new_head:  {'blocks.5.layer.bias': tensor([-0.0005, -0.0160], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0531,  0.0097,  0.0127,  ...,  0.0032,  0.0010,  0.0103],
        [-0.0248,  0.0129, -0.0082,  ..., -0.0005,  0.0202, -0.0025]],
       device='cuda:0')}
BLOCKS IN SUP:  [5]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c/models
SAVED HEADS THRESHOLD:  0.91625
SAVED model.topk_kernels {'conv0': [25, 59, 77, 30, 5, 82, 33, 16, 89, 42, 68, 70, 53, 35, 93], 'conv1': [50, 75, 56, 206, 354, 336, 365, 94, 118, 180, 345, 41, 191, 382, 140, 317, 104, 121, 352, 324, 92, 99, 264, 190, 304, 233, 105, 100, 117, 88, 202, 67, 144, 357, 26, 355, 326, 163, 130, 137, 146, 62, 36, 6, 312, 68, 217, 219, 39, 35, 296, 349, 197, 134, 171, 131, 33, 193, 292], 'conv2': [761, 958, 937, 307, 176, 195, 1317, 622, 95, 856, 1486, 731, 1385, 544, 1436, 1297, 612, 636, 1274, 311, 673, 407, 1294, 490, 348, 700, 1278, 421, 1171, 450, 35, 921, 626, 405, 250, 1281, 733, 353, 14, 499, 93, 1292, 312, 395, 380, 427, 1046, 905, 208, 1235, 1452, 1086, 1006, 61, 799, 457, 932, 869, 56, 1307, 1034, 851, 484, 981, 548, 1123, 855, 12, 1360, 639, 359, 1508, 209, 580, 1159, 1442, 743, 1404, 635, 1236, 1412, 1345, 577, 144, 66, 361, 1291, 1359, 276, 796, 1534, 1450, 1061, 866, 1306, 783, 900, 289, 123, 846, 1203, 730, 838, 818, 164, 1388, 1125, 1193, 1227, 875, 1100, 881, 105, 520, 67, 287, 1049, 1113, 574, 1105, 1083, 1000, 683, 1071, 667, 1521, 1286, 1126, 11, 243, 916, 449, 618, 698, 1466, 735, 1272, 952, 1405, 896, 229, 670, 294, 794, 1232, 542, 460, 198, 924, 850, 141, 746, 614, 1371, 1400, 773, 1242, 810, 268, 1172, 973, 155, 534, 167, 1421, 524, 1480, 711, 754, 1434, 454, 1535, 246, 911, 109, 379, 560, 681, 1500, 434, 906, 341, 692, 261, 994, 726, 942, 801, 302, 728, 598, 1030, 1233, 100, 1441, 1265, 852, 1238, 408, 991, 805, 368, 217, 687, 1008, 1116, 1464, 854, 1386, 313, 1372, 523, 1230, 935, 423, 812, 329, 1033, 199, 1325, 1516, 309, 1180, 1314, 15, 1472, 1361, 1455, 461, 640, 1489], 'conv3': [499, 1755, 1447, 505, 1394, 507, 1567, 1720, 653, 2500, 861, 2300, 398, 421, 689, 2309, 1762, 1554, 1137, 2460, 2154, 1348, 1843, 1506, 28, 1403, 1873, 340, 652, 2448, 589, 1783, 1326, 1127, 878, 648, 2106, 468, 2203, 2181, 2285, 2340, 43, 2062, 1943, 1894, 2185, 938, 1831, 828, 1617, 1536, 419, 1500, 1179, 1558, 1226, 512, 2317, 1100, 533, 80, 935, 1723, 1162, 1845, 758, 481, 1887, 2000, 1938, 1440, 792, 2241, 2282, 1867, 646, 2502, 926, 2518, 1599, 1359, 284, 540, 555, 1187, 1375, 2515, 390, 1406, 1433, 1054, 1619, 607, 1457, 1968, 1405, 971, 225, 2008, 746, 1789, 1206, 1438, 96, 593, 2072, 2086, 1164, 2117, 1125, 810, 1339, 2523, 1608, 1713, 2095, 1794, 2494, 1080, 526, 2391, 425, 1638, 1868, 446, 1597, 819, 520, 2, 1561, 2112, 2223, 2078, 1616, 1555, 177, 564, 1088, 194, 1533, 1942, 1253, 103, 2545, 2162, 1109, 354, 1218, 1275, 1448, 2254, 1380, 1698, 1217, 2114, 339, 781, 2441, 1280, 151, 1324, 2037, 1677, 775, 1964, 280, 2081, 1872, 36, 724, 81, 509, 669, 1061, 1759, 1781, 2151, 193, 277, 2456, 1644, 761, 2442, 231, 1341, 2315, 1770, 2453, 1647, 2183, 907, 1248, 2474, 1516, 362, 1492, 1449, 454, 240, 1995, 1874, 393, 559, 508, 2271, 472, 624, 1543, 2170, 470, 687, 2511, 2110, 1319, 2559, 1363, 884, 546, 291, 1424, 1097, 990, 1958, 2367, 2479, 145, 45, 1656, 1548, 2060, 943, 1684, 2084, 129, 1776, 1824, 2050, 613, 1613, 2001, 252, 780, 1153, 655, 818, 867, 1436, 1689, 2270, 1651, 1992, 6, 1385, 1346, 2328, 1170, 1788, 2067, 2134, 450, 1038, 1694, 962, 934, 697, 1132, 2283, 615, 1409, 1439, 37, 1510, 1799, 2525, 484, 29, 480, 611, 1150, 195, 1283, 236, 2305, 1664, 272, 313, 2465, 632, 1303, 683, 1194, 665, 202, 660, 365, 674, 730, 737, 1497, 1465, 2352, 63, 1926, 2028, 1646, 548, 2555, 1378, 172, 720, 1443, 980, 2323, 2482, 299, 1660, 1360, 557, 921, 40, 939, 1862, 2098, 1267, 995, 832, 1, 1393, 423, 1808, 1495, 399, 356, 1446, 662, 487, 1224, 2013, 1469, 663, 2090, 1225, 1924, 409, 1340, 2103, 2302, 1830, 588, 2357, 2093, 2003, 2033, 477, 2046, 394, 1266, 616, 1758, 2188, 1119, 370, 360, 2295, 297, 1415, 1292, 1499, 991, 1381, 2414, 11, 2031, 1659, 1998, 1670, 431, 1844, 1431, 598, 1370, 2532, 2011, 1996], 'conv4': [1323, 433, 929, 593, 448, 671, 2211, 2271, 2518, 2285, 2385, 1620, 2268, 397, 1029, 1250, 1887, 2121, 1473, 966, 881, 1898, 499, 99, 1026, 1867, 2214, 91, 783, 546, 1572, 1629, 997, 514, 188, 784, 1784, 2038, 13, 1148, 2158, 624, 1996, 2046, 1989, 615, 969, 1167, 2039, 2356, 1908, 1149, 2395, 2519, 1654, 1376, 2528, 1741, 754, 603, 910, 673, 1519, 161, 1364, 139, 674, 608, 507, 372, 808, 1657, 1610, 2258, 1019, 1925, 2308, 1285, 1498, 2388, 1090, 2548, 1764, 721, 6, 2537, 137, 2015, 1356, 1822, 1131, 2461, 1791, 1708, 2276, 1617, 1067, 1730, 1267, 411, 2224, 348, 2253, 1712, 21, 696, 1261, 2006, 1522, 1942, 1485, 1706, 1064, 610, 1772, 1100, 218, 1710, 1007, 551, 1739, 1153, 1348, 1622, 567, 437, 3, 82, 1797, 1539, 633, 1990, 2012, 1366, 1184, 605, 1284, 1524, 978, 725, 183, 1830, 468, 2423, 1412, 67, 398, 607, 193, 2022, 1727, 34, 113, 774, 258, 1575, 788, 2307, 1471, 2551, 2320, 1218, 2326, 1360, 1777, 1424, 2476, 489, 1832, 1418, 1490, 1135, 2265, 843, 412, 1237, 2209, 1425, 1492, 2366, 1223, 391, 1929, 2145, 2210, 1169, 2557, 530, 2212, 29, 1789, 535, 761, 772, 147, 1095, 381, 426, 1583, 627, 1483, 1746, 2064, 865, 1246, 2327, 1513, 1724, 1897, 1819, 2227, 935, 2438, 990, 2558, 1193, 687, 1139, 1221, 926, 589, 945, 2355, 2544, 2321, 586, 1280, 1817, 2176, 494, 2471, 959, 1310, 340, 2135, 1774, 704, 413, 1080, 347, 181, 415, 702, 712, 1521, 2120, 366, 993, 1511, 1058, 1025, 2543, 1390, 1641, 24, 152, 312, 1801, 1795, 2552, 2415, 880, 1719, 2345, 1416, 28, 1375, 1904, 1354, 270, 2482, 2237, 1692, 1686, 1656, 2436, 1387, 806, 635, 1969, 1536, 918, 225, 142, 2510, 379, 1011, 395, 675, 963, 1063, 2256, 2066, 1214, 1691, 1846, 641, 1550, 1342, 1152, 2361, 329, 493, 2202, 1127, 339, 241, 2251, 666, 70, 126, 1966, 813, 2484, 830, 708, 85, 429, 637, 648, 315, 1400, 470, 656, 864, 2445, 2333, 1868, 1071, 2505, 2133, 1403, 2207, 2393, 1509, 16, 95, 716, 1036, 796, 975, 778, 1847, 2305, 1748, 1813, 2228, 949, 263, 1255, 2208, 1420, 1085, 1472, 802, 26, 2070, 2435, 558, 1053, 1434, 2138, 2238, 1216, 1144, 1683, 2538, 544, 2406, 477, 1119, 497, 1099, 625, 240, 455, 2334, 276, 863, 1892, 1234, 1596, 1578, 592, 1547]}
SAVED model.heads_thresh 0.91625
RESULT:  {'train_loss': 0.005003861151635647, 'train_acc': 96.07999920845032, 'test_loss': 0.0020557385869324207, 'test_acc': 96.0, 'convergence': 22.7949275970459, 'R1': 0, 'dataset_sup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [2, 56]}, 'dataset_unsup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 50, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [2, 56]}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3, 4], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 50, 'lr': 0.005}, 't1': {'blocks': [5], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}}
IN R3:  {'count': 3, 'R0': {'train_loss': 0.014464263804256916, 'train_acc': 84.64999794960022, 'test_loss': 0.005055953748524189, 'test_acc': 82.0, 'convergence': 23.65986442565918, 'R1': 0, 'dataset_sup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [32, 93]}, 'dataset_unsup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 50, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [32, 93]}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3, 4], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 50, 'lr': 0.005}, 't1': {'blocks': [5], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}}, 'model_config': {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}, 'R1': {'train_loss': 0.0061482023447752, 'train_acc': 94.37000155448914, 'test_loss': 0.007179258391261101, 'test_acc': 88.0, 'convergence': 23.331533432006836, 'R1': 0, 'dataset_sup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [35, 92]}, 'dataset_unsup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 50, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [35, 92]}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3, 4], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 50, 'lr': 0.005}, 't1': {'blocks': [5], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}}, 'R2': {'train_loss': 0.006392252631485462, 'train_acc': 94.67999935150146, 'test_loss': 0.003225994296371937, 'test_acc': 92.5, 'convergence': 23.05847930908203, 'R1': 0, 'dataset_sup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [16, 41]}, 'dataset_unsup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 50, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [16, 41]}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3, 4], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 50, 'lr': 0.005}, 't1': {'blocks': [5], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}}, 'R3': {'train_loss': 0.005003861151635647, 'train_acc': 96.07999920845032, 'test_loss': 0.0020557385869324207, 'test_acc': 96.0, 'convergence': 22.7949275970459, 'R1': 0, 'dataset_sup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [2, 56]}, 'dataset_unsup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 50, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [2, 56]}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3, 4], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 50, 'lr': 0.005}, 't1': {'blocks': [5], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}}}
################################## TASK 4 ############################################
[51, 79]
SEED:  0
block 0, size : 96 16 16
96
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 2.886751345948129
block 1, size : 384 8 8
384
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.8505172717997146
block 2, size : 1536 4 4
1536
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.4252586358998573
block 3, size : 2560 2 2
2560
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.21262931794992865
block 4, size : 2560 1 1
2560
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.1647019614671031
NOWWWW
1 1
2
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
blocks['b%s' % 5] 2560 2560
range = 0.3423265984407288
train_layer_order: consecutive {'t0': {'blocks': [0, 1, 2, 3, 4], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 50, 'lr': 0.005}, 't1': {'blocks': [5], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}
The device used will be: 
True
cuda:0
PARAMSSSS IN LOAD:  {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
avg_deltas:  {'blocks.0.layer.weight': tensor([-1.6922e-01,  3.0558e-02,  1.2049e-02,  3.9516e-02, -4.9969e-01,
        -3.0439e+00,  5.1505e-02, -3.2593e-02, -4.2405e-01,  8.6483e-02,
         1.9807e-01,  2.5165e-01,  7.0195e-03,  5.2228e-02,  2.0366e-01,
        -1.3184e-01, -2.2031e+00,  2.3888e-03, -2.9824e-01, -6.7781e-01,
         2.5472e-01,  7.0496e-01, -7.8939e-02, -1.3264e-03,  2.1478e-01,
         0.0000e+00,  1.1430e-02, -6.0632e-02,  9.4325e-02, -6.8987e-02,
         5.0454e-01,  4.6363e-01, -1.5180e-01, -1.3133e+00,  3.8451e-01,
        -1.7383e-01,  2.5563e-01, -7.4148e-02,  6.5031e-01,  1.4578e-02,
         3.4526e-01,  6.0933e-02,  4.4867e-01, -4.0021e-01,  2.9619e-01,
        -3.1233e+00, -7.6284e-02,  3.1036e-01, -1.6988e-02,  1.0293e-01,
        -3.4339e-01,  2.2018e-01,  3.4006e-01,  1.2156e-02, -5.3072e-02,
         1.0000e+00, -2.3795e-01,  2.9973e-01, -1.0210e+00, -1.6296e-01,
        -2.3338e-01, -5.4405e-03, -8.6858e-02,  3.8959e-02, -4.2423e+00,
        -1.9021e-01, -8.1600e-01, -2.1660e-02,  5.0904e-01,  4.2375e-01,
         1.3137e-01,  2.5041e-01, -1.6153e-02, -5.5677e-01, -1.5058e-02,
        -5.2150e-01,  9.5023e-02,  4.5953e-01, -1.8004e-01,  2.4009e-01,
        -2.8477e-01,  3.4812e-01, -4.4845e+00, -1.4654e-02,  1.9894e-02,
        -1.3805e-01, -8.9614e-01, -1.8058e-01, -2.8345e-02, -4.1761e+00,
        -4.4666e-03, -9.4953e-02, -1.1751e-01,  4.8726e-01, -8.0669e-02,
         2.5559e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.2612e-02,  5.0463e-03,  1.1843e-02,  1.1229e-02,  4.2382e-01,
        -8.5354e-02,  1.8597e-02, -9.0869e-02, -2.2220e-02,  5.3421e-02,
        -7.7980e-03,  8.1232e-02,  5.0851e-02,  4.5548e-02, -2.6792e-03,
         4.6737e-02, -1.3556e-02,  2.2028e-01, -1.6829e-01, -1.1562e-01,
         3.2501e-02,  1.5003e-02,  1.7049e-01,  1.5556e-01,  2.4546e-01,
         1.5099e-01, -6.5082e-02, -5.6902e-02, -3.1862e-02,  3.1719e-02,
         1.7966e-02,  0.0000e+00, -1.4299e-01, -1.1973e-02, -2.4082e-03,
        -2.1826e-02,  4.3584e-01,  6.0378e-02,  9.1899e-04,  3.9139e-03,
         4.3262e-02, -1.3585e-04,  4.8478e-03,  1.0729e-03,  5.0431e-02,
         2.7299e-05, -1.9866e-01,  3.2564e-02,  0.0000e+00, -1.2617e-01,
         0.0000e+00,  1.3689e-02, -1.0376e-02, -7.9788e-03, -8.1740e-02,
         1.2038e-01,  0.0000e+00, -4.4446e-02,  4.0616e-02, -7.4789e-03,
         3.9668e-03, -3.1809e-01,  1.8182e-02, -1.5534e-01, -5.3508e-03,
        -6.0983e-04, -1.3302e-01, -6.1762e-04,  1.0554e-01, -5.2302e-02,
        -1.8518e-01, -1.4657e-03, -1.6221e-01,  2.4042e-01, -1.9007e-02,
         0.0000e+00,  1.9875e-01, -2.0358e-01, -1.7460e-01,  1.1516e-01,
        -2.7575e-02,  1.8062e-02,  3.1505e-02, -7.2344e-04,  8.8336e-02,
        -6.1551e-04,  2.7940e-03, -1.6287e-01, -4.0950e-02,  1.0601e-02,
        -1.4954e-03,  3.5880e-03, -6.9619e-03,  3.6285e-03,  0.0000e+00,
        -6.8083e-02, -1.2729e-04,  6.5316e-03, -1.7149e-01, -6.2475e-03,
         4.9682e-01,  8.4390e-03,  5.9232e-03, -4.8515e-02, -1.5981e-02,
         2.1343e-01, -6.5048e-03, -6.2808e-02, -2.2987e-02,  1.2035e-01,
        -7.5936e-02,  6.3414e-03, -2.5383e-03,  0.0000e+00,  5.4144e-02,
        -5.0808e-02, -3.3747e-02,  2.1345e-02,  0.0000e+00, -6.5218e-02,
        -1.7098e-01, -4.8858e-02,  1.6343e-02,  1.5772e-01,  2.0652e-02,
        -1.2693e-01, -2.5862e-02, -9.7612e-03,  3.8284e-02,  3.4639e-03,
         3.8866e-03,  9.4673e-02,  6.9673e-02,  8.2724e-03,  7.6764e-02,
         1.6617e-02,  3.2292e-03,  3.2784e-02,  1.6169e-01,  4.2651e-02,
         7.3716e-02,  9.5524e-03,  8.0320e-02, -2.3239e-02,  7.5706e-02,
         8.8135e-03,  1.0000e+00, -4.2485e-02,  4.9043e-02, -6.3019e-04,
        -2.9946e-02, -5.8974e-02,  3.3567e-02,  1.6153e-02,  6.6085e-04,
        -5.8661e-02, -3.0275e-01, -3.2326e-01,  2.3941e-02, -1.9420e-01,
        -1.1663e-02,  4.2509e-03,  6.2793e-02, -2.8707e-01, -4.9081e-02,
         5.7958e-02, -9.3626e-02,  1.7704e-03,  4.2491e-02, -1.0071e-01,
        -2.9259e-02,  4.1614e-02,  3.5754e-02,  8.1689e-02,  9.7101e-02,
        -5.7368e-02,  4.8957e-02, -2.5140e-04,  2.7846e-02,  1.4198e-01,
        -1.2218e-01,  4.6678e-02,  2.2657e-02,  4.9386e-04, -1.5390e-02,
         2.4182e-03,  9.1135e-02,  6.1158e-03,  4.3874e-02, -1.2338e-01,
         1.7840e-01, -2.4049e-02,  5.8216e-03,  3.3235e-02,  1.2123e-02,
         5.1059e-02,  2.2466e-03,  1.3988e-01,  2.7036e-03, -8.7844e-03,
         1.3434e-01, -2.8288e-02, -1.9754e-01,  2.2197e-01, -1.7031e-02,
         3.4277e-02,  5.2277e-02, -6.9695e-04, -1.1420e-02,  1.0283e-01,
        -4.5814e-02,  2.2652e-02,  4.9007e-04, -7.3844e-02, -1.5797e-03,
         1.3247e-02,  2.0230e-02,  7.7416e-02, -1.7772e-01,  5.2795e-02,
         2.9188e-03,  9.6453e-02, -6.5041e-02, -2.7500e-02,  1.2911e-01,
        -1.5009e-01,  5.1640e-02, -1.5535e-01,  9.9318e-03,  7.3771e-02,
        -4.1252e-03,  7.2684e-05,  1.1450e-02,  1.7932e-02,  4.1558e-02,
        -6.0060e-02,  1.0553e-01, -1.0453e-04,  9.7959e-02,  5.0164e-02,
        -6.1456e-02, -1.3501e-02, -2.1230e-03, -1.0466e-01, -5.5882e-03,
        -5.2600e-02,  1.6193e-02, -2.8089e-02,  1.3411e-01, -5.8069e-03,
         1.1590e-01,  4.8417e-03, -2.3923e-02,  1.4949e-02, -1.4830e-01,
         4.7705e-02,  1.1533e-03,  8.1029e-03, -6.7280e-03,  2.4407e-03,
        -7.0826e-04, -3.5576e-03,  1.8757e-01,  2.8498e-02,  0.0000e+00,
         1.5432e-02,  0.0000e+00,  2.8411e-02, -6.4669e-04,  1.8844e-04,
        -6.6458e-02,  2.0176e-01,  1.0405e-02, -1.9453e-04,  1.4030e-01,
        -1.4373e-02,  5.0779e-02,  6.9244e-02,  6.0564e-02, -3.6282e-03,
        -2.0234e-02,  8.2206e-02,  8.9636e-02,  1.6475e-03,  1.1944e-02,
         1.5059e-01, -1.1227e-02, -7.8199e-02,  4.4929e-02, -6.2888e-02,
        -2.3575e-02, -1.6898e-01,  2.5362e-02,  2.5342e-03, -1.4375e-02,
        -1.2812e-01, -2.8508e-01, -2.1058e-02,  6.6832e-02,  4.2266e-02,
        -1.6478e-01, -1.5021e-01,  1.8009e-03, -7.5933e-02,  0.0000e+00,
         4.6073e-02,  2.1162e-01,  1.4844e-03,  3.5720e-02, -1.3482e-02,
        -1.6184e-02, -3.2782e-03,  4.6514e-02, -2.7451e-02,  4.8634e-02,
        -7.4128e-04,  1.1159e-02, -4.4139e-02, -1.0003e-02,  2.6175e-03,
        -8.0240e-03,  1.2042e-02,  9.4057e-02, -4.9514e-02, -2.8404e-01,
         8.3525e-02,  3.8030e-02,  1.8998e-01, -5.0094e-02,  3.1170e-01,
        -2.0165e-01,  4.6826e-02, -1.8869e-01,  4.2256e-02,  2.7622e-03,
         4.5240e-02,  0.0000e+00, -6.4334e-03,  4.8887e-02,  1.4797e-02,
         1.2648e-02,  8.0791e-03, -2.1101e-02,  1.7169e-01, -8.4205e-02,
        -3.5582e-02,  1.5490e-02,  2.9487e-01,  4.4380e-02, -4.9195e-01,
         1.4262e-02, -3.7070e-03,  0.0000e+00,  3.9846e-02,  2.1262e-02,
         0.0000e+00, -7.5691e-03,  1.4452e-01,  9.4313e-02, -2.0882e-02,
         5.2084e-03,  2.7375e-03, -1.9288e-02,  1.5374e-02,  4.8799e-02,
        -7.4174e-02, -4.1728e-03,  1.1336e-01,  3.7727e-02, -6.2218e-03,
         1.1682e-02,  1.2150e-01,  4.3690e-02, -2.0061e-01, -4.3121e-02,
         2.1884e-02, -3.1343e-02,  2.8401e-02,  6.8034e-03,  4.7003e-02,
         1.0937e-01,  7.4545e-02, -4.4829e-02,  1.7538e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0028, -0.0071,  0.0568,  ...,  0.0337,  0.0000,  0.0398],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0093, -0.0104, -0.0531,  ..., -0.0193, -0.0318, -0.0834],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([-0.0032, -0.0780,  0.0386,  ...,  0.1096,  0.0361, -0.0318],
       device='cuda:0')}
topk_layer:  None
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-1.6922e-01,  3.0558e-02,  1.2049e-02,  3.9516e-02, -4.9969e-01,
        -3.0439e+00,  5.1505e-02, -3.2593e-02, -4.2405e-01,  8.6483e-02,
         1.9807e-01,  2.5165e-01,  7.0195e-03,  5.2228e-02,  2.0366e-01,
        -1.3184e-01, -2.2031e+00,  2.3888e-03, -2.9824e-01, -6.7781e-01,
         2.5472e-01,  7.0496e-01, -7.8939e-02, -1.3264e-03,  2.1478e-01,
         0.0000e+00,  1.1430e-02, -6.0632e-02,  9.4325e-02, -6.8987e-02,
         5.0454e-01,  4.6363e-01, -1.5180e-01, -1.3133e+00,  3.8451e-01,
        -1.7383e-01,  2.5563e-01, -7.4148e-02,  6.5031e-01,  1.4578e-02,
         3.4526e-01,  6.0933e-02,  4.4867e-01, -4.0021e-01,  2.9619e-01,
        -3.1233e+00, -7.6284e-02,  3.1036e-01, -1.6988e-02,  1.0293e-01,
        -3.4339e-01,  2.2018e-01,  3.4006e-01,  1.2156e-02, -5.3072e-02,
         1.0000e+00, -2.3795e-01,  2.9973e-01, -1.0210e+00, -1.6296e-01,
        -2.3338e-01, -5.4405e-03, -8.6858e-02,  3.8959e-02, -4.2423e+00,
        -1.9021e-01, -8.1600e-01, -2.1660e-02,  5.0904e-01,  4.2375e-01,
         1.3137e-01,  2.5041e-01, -1.6153e-02, -5.5677e-01, -1.5058e-02,
        -5.2150e-01,  9.5023e-02,  4.5953e-01, -1.8004e-01,  2.4009e-01,
        -2.8477e-01,  3.4812e-01, -4.4845e+00, -1.4654e-02,  1.9894e-02,
        -1.3805e-01, -8.9614e-01, -1.8058e-01, -2.8345e-02, -4.1761e+00,
        -4.4666e-03, -9.4953e-02, -1.1751e-01,  4.8726e-01, -8.0669e-02,
         2.5559e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.2612e-02,  5.0463e-03,  1.1843e-02,  1.1229e-02,  4.2382e-01,
        -8.5354e-02,  1.8597e-02, -9.0869e-02, -2.2220e-02,  5.3421e-02,
        -7.7980e-03,  8.1232e-02,  5.0851e-02,  4.5548e-02, -2.6792e-03,
         4.6737e-02, -1.3556e-02,  2.2028e-01, -1.6829e-01, -1.1562e-01,
         3.2501e-02,  1.5003e-02,  1.7049e-01,  1.5556e-01,  2.4546e-01,
         1.5099e-01, -6.5082e-02, -5.6902e-02, -3.1862e-02,  3.1719e-02,
         1.7966e-02,  0.0000e+00, -1.4299e-01, -1.1973e-02, -2.4082e-03,
        -2.1826e-02,  4.3584e-01,  6.0378e-02,  9.1899e-04,  3.9139e-03,
         4.3262e-02, -1.3585e-04,  4.8478e-03,  1.0729e-03,  5.0431e-02,
         2.7299e-05, -1.9866e-01,  3.2564e-02,  0.0000e+00, -1.2617e-01,
         0.0000e+00,  1.3689e-02, -1.0376e-02, -7.9788e-03, -8.1740e-02,
         1.2038e-01,  0.0000e+00, -4.4446e-02,  4.0616e-02, -7.4789e-03,
         3.9668e-03, -3.1809e-01,  1.8182e-02, -1.5534e-01, -5.3508e-03,
        -6.0983e-04, -1.3302e-01, -6.1762e-04,  1.0554e-01, -5.2302e-02,
        -1.8518e-01, -1.4657e-03, -1.6221e-01,  2.4042e-01, -1.9007e-02,
         0.0000e+00,  1.9875e-01, -2.0358e-01, -1.7460e-01,  1.1516e-01,
        -2.7575e-02,  1.8062e-02,  3.1505e-02, -7.2344e-04,  8.8336e-02,
        -6.1551e-04,  2.7940e-03, -1.6287e-01, -4.0950e-02,  1.0601e-02,
        -1.4954e-03,  3.5880e-03, -6.9619e-03,  3.6285e-03,  0.0000e+00,
        -6.8083e-02, -1.2729e-04,  6.5316e-03, -1.7149e-01, -6.2475e-03,
         4.9682e-01,  8.4390e-03,  5.9232e-03, -4.8515e-02, -1.5981e-02,
         2.1343e-01, -6.5048e-03, -6.2808e-02, -2.2987e-02,  1.2035e-01,
        -7.5936e-02,  6.3414e-03, -2.5383e-03,  0.0000e+00,  5.4144e-02,
        -5.0808e-02, -3.3747e-02,  2.1345e-02,  0.0000e+00, -6.5218e-02,
        -1.7098e-01, -4.8858e-02,  1.6343e-02,  1.5772e-01,  2.0652e-02,
        -1.2693e-01, -2.5862e-02, -9.7612e-03,  3.8284e-02,  3.4639e-03,
         3.8866e-03,  9.4673e-02,  6.9673e-02,  8.2724e-03,  7.6764e-02,
         1.6617e-02,  3.2292e-03,  3.2784e-02,  1.6169e-01,  4.2651e-02,
         7.3716e-02,  9.5524e-03,  8.0320e-02, -2.3239e-02,  7.5706e-02,
         8.8135e-03,  1.0000e+00, -4.2485e-02,  4.9043e-02, -6.3019e-04,
        -2.9946e-02, -5.8974e-02,  3.3567e-02,  1.6153e-02,  6.6085e-04,
        -5.8661e-02, -3.0275e-01, -3.2326e-01,  2.3941e-02, -1.9420e-01,
        -1.1663e-02,  4.2509e-03,  6.2793e-02, -2.8707e-01, -4.9081e-02,
         5.7958e-02, -9.3626e-02,  1.7704e-03,  4.2491e-02, -1.0071e-01,
        -2.9259e-02,  4.1614e-02,  3.5754e-02,  8.1689e-02,  9.7101e-02,
        -5.7368e-02,  4.8957e-02, -2.5140e-04,  2.7846e-02,  1.4198e-01,
        -1.2218e-01,  4.6678e-02,  2.2657e-02,  4.9386e-04, -1.5390e-02,
         2.4182e-03,  9.1135e-02,  6.1158e-03,  4.3874e-02, -1.2338e-01,
         1.7840e-01, -2.4049e-02,  5.8216e-03,  3.3235e-02,  1.2123e-02,
         5.1059e-02,  2.2466e-03,  1.3988e-01,  2.7036e-03, -8.7844e-03,
         1.3434e-01, -2.8288e-02, -1.9754e-01,  2.2197e-01, -1.7031e-02,
         3.4277e-02,  5.2277e-02, -6.9695e-04, -1.1420e-02,  1.0283e-01,
        -4.5814e-02,  2.2652e-02,  4.9007e-04, -7.3844e-02, -1.5797e-03,
         1.3247e-02,  2.0230e-02,  7.7416e-02, -1.7772e-01,  5.2795e-02,
         2.9188e-03,  9.6453e-02, -6.5041e-02, -2.7500e-02,  1.2911e-01,
        -1.5009e-01,  5.1640e-02, -1.5535e-01,  9.9318e-03,  7.3771e-02,
        -4.1252e-03,  7.2684e-05,  1.1450e-02,  1.7932e-02,  4.1558e-02,
        -6.0060e-02,  1.0553e-01, -1.0453e-04,  9.7959e-02,  5.0164e-02,
        -6.1456e-02, -1.3501e-02, -2.1230e-03, -1.0466e-01, -5.5882e-03,
        -5.2600e-02,  1.6193e-02, -2.8089e-02,  1.3411e-01, -5.8069e-03,
         1.1590e-01,  4.8417e-03, -2.3923e-02,  1.4949e-02, -1.4830e-01,
         4.7705e-02,  1.1533e-03,  8.1029e-03, -6.7280e-03,  2.4407e-03,
        -7.0826e-04, -3.5576e-03,  1.8757e-01,  2.8498e-02,  0.0000e+00,
         1.5432e-02,  0.0000e+00,  2.8411e-02, -6.4669e-04,  1.8844e-04,
        -6.6458e-02,  2.0176e-01,  1.0405e-02, -1.9453e-04,  1.4030e-01,
        -1.4373e-02,  5.0779e-02,  6.9244e-02,  6.0564e-02, -3.6282e-03,
        -2.0234e-02,  8.2206e-02,  8.9636e-02,  1.6475e-03,  1.1944e-02,
         1.5059e-01, -1.1227e-02, -7.8199e-02,  4.4929e-02, -6.2888e-02,
        -2.3575e-02, -1.6898e-01,  2.5362e-02,  2.5342e-03, -1.4375e-02,
        -1.2812e-01, -2.8508e-01, -2.1058e-02,  6.6832e-02,  4.2266e-02,
        -1.6478e-01, -1.5021e-01,  1.8009e-03, -7.5933e-02,  0.0000e+00,
         4.6073e-02,  2.1162e-01,  1.4844e-03,  3.5720e-02, -1.3482e-02,
        -1.6184e-02, -3.2782e-03,  4.6514e-02, -2.7451e-02,  4.8634e-02,
        -7.4128e-04,  1.1159e-02, -4.4139e-02, -1.0003e-02,  2.6175e-03,
        -8.0240e-03,  1.2042e-02,  9.4057e-02, -4.9514e-02, -2.8404e-01,
         8.3525e-02,  3.8030e-02,  1.8998e-01, -5.0094e-02,  3.1170e-01,
        -2.0165e-01,  4.6826e-02, -1.8869e-01,  4.2256e-02,  2.7622e-03,
         4.5240e-02,  0.0000e+00, -6.4334e-03,  4.8887e-02,  1.4797e-02,
         1.2648e-02,  8.0791e-03, -2.1101e-02,  1.7169e-01, -8.4205e-02,
        -3.5582e-02,  1.5490e-02,  2.9487e-01,  4.4380e-02, -4.9195e-01,
         1.4262e-02, -3.7070e-03,  0.0000e+00,  3.9846e-02,  2.1262e-02,
         0.0000e+00, -7.5691e-03,  1.4452e-01,  9.4313e-02, -2.0882e-02,
         5.2084e-03,  2.7375e-03, -1.9288e-02,  1.5374e-02,  4.8799e-02,
        -7.4174e-02, -4.1728e-03,  1.1336e-01,  3.7727e-02, -6.2218e-03,
         1.1682e-02,  1.2150e-01,  4.3690e-02, -2.0061e-01, -4.3121e-02,
         2.1884e-02, -3.1343e-02,  2.8401e-02,  6.8034e-03,  4.7003e-02,
         1.0937e-01,  7.4545e-02, -4.4829e-02,  1.7538e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0028, -0.0071,  0.0568,  ...,  0.0337,  0.0000,  0.0398],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0093, -0.0104, -0.0531,  ..., -0.0193, -0.0318, -0.0834],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([-0.0032, -0.0780,  0.0386,  ...,  0.1096,  0.0361, -0.0318],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.0.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([-1.6922e-01,  3.0558e-02,  1.2049e-02,  3.9516e-02, -4.9969e-01,
        -3.0439e+00,  5.1505e-02, -3.2593e-02, -4.2405e-01,  8.6483e-02,
         1.9807e-01,  2.5165e-01,  7.0195e-03,  5.2228e-02,  2.0366e-01,
        -1.3184e-01, -2.2031e+00,  2.3888e-03, -2.9824e-01, -6.7781e-01,
         2.5472e-01,  7.0496e-01, -7.8939e-02, -1.3264e-03,  2.1478e-01,
         0.0000e+00,  1.1430e-02, -6.0632e-02,  9.4325e-02, -6.8987e-02,
         5.0454e-01,  4.6363e-01, -1.5180e-01, -1.3133e+00,  3.8451e-01,
        -1.7383e-01,  2.5563e-01, -7.4148e-02,  6.5031e-01,  1.4578e-02,
         3.4526e-01,  6.0933e-02,  4.4867e-01, -4.0021e-01,  2.9619e-01,
        -3.1233e+00, -7.6284e-02,  3.1036e-01, -1.6988e-02,  1.0293e-01,
        -3.4339e-01,  2.2018e-01,  3.4006e-01,  1.2156e-02, -5.3072e-02,
         1.0000e+00, -2.3795e-01,  2.9973e-01, -1.0210e+00, -1.6296e-01,
        -2.3338e-01, -5.4405e-03, -8.6858e-02,  3.8959e-02, -4.2423e+00,
        -1.9021e-01, -8.1600e-01, -2.1660e-02,  5.0904e-01,  4.2375e-01,
         1.3137e-01,  2.5041e-01, -1.6153e-02, -5.5677e-01, -1.5058e-02,
        -5.2150e-01,  9.5023e-02,  4.5953e-01, -1.8004e-01,  2.4009e-01,
        -2.8477e-01,  3.4812e-01, -4.4845e+00, -1.4654e-02,  1.9894e-02,
        -1.3805e-01, -8.9614e-01, -1.8058e-01, -2.8345e-02, -4.1761e+00,
        -4.4666e-03, -9.4953e-02, -1.1751e-01,  4.8726e-01, -8.0669e-02,
         2.5559e-01], device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-1.6922e-01,  3.0558e-02,  1.2049e-02,  3.9516e-02, -4.9969e-01,
        -3.0439e+00,  5.1505e-02, -3.2593e-02, -4.2405e-01,  8.6483e-02,
         1.9807e-01,  2.5165e-01,  7.0195e-03,  5.2228e-02,  2.0366e-01,
        -1.3184e-01, -2.2031e+00,  2.3888e-03, -2.9824e-01, -6.7781e-01,
         2.5472e-01,  7.0496e-01, -7.8939e-02, -1.3264e-03,  2.1478e-01,
         0.0000e+00,  1.1430e-02, -6.0632e-02,  9.4325e-02, -6.8987e-02,
         5.0454e-01,  4.6363e-01, -1.5180e-01, -1.3133e+00,  3.8451e-01,
        -1.7383e-01,  2.5563e-01, -7.4148e-02,  6.5031e-01,  1.4578e-02,
         3.4526e-01,  6.0933e-02,  4.4867e-01, -4.0021e-01,  2.9619e-01,
        -3.1233e+00, -7.6284e-02,  3.1036e-01, -1.6988e-02,  1.0293e-01,
        -3.4339e-01,  2.2018e-01,  3.4006e-01,  1.2156e-02, -5.3072e-02,
         1.0000e+00, -2.3795e-01,  2.9973e-01, -1.0210e+00, -1.6296e-01,
        -2.3338e-01, -5.4405e-03, -8.6858e-02,  3.8959e-02, -4.2423e+00,
        -1.9021e-01, -8.1600e-01, -2.1660e-02,  5.0904e-01,  4.2375e-01,
         1.3137e-01,  2.5041e-01, -1.6153e-02, -5.5677e-01, -1.5058e-02,
        -5.2150e-01,  9.5023e-02,  4.5953e-01, -1.8004e-01,  2.4009e-01,
        -2.8477e-01,  3.4812e-01, -4.4845e+00, -1.4654e-02,  1.9894e-02,
        -1.3805e-01, -8.9614e-01, -1.8058e-01, -2.8345e-02, -4.1761e+00,
        -4.4666e-03, -9.4953e-02, -1.1751e-01,  4.8726e-01, -8.0669e-02,
         2.5559e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.2612e-02,  5.0463e-03,  1.1843e-02,  1.1229e-02,  4.2382e-01,
        -8.5354e-02,  1.8597e-02, -9.0869e-02, -2.2220e-02,  5.3421e-02,
        -7.7980e-03,  8.1232e-02,  5.0851e-02,  4.5548e-02, -2.6792e-03,
         4.6737e-02, -1.3556e-02,  2.2028e-01, -1.6829e-01, -1.1562e-01,
         3.2501e-02,  1.5003e-02,  1.7049e-01,  1.5556e-01,  2.4546e-01,
         1.5099e-01, -6.5082e-02, -5.6902e-02, -3.1862e-02,  3.1719e-02,
         1.7966e-02,  0.0000e+00, -1.4299e-01, -1.1973e-02, -2.4082e-03,
        -2.1826e-02,  4.3584e-01,  6.0378e-02,  9.1899e-04,  3.9139e-03,
         4.3262e-02, -1.3585e-04,  4.8478e-03,  1.0729e-03,  5.0431e-02,
         2.7299e-05, -1.9866e-01,  3.2564e-02,  0.0000e+00, -1.2617e-01,
         0.0000e+00,  1.3689e-02, -1.0376e-02, -7.9788e-03, -8.1740e-02,
         1.2038e-01,  0.0000e+00, -4.4446e-02,  4.0616e-02, -7.4789e-03,
         3.9668e-03, -3.1809e-01,  1.8182e-02, -1.5534e-01, -5.3508e-03,
        -6.0983e-04, -1.3302e-01, -6.1762e-04,  1.0554e-01, -5.2302e-02,
        -1.8518e-01, -1.4657e-03, -1.6221e-01,  2.4042e-01, -1.9007e-02,
         0.0000e+00,  1.9875e-01, -2.0358e-01, -1.7460e-01,  1.1516e-01,
        -2.7575e-02,  1.8062e-02,  3.1505e-02, -7.2344e-04,  8.8336e-02,
        -6.1551e-04,  2.7940e-03, -1.6287e-01, -4.0950e-02,  1.0601e-02,
        -1.4954e-03,  3.5880e-03, -6.9619e-03,  3.6285e-03,  0.0000e+00,
        -6.8083e-02, -1.2729e-04,  6.5316e-03, -1.7149e-01, -6.2475e-03,
         4.9682e-01,  8.4390e-03,  5.9232e-03, -4.8515e-02, -1.5981e-02,
         2.1343e-01, -6.5048e-03, -6.2808e-02, -2.2987e-02,  1.2035e-01,
        -7.5936e-02,  6.3414e-03, -2.5383e-03,  0.0000e+00,  5.4144e-02,
        -5.0808e-02, -3.3747e-02,  2.1345e-02,  0.0000e+00, -6.5218e-02,
        -1.7098e-01, -4.8858e-02,  1.6343e-02,  1.5772e-01,  2.0652e-02,
        -1.2693e-01, -2.5862e-02, -9.7612e-03,  3.8284e-02,  3.4639e-03,
         3.8866e-03,  9.4673e-02,  6.9673e-02,  8.2724e-03,  7.6764e-02,
         1.6617e-02,  3.2292e-03,  3.2784e-02,  1.6169e-01,  4.2651e-02,
         7.3716e-02,  9.5524e-03,  8.0320e-02, -2.3239e-02,  7.5706e-02,
         8.8135e-03,  1.0000e+00, -4.2485e-02,  4.9043e-02, -6.3019e-04,
        -2.9946e-02, -5.8974e-02,  3.3567e-02,  1.6153e-02,  6.6085e-04,
        -5.8661e-02, -3.0275e-01, -3.2326e-01,  2.3941e-02, -1.9420e-01,
        -1.1663e-02,  4.2509e-03,  6.2793e-02, -2.8707e-01, -4.9081e-02,
         5.7958e-02, -9.3626e-02,  1.7704e-03,  4.2491e-02, -1.0071e-01,
        -2.9259e-02,  4.1614e-02,  3.5754e-02,  8.1689e-02,  9.7101e-02,
        -5.7368e-02,  4.8957e-02, -2.5140e-04,  2.7846e-02,  1.4198e-01,
        -1.2218e-01,  4.6678e-02,  2.2657e-02,  4.9386e-04, -1.5390e-02,
         2.4182e-03,  9.1135e-02,  6.1158e-03,  4.3874e-02, -1.2338e-01,
         1.7840e-01, -2.4049e-02,  5.8216e-03,  3.3235e-02,  1.2123e-02,
         5.1059e-02,  2.2466e-03,  1.3988e-01,  2.7036e-03, -8.7844e-03,
         1.3434e-01, -2.8288e-02, -1.9754e-01,  2.2197e-01, -1.7031e-02,
         3.4277e-02,  5.2277e-02, -6.9695e-04, -1.1420e-02,  1.0283e-01,
        -4.5814e-02,  2.2652e-02,  4.9007e-04, -7.3844e-02, -1.5797e-03,
         1.3247e-02,  2.0230e-02,  7.7416e-02, -1.7772e-01,  5.2795e-02,
         2.9188e-03,  9.6453e-02, -6.5041e-02, -2.7500e-02,  1.2911e-01,
        -1.5009e-01,  5.1640e-02, -1.5535e-01,  9.9318e-03,  7.3771e-02,
        -4.1252e-03,  7.2684e-05,  1.1450e-02,  1.7932e-02,  4.1558e-02,
        -6.0060e-02,  1.0553e-01, -1.0453e-04,  9.7959e-02,  5.0164e-02,
        -6.1456e-02, -1.3501e-02, -2.1230e-03, -1.0466e-01, -5.5882e-03,
        -5.2600e-02,  1.6193e-02, -2.8089e-02,  1.3411e-01, -5.8069e-03,
         1.1590e-01,  4.8417e-03, -2.3923e-02,  1.4949e-02, -1.4830e-01,
         4.7705e-02,  1.1533e-03,  8.1029e-03, -6.7280e-03,  2.4407e-03,
        -7.0826e-04, -3.5576e-03,  1.8757e-01,  2.8498e-02,  0.0000e+00,
         1.5432e-02,  0.0000e+00,  2.8411e-02, -6.4669e-04,  1.8844e-04,
        -6.6458e-02,  2.0176e-01,  1.0405e-02, -1.9453e-04,  1.4030e-01,
        -1.4373e-02,  5.0779e-02,  6.9244e-02,  6.0564e-02, -3.6282e-03,
        -2.0234e-02,  8.2206e-02,  8.9636e-02,  1.6475e-03,  1.1944e-02,
         1.5059e-01, -1.1227e-02, -7.8199e-02,  4.4929e-02, -6.2888e-02,
        -2.3575e-02, -1.6898e-01,  2.5362e-02,  2.5342e-03, -1.4375e-02,
        -1.2812e-01, -2.8508e-01, -2.1058e-02,  6.6832e-02,  4.2266e-02,
        -1.6478e-01, -1.5021e-01,  1.8009e-03, -7.5933e-02,  0.0000e+00,
         4.6073e-02,  2.1162e-01,  1.4844e-03,  3.5720e-02, -1.3482e-02,
        -1.6184e-02, -3.2782e-03,  4.6514e-02, -2.7451e-02,  4.8634e-02,
        -7.4128e-04,  1.1159e-02, -4.4139e-02, -1.0003e-02,  2.6175e-03,
        -8.0240e-03,  1.2042e-02,  9.4057e-02, -4.9514e-02, -2.8404e-01,
         8.3525e-02,  3.8030e-02,  1.8998e-01, -5.0094e-02,  3.1170e-01,
        -2.0165e-01,  4.6826e-02, -1.8869e-01,  4.2256e-02,  2.7622e-03,
         4.5240e-02,  0.0000e+00, -6.4334e-03,  4.8887e-02,  1.4797e-02,
         1.2648e-02,  8.0791e-03, -2.1101e-02,  1.7169e-01, -8.4205e-02,
        -3.5582e-02,  1.5490e-02,  2.9487e-01,  4.4380e-02, -4.9195e-01,
         1.4262e-02, -3.7070e-03,  0.0000e+00,  3.9846e-02,  2.1262e-02,
         0.0000e+00, -7.5691e-03,  1.4452e-01,  9.4313e-02, -2.0882e-02,
         5.2084e-03,  2.7375e-03, -1.9288e-02,  1.5374e-02,  4.8799e-02,
        -7.4174e-02, -4.1728e-03,  1.1336e-01,  3.7727e-02, -6.2218e-03,
         1.1682e-02,  1.2150e-01,  4.3690e-02, -2.0061e-01, -4.3121e-02,
         2.1884e-02, -3.1343e-02,  2.8401e-02,  6.8034e-03,  4.7003e-02,
         1.0937e-01,  7.4545e-02, -4.4829e-02,  1.7538e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0028, -0.0071,  0.0568,  ...,  0.0337,  0.0000,  0.0398],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0093, -0.0104, -0.0531,  ..., -0.0193, -0.0318, -0.0834],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([-0.0032, -0.0780,  0.0386,  ...,  0.1096,  0.0361, -0.0318],
       device='cuda:0')}
topk_layer inside model after retrieval:  [25, 59, 77, 30, 5, 82, 33, 16, 89, 42, 68, 70, 53, 35, 93]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-1.6922e-01,  3.0558e-02,  1.2049e-02,  3.9516e-02, -4.9969e-01,
        -3.0439e+00,  5.1505e-02, -3.2593e-02, -4.2405e-01,  8.6483e-02,
         1.9807e-01,  2.5165e-01,  7.0195e-03,  5.2228e-02,  2.0366e-01,
        -1.3184e-01, -2.2031e+00,  2.3888e-03, -2.9824e-01, -6.7781e-01,
         2.5472e-01,  7.0496e-01, -7.8939e-02, -1.3264e-03,  2.1478e-01,
         0.0000e+00,  1.1430e-02, -6.0632e-02,  9.4325e-02, -6.8987e-02,
         5.0454e-01,  4.6363e-01, -1.5180e-01, -1.3133e+00,  3.8451e-01,
        -1.7383e-01,  2.5563e-01, -7.4148e-02,  6.5031e-01,  1.4578e-02,
         3.4526e-01,  6.0933e-02,  4.4867e-01, -4.0021e-01,  2.9619e-01,
        -3.1233e+00, -7.6284e-02,  3.1036e-01, -1.6988e-02,  1.0293e-01,
        -3.4339e-01,  2.2018e-01,  3.4006e-01,  1.2156e-02, -5.3072e-02,
         1.0000e+00, -2.3795e-01,  2.9973e-01, -1.0210e+00, -1.6296e-01,
        -2.3338e-01, -5.4405e-03, -8.6858e-02,  3.8959e-02, -4.2423e+00,
        -1.9021e-01, -8.1600e-01, -2.1660e-02,  5.0904e-01,  4.2375e-01,
         1.3137e-01,  2.5041e-01, -1.6153e-02, -5.5677e-01, -1.5058e-02,
        -5.2150e-01,  9.5023e-02,  4.5953e-01, -1.8004e-01,  2.4009e-01,
        -2.8477e-01,  3.4812e-01, -4.4845e+00, -1.4654e-02,  1.9894e-02,
        -1.3805e-01, -8.9614e-01, -1.8058e-01, -2.8345e-02, -4.1761e+00,
        -4.4666e-03, -9.4953e-02, -1.1751e-01,  4.8726e-01, -8.0669e-02,
         2.5559e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.2612e-02,  5.0463e-03,  1.1843e-02,  1.1229e-02,  4.2382e-01,
        -8.5354e-02,  1.8597e-02, -9.0869e-02, -2.2220e-02,  5.3421e-02,
        -7.7980e-03,  8.1232e-02,  5.0851e-02,  4.5548e-02, -2.6792e-03,
         4.6737e-02, -1.3556e-02,  2.2028e-01, -1.6829e-01, -1.1562e-01,
         3.2501e-02,  1.5003e-02,  1.7049e-01,  1.5556e-01,  2.4546e-01,
         1.5099e-01, -6.5082e-02, -5.6902e-02, -3.1862e-02,  3.1719e-02,
         1.7966e-02,  0.0000e+00, -1.4299e-01, -1.1973e-02, -2.4082e-03,
        -2.1826e-02,  4.3584e-01,  6.0378e-02,  9.1899e-04,  3.9139e-03,
         4.3262e-02, -1.3585e-04,  4.8478e-03,  1.0729e-03,  5.0431e-02,
         2.7299e-05, -1.9866e-01,  3.2564e-02,  0.0000e+00, -1.2617e-01,
         0.0000e+00,  1.3689e-02, -1.0376e-02, -7.9788e-03, -8.1740e-02,
         1.2038e-01,  0.0000e+00, -4.4446e-02,  4.0616e-02, -7.4789e-03,
         3.9668e-03, -3.1809e-01,  1.8182e-02, -1.5534e-01, -5.3508e-03,
        -6.0983e-04, -1.3302e-01, -6.1762e-04,  1.0554e-01, -5.2302e-02,
        -1.8518e-01, -1.4657e-03, -1.6221e-01,  2.4042e-01, -1.9007e-02,
         0.0000e+00,  1.9875e-01, -2.0358e-01, -1.7460e-01,  1.1516e-01,
        -2.7575e-02,  1.8062e-02,  3.1505e-02, -7.2344e-04,  8.8336e-02,
        -6.1551e-04,  2.7940e-03, -1.6287e-01, -4.0950e-02,  1.0601e-02,
        -1.4954e-03,  3.5880e-03, -6.9619e-03,  3.6285e-03,  0.0000e+00,
        -6.8083e-02, -1.2729e-04,  6.5316e-03, -1.7149e-01, -6.2475e-03,
         4.9682e-01,  8.4390e-03,  5.9232e-03, -4.8515e-02, -1.5981e-02,
         2.1343e-01, -6.5048e-03, -6.2808e-02, -2.2987e-02,  1.2035e-01,
        -7.5936e-02,  6.3414e-03, -2.5383e-03,  0.0000e+00,  5.4144e-02,
        -5.0808e-02, -3.3747e-02,  2.1345e-02,  0.0000e+00, -6.5218e-02,
        -1.7098e-01, -4.8858e-02,  1.6343e-02,  1.5772e-01,  2.0652e-02,
        -1.2693e-01, -2.5862e-02, -9.7612e-03,  3.8284e-02,  3.4639e-03,
         3.8866e-03,  9.4673e-02,  6.9673e-02,  8.2724e-03,  7.6764e-02,
         1.6617e-02,  3.2292e-03,  3.2784e-02,  1.6169e-01,  4.2651e-02,
         7.3716e-02,  9.5524e-03,  8.0320e-02, -2.3239e-02,  7.5706e-02,
         8.8135e-03,  1.0000e+00, -4.2485e-02,  4.9043e-02, -6.3019e-04,
        -2.9946e-02, -5.8974e-02,  3.3567e-02,  1.6153e-02,  6.6085e-04,
        -5.8661e-02, -3.0275e-01, -3.2326e-01,  2.3941e-02, -1.9420e-01,
        -1.1663e-02,  4.2509e-03,  6.2793e-02, -2.8707e-01, -4.9081e-02,
         5.7958e-02, -9.3626e-02,  1.7704e-03,  4.2491e-02, -1.0071e-01,
        -2.9259e-02,  4.1614e-02,  3.5754e-02,  8.1689e-02,  9.7101e-02,
        -5.7368e-02,  4.8957e-02, -2.5140e-04,  2.7846e-02,  1.4198e-01,
        -1.2218e-01,  4.6678e-02,  2.2657e-02,  4.9386e-04, -1.5390e-02,
         2.4182e-03,  9.1135e-02,  6.1158e-03,  4.3874e-02, -1.2338e-01,
         1.7840e-01, -2.4049e-02,  5.8216e-03,  3.3235e-02,  1.2123e-02,
         5.1059e-02,  2.2466e-03,  1.3988e-01,  2.7036e-03, -8.7844e-03,
         1.3434e-01, -2.8288e-02, -1.9754e-01,  2.2197e-01, -1.7031e-02,
         3.4277e-02,  5.2277e-02, -6.9695e-04, -1.1420e-02,  1.0283e-01,
        -4.5814e-02,  2.2652e-02,  4.9007e-04, -7.3844e-02, -1.5797e-03,
         1.3247e-02,  2.0230e-02,  7.7416e-02, -1.7772e-01,  5.2795e-02,
         2.9188e-03,  9.6453e-02, -6.5041e-02, -2.7500e-02,  1.2911e-01,
        -1.5009e-01,  5.1640e-02, -1.5535e-01,  9.9318e-03,  7.3771e-02,
        -4.1252e-03,  7.2684e-05,  1.1450e-02,  1.7932e-02,  4.1558e-02,
        -6.0060e-02,  1.0553e-01, -1.0453e-04,  9.7959e-02,  5.0164e-02,
        -6.1456e-02, -1.3501e-02, -2.1230e-03, -1.0466e-01, -5.5882e-03,
        -5.2600e-02,  1.6193e-02, -2.8089e-02,  1.3411e-01, -5.8069e-03,
         1.1590e-01,  4.8417e-03, -2.3923e-02,  1.4949e-02, -1.4830e-01,
         4.7705e-02,  1.1533e-03,  8.1029e-03, -6.7280e-03,  2.4407e-03,
        -7.0826e-04, -3.5576e-03,  1.8757e-01,  2.8498e-02,  0.0000e+00,
         1.5432e-02,  0.0000e+00,  2.8411e-02, -6.4669e-04,  1.8844e-04,
        -6.6458e-02,  2.0176e-01,  1.0405e-02, -1.9453e-04,  1.4030e-01,
        -1.4373e-02,  5.0779e-02,  6.9244e-02,  6.0564e-02, -3.6282e-03,
        -2.0234e-02,  8.2206e-02,  8.9636e-02,  1.6475e-03,  1.1944e-02,
         1.5059e-01, -1.1227e-02, -7.8199e-02,  4.4929e-02, -6.2888e-02,
        -2.3575e-02, -1.6898e-01,  2.5362e-02,  2.5342e-03, -1.4375e-02,
        -1.2812e-01, -2.8508e-01, -2.1058e-02,  6.6832e-02,  4.2266e-02,
        -1.6478e-01, -1.5021e-01,  1.8009e-03, -7.5933e-02,  0.0000e+00,
         4.6073e-02,  2.1162e-01,  1.4844e-03,  3.5720e-02, -1.3482e-02,
        -1.6184e-02, -3.2782e-03,  4.6514e-02, -2.7451e-02,  4.8634e-02,
        -7.4128e-04,  1.1159e-02, -4.4139e-02, -1.0003e-02,  2.6175e-03,
        -8.0240e-03,  1.2042e-02,  9.4057e-02, -4.9514e-02, -2.8404e-01,
         8.3525e-02,  3.8030e-02,  1.8998e-01, -5.0094e-02,  3.1170e-01,
        -2.0165e-01,  4.6826e-02, -1.8869e-01,  4.2256e-02,  2.7622e-03,
         4.5240e-02,  0.0000e+00, -6.4334e-03,  4.8887e-02,  1.4797e-02,
         1.2648e-02,  8.0791e-03, -2.1101e-02,  1.7169e-01, -8.4205e-02,
        -3.5582e-02,  1.5490e-02,  2.9487e-01,  4.4380e-02, -4.9195e-01,
         1.4262e-02, -3.7070e-03,  0.0000e+00,  3.9846e-02,  2.1262e-02,
         0.0000e+00, -7.5691e-03,  1.4452e-01,  9.4313e-02, -2.0882e-02,
         5.2084e-03,  2.7375e-03, -1.9288e-02,  1.5374e-02,  4.8799e-02,
        -7.4174e-02, -4.1728e-03,  1.1336e-01,  3.7727e-02, -6.2218e-03,
         1.1682e-02,  1.2150e-01,  4.3690e-02, -2.0061e-01, -4.3121e-02,
         2.1884e-02, -3.1343e-02,  2.8401e-02,  6.8034e-03,  4.7003e-02,
         1.0937e-01,  7.4545e-02, -4.4829e-02,  1.7538e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0028, -0.0071,  0.0568,  ...,  0.0337,  0.0000,  0.0398],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0093, -0.0104, -0.0531,  ..., -0.0193, -0.0318, -0.0834],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([-0.0032, -0.0780,  0.0386,  ...,  0.1096,  0.0361, -0.0318],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.1.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 3.2612e-02,  5.0463e-03,  1.1843e-02,  1.1229e-02,  4.2382e-01,
        -8.5354e-02,  1.8597e-02, -9.0869e-02, -2.2220e-02,  5.3421e-02,
        -7.7980e-03,  8.1232e-02,  5.0851e-02,  4.5548e-02, -2.6792e-03,
         4.6737e-02, -1.3556e-02,  2.2028e-01, -1.6829e-01, -1.1562e-01,
         3.2501e-02,  1.5003e-02,  1.7049e-01,  1.5556e-01,  2.4546e-01,
         1.5099e-01, -6.5082e-02, -5.6902e-02, -3.1862e-02,  3.1719e-02,
         1.7966e-02,  0.0000e+00, -1.4299e-01, -1.1973e-02, -2.4082e-03,
        -2.1826e-02,  4.3584e-01,  6.0378e-02,  9.1899e-04,  3.9139e-03,
         4.3262e-02, -1.3585e-04,  4.8478e-03,  1.0729e-03,  5.0431e-02,
         2.7299e-05, -1.9866e-01,  3.2564e-02,  0.0000e+00, -1.2617e-01,
         0.0000e+00,  1.3689e-02, -1.0376e-02, -7.9788e-03, -8.1740e-02,
         1.2038e-01,  0.0000e+00, -4.4446e-02,  4.0616e-02, -7.4789e-03,
         3.9668e-03, -3.1809e-01,  1.8182e-02, -1.5534e-01, -5.3508e-03,
        -6.0983e-04, -1.3302e-01, -6.1762e-04,  1.0554e-01, -5.2302e-02,
        -1.8518e-01, -1.4657e-03, -1.6221e-01,  2.4042e-01, -1.9007e-02,
         0.0000e+00,  1.9875e-01, -2.0358e-01, -1.7460e-01,  1.1516e-01,
        -2.7575e-02,  1.8062e-02,  3.1505e-02, -7.2344e-04,  8.8336e-02,
        -6.1551e-04,  2.7940e-03, -1.6287e-01, -4.0950e-02,  1.0601e-02,
        -1.4954e-03,  3.5880e-03, -6.9619e-03,  3.6285e-03,  0.0000e+00,
        -6.8083e-02, -1.2729e-04,  6.5316e-03, -1.7149e-01, -6.2475e-03,
         4.9682e-01,  8.4390e-03,  5.9232e-03, -4.8515e-02, -1.5981e-02,
         2.1343e-01, -6.5048e-03, -6.2808e-02, -2.2987e-02,  1.2035e-01,
        -7.5936e-02,  6.3414e-03, -2.5383e-03,  0.0000e+00,  5.4144e-02,
        -5.0808e-02, -3.3747e-02,  2.1345e-02,  0.0000e+00, -6.5218e-02,
        -1.7098e-01, -4.8858e-02,  1.6343e-02,  1.5772e-01,  2.0652e-02,
        -1.2693e-01, -2.5862e-02, -9.7612e-03,  3.8284e-02,  3.4639e-03,
         3.8866e-03,  9.4673e-02,  6.9673e-02,  8.2724e-03,  7.6764e-02,
         1.6617e-02,  3.2292e-03,  3.2784e-02,  1.6169e-01,  4.2651e-02,
         7.3716e-02,  9.5524e-03,  8.0320e-02, -2.3239e-02,  7.5706e-02,
         8.8135e-03,  1.0000e+00, -4.2485e-02,  4.9043e-02, -6.3019e-04,
        -2.9946e-02, -5.8974e-02,  3.3567e-02,  1.6153e-02,  6.6085e-04,
        -5.8661e-02, -3.0275e-01, -3.2326e-01,  2.3941e-02, -1.9420e-01,
        -1.1663e-02,  4.2509e-03,  6.2793e-02, -2.8707e-01, -4.9081e-02,
         5.7958e-02, -9.3626e-02,  1.7704e-03,  4.2491e-02, -1.0071e-01,
        -2.9259e-02,  4.1614e-02,  3.5754e-02,  8.1689e-02,  9.7101e-02,
        -5.7368e-02,  4.8957e-02, -2.5140e-04,  2.7846e-02,  1.4198e-01,
        -1.2218e-01,  4.6678e-02,  2.2657e-02,  4.9386e-04, -1.5390e-02,
         2.4182e-03,  9.1135e-02,  6.1158e-03,  4.3874e-02, -1.2338e-01,
         1.7840e-01, -2.4049e-02,  5.8216e-03,  3.3235e-02,  1.2123e-02,
         5.1059e-02,  2.2466e-03,  1.3988e-01,  2.7036e-03, -8.7844e-03,
         1.3434e-01, -2.8288e-02, -1.9754e-01,  2.2197e-01, -1.7031e-02,
         3.4277e-02,  5.2277e-02, -6.9695e-04, -1.1420e-02,  1.0283e-01,
        -4.5814e-02,  2.2652e-02,  4.9007e-04, -7.3844e-02, -1.5797e-03,
         1.3247e-02,  2.0230e-02,  7.7416e-02, -1.7772e-01,  5.2795e-02,
         2.9188e-03,  9.6453e-02, -6.5041e-02, -2.7500e-02,  1.2911e-01,
        -1.5009e-01,  5.1640e-02, -1.5535e-01,  9.9318e-03,  7.3771e-02,
        -4.1252e-03,  7.2684e-05,  1.1450e-02,  1.7932e-02,  4.1558e-02,
        -6.0060e-02,  1.0553e-01, -1.0453e-04,  9.7959e-02,  5.0164e-02,
        -6.1456e-02, -1.3501e-02, -2.1230e-03, -1.0466e-01, -5.5882e-03,
        -5.2600e-02,  1.6193e-02, -2.8089e-02,  1.3411e-01, -5.8069e-03,
         1.1590e-01,  4.8417e-03, -2.3923e-02,  1.4949e-02, -1.4830e-01,
         4.7705e-02,  1.1533e-03,  8.1029e-03, -6.7280e-03,  2.4407e-03,
        -7.0826e-04, -3.5576e-03,  1.8757e-01,  2.8498e-02,  0.0000e+00,
         1.5432e-02,  0.0000e+00,  2.8411e-02, -6.4669e-04,  1.8844e-04,
        -6.6458e-02,  2.0176e-01,  1.0405e-02, -1.9453e-04,  1.4030e-01,
        -1.4373e-02,  5.0779e-02,  6.9244e-02,  6.0564e-02, -3.6282e-03,
        -2.0234e-02,  8.2206e-02,  8.9636e-02,  1.6475e-03,  1.1944e-02,
         1.5059e-01, -1.1227e-02, -7.8199e-02,  4.4929e-02, -6.2888e-02,
        -2.3575e-02, -1.6898e-01,  2.5362e-02,  2.5342e-03, -1.4375e-02,
        -1.2812e-01, -2.8508e-01, -2.1058e-02,  6.6832e-02,  4.2266e-02,
        -1.6478e-01, -1.5021e-01,  1.8009e-03, -7.5933e-02,  0.0000e+00,
         4.6073e-02,  2.1162e-01,  1.4844e-03,  3.5720e-02, -1.3482e-02,
        -1.6184e-02, -3.2782e-03,  4.6514e-02, -2.7451e-02,  4.8634e-02,
        -7.4128e-04,  1.1159e-02, -4.4139e-02, -1.0003e-02,  2.6175e-03,
        -8.0240e-03,  1.2042e-02,  9.4057e-02, -4.9514e-02, -2.8404e-01,
         8.3525e-02,  3.8030e-02,  1.8998e-01, -5.0094e-02,  3.1170e-01,
        -2.0165e-01,  4.6826e-02, -1.8869e-01,  4.2256e-02,  2.7622e-03,
         4.5240e-02,  0.0000e+00, -6.4334e-03,  4.8887e-02,  1.4797e-02,
         1.2648e-02,  8.0791e-03, -2.1101e-02,  1.7169e-01, -8.4205e-02,
        -3.5582e-02,  1.5490e-02,  2.9487e-01,  4.4380e-02, -4.9195e-01,
         1.4262e-02, -3.7070e-03,  0.0000e+00,  3.9846e-02,  2.1262e-02,
         0.0000e+00, -7.5691e-03,  1.4452e-01,  9.4313e-02, -2.0882e-02,
         5.2084e-03,  2.7375e-03, -1.9288e-02,  1.5374e-02,  4.8799e-02,
        -7.4174e-02, -4.1728e-03,  1.1336e-01,  3.7727e-02, -6.2218e-03,
         1.1682e-02,  1.2150e-01,  4.3690e-02, -2.0061e-01, -4.3121e-02,
         2.1884e-02, -3.1343e-02,  2.8401e-02,  6.8034e-03,  4.7003e-02,
         1.0937e-01,  7.4545e-02, -4.4829e-02,  1.7538e-02], device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-1.6922e-01,  3.0558e-02,  1.2049e-02,  3.9516e-02, -4.9969e-01,
        -3.0439e+00,  5.1505e-02, -3.2593e-02, -4.2405e-01,  8.6483e-02,
         1.9807e-01,  2.5165e-01,  7.0195e-03,  5.2228e-02,  2.0366e-01,
        -1.3184e-01, -2.2031e+00,  2.3888e-03, -2.9824e-01, -6.7781e-01,
         2.5472e-01,  7.0496e-01, -7.8939e-02, -1.3264e-03,  2.1478e-01,
         0.0000e+00,  1.1430e-02, -6.0632e-02,  9.4325e-02, -6.8987e-02,
         5.0454e-01,  4.6363e-01, -1.5180e-01, -1.3133e+00,  3.8451e-01,
        -1.7383e-01,  2.5563e-01, -7.4148e-02,  6.5031e-01,  1.4578e-02,
         3.4526e-01,  6.0933e-02,  4.4867e-01, -4.0021e-01,  2.9619e-01,
        -3.1233e+00, -7.6284e-02,  3.1036e-01, -1.6988e-02,  1.0293e-01,
        -3.4339e-01,  2.2018e-01,  3.4006e-01,  1.2156e-02, -5.3072e-02,
         1.0000e+00, -2.3795e-01,  2.9973e-01, -1.0210e+00, -1.6296e-01,
        -2.3338e-01, -5.4405e-03, -8.6858e-02,  3.8959e-02, -4.2423e+00,
        -1.9021e-01, -8.1600e-01, -2.1660e-02,  5.0904e-01,  4.2375e-01,
         1.3137e-01,  2.5041e-01, -1.6153e-02, -5.5677e-01, -1.5058e-02,
        -5.2150e-01,  9.5023e-02,  4.5953e-01, -1.8004e-01,  2.4009e-01,
        -2.8477e-01,  3.4812e-01, -4.4845e+00, -1.4654e-02,  1.9894e-02,
        -1.3805e-01, -8.9614e-01, -1.8058e-01, -2.8345e-02, -4.1761e+00,
        -4.4666e-03, -9.4953e-02, -1.1751e-01,  4.8726e-01, -8.0669e-02,
         2.5559e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.2612e-02,  5.0463e-03,  1.1843e-02,  1.1229e-02,  4.2382e-01,
        -8.5354e-02,  1.8597e-02, -9.0869e-02, -2.2220e-02,  5.3421e-02,
        -7.7980e-03,  8.1232e-02,  5.0851e-02,  4.5548e-02, -2.6792e-03,
         4.6737e-02, -1.3556e-02,  2.2028e-01, -1.6829e-01, -1.1562e-01,
         3.2501e-02,  1.5003e-02,  1.7049e-01,  1.5556e-01,  2.4546e-01,
         1.5099e-01, -6.5082e-02, -5.6902e-02, -3.1862e-02,  3.1719e-02,
         1.7966e-02,  0.0000e+00, -1.4299e-01, -1.1973e-02, -2.4082e-03,
        -2.1826e-02,  4.3584e-01,  6.0378e-02,  9.1899e-04,  3.9139e-03,
         4.3262e-02, -1.3585e-04,  4.8478e-03,  1.0729e-03,  5.0431e-02,
         2.7299e-05, -1.9866e-01,  3.2564e-02,  0.0000e+00, -1.2617e-01,
         0.0000e+00,  1.3689e-02, -1.0376e-02, -7.9788e-03, -8.1740e-02,
         1.2038e-01,  0.0000e+00, -4.4446e-02,  4.0616e-02, -7.4789e-03,
         3.9668e-03, -3.1809e-01,  1.8182e-02, -1.5534e-01, -5.3508e-03,
        -6.0983e-04, -1.3302e-01, -6.1762e-04,  1.0554e-01, -5.2302e-02,
        -1.8518e-01, -1.4657e-03, -1.6221e-01,  2.4042e-01, -1.9007e-02,
         0.0000e+00,  1.9875e-01, -2.0358e-01, -1.7460e-01,  1.1516e-01,
        -2.7575e-02,  1.8062e-02,  3.1505e-02, -7.2344e-04,  8.8336e-02,
        -6.1551e-04,  2.7940e-03, -1.6287e-01, -4.0950e-02,  1.0601e-02,
        -1.4954e-03,  3.5880e-03, -6.9619e-03,  3.6285e-03,  0.0000e+00,
        -6.8083e-02, -1.2729e-04,  6.5316e-03, -1.7149e-01, -6.2475e-03,
         4.9682e-01,  8.4390e-03,  5.9232e-03, -4.8515e-02, -1.5981e-02,
         2.1343e-01, -6.5048e-03, -6.2808e-02, -2.2987e-02,  1.2035e-01,
        -7.5936e-02,  6.3414e-03, -2.5383e-03,  0.0000e+00,  5.4144e-02,
        -5.0808e-02, -3.3747e-02,  2.1345e-02,  0.0000e+00, -6.5218e-02,
        -1.7098e-01, -4.8858e-02,  1.6343e-02,  1.5772e-01,  2.0652e-02,
        -1.2693e-01, -2.5862e-02, -9.7612e-03,  3.8284e-02,  3.4639e-03,
         3.8866e-03,  9.4673e-02,  6.9673e-02,  8.2724e-03,  7.6764e-02,
         1.6617e-02,  3.2292e-03,  3.2784e-02,  1.6169e-01,  4.2651e-02,
         7.3716e-02,  9.5524e-03,  8.0320e-02, -2.3239e-02,  7.5706e-02,
         8.8135e-03,  1.0000e+00, -4.2485e-02,  4.9043e-02, -6.3019e-04,
        -2.9946e-02, -5.8974e-02,  3.3567e-02,  1.6153e-02,  6.6085e-04,
        -5.8661e-02, -3.0275e-01, -3.2326e-01,  2.3941e-02, -1.9420e-01,
        -1.1663e-02,  4.2509e-03,  6.2793e-02, -2.8707e-01, -4.9081e-02,
         5.7958e-02, -9.3626e-02,  1.7704e-03,  4.2491e-02, -1.0071e-01,
        -2.9259e-02,  4.1614e-02,  3.5754e-02,  8.1689e-02,  9.7101e-02,
        -5.7368e-02,  4.8957e-02, -2.5140e-04,  2.7846e-02,  1.4198e-01,
        -1.2218e-01,  4.6678e-02,  2.2657e-02,  4.9386e-04, -1.5390e-02,
         2.4182e-03,  9.1135e-02,  6.1158e-03,  4.3874e-02, -1.2338e-01,
         1.7840e-01, -2.4049e-02,  5.8216e-03,  3.3235e-02,  1.2123e-02,
         5.1059e-02,  2.2466e-03,  1.3988e-01,  2.7036e-03, -8.7844e-03,
         1.3434e-01, -2.8288e-02, -1.9754e-01,  2.2197e-01, -1.7031e-02,
         3.4277e-02,  5.2277e-02, -6.9695e-04, -1.1420e-02,  1.0283e-01,
        -4.5814e-02,  2.2652e-02,  4.9007e-04, -7.3844e-02, -1.5797e-03,
         1.3247e-02,  2.0230e-02,  7.7416e-02, -1.7772e-01,  5.2795e-02,
         2.9188e-03,  9.6453e-02, -6.5041e-02, -2.7500e-02,  1.2911e-01,
        -1.5009e-01,  5.1640e-02, -1.5535e-01,  9.9318e-03,  7.3771e-02,
        -4.1252e-03,  7.2684e-05,  1.1450e-02,  1.7932e-02,  4.1558e-02,
        -6.0060e-02,  1.0553e-01, -1.0453e-04,  9.7959e-02,  5.0164e-02,
        -6.1456e-02, -1.3501e-02, -2.1230e-03, -1.0466e-01, -5.5882e-03,
        -5.2600e-02,  1.6193e-02, -2.8089e-02,  1.3411e-01, -5.8069e-03,
         1.1590e-01,  4.8417e-03, -2.3923e-02,  1.4949e-02, -1.4830e-01,
         4.7705e-02,  1.1533e-03,  8.1029e-03, -6.7280e-03,  2.4407e-03,
        -7.0826e-04, -3.5576e-03,  1.8757e-01,  2.8498e-02,  0.0000e+00,
         1.5432e-02,  0.0000e+00,  2.8411e-02, -6.4669e-04,  1.8844e-04,
        -6.6458e-02,  2.0176e-01,  1.0405e-02, -1.9453e-04,  1.4030e-01,
        -1.4373e-02,  5.0779e-02,  6.9244e-02,  6.0564e-02, -3.6282e-03,
        -2.0234e-02,  8.2206e-02,  8.9636e-02,  1.6475e-03,  1.1944e-02,
         1.5059e-01, -1.1227e-02, -7.8199e-02,  4.4929e-02, -6.2888e-02,
        -2.3575e-02, -1.6898e-01,  2.5362e-02,  2.5342e-03, -1.4375e-02,
        -1.2812e-01, -2.8508e-01, -2.1058e-02,  6.6832e-02,  4.2266e-02,
        -1.6478e-01, -1.5021e-01,  1.8009e-03, -7.5933e-02,  0.0000e+00,
         4.6073e-02,  2.1162e-01,  1.4844e-03,  3.5720e-02, -1.3482e-02,
        -1.6184e-02, -3.2782e-03,  4.6514e-02, -2.7451e-02,  4.8634e-02,
        -7.4128e-04,  1.1159e-02, -4.4139e-02, -1.0003e-02,  2.6175e-03,
        -8.0240e-03,  1.2042e-02,  9.4057e-02, -4.9514e-02, -2.8404e-01,
         8.3525e-02,  3.8030e-02,  1.8998e-01, -5.0094e-02,  3.1170e-01,
        -2.0165e-01,  4.6826e-02, -1.8869e-01,  4.2256e-02,  2.7622e-03,
         4.5240e-02,  0.0000e+00, -6.4334e-03,  4.8887e-02,  1.4797e-02,
         1.2648e-02,  8.0791e-03, -2.1101e-02,  1.7169e-01, -8.4205e-02,
        -3.5582e-02,  1.5490e-02,  2.9487e-01,  4.4380e-02, -4.9195e-01,
         1.4262e-02, -3.7070e-03,  0.0000e+00,  3.9846e-02,  2.1262e-02,
         0.0000e+00, -7.5691e-03,  1.4452e-01,  9.4313e-02, -2.0882e-02,
         5.2084e-03,  2.7375e-03, -1.9288e-02,  1.5374e-02,  4.8799e-02,
        -7.4174e-02, -4.1728e-03,  1.1336e-01,  3.7727e-02, -6.2218e-03,
         1.1682e-02,  1.2150e-01,  4.3690e-02, -2.0061e-01, -4.3121e-02,
         2.1884e-02, -3.1343e-02,  2.8401e-02,  6.8034e-03,  4.7003e-02,
         1.0937e-01,  7.4545e-02, -4.4829e-02,  1.7538e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0028, -0.0071,  0.0568,  ...,  0.0337,  0.0000,  0.0398],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0093, -0.0104, -0.0531,  ..., -0.0193, -0.0318, -0.0834],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([-0.0032, -0.0780,  0.0386,  ...,  0.1096,  0.0361, -0.0318],
       device='cuda:0')}
topk_layer inside model after retrieval:  [50, 75, 56, 206, 354, 336, 365, 94, 118, 180, 345, 41, 191, 382, 140, 317, 104, 121, 352, 324, 92, 99, 264, 190, 304, 233, 105, 100, 117, 88, 202, 67, 144, 357, 26, 355, 326, 163, 130, 137, 146, 62, 36, 6, 312, 68, 217, 219, 39, 35, 296, 349, 197, 134, 171, 131, 33, 193, 292]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-1.6922e-01,  3.0558e-02,  1.2049e-02,  3.9516e-02, -4.9969e-01,
        -3.0439e+00,  5.1505e-02, -3.2593e-02, -4.2405e-01,  8.6483e-02,
         1.9807e-01,  2.5165e-01,  7.0195e-03,  5.2228e-02,  2.0366e-01,
        -1.3184e-01, -2.2031e+00,  2.3888e-03, -2.9824e-01, -6.7781e-01,
         2.5472e-01,  7.0496e-01, -7.8939e-02, -1.3264e-03,  2.1478e-01,
         0.0000e+00,  1.1430e-02, -6.0632e-02,  9.4325e-02, -6.8987e-02,
         5.0454e-01,  4.6363e-01, -1.5180e-01, -1.3133e+00,  3.8451e-01,
        -1.7383e-01,  2.5563e-01, -7.4148e-02,  6.5031e-01,  1.4578e-02,
         3.4526e-01,  6.0933e-02,  4.4867e-01, -4.0021e-01,  2.9619e-01,
        -3.1233e+00, -7.6284e-02,  3.1036e-01, -1.6988e-02,  1.0293e-01,
        -3.4339e-01,  2.2018e-01,  3.4006e-01,  1.2156e-02, -5.3072e-02,
         1.0000e+00, -2.3795e-01,  2.9973e-01, -1.0210e+00, -1.6296e-01,
        -2.3338e-01, -5.4405e-03, -8.6858e-02,  3.8959e-02, -4.2423e+00,
        -1.9021e-01, -8.1600e-01, -2.1660e-02,  5.0904e-01,  4.2375e-01,
         1.3137e-01,  2.5041e-01, -1.6153e-02, -5.5677e-01, -1.5058e-02,
        -5.2150e-01,  9.5023e-02,  4.5953e-01, -1.8004e-01,  2.4009e-01,
        -2.8477e-01,  3.4812e-01, -4.4845e+00, -1.4654e-02,  1.9894e-02,
        -1.3805e-01, -8.9614e-01, -1.8058e-01, -2.8345e-02, -4.1761e+00,
        -4.4666e-03, -9.4953e-02, -1.1751e-01,  4.8726e-01, -8.0669e-02,
         2.5559e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.2612e-02,  5.0463e-03,  1.1843e-02,  1.1229e-02,  4.2382e-01,
        -8.5354e-02,  1.8597e-02, -9.0869e-02, -2.2220e-02,  5.3421e-02,
        -7.7980e-03,  8.1232e-02,  5.0851e-02,  4.5548e-02, -2.6792e-03,
         4.6737e-02, -1.3556e-02,  2.2028e-01, -1.6829e-01, -1.1562e-01,
         3.2501e-02,  1.5003e-02,  1.7049e-01,  1.5556e-01,  2.4546e-01,
         1.5099e-01, -6.5082e-02, -5.6902e-02, -3.1862e-02,  3.1719e-02,
         1.7966e-02,  0.0000e+00, -1.4299e-01, -1.1973e-02, -2.4082e-03,
        -2.1826e-02,  4.3584e-01,  6.0378e-02,  9.1899e-04,  3.9139e-03,
         4.3262e-02, -1.3585e-04,  4.8478e-03,  1.0729e-03,  5.0431e-02,
         2.7299e-05, -1.9866e-01,  3.2564e-02,  0.0000e+00, -1.2617e-01,
         0.0000e+00,  1.3689e-02, -1.0376e-02, -7.9788e-03, -8.1740e-02,
         1.2038e-01,  0.0000e+00, -4.4446e-02,  4.0616e-02, -7.4789e-03,
         3.9668e-03, -3.1809e-01,  1.8182e-02, -1.5534e-01, -5.3508e-03,
        -6.0983e-04, -1.3302e-01, -6.1762e-04,  1.0554e-01, -5.2302e-02,
        -1.8518e-01, -1.4657e-03, -1.6221e-01,  2.4042e-01, -1.9007e-02,
         0.0000e+00,  1.9875e-01, -2.0358e-01, -1.7460e-01,  1.1516e-01,
        -2.7575e-02,  1.8062e-02,  3.1505e-02, -7.2344e-04,  8.8336e-02,
        -6.1551e-04,  2.7940e-03, -1.6287e-01, -4.0950e-02,  1.0601e-02,
        -1.4954e-03,  3.5880e-03, -6.9619e-03,  3.6285e-03,  0.0000e+00,
        -6.8083e-02, -1.2729e-04,  6.5316e-03, -1.7149e-01, -6.2475e-03,
         4.9682e-01,  8.4390e-03,  5.9232e-03, -4.8515e-02, -1.5981e-02,
         2.1343e-01, -6.5048e-03, -6.2808e-02, -2.2987e-02,  1.2035e-01,
        -7.5936e-02,  6.3414e-03, -2.5383e-03,  0.0000e+00,  5.4144e-02,
        -5.0808e-02, -3.3747e-02,  2.1345e-02,  0.0000e+00, -6.5218e-02,
        -1.7098e-01, -4.8858e-02,  1.6343e-02,  1.5772e-01,  2.0652e-02,
        -1.2693e-01, -2.5862e-02, -9.7612e-03,  3.8284e-02,  3.4639e-03,
         3.8866e-03,  9.4673e-02,  6.9673e-02,  8.2724e-03,  7.6764e-02,
         1.6617e-02,  3.2292e-03,  3.2784e-02,  1.6169e-01,  4.2651e-02,
         7.3716e-02,  9.5524e-03,  8.0320e-02, -2.3239e-02,  7.5706e-02,
         8.8135e-03,  1.0000e+00, -4.2485e-02,  4.9043e-02, -6.3019e-04,
        -2.9946e-02, -5.8974e-02,  3.3567e-02,  1.6153e-02,  6.6085e-04,
        -5.8661e-02, -3.0275e-01, -3.2326e-01,  2.3941e-02, -1.9420e-01,
        -1.1663e-02,  4.2509e-03,  6.2793e-02, -2.8707e-01, -4.9081e-02,
         5.7958e-02, -9.3626e-02,  1.7704e-03,  4.2491e-02, -1.0071e-01,
        -2.9259e-02,  4.1614e-02,  3.5754e-02,  8.1689e-02,  9.7101e-02,
        -5.7368e-02,  4.8957e-02, -2.5140e-04,  2.7846e-02,  1.4198e-01,
        -1.2218e-01,  4.6678e-02,  2.2657e-02,  4.9386e-04, -1.5390e-02,
         2.4182e-03,  9.1135e-02,  6.1158e-03,  4.3874e-02, -1.2338e-01,
         1.7840e-01, -2.4049e-02,  5.8216e-03,  3.3235e-02,  1.2123e-02,
         5.1059e-02,  2.2466e-03,  1.3988e-01,  2.7036e-03, -8.7844e-03,
         1.3434e-01, -2.8288e-02, -1.9754e-01,  2.2197e-01, -1.7031e-02,
         3.4277e-02,  5.2277e-02, -6.9695e-04, -1.1420e-02,  1.0283e-01,
        -4.5814e-02,  2.2652e-02,  4.9007e-04, -7.3844e-02, -1.5797e-03,
         1.3247e-02,  2.0230e-02,  7.7416e-02, -1.7772e-01,  5.2795e-02,
         2.9188e-03,  9.6453e-02, -6.5041e-02, -2.7500e-02,  1.2911e-01,
        -1.5009e-01,  5.1640e-02, -1.5535e-01,  9.9318e-03,  7.3771e-02,
        -4.1252e-03,  7.2684e-05,  1.1450e-02,  1.7932e-02,  4.1558e-02,
        -6.0060e-02,  1.0553e-01, -1.0453e-04,  9.7959e-02,  5.0164e-02,
        -6.1456e-02, -1.3501e-02, -2.1230e-03, -1.0466e-01, -5.5882e-03,
        -5.2600e-02,  1.6193e-02, -2.8089e-02,  1.3411e-01, -5.8069e-03,
         1.1590e-01,  4.8417e-03, -2.3923e-02,  1.4949e-02, -1.4830e-01,
         4.7705e-02,  1.1533e-03,  8.1029e-03, -6.7280e-03,  2.4407e-03,
        -7.0826e-04, -3.5576e-03,  1.8757e-01,  2.8498e-02,  0.0000e+00,
         1.5432e-02,  0.0000e+00,  2.8411e-02, -6.4669e-04,  1.8844e-04,
        -6.6458e-02,  2.0176e-01,  1.0405e-02, -1.9453e-04,  1.4030e-01,
        -1.4373e-02,  5.0779e-02,  6.9244e-02,  6.0564e-02, -3.6282e-03,
        -2.0234e-02,  8.2206e-02,  8.9636e-02,  1.6475e-03,  1.1944e-02,
         1.5059e-01, -1.1227e-02, -7.8199e-02,  4.4929e-02, -6.2888e-02,
        -2.3575e-02, -1.6898e-01,  2.5362e-02,  2.5342e-03, -1.4375e-02,
        -1.2812e-01, -2.8508e-01, -2.1058e-02,  6.6832e-02,  4.2266e-02,
        -1.6478e-01, -1.5021e-01,  1.8009e-03, -7.5933e-02,  0.0000e+00,
         4.6073e-02,  2.1162e-01,  1.4844e-03,  3.5720e-02, -1.3482e-02,
        -1.6184e-02, -3.2782e-03,  4.6514e-02, -2.7451e-02,  4.8634e-02,
        -7.4128e-04,  1.1159e-02, -4.4139e-02, -1.0003e-02,  2.6175e-03,
        -8.0240e-03,  1.2042e-02,  9.4057e-02, -4.9514e-02, -2.8404e-01,
         8.3525e-02,  3.8030e-02,  1.8998e-01, -5.0094e-02,  3.1170e-01,
        -2.0165e-01,  4.6826e-02, -1.8869e-01,  4.2256e-02,  2.7622e-03,
         4.5240e-02,  0.0000e+00, -6.4334e-03,  4.8887e-02,  1.4797e-02,
         1.2648e-02,  8.0791e-03, -2.1101e-02,  1.7169e-01, -8.4205e-02,
        -3.5582e-02,  1.5490e-02,  2.9487e-01,  4.4380e-02, -4.9195e-01,
         1.4262e-02, -3.7070e-03,  0.0000e+00,  3.9846e-02,  2.1262e-02,
         0.0000e+00, -7.5691e-03,  1.4452e-01,  9.4313e-02, -2.0882e-02,
         5.2084e-03,  2.7375e-03, -1.9288e-02,  1.5374e-02,  4.8799e-02,
        -7.4174e-02, -4.1728e-03,  1.1336e-01,  3.7727e-02, -6.2218e-03,
         1.1682e-02,  1.2150e-01,  4.3690e-02, -2.0061e-01, -4.3121e-02,
         2.1884e-02, -3.1343e-02,  2.8401e-02,  6.8034e-03,  4.7003e-02,
         1.0937e-01,  7.4545e-02, -4.4829e-02,  1.7538e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0028, -0.0071,  0.0568,  ...,  0.0337,  0.0000,  0.0398],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0093, -0.0104, -0.0531,  ..., -0.0193, -0.0318, -0.0834],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([-0.0032, -0.0780,  0.0386,  ...,  0.1096,  0.0361, -0.0318],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.2.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 0.0028, -0.0071,  0.0568,  ...,  0.0337,  0.0000,  0.0398],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-1.6922e-01,  3.0558e-02,  1.2049e-02,  3.9516e-02, -4.9969e-01,
        -3.0439e+00,  5.1505e-02, -3.2593e-02, -4.2405e-01,  8.6483e-02,
         1.9807e-01,  2.5165e-01,  7.0195e-03,  5.2228e-02,  2.0366e-01,
        -1.3184e-01, -2.2031e+00,  2.3888e-03, -2.9824e-01, -6.7781e-01,
         2.5472e-01,  7.0496e-01, -7.8939e-02, -1.3264e-03,  2.1478e-01,
         0.0000e+00,  1.1430e-02, -6.0632e-02,  9.4325e-02, -6.8987e-02,
         5.0454e-01,  4.6363e-01, -1.5180e-01, -1.3133e+00,  3.8451e-01,
        -1.7383e-01,  2.5563e-01, -7.4148e-02,  6.5031e-01,  1.4578e-02,
         3.4526e-01,  6.0933e-02,  4.4867e-01, -4.0021e-01,  2.9619e-01,
        -3.1233e+00, -7.6284e-02,  3.1036e-01, -1.6988e-02,  1.0293e-01,
        -3.4339e-01,  2.2018e-01,  3.4006e-01,  1.2156e-02, -5.3072e-02,
         1.0000e+00, -2.3795e-01,  2.9973e-01, -1.0210e+00, -1.6296e-01,
        -2.3338e-01, -5.4405e-03, -8.6858e-02,  3.8959e-02, -4.2423e+00,
        -1.9021e-01, -8.1600e-01, -2.1660e-02,  5.0904e-01,  4.2375e-01,
         1.3137e-01,  2.5041e-01, -1.6153e-02, -5.5677e-01, -1.5058e-02,
        -5.2150e-01,  9.5023e-02,  4.5953e-01, -1.8004e-01,  2.4009e-01,
        -2.8477e-01,  3.4812e-01, -4.4845e+00, -1.4654e-02,  1.9894e-02,
        -1.3805e-01, -8.9614e-01, -1.8058e-01, -2.8345e-02, -4.1761e+00,
        -4.4666e-03, -9.4953e-02, -1.1751e-01,  4.8726e-01, -8.0669e-02,
         2.5559e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.2612e-02,  5.0463e-03,  1.1843e-02,  1.1229e-02,  4.2382e-01,
        -8.5354e-02,  1.8597e-02, -9.0869e-02, -2.2220e-02,  5.3421e-02,
        -7.7980e-03,  8.1232e-02,  5.0851e-02,  4.5548e-02, -2.6792e-03,
         4.6737e-02, -1.3556e-02,  2.2028e-01, -1.6829e-01, -1.1562e-01,
         3.2501e-02,  1.5003e-02,  1.7049e-01,  1.5556e-01,  2.4546e-01,
         1.5099e-01, -6.5082e-02, -5.6902e-02, -3.1862e-02,  3.1719e-02,
         1.7966e-02,  0.0000e+00, -1.4299e-01, -1.1973e-02, -2.4082e-03,
        -2.1826e-02,  4.3584e-01,  6.0378e-02,  9.1899e-04,  3.9139e-03,
         4.3262e-02, -1.3585e-04,  4.8478e-03,  1.0729e-03,  5.0431e-02,
         2.7299e-05, -1.9866e-01,  3.2564e-02,  0.0000e+00, -1.2617e-01,
         0.0000e+00,  1.3689e-02, -1.0376e-02, -7.9788e-03, -8.1740e-02,
         1.2038e-01,  0.0000e+00, -4.4446e-02,  4.0616e-02, -7.4789e-03,
         3.9668e-03, -3.1809e-01,  1.8182e-02, -1.5534e-01, -5.3508e-03,
        -6.0983e-04, -1.3302e-01, -6.1762e-04,  1.0554e-01, -5.2302e-02,
        -1.8518e-01, -1.4657e-03, -1.6221e-01,  2.4042e-01, -1.9007e-02,
         0.0000e+00,  1.9875e-01, -2.0358e-01, -1.7460e-01,  1.1516e-01,
        -2.7575e-02,  1.8062e-02,  3.1505e-02, -7.2344e-04,  8.8336e-02,
        -6.1551e-04,  2.7940e-03, -1.6287e-01, -4.0950e-02,  1.0601e-02,
        -1.4954e-03,  3.5880e-03, -6.9619e-03,  3.6285e-03,  0.0000e+00,
        -6.8083e-02, -1.2729e-04,  6.5316e-03, -1.7149e-01, -6.2475e-03,
         4.9682e-01,  8.4390e-03,  5.9232e-03, -4.8515e-02, -1.5981e-02,
         2.1343e-01, -6.5048e-03, -6.2808e-02, -2.2987e-02,  1.2035e-01,
        -7.5936e-02,  6.3414e-03, -2.5383e-03,  0.0000e+00,  5.4144e-02,
        -5.0808e-02, -3.3747e-02,  2.1345e-02,  0.0000e+00, -6.5218e-02,
        -1.7098e-01, -4.8858e-02,  1.6343e-02,  1.5772e-01,  2.0652e-02,
        -1.2693e-01, -2.5862e-02, -9.7612e-03,  3.8284e-02,  3.4639e-03,
         3.8866e-03,  9.4673e-02,  6.9673e-02,  8.2724e-03,  7.6764e-02,
         1.6617e-02,  3.2292e-03,  3.2784e-02,  1.6169e-01,  4.2651e-02,
         7.3716e-02,  9.5524e-03,  8.0320e-02, -2.3239e-02,  7.5706e-02,
         8.8135e-03,  1.0000e+00, -4.2485e-02,  4.9043e-02, -6.3019e-04,
        -2.9946e-02, -5.8974e-02,  3.3567e-02,  1.6153e-02,  6.6085e-04,
        -5.8661e-02, -3.0275e-01, -3.2326e-01,  2.3941e-02, -1.9420e-01,
        -1.1663e-02,  4.2509e-03,  6.2793e-02, -2.8707e-01, -4.9081e-02,
         5.7958e-02, -9.3626e-02,  1.7704e-03,  4.2491e-02, -1.0071e-01,
        -2.9259e-02,  4.1614e-02,  3.5754e-02,  8.1689e-02,  9.7101e-02,
        -5.7368e-02,  4.8957e-02, -2.5140e-04,  2.7846e-02,  1.4198e-01,
        -1.2218e-01,  4.6678e-02,  2.2657e-02,  4.9386e-04, -1.5390e-02,
         2.4182e-03,  9.1135e-02,  6.1158e-03,  4.3874e-02, -1.2338e-01,
         1.7840e-01, -2.4049e-02,  5.8216e-03,  3.3235e-02,  1.2123e-02,
         5.1059e-02,  2.2466e-03,  1.3988e-01,  2.7036e-03, -8.7844e-03,
         1.3434e-01, -2.8288e-02, -1.9754e-01,  2.2197e-01, -1.7031e-02,
         3.4277e-02,  5.2277e-02, -6.9695e-04, -1.1420e-02,  1.0283e-01,
        -4.5814e-02,  2.2652e-02,  4.9007e-04, -7.3844e-02, -1.5797e-03,
         1.3247e-02,  2.0230e-02,  7.7416e-02, -1.7772e-01,  5.2795e-02,
         2.9188e-03,  9.6453e-02, -6.5041e-02, -2.7500e-02,  1.2911e-01,
        -1.5009e-01,  5.1640e-02, -1.5535e-01,  9.9318e-03,  7.3771e-02,
        -4.1252e-03,  7.2684e-05,  1.1450e-02,  1.7932e-02,  4.1558e-02,
        -6.0060e-02,  1.0553e-01, -1.0453e-04,  9.7959e-02,  5.0164e-02,
        -6.1456e-02, -1.3501e-02, -2.1230e-03, -1.0466e-01, -5.5882e-03,
        -5.2600e-02,  1.6193e-02, -2.8089e-02,  1.3411e-01, -5.8069e-03,
         1.1590e-01,  4.8417e-03, -2.3923e-02,  1.4949e-02, -1.4830e-01,
         4.7705e-02,  1.1533e-03,  8.1029e-03, -6.7280e-03,  2.4407e-03,
        -7.0826e-04, -3.5576e-03,  1.8757e-01,  2.8498e-02,  0.0000e+00,
         1.5432e-02,  0.0000e+00,  2.8411e-02, -6.4669e-04,  1.8844e-04,
        -6.6458e-02,  2.0176e-01,  1.0405e-02, -1.9453e-04,  1.4030e-01,
        -1.4373e-02,  5.0779e-02,  6.9244e-02,  6.0564e-02, -3.6282e-03,
        -2.0234e-02,  8.2206e-02,  8.9636e-02,  1.6475e-03,  1.1944e-02,
         1.5059e-01, -1.1227e-02, -7.8199e-02,  4.4929e-02, -6.2888e-02,
        -2.3575e-02, -1.6898e-01,  2.5362e-02,  2.5342e-03, -1.4375e-02,
        -1.2812e-01, -2.8508e-01, -2.1058e-02,  6.6832e-02,  4.2266e-02,
        -1.6478e-01, -1.5021e-01,  1.8009e-03, -7.5933e-02,  0.0000e+00,
         4.6073e-02,  2.1162e-01,  1.4844e-03,  3.5720e-02, -1.3482e-02,
        -1.6184e-02, -3.2782e-03,  4.6514e-02, -2.7451e-02,  4.8634e-02,
        -7.4128e-04,  1.1159e-02, -4.4139e-02, -1.0003e-02,  2.6175e-03,
        -8.0240e-03,  1.2042e-02,  9.4057e-02, -4.9514e-02, -2.8404e-01,
         8.3525e-02,  3.8030e-02,  1.8998e-01, -5.0094e-02,  3.1170e-01,
        -2.0165e-01,  4.6826e-02, -1.8869e-01,  4.2256e-02,  2.7622e-03,
         4.5240e-02,  0.0000e+00, -6.4334e-03,  4.8887e-02,  1.4797e-02,
         1.2648e-02,  8.0791e-03, -2.1101e-02,  1.7169e-01, -8.4205e-02,
        -3.5582e-02,  1.5490e-02,  2.9487e-01,  4.4380e-02, -4.9195e-01,
         1.4262e-02, -3.7070e-03,  0.0000e+00,  3.9846e-02,  2.1262e-02,
         0.0000e+00, -7.5691e-03,  1.4452e-01,  9.4313e-02, -2.0882e-02,
         5.2084e-03,  2.7375e-03, -1.9288e-02,  1.5374e-02,  4.8799e-02,
        -7.4174e-02, -4.1728e-03,  1.1336e-01,  3.7727e-02, -6.2218e-03,
         1.1682e-02,  1.2150e-01,  4.3690e-02, -2.0061e-01, -4.3121e-02,
         2.1884e-02, -3.1343e-02,  2.8401e-02,  6.8034e-03,  4.7003e-02,
         1.0937e-01,  7.4545e-02, -4.4829e-02,  1.7538e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0028, -0.0071,  0.0568,  ...,  0.0337,  0.0000,  0.0398],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0093, -0.0104, -0.0531,  ..., -0.0193, -0.0318, -0.0834],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([-0.0032, -0.0780,  0.0386,  ...,  0.1096,  0.0361, -0.0318],
       device='cuda:0')}
topk_layer inside model after retrieval:  [761, 958, 937, 307, 176, 195, 1317, 622, 95, 856, 1486, 731, 1385, 544, 1436, 1297, 612, 636, 1274, 311, 673, 407, 1294, 490, 348, 700, 1278, 421, 1171, 450, 35, 921, 626, 405, 250, 1281, 733, 353, 14, 499, 93, 1292, 312, 395, 380, 427, 1046, 905, 208, 1235, 1452, 1086, 1006, 61, 799, 457, 932, 869, 56, 1307, 1034, 851, 484, 981, 548, 1123, 855, 12, 1360, 639, 359, 1508, 209, 580, 1159, 1442, 743, 1404, 635, 1236, 1412, 1345, 577, 144, 66, 361, 1291, 1359, 276, 796, 1534, 1450, 1061, 866, 1306, 783, 900, 289, 123, 846, 1203, 730, 838, 818, 164, 1388, 1125, 1193, 1227, 875, 1100, 881, 105, 520, 67, 287, 1049, 1113, 574, 1105, 1083, 1000, 683, 1071, 667, 1521, 1286, 1126, 11, 243, 916, 449, 618, 698, 1466, 735, 1272, 952, 1405, 896, 229, 670, 294, 794, 1232, 542, 460, 198, 924, 850, 141, 746, 614, 1371, 1400, 773, 1242, 810, 268, 1172, 973, 155, 534, 167, 1421, 524, 1480, 711, 754, 1434, 454, 1535, 246, 911, 109, 379, 560, 681, 1500, 434, 906, 341, 692, 261, 994, 726, 942, 801, 302, 728, 598, 1030, 1233, 100, 1441, 1265, 852, 1238, 408, 991, 805, 368, 217, 687, 1008, 1116, 1464, 854, 1386, 313, 1372, 523, 1230, 935, 423, 812, 329, 1033, 199, 1325, 1516, 309, 1180, 1314, 15, 1472, 1361, 1455, 461, 640, 1489]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-1.6922e-01,  3.0558e-02,  1.2049e-02,  3.9516e-02, -4.9969e-01,
        -3.0439e+00,  5.1505e-02, -3.2593e-02, -4.2405e-01,  8.6483e-02,
         1.9807e-01,  2.5165e-01,  7.0195e-03,  5.2228e-02,  2.0366e-01,
        -1.3184e-01, -2.2031e+00,  2.3888e-03, -2.9824e-01, -6.7781e-01,
         2.5472e-01,  7.0496e-01, -7.8939e-02, -1.3264e-03,  2.1478e-01,
         0.0000e+00,  1.1430e-02, -6.0632e-02,  9.4325e-02, -6.8987e-02,
         5.0454e-01,  4.6363e-01, -1.5180e-01, -1.3133e+00,  3.8451e-01,
        -1.7383e-01,  2.5563e-01, -7.4148e-02,  6.5031e-01,  1.4578e-02,
         3.4526e-01,  6.0933e-02,  4.4867e-01, -4.0021e-01,  2.9619e-01,
        -3.1233e+00, -7.6284e-02,  3.1036e-01, -1.6988e-02,  1.0293e-01,
        -3.4339e-01,  2.2018e-01,  3.4006e-01,  1.2156e-02, -5.3072e-02,
         1.0000e+00, -2.3795e-01,  2.9973e-01, -1.0210e+00, -1.6296e-01,
        -2.3338e-01, -5.4405e-03, -8.6858e-02,  3.8959e-02, -4.2423e+00,
        -1.9021e-01, -8.1600e-01, -2.1660e-02,  5.0904e-01,  4.2375e-01,
         1.3137e-01,  2.5041e-01, -1.6153e-02, -5.5677e-01, -1.5058e-02,
        -5.2150e-01,  9.5023e-02,  4.5953e-01, -1.8004e-01,  2.4009e-01,
        -2.8477e-01,  3.4812e-01, -4.4845e+00, -1.4654e-02,  1.9894e-02,
        -1.3805e-01, -8.9614e-01, -1.8058e-01, -2.8345e-02, -4.1761e+00,
        -4.4666e-03, -9.4953e-02, -1.1751e-01,  4.8726e-01, -8.0669e-02,
         2.5559e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.2612e-02,  5.0463e-03,  1.1843e-02,  1.1229e-02,  4.2382e-01,
        -8.5354e-02,  1.8597e-02, -9.0869e-02, -2.2220e-02,  5.3421e-02,
        -7.7980e-03,  8.1232e-02,  5.0851e-02,  4.5548e-02, -2.6792e-03,
         4.6737e-02, -1.3556e-02,  2.2028e-01, -1.6829e-01, -1.1562e-01,
         3.2501e-02,  1.5003e-02,  1.7049e-01,  1.5556e-01,  2.4546e-01,
         1.5099e-01, -6.5082e-02, -5.6902e-02, -3.1862e-02,  3.1719e-02,
         1.7966e-02,  0.0000e+00, -1.4299e-01, -1.1973e-02, -2.4082e-03,
        -2.1826e-02,  4.3584e-01,  6.0378e-02,  9.1899e-04,  3.9139e-03,
         4.3262e-02, -1.3585e-04,  4.8478e-03,  1.0729e-03,  5.0431e-02,
         2.7299e-05, -1.9866e-01,  3.2564e-02,  0.0000e+00, -1.2617e-01,
         0.0000e+00,  1.3689e-02, -1.0376e-02, -7.9788e-03, -8.1740e-02,
         1.2038e-01,  0.0000e+00, -4.4446e-02,  4.0616e-02, -7.4789e-03,
         3.9668e-03, -3.1809e-01,  1.8182e-02, -1.5534e-01, -5.3508e-03,
        -6.0983e-04, -1.3302e-01, -6.1762e-04,  1.0554e-01, -5.2302e-02,
        -1.8518e-01, -1.4657e-03, -1.6221e-01,  2.4042e-01, -1.9007e-02,
         0.0000e+00,  1.9875e-01, -2.0358e-01, -1.7460e-01,  1.1516e-01,
        -2.7575e-02,  1.8062e-02,  3.1505e-02, -7.2344e-04,  8.8336e-02,
        -6.1551e-04,  2.7940e-03, -1.6287e-01, -4.0950e-02,  1.0601e-02,
        -1.4954e-03,  3.5880e-03, -6.9619e-03,  3.6285e-03,  0.0000e+00,
        -6.8083e-02, -1.2729e-04,  6.5316e-03, -1.7149e-01, -6.2475e-03,
         4.9682e-01,  8.4390e-03,  5.9232e-03, -4.8515e-02, -1.5981e-02,
         2.1343e-01, -6.5048e-03, -6.2808e-02, -2.2987e-02,  1.2035e-01,
        -7.5936e-02,  6.3414e-03, -2.5383e-03,  0.0000e+00,  5.4144e-02,
        -5.0808e-02, -3.3747e-02,  2.1345e-02,  0.0000e+00, -6.5218e-02,
        -1.7098e-01, -4.8858e-02,  1.6343e-02,  1.5772e-01,  2.0652e-02,
        -1.2693e-01, -2.5862e-02, -9.7612e-03,  3.8284e-02,  3.4639e-03,
         3.8866e-03,  9.4673e-02,  6.9673e-02,  8.2724e-03,  7.6764e-02,
         1.6617e-02,  3.2292e-03,  3.2784e-02,  1.6169e-01,  4.2651e-02,
         7.3716e-02,  9.5524e-03,  8.0320e-02, -2.3239e-02,  7.5706e-02,
         8.8135e-03,  1.0000e+00, -4.2485e-02,  4.9043e-02, -6.3019e-04,
        -2.9946e-02, -5.8974e-02,  3.3567e-02,  1.6153e-02,  6.6085e-04,
        -5.8661e-02, -3.0275e-01, -3.2326e-01,  2.3941e-02, -1.9420e-01,
        -1.1663e-02,  4.2509e-03,  6.2793e-02, -2.8707e-01, -4.9081e-02,
         5.7958e-02, -9.3626e-02,  1.7704e-03,  4.2491e-02, -1.0071e-01,
        -2.9259e-02,  4.1614e-02,  3.5754e-02,  8.1689e-02,  9.7101e-02,
        -5.7368e-02,  4.8957e-02, -2.5140e-04,  2.7846e-02,  1.4198e-01,
        -1.2218e-01,  4.6678e-02,  2.2657e-02,  4.9386e-04, -1.5390e-02,
         2.4182e-03,  9.1135e-02,  6.1158e-03,  4.3874e-02, -1.2338e-01,
         1.7840e-01, -2.4049e-02,  5.8216e-03,  3.3235e-02,  1.2123e-02,
         5.1059e-02,  2.2466e-03,  1.3988e-01,  2.7036e-03, -8.7844e-03,
         1.3434e-01, -2.8288e-02, -1.9754e-01,  2.2197e-01, -1.7031e-02,
         3.4277e-02,  5.2277e-02, -6.9695e-04, -1.1420e-02,  1.0283e-01,
        -4.5814e-02,  2.2652e-02,  4.9007e-04, -7.3844e-02, -1.5797e-03,
         1.3247e-02,  2.0230e-02,  7.7416e-02, -1.7772e-01,  5.2795e-02,
         2.9188e-03,  9.6453e-02, -6.5041e-02, -2.7500e-02,  1.2911e-01,
        -1.5009e-01,  5.1640e-02, -1.5535e-01,  9.9318e-03,  7.3771e-02,
        -4.1252e-03,  7.2684e-05,  1.1450e-02,  1.7932e-02,  4.1558e-02,
        -6.0060e-02,  1.0553e-01, -1.0453e-04,  9.7959e-02,  5.0164e-02,
        -6.1456e-02, -1.3501e-02, -2.1230e-03, -1.0466e-01, -5.5882e-03,
        -5.2600e-02,  1.6193e-02, -2.8089e-02,  1.3411e-01, -5.8069e-03,
         1.1590e-01,  4.8417e-03, -2.3923e-02,  1.4949e-02, -1.4830e-01,
         4.7705e-02,  1.1533e-03,  8.1029e-03, -6.7280e-03,  2.4407e-03,
        -7.0826e-04, -3.5576e-03,  1.8757e-01,  2.8498e-02,  0.0000e+00,
         1.5432e-02,  0.0000e+00,  2.8411e-02, -6.4669e-04,  1.8844e-04,
        -6.6458e-02,  2.0176e-01,  1.0405e-02, -1.9453e-04,  1.4030e-01,
        -1.4373e-02,  5.0779e-02,  6.9244e-02,  6.0564e-02, -3.6282e-03,
        -2.0234e-02,  8.2206e-02,  8.9636e-02,  1.6475e-03,  1.1944e-02,
         1.5059e-01, -1.1227e-02, -7.8199e-02,  4.4929e-02, -6.2888e-02,
        -2.3575e-02, -1.6898e-01,  2.5362e-02,  2.5342e-03, -1.4375e-02,
        -1.2812e-01, -2.8508e-01, -2.1058e-02,  6.6832e-02,  4.2266e-02,
        -1.6478e-01, -1.5021e-01,  1.8009e-03, -7.5933e-02,  0.0000e+00,
         4.6073e-02,  2.1162e-01,  1.4844e-03,  3.5720e-02, -1.3482e-02,
        -1.6184e-02, -3.2782e-03,  4.6514e-02, -2.7451e-02,  4.8634e-02,
        -7.4128e-04,  1.1159e-02, -4.4139e-02, -1.0003e-02,  2.6175e-03,
        -8.0240e-03,  1.2042e-02,  9.4057e-02, -4.9514e-02, -2.8404e-01,
         8.3525e-02,  3.8030e-02,  1.8998e-01, -5.0094e-02,  3.1170e-01,
        -2.0165e-01,  4.6826e-02, -1.8869e-01,  4.2256e-02,  2.7622e-03,
         4.5240e-02,  0.0000e+00, -6.4334e-03,  4.8887e-02,  1.4797e-02,
         1.2648e-02,  8.0791e-03, -2.1101e-02,  1.7169e-01, -8.4205e-02,
        -3.5582e-02,  1.5490e-02,  2.9487e-01,  4.4380e-02, -4.9195e-01,
         1.4262e-02, -3.7070e-03,  0.0000e+00,  3.9846e-02,  2.1262e-02,
         0.0000e+00, -7.5691e-03,  1.4452e-01,  9.4313e-02, -2.0882e-02,
         5.2084e-03,  2.7375e-03, -1.9288e-02,  1.5374e-02,  4.8799e-02,
        -7.4174e-02, -4.1728e-03,  1.1336e-01,  3.7727e-02, -6.2218e-03,
         1.1682e-02,  1.2150e-01,  4.3690e-02, -2.0061e-01, -4.3121e-02,
         2.1884e-02, -3.1343e-02,  2.8401e-02,  6.8034e-03,  4.7003e-02,
         1.0937e-01,  7.4545e-02, -4.4829e-02,  1.7538e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0028, -0.0071,  0.0568,  ...,  0.0337,  0.0000,  0.0398],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0093, -0.0104, -0.0531,  ..., -0.0193, -0.0318, -0.0834],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([-0.0032, -0.0780,  0.0386,  ...,  0.1096,  0.0361, -0.0318],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.3.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 0.0093, -0.0104, -0.0531,  ..., -0.0193, -0.0318, -0.0834],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-1.6922e-01,  3.0558e-02,  1.2049e-02,  3.9516e-02, -4.9969e-01,
        -3.0439e+00,  5.1505e-02, -3.2593e-02, -4.2405e-01,  8.6483e-02,
         1.9807e-01,  2.5165e-01,  7.0195e-03,  5.2228e-02,  2.0366e-01,
        -1.3184e-01, -2.2031e+00,  2.3888e-03, -2.9824e-01, -6.7781e-01,
         2.5472e-01,  7.0496e-01, -7.8939e-02, -1.3264e-03,  2.1478e-01,
         0.0000e+00,  1.1430e-02, -6.0632e-02,  9.4325e-02, -6.8987e-02,
         5.0454e-01,  4.6363e-01, -1.5180e-01, -1.3133e+00,  3.8451e-01,
        -1.7383e-01,  2.5563e-01, -7.4148e-02,  6.5031e-01,  1.4578e-02,
         3.4526e-01,  6.0933e-02,  4.4867e-01, -4.0021e-01,  2.9619e-01,
        -3.1233e+00, -7.6284e-02,  3.1036e-01, -1.6988e-02,  1.0293e-01,
        -3.4339e-01,  2.2018e-01,  3.4006e-01,  1.2156e-02, -5.3072e-02,
         1.0000e+00, -2.3795e-01,  2.9973e-01, -1.0210e+00, -1.6296e-01,
        -2.3338e-01, -5.4405e-03, -8.6858e-02,  3.8959e-02, -4.2423e+00,
        -1.9021e-01, -8.1600e-01, -2.1660e-02,  5.0904e-01,  4.2375e-01,
         1.3137e-01,  2.5041e-01, -1.6153e-02, -5.5677e-01, -1.5058e-02,
        -5.2150e-01,  9.5023e-02,  4.5953e-01, -1.8004e-01,  2.4009e-01,
        -2.8477e-01,  3.4812e-01, -4.4845e+00, -1.4654e-02,  1.9894e-02,
        -1.3805e-01, -8.9614e-01, -1.8058e-01, -2.8345e-02, -4.1761e+00,
        -4.4666e-03, -9.4953e-02, -1.1751e-01,  4.8726e-01, -8.0669e-02,
         2.5559e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.2612e-02,  5.0463e-03,  1.1843e-02,  1.1229e-02,  4.2382e-01,
        -8.5354e-02,  1.8597e-02, -9.0869e-02, -2.2220e-02,  5.3421e-02,
        -7.7980e-03,  8.1232e-02,  5.0851e-02,  4.5548e-02, -2.6792e-03,
         4.6737e-02, -1.3556e-02,  2.2028e-01, -1.6829e-01, -1.1562e-01,
         3.2501e-02,  1.5003e-02,  1.7049e-01,  1.5556e-01,  2.4546e-01,
         1.5099e-01, -6.5082e-02, -5.6902e-02, -3.1862e-02,  3.1719e-02,
         1.7966e-02,  0.0000e+00, -1.4299e-01, -1.1973e-02, -2.4082e-03,
        -2.1826e-02,  4.3584e-01,  6.0378e-02,  9.1899e-04,  3.9139e-03,
         4.3262e-02, -1.3585e-04,  4.8478e-03,  1.0729e-03,  5.0431e-02,
         2.7299e-05, -1.9866e-01,  3.2564e-02,  0.0000e+00, -1.2617e-01,
         0.0000e+00,  1.3689e-02, -1.0376e-02, -7.9788e-03, -8.1740e-02,
         1.2038e-01,  0.0000e+00, -4.4446e-02,  4.0616e-02, -7.4789e-03,
         3.9668e-03, -3.1809e-01,  1.8182e-02, -1.5534e-01, -5.3508e-03,
        -6.0983e-04, -1.3302e-01, -6.1762e-04,  1.0554e-01, -5.2302e-02,
        -1.8518e-01, -1.4657e-03, -1.6221e-01,  2.4042e-01, -1.9007e-02,
         0.0000e+00,  1.9875e-01, -2.0358e-01, -1.7460e-01,  1.1516e-01,
        -2.7575e-02,  1.8062e-02,  3.1505e-02, -7.2344e-04,  8.8336e-02,
        -6.1551e-04,  2.7940e-03, -1.6287e-01, -4.0950e-02,  1.0601e-02,
        -1.4954e-03,  3.5880e-03, -6.9619e-03,  3.6285e-03,  0.0000e+00,
        -6.8083e-02, -1.2729e-04,  6.5316e-03, -1.7149e-01, -6.2475e-03,
         4.9682e-01,  8.4390e-03,  5.9232e-03, -4.8515e-02, -1.5981e-02,
         2.1343e-01, -6.5048e-03, -6.2808e-02, -2.2987e-02,  1.2035e-01,
        -7.5936e-02,  6.3414e-03, -2.5383e-03,  0.0000e+00,  5.4144e-02,
        -5.0808e-02, -3.3747e-02,  2.1345e-02,  0.0000e+00, -6.5218e-02,
        -1.7098e-01, -4.8858e-02,  1.6343e-02,  1.5772e-01,  2.0652e-02,
        -1.2693e-01, -2.5862e-02, -9.7612e-03,  3.8284e-02,  3.4639e-03,
         3.8866e-03,  9.4673e-02,  6.9673e-02,  8.2724e-03,  7.6764e-02,
         1.6617e-02,  3.2292e-03,  3.2784e-02,  1.6169e-01,  4.2651e-02,
         7.3716e-02,  9.5524e-03,  8.0320e-02, -2.3239e-02,  7.5706e-02,
         8.8135e-03,  1.0000e+00, -4.2485e-02,  4.9043e-02, -6.3019e-04,
        -2.9946e-02, -5.8974e-02,  3.3567e-02,  1.6153e-02,  6.6085e-04,
        -5.8661e-02, -3.0275e-01, -3.2326e-01,  2.3941e-02, -1.9420e-01,
        -1.1663e-02,  4.2509e-03,  6.2793e-02, -2.8707e-01, -4.9081e-02,
         5.7958e-02, -9.3626e-02,  1.7704e-03,  4.2491e-02, -1.0071e-01,
        -2.9259e-02,  4.1614e-02,  3.5754e-02,  8.1689e-02,  9.7101e-02,
        -5.7368e-02,  4.8957e-02, -2.5140e-04,  2.7846e-02,  1.4198e-01,
        -1.2218e-01,  4.6678e-02,  2.2657e-02,  4.9386e-04, -1.5390e-02,
         2.4182e-03,  9.1135e-02,  6.1158e-03,  4.3874e-02, -1.2338e-01,
         1.7840e-01, -2.4049e-02,  5.8216e-03,  3.3235e-02,  1.2123e-02,
         5.1059e-02,  2.2466e-03,  1.3988e-01,  2.7036e-03, -8.7844e-03,
         1.3434e-01, -2.8288e-02, -1.9754e-01,  2.2197e-01, -1.7031e-02,
         3.4277e-02,  5.2277e-02, -6.9695e-04, -1.1420e-02,  1.0283e-01,
        -4.5814e-02,  2.2652e-02,  4.9007e-04, -7.3844e-02, -1.5797e-03,
         1.3247e-02,  2.0230e-02,  7.7416e-02, -1.7772e-01,  5.2795e-02,
         2.9188e-03,  9.6453e-02, -6.5041e-02, -2.7500e-02,  1.2911e-01,
        -1.5009e-01,  5.1640e-02, -1.5535e-01,  9.9318e-03,  7.3771e-02,
        -4.1252e-03,  7.2684e-05,  1.1450e-02,  1.7932e-02,  4.1558e-02,
        -6.0060e-02,  1.0553e-01, -1.0453e-04,  9.7959e-02,  5.0164e-02,
        -6.1456e-02, -1.3501e-02, -2.1230e-03, -1.0466e-01, -5.5882e-03,
        -5.2600e-02,  1.6193e-02, -2.8089e-02,  1.3411e-01, -5.8069e-03,
         1.1590e-01,  4.8417e-03, -2.3923e-02,  1.4949e-02, -1.4830e-01,
         4.7705e-02,  1.1533e-03,  8.1029e-03, -6.7280e-03,  2.4407e-03,
        -7.0826e-04, -3.5576e-03,  1.8757e-01,  2.8498e-02,  0.0000e+00,
         1.5432e-02,  0.0000e+00,  2.8411e-02, -6.4669e-04,  1.8844e-04,
        -6.6458e-02,  2.0176e-01,  1.0405e-02, -1.9453e-04,  1.4030e-01,
        -1.4373e-02,  5.0779e-02,  6.9244e-02,  6.0564e-02, -3.6282e-03,
        -2.0234e-02,  8.2206e-02,  8.9636e-02,  1.6475e-03,  1.1944e-02,
         1.5059e-01, -1.1227e-02, -7.8199e-02,  4.4929e-02, -6.2888e-02,
        -2.3575e-02, -1.6898e-01,  2.5362e-02,  2.5342e-03, -1.4375e-02,
        -1.2812e-01, -2.8508e-01, -2.1058e-02,  6.6832e-02,  4.2266e-02,
        -1.6478e-01, -1.5021e-01,  1.8009e-03, -7.5933e-02,  0.0000e+00,
         4.6073e-02,  2.1162e-01,  1.4844e-03,  3.5720e-02, -1.3482e-02,
        -1.6184e-02, -3.2782e-03,  4.6514e-02, -2.7451e-02,  4.8634e-02,
        -7.4128e-04,  1.1159e-02, -4.4139e-02, -1.0003e-02,  2.6175e-03,
        -8.0240e-03,  1.2042e-02,  9.4057e-02, -4.9514e-02, -2.8404e-01,
         8.3525e-02,  3.8030e-02,  1.8998e-01, -5.0094e-02,  3.1170e-01,
        -2.0165e-01,  4.6826e-02, -1.8869e-01,  4.2256e-02,  2.7622e-03,
         4.5240e-02,  0.0000e+00, -6.4334e-03,  4.8887e-02,  1.4797e-02,
         1.2648e-02,  8.0791e-03, -2.1101e-02,  1.7169e-01, -8.4205e-02,
        -3.5582e-02,  1.5490e-02,  2.9487e-01,  4.4380e-02, -4.9195e-01,
         1.4262e-02, -3.7070e-03,  0.0000e+00,  3.9846e-02,  2.1262e-02,
         0.0000e+00, -7.5691e-03,  1.4452e-01,  9.4313e-02, -2.0882e-02,
         5.2084e-03,  2.7375e-03, -1.9288e-02,  1.5374e-02,  4.8799e-02,
        -7.4174e-02, -4.1728e-03,  1.1336e-01,  3.7727e-02, -6.2218e-03,
         1.1682e-02,  1.2150e-01,  4.3690e-02, -2.0061e-01, -4.3121e-02,
         2.1884e-02, -3.1343e-02,  2.8401e-02,  6.8034e-03,  4.7003e-02,
         1.0937e-01,  7.4545e-02, -4.4829e-02,  1.7538e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0028, -0.0071,  0.0568,  ...,  0.0337,  0.0000,  0.0398],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0093, -0.0104, -0.0531,  ..., -0.0193, -0.0318, -0.0834],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([-0.0032, -0.0780,  0.0386,  ...,  0.1096,  0.0361, -0.0318],
       device='cuda:0')}
topk_layer inside model after retrieval:  [499, 1755, 1447, 505, 1394, 507, 1567, 1720, 653, 2500, 861, 2300, 398, 421, 689, 2309, 1762, 1554, 1137, 2460, 2154, 1348, 1843, 1506, 28, 1403, 1873, 340, 652, 2448, 589, 1783, 1326, 1127, 878, 648, 2106, 468, 2203, 2181, 2285, 2340, 43, 2062, 1943, 1894, 2185, 938, 1831, 828, 1617, 1536, 419, 1500, 1179, 1558, 1226, 512, 2317, 1100, 533, 80, 935, 1723, 1162, 1845, 758, 481, 1887, 2000, 1938, 1440, 792, 2241, 2282, 1867, 646, 2502, 926, 2518, 1599, 1359, 284, 540, 555, 1187, 1375, 2515, 390, 1406, 1433, 1054, 1619, 607, 1457, 1968, 1405, 971, 225, 2008, 746, 1789, 1206, 1438, 96, 593, 2072, 2086, 1164, 2117, 1125, 810, 1339, 2523, 1608, 1713, 2095, 1794, 2494, 1080, 526, 2391, 425, 1638, 1868, 446, 1597, 819, 520, 2, 1561, 2112, 2223, 2078, 1616, 1555, 177, 564, 1088, 194, 1533, 1942, 1253, 103, 2545, 2162, 1109, 354, 1218, 1275, 1448, 2254, 1380, 1698, 1217, 2114, 339, 781, 2441, 1280, 151, 1324, 2037, 1677, 775, 1964, 280, 2081, 1872, 36, 724, 81, 509, 669, 1061, 1759, 1781, 2151, 193, 277, 2456, 1644, 761, 2442, 231, 1341, 2315, 1770, 2453, 1647, 2183, 907, 1248, 2474, 1516, 362, 1492, 1449, 454, 240, 1995, 1874, 393, 559, 508, 2271, 472, 624, 1543, 2170, 470, 687, 2511, 2110, 1319, 2559, 1363, 884, 546, 291, 1424, 1097, 990, 1958, 2367, 2479, 145, 45, 1656, 1548, 2060, 943, 1684, 2084, 129, 1776, 1824, 2050, 613, 1613, 2001, 252, 780, 1153, 655, 818, 867, 1436, 1689, 2270, 1651, 1992, 6, 1385, 1346, 2328, 1170, 1788, 2067, 2134, 450, 1038, 1694, 962, 934, 697, 1132, 2283, 615, 1409, 1439, 37, 1510, 1799, 2525, 484, 29, 480, 611, 1150, 195, 1283, 236, 2305, 1664, 272, 313, 2465, 632, 1303, 683, 1194, 665, 202, 660, 365, 674, 730, 737, 1497, 1465, 2352, 63, 1926, 2028, 1646, 548, 2555, 1378, 172, 720, 1443, 980, 2323, 2482, 299, 1660, 1360, 557, 921, 40, 939, 1862, 2098, 1267, 995, 832, 1, 1393, 423, 1808, 1495, 399, 356, 1446, 662, 487, 1224, 2013, 1469, 663, 2090, 1225, 1924, 409, 1340, 2103, 2302, 1830, 588, 2357, 2093, 2003, 2033, 477, 2046, 394, 1266, 616, 1758, 2188, 1119, 370, 360, 2295, 297, 1415, 1292, 1499, 991, 1381, 2414, 11, 2031, 1659, 1998, 1670, 431, 1844, 1431, 598, 1370, 2532, 2011, 1996]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-1.6922e-01,  3.0558e-02,  1.2049e-02,  3.9516e-02, -4.9969e-01,
        -3.0439e+00,  5.1505e-02, -3.2593e-02, -4.2405e-01,  8.6483e-02,
         1.9807e-01,  2.5165e-01,  7.0195e-03,  5.2228e-02,  2.0366e-01,
        -1.3184e-01, -2.2031e+00,  2.3888e-03, -2.9824e-01, -6.7781e-01,
         2.5472e-01,  7.0496e-01, -7.8939e-02, -1.3264e-03,  2.1478e-01,
         0.0000e+00,  1.1430e-02, -6.0632e-02,  9.4325e-02, -6.8987e-02,
         5.0454e-01,  4.6363e-01, -1.5180e-01, -1.3133e+00,  3.8451e-01,
        -1.7383e-01,  2.5563e-01, -7.4148e-02,  6.5031e-01,  1.4578e-02,
         3.4526e-01,  6.0933e-02,  4.4867e-01, -4.0021e-01,  2.9619e-01,
        -3.1233e+00, -7.6284e-02,  3.1036e-01, -1.6988e-02,  1.0293e-01,
        -3.4339e-01,  2.2018e-01,  3.4006e-01,  1.2156e-02, -5.3072e-02,
         1.0000e+00, -2.3795e-01,  2.9973e-01, -1.0210e+00, -1.6296e-01,
        -2.3338e-01, -5.4405e-03, -8.6858e-02,  3.8959e-02, -4.2423e+00,
        -1.9021e-01, -8.1600e-01, -2.1660e-02,  5.0904e-01,  4.2375e-01,
         1.3137e-01,  2.5041e-01, -1.6153e-02, -5.5677e-01, -1.5058e-02,
        -5.2150e-01,  9.5023e-02,  4.5953e-01, -1.8004e-01,  2.4009e-01,
        -2.8477e-01,  3.4812e-01, -4.4845e+00, -1.4654e-02,  1.9894e-02,
        -1.3805e-01, -8.9614e-01, -1.8058e-01, -2.8345e-02, -4.1761e+00,
        -4.4666e-03, -9.4953e-02, -1.1751e-01,  4.8726e-01, -8.0669e-02,
         2.5559e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.2612e-02,  5.0463e-03,  1.1843e-02,  1.1229e-02,  4.2382e-01,
        -8.5354e-02,  1.8597e-02, -9.0869e-02, -2.2220e-02,  5.3421e-02,
        -7.7980e-03,  8.1232e-02,  5.0851e-02,  4.5548e-02, -2.6792e-03,
         4.6737e-02, -1.3556e-02,  2.2028e-01, -1.6829e-01, -1.1562e-01,
         3.2501e-02,  1.5003e-02,  1.7049e-01,  1.5556e-01,  2.4546e-01,
         1.5099e-01, -6.5082e-02, -5.6902e-02, -3.1862e-02,  3.1719e-02,
         1.7966e-02,  0.0000e+00, -1.4299e-01, -1.1973e-02, -2.4082e-03,
        -2.1826e-02,  4.3584e-01,  6.0378e-02,  9.1899e-04,  3.9139e-03,
         4.3262e-02, -1.3585e-04,  4.8478e-03,  1.0729e-03,  5.0431e-02,
         2.7299e-05, -1.9866e-01,  3.2564e-02,  0.0000e+00, -1.2617e-01,
         0.0000e+00,  1.3689e-02, -1.0376e-02, -7.9788e-03, -8.1740e-02,
         1.2038e-01,  0.0000e+00, -4.4446e-02,  4.0616e-02, -7.4789e-03,
         3.9668e-03, -3.1809e-01,  1.8182e-02, -1.5534e-01, -5.3508e-03,
        -6.0983e-04, -1.3302e-01, -6.1762e-04,  1.0554e-01, -5.2302e-02,
        -1.8518e-01, -1.4657e-03, -1.6221e-01,  2.4042e-01, -1.9007e-02,
         0.0000e+00,  1.9875e-01, -2.0358e-01, -1.7460e-01,  1.1516e-01,
        -2.7575e-02,  1.8062e-02,  3.1505e-02, -7.2344e-04,  8.8336e-02,
        -6.1551e-04,  2.7940e-03, -1.6287e-01, -4.0950e-02,  1.0601e-02,
        -1.4954e-03,  3.5880e-03, -6.9619e-03,  3.6285e-03,  0.0000e+00,
        -6.8083e-02, -1.2729e-04,  6.5316e-03, -1.7149e-01, -6.2475e-03,
         4.9682e-01,  8.4390e-03,  5.9232e-03, -4.8515e-02, -1.5981e-02,
         2.1343e-01, -6.5048e-03, -6.2808e-02, -2.2987e-02,  1.2035e-01,
        -7.5936e-02,  6.3414e-03, -2.5383e-03,  0.0000e+00,  5.4144e-02,
        -5.0808e-02, -3.3747e-02,  2.1345e-02,  0.0000e+00, -6.5218e-02,
        -1.7098e-01, -4.8858e-02,  1.6343e-02,  1.5772e-01,  2.0652e-02,
        -1.2693e-01, -2.5862e-02, -9.7612e-03,  3.8284e-02,  3.4639e-03,
         3.8866e-03,  9.4673e-02,  6.9673e-02,  8.2724e-03,  7.6764e-02,
         1.6617e-02,  3.2292e-03,  3.2784e-02,  1.6169e-01,  4.2651e-02,
         7.3716e-02,  9.5524e-03,  8.0320e-02, -2.3239e-02,  7.5706e-02,
         8.8135e-03,  1.0000e+00, -4.2485e-02,  4.9043e-02, -6.3019e-04,
        -2.9946e-02, -5.8974e-02,  3.3567e-02,  1.6153e-02,  6.6085e-04,
        -5.8661e-02, -3.0275e-01, -3.2326e-01,  2.3941e-02, -1.9420e-01,
        -1.1663e-02,  4.2509e-03,  6.2793e-02, -2.8707e-01, -4.9081e-02,
         5.7958e-02, -9.3626e-02,  1.7704e-03,  4.2491e-02, -1.0071e-01,
        -2.9259e-02,  4.1614e-02,  3.5754e-02,  8.1689e-02,  9.7101e-02,
        -5.7368e-02,  4.8957e-02, -2.5140e-04,  2.7846e-02,  1.4198e-01,
        -1.2218e-01,  4.6678e-02,  2.2657e-02,  4.9386e-04, -1.5390e-02,
         2.4182e-03,  9.1135e-02,  6.1158e-03,  4.3874e-02, -1.2338e-01,
         1.7840e-01, -2.4049e-02,  5.8216e-03,  3.3235e-02,  1.2123e-02,
         5.1059e-02,  2.2466e-03,  1.3988e-01,  2.7036e-03, -8.7844e-03,
         1.3434e-01, -2.8288e-02, -1.9754e-01,  2.2197e-01, -1.7031e-02,
         3.4277e-02,  5.2277e-02, -6.9695e-04, -1.1420e-02,  1.0283e-01,
        -4.5814e-02,  2.2652e-02,  4.9007e-04, -7.3844e-02, -1.5797e-03,
         1.3247e-02,  2.0230e-02,  7.7416e-02, -1.7772e-01,  5.2795e-02,
         2.9188e-03,  9.6453e-02, -6.5041e-02, -2.7500e-02,  1.2911e-01,
        -1.5009e-01,  5.1640e-02, -1.5535e-01,  9.9318e-03,  7.3771e-02,
        -4.1252e-03,  7.2684e-05,  1.1450e-02,  1.7932e-02,  4.1558e-02,
        -6.0060e-02,  1.0553e-01, -1.0453e-04,  9.7959e-02,  5.0164e-02,
        -6.1456e-02, -1.3501e-02, -2.1230e-03, -1.0466e-01, -5.5882e-03,
        -5.2600e-02,  1.6193e-02, -2.8089e-02,  1.3411e-01, -5.8069e-03,
         1.1590e-01,  4.8417e-03, -2.3923e-02,  1.4949e-02, -1.4830e-01,
         4.7705e-02,  1.1533e-03,  8.1029e-03, -6.7280e-03,  2.4407e-03,
        -7.0826e-04, -3.5576e-03,  1.8757e-01,  2.8498e-02,  0.0000e+00,
         1.5432e-02,  0.0000e+00,  2.8411e-02, -6.4669e-04,  1.8844e-04,
        -6.6458e-02,  2.0176e-01,  1.0405e-02, -1.9453e-04,  1.4030e-01,
        -1.4373e-02,  5.0779e-02,  6.9244e-02,  6.0564e-02, -3.6282e-03,
        -2.0234e-02,  8.2206e-02,  8.9636e-02,  1.6475e-03,  1.1944e-02,
         1.5059e-01, -1.1227e-02, -7.8199e-02,  4.4929e-02, -6.2888e-02,
        -2.3575e-02, -1.6898e-01,  2.5362e-02,  2.5342e-03, -1.4375e-02,
        -1.2812e-01, -2.8508e-01, -2.1058e-02,  6.6832e-02,  4.2266e-02,
        -1.6478e-01, -1.5021e-01,  1.8009e-03, -7.5933e-02,  0.0000e+00,
         4.6073e-02,  2.1162e-01,  1.4844e-03,  3.5720e-02, -1.3482e-02,
        -1.6184e-02, -3.2782e-03,  4.6514e-02, -2.7451e-02,  4.8634e-02,
        -7.4128e-04,  1.1159e-02, -4.4139e-02, -1.0003e-02,  2.6175e-03,
        -8.0240e-03,  1.2042e-02,  9.4057e-02, -4.9514e-02, -2.8404e-01,
         8.3525e-02,  3.8030e-02,  1.8998e-01, -5.0094e-02,  3.1170e-01,
        -2.0165e-01,  4.6826e-02, -1.8869e-01,  4.2256e-02,  2.7622e-03,
         4.5240e-02,  0.0000e+00, -6.4334e-03,  4.8887e-02,  1.4797e-02,
         1.2648e-02,  8.0791e-03, -2.1101e-02,  1.7169e-01, -8.4205e-02,
        -3.5582e-02,  1.5490e-02,  2.9487e-01,  4.4380e-02, -4.9195e-01,
         1.4262e-02, -3.7070e-03,  0.0000e+00,  3.9846e-02,  2.1262e-02,
         0.0000e+00, -7.5691e-03,  1.4452e-01,  9.4313e-02, -2.0882e-02,
         5.2084e-03,  2.7375e-03, -1.9288e-02,  1.5374e-02,  4.8799e-02,
        -7.4174e-02, -4.1728e-03,  1.1336e-01,  3.7727e-02, -6.2218e-03,
         1.1682e-02,  1.2150e-01,  4.3690e-02, -2.0061e-01, -4.3121e-02,
         2.1884e-02, -3.1343e-02,  2.8401e-02,  6.8034e-03,  4.7003e-02,
         1.0937e-01,  7.4545e-02, -4.4829e-02,  1.7538e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0028, -0.0071,  0.0568,  ...,  0.0337,  0.0000,  0.0398],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0093, -0.0104, -0.0531,  ..., -0.0193, -0.0318, -0.0834],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([-0.0032, -0.0780,  0.0386,  ...,  0.1096,  0.0361, -0.0318],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.4.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([-0.0032, -0.0780,  0.0386,  ...,  0.1096,  0.0361, -0.0318],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-1.6922e-01,  3.0558e-02,  1.2049e-02,  3.9516e-02, -4.9969e-01,
        -3.0439e+00,  5.1505e-02, -3.2593e-02, -4.2405e-01,  8.6483e-02,
         1.9807e-01,  2.5165e-01,  7.0195e-03,  5.2228e-02,  2.0366e-01,
        -1.3184e-01, -2.2031e+00,  2.3888e-03, -2.9824e-01, -6.7781e-01,
         2.5472e-01,  7.0496e-01, -7.8939e-02, -1.3264e-03,  2.1478e-01,
         0.0000e+00,  1.1430e-02, -6.0632e-02,  9.4325e-02, -6.8987e-02,
         5.0454e-01,  4.6363e-01, -1.5180e-01, -1.3133e+00,  3.8451e-01,
        -1.7383e-01,  2.5563e-01, -7.4148e-02,  6.5031e-01,  1.4578e-02,
         3.4526e-01,  6.0933e-02,  4.4867e-01, -4.0021e-01,  2.9619e-01,
        -3.1233e+00, -7.6284e-02,  3.1036e-01, -1.6988e-02,  1.0293e-01,
        -3.4339e-01,  2.2018e-01,  3.4006e-01,  1.2156e-02, -5.3072e-02,
         1.0000e+00, -2.3795e-01,  2.9973e-01, -1.0210e+00, -1.6296e-01,
        -2.3338e-01, -5.4405e-03, -8.6858e-02,  3.8959e-02, -4.2423e+00,
        -1.9021e-01, -8.1600e-01, -2.1660e-02,  5.0904e-01,  4.2375e-01,
         1.3137e-01,  2.5041e-01, -1.6153e-02, -5.5677e-01, -1.5058e-02,
        -5.2150e-01,  9.5023e-02,  4.5953e-01, -1.8004e-01,  2.4009e-01,
        -2.8477e-01,  3.4812e-01, -4.4845e+00, -1.4654e-02,  1.9894e-02,
        -1.3805e-01, -8.9614e-01, -1.8058e-01, -2.8345e-02, -4.1761e+00,
        -4.4666e-03, -9.4953e-02, -1.1751e-01,  4.8726e-01, -8.0669e-02,
         2.5559e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 3.2612e-02,  5.0463e-03,  1.1843e-02,  1.1229e-02,  4.2382e-01,
        -8.5354e-02,  1.8597e-02, -9.0869e-02, -2.2220e-02,  5.3421e-02,
        -7.7980e-03,  8.1232e-02,  5.0851e-02,  4.5548e-02, -2.6792e-03,
         4.6737e-02, -1.3556e-02,  2.2028e-01, -1.6829e-01, -1.1562e-01,
         3.2501e-02,  1.5003e-02,  1.7049e-01,  1.5556e-01,  2.4546e-01,
         1.5099e-01, -6.5082e-02, -5.6902e-02, -3.1862e-02,  3.1719e-02,
         1.7966e-02,  0.0000e+00, -1.4299e-01, -1.1973e-02, -2.4082e-03,
        -2.1826e-02,  4.3584e-01,  6.0378e-02,  9.1899e-04,  3.9139e-03,
         4.3262e-02, -1.3585e-04,  4.8478e-03,  1.0729e-03,  5.0431e-02,
         2.7299e-05, -1.9866e-01,  3.2564e-02,  0.0000e+00, -1.2617e-01,
         0.0000e+00,  1.3689e-02, -1.0376e-02, -7.9788e-03, -8.1740e-02,
         1.2038e-01,  0.0000e+00, -4.4446e-02,  4.0616e-02, -7.4789e-03,
         3.9668e-03, -3.1809e-01,  1.8182e-02, -1.5534e-01, -5.3508e-03,
        -6.0983e-04, -1.3302e-01, -6.1762e-04,  1.0554e-01, -5.2302e-02,
        -1.8518e-01, -1.4657e-03, -1.6221e-01,  2.4042e-01, -1.9007e-02,
         0.0000e+00,  1.9875e-01, -2.0358e-01, -1.7460e-01,  1.1516e-01,
        -2.7575e-02,  1.8062e-02,  3.1505e-02, -7.2344e-04,  8.8336e-02,
        -6.1551e-04,  2.7940e-03, -1.6287e-01, -4.0950e-02,  1.0601e-02,
        -1.4954e-03,  3.5880e-03, -6.9619e-03,  3.6285e-03,  0.0000e+00,
        -6.8083e-02, -1.2729e-04,  6.5316e-03, -1.7149e-01, -6.2475e-03,
         4.9682e-01,  8.4390e-03,  5.9232e-03, -4.8515e-02, -1.5981e-02,
         2.1343e-01, -6.5048e-03, -6.2808e-02, -2.2987e-02,  1.2035e-01,
        -7.5936e-02,  6.3414e-03, -2.5383e-03,  0.0000e+00,  5.4144e-02,
        -5.0808e-02, -3.3747e-02,  2.1345e-02,  0.0000e+00, -6.5218e-02,
        -1.7098e-01, -4.8858e-02,  1.6343e-02,  1.5772e-01,  2.0652e-02,
        -1.2693e-01, -2.5862e-02, -9.7612e-03,  3.8284e-02,  3.4639e-03,
         3.8866e-03,  9.4673e-02,  6.9673e-02,  8.2724e-03,  7.6764e-02,
         1.6617e-02,  3.2292e-03,  3.2784e-02,  1.6169e-01,  4.2651e-02,
         7.3716e-02,  9.5524e-03,  8.0320e-02, -2.3239e-02,  7.5706e-02,
         8.8135e-03,  1.0000e+00, -4.2485e-02,  4.9043e-02, -6.3019e-04,
        -2.9946e-02, -5.8974e-02,  3.3567e-02,  1.6153e-02,  6.6085e-04,
        -5.8661e-02, -3.0275e-01, -3.2326e-01,  2.3941e-02, -1.9420e-01,
        -1.1663e-02,  4.2509e-03,  6.2793e-02, -2.8707e-01, -4.9081e-02,
         5.7958e-02, -9.3626e-02,  1.7704e-03,  4.2491e-02, -1.0071e-01,
        -2.9259e-02,  4.1614e-02,  3.5754e-02,  8.1689e-02,  9.7101e-02,
        -5.7368e-02,  4.8957e-02, -2.5140e-04,  2.7846e-02,  1.4198e-01,
        -1.2218e-01,  4.6678e-02,  2.2657e-02,  4.9386e-04, -1.5390e-02,
         2.4182e-03,  9.1135e-02,  6.1158e-03,  4.3874e-02, -1.2338e-01,
         1.7840e-01, -2.4049e-02,  5.8216e-03,  3.3235e-02,  1.2123e-02,
         5.1059e-02,  2.2466e-03,  1.3988e-01,  2.7036e-03, -8.7844e-03,
         1.3434e-01, -2.8288e-02, -1.9754e-01,  2.2197e-01, -1.7031e-02,
         3.4277e-02,  5.2277e-02, -6.9695e-04, -1.1420e-02,  1.0283e-01,
        -4.5814e-02,  2.2652e-02,  4.9007e-04, -7.3844e-02, -1.5797e-03,
         1.3247e-02,  2.0230e-02,  7.7416e-02, -1.7772e-01,  5.2795e-02,
         2.9188e-03,  9.6453e-02, -6.5041e-02, -2.7500e-02,  1.2911e-01,
        -1.5009e-01,  5.1640e-02, -1.5535e-01,  9.9318e-03,  7.3771e-02,
        -4.1252e-03,  7.2684e-05,  1.1450e-02,  1.7932e-02,  4.1558e-02,
        -6.0060e-02,  1.0553e-01, -1.0453e-04,  9.7959e-02,  5.0164e-02,
        -6.1456e-02, -1.3501e-02, -2.1230e-03, -1.0466e-01, -5.5882e-03,
        -5.2600e-02,  1.6193e-02, -2.8089e-02,  1.3411e-01, -5.8069e-03,
         1.1590e-01,  4.8417e-03, -2.3923e-02,  1.4949e-02, -1.4830e-01,
         4.7705e-02,  1.1533e-03,  8.1029e-03, -6.7280e-03,  2.4407e-03,
        -7.0826e-04, -3.5576e-03,  1.8757e-01,  2.8498e-02,  0.0000e+00,
         1.5432e-02,  0.0000e+00,  2.8411e-02, -6.4669e-04,  1.8844e-04,
        -6.6458e-02,  2.0176e-01,  1.0405e-02, -1.9453e-04,  1.4030e-01,
        -1.4373e-02,  5.0779e-02,  6.9244e-02,  6.0564e-02, -3.6282e-03,
        -2.0234e-02,  8.2206e-02,  8.9636e-02,  1.6475e-03,  1.1944e-02,
         1.5059e-01, -1.1227e-02, -7.8199e-02,  4.4929e-02, -6.2888e-02,
        -2.3575e-02, -1.6898e-01,  2.5362e-02,  2.5342e-03, -1.4375e-02,
        -1.2812e-01, -2.8508e-01, -2.1058e-02,  6.6832e-02,  4.2266e-02,
        -1.6478e-01, -1.5021e-01,  1.8009e-03, -7.5933e-02,  0.0000e+00,
         4.6073e-02,  2.1162e-01,  1.4844e-03,  3.5720e-02, -1.3482e-02,
        -1.6184e-02, -3.2782e-03,  4.6514e-02, -2.7451e-02,  4.8634e-02,
        -7.4128e-04,  1.1159e-02, -4.4139e-02, -1.0003e-02,  2.6175e-03,
        -8.0240e-03,  1.2042e-02,  9.4057e-02, -4.9514e-02, -2.8404e-01,
         8.3525e-02,  3.8030e-02,  1.8998e-01, -5.0094e-02,  3.1170e-01,
        -2.0165e-01,  4.6826e-02, -1.8869e-01,  4.2256e-02,  2.7622e-03,
         4.5240e-02,  0.0000e+00, -6.4334e-03,  4.8887e-02,  1.4797e-02,
         1.2648e-02,  8.0791e-03, -2.1101e-02,  1.7169e-01, -8.4205e-02,
        -3.5582e-02,  1.5490e-02,  2.9487e-01,  4.4380e-02, -4.9195e-01,
         1.4262e-02, -3.7070e-03,  0.0000e+00,  3.9846e-02,  2.1262e-02,
         0.0000e+00, -7.5691e-03,  1.4452e-01,  9.4313e-02, -2.0882e-02,
         5.2084e-03,  2.7375e-03, -1.9288e-02,  1.5374e-02,  4.8799e-02,
        -7.4174e-02, -4.1728e-03,  1.1336e-01,  3.7727e-02, -6.2218e-03,
         1.1682e-02,  1.2150e-01,  4.3690e-02, -2.0061e-01, -4.3121e-02,
         2.1884e-02, -3.1343e-02,  2.8401e-02,  6.8034e-03,  4.7003e-02,
         1.0937e-01,  7.4545e-02, -4.4829e-02,  1.7538e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0028, -0.0071,  0.0568,  ...,  0.0337,  0.0000,  0.0398],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([ 0.0093, -0.0104, -0.0531,  ..., -0.0193, -0.0318, -0.0834],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([-0.0032, -0.0780,  0.0386,  ...,  0.1096,  0.0361, -0.0318],
       device='cuda:0')}
topk_layer inside model after retrieval:  [1323, 433, 929, 593, 448, 671, 2211, 2271, 2518, 2285, 2385, 1620, 2268, 397, 1029, 1250, 1887, 2121, 1473, 966, 881, 1898, 499, 99, 1026, 1867, 2214, 91, 783, 546, 1572, 1629, 997, 514, 188, 784, 1784, 2038, 13, 1148, 2158, 624, 1996, 2046, 1989, 615, 969, 1167, 2039, 2356, 1908, 1149, 2395, 2519, 1654, 1376, 2528, 1741, 754, 603, 910, 673, 1519, 161, 1364, 139, 674, 608, 507, 372, 808, 1657, 1610, 2258, 1019, 1925, 2308, 1285, 1498, 2388, 1090, 2548, 1764, 721, 6, 2537, 137, 2015, 1356, 1822, 1131, 2461, 1791, 1708, 2276, 1617, 1067, 1730, 1267, 411, 2224, 348, 2253, 1712, 21, 696, 1261, 2006, 1522, 1942, 1485, 1706, 1064, 610, 1772, 1100, 218, 1710, 1007, 551, 1739, 1153, 1348, 1622, 567, 437, 3, 82, 1797, 1539, 633, 1990, 2012, 1366, 1184, 605, 1284, 1524, 978, 725, 183, 1830, 468, 2423, 1412, 67, 398, 607, 193, 2022, 1727, 34, 113, 774, 258, 1575, 788, 2307, 1471, 2551, 2320, 1218, 2326, 1360, 1777, 1424, 2476, 489, 1832, 1418, 1490, 1135, 2265, 843, 412, 1237, 2209, 1425, 1492, 2366, 1223, 391, 1929, 2145, 2210, 1169, 2557, 530, 2212, 29, 1789, 535, 761, 772, 147, 1095, 381, 426, 1583, 627, 1483, 1746, 2064, 865, 1246, 2327, 1513, 1724, 1897, 1819, 2227, 935, 2438, 990, 2558, 1193, 687, 1139, 1221, 926, 589, 945, 2355, 2544, 2321, 586, 1280, 1817, 2176, 494, 2471, 959, 1310, 340, 2135, 1774, 704, 413, 1080, 347, 181, 415, 702, 712, 1521, 2120, 366, 993, 1511, 1058, 1025, 2543, 1390, 1641, 24, 152, 312, 1801, 1795, 2552, 2415, 880, 1719, 2345, 1416, 28, 1375, 1904, 1354, 270, 2482, 2237, 1692, 1686, 1656, 2436, 1387, 806, 635, 1969, 1536, 918, 225, 142, 2510, 379, 1011, 395, 675, 963, 1063, 2256, 2066, 1214, 1691, 1846, 641, 1550, 1342, 1152, 2361, 329, 493, 2202, 1127, 339, 241, 2251, 666, 70, 126, 1966, 813, 2484, 830, 708, 85, 429, 637, 648, 315, 1400, 470, 656, 864, 2445, 2333, 1868, 1071, 2505, 2133, 1403, 2207, 2393, 1509, 16, 95, 716, 1036, 796, 975, 778, 1847, 2305, 1748, 1813, 2228, 949, 263, 1255, 2208, 1420, 1085, 1472, 802, 26, 2070, 2435, 558, 1053, 1434, 2138, 2238, 1216, 1144, 1683, 2538, 544, 2406, 477, 1119, 497, 1099, 625, 240, 455, 2334, 276, 863, 1892, 1234, 1596, 1578, 592, 1547]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
The device used will be: 
True
cuda:0
The device used will be: 
True
cuda:0
chosen_head:  {'blocks.5.layer.bias': tensor([-0.0005, -0.0160], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0531,  0.0097,  0.0127,  ...,  0.0032,  0.0010,  0.0103],
        [-0.0248,  0.0129, -0.0082,  ..., -0.0005,  0.0202, -0.0025]],
       device='cuda:0')}

 Model C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c loaded successfuly with best perf



 ----- Architecture Block BatchNorm2dSK3962(5, 5)1.0reflect, number 0 -----
- BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK963842(3, 3)0.6499999761581421reflect, number 1 -----
- BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.4)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK38415362(3, 3)0.25reflect, number 2 -----
- BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block BatchNorm2dSK153625602(3, 3)0.25reflect, number 3 -----
- BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block BatchNorm2dSK256025602(3, 3)0.25reflect, number 4 -----
- BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 5 -----
- Flatten(start_dim=1, end_dim=-1)
- Dropout(p=0.5, inplace=False)
- Linear(in_features=2560, out_features=2, bias=True)
model.heads:  [{'blocks.5.layer.bias': tensor([ 1.5791e-05, -1.6506e-02], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0120,  0.0150,  0.0043,  ...,  0.0072,  0.0111, -0.0045],
        [ 0.0163,  0.0075,  0.0001,  ..., -0.0045,  0.0101,  0.0122]],
       device='cuda:0')}, {'blocks.5.layer.bias': tensor([ 0.0040, -0.0205], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 1.5765e-02,  1.0660e-02,  4.3432e-03,  ...,  1.0930e-02,
          1.1431e-02, -3.5607e-03],
        [ 1.2527e-02,  1.1907e-02,  8.8815e-05,  ..., -8.2031e-03,
          9.7858e-03,  1.1328e-02]], device='cuda:0')}, {'blocks.5.layer.bias': tensor([ 0.0005, -0.0170], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0349,  0.0030,  0.0162,  ..., -0.0045,  0.0023,  0.0014],
        [-0.0066,  0.0195, -0.0117,  ...,  0.0072,  0.0189,  0.0064]],
       device='cuda:0')}, {'blocks.5.layer.bias': tensor([-0.0005, -0.0160], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0531,  0.0097,  0.0127,  ...,  0.0032,  0.0010,  0.0103],
        [-0.0248,  0.0129, -0.0082,  ..., -0.0005,  0.0202, -0.0025]],
       device='cuda:0')}]
[Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (5): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=2560, out_features=2, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)]
LAYER NAME:  Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (5): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=2560, out_features=2, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)
LAYER CHILDREN:  [BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=1.4)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
  )
  (layer): Linear(in_features=2560, out_features=2, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)]
subsubl NAME:  Sequential(
  (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=1.4)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): Flatten(start_dim=1, end_dim=-1)
)
subsubl NAME:  Linear(in_features=2560, out_features=2, bias=True)
subsubl NAME:  Dropout(p=0.5, inplace=False)
CONFIG MODE:  unsupervised

 ********** Hebbian Unsupervised learning of blocks [0, 1, 2, 3, 4] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([49, 33, 72, 51, 71, 92, 15, 14, 23,  0, 71, 75, 81, 69, 40, 43, 92, 97,
        70, 53], device='cuda:0')
TARGETS AFTER SUB:  tensor([51, 51, 51, 79, 51, 79, 51, 79, 79, 79, 79, 51, 51, 51, 51, 51, 51, 51,
        79, 51])
[51, 79]
TARGETS AFTER CLEANER:  tensor([0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0])
200
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 200
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               None
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([19, 29,  0, 11,  1, 86, 90, 28, 23, 31, 39, 96, 82, 17, 71, 39,  8, 97,
        80, 71], device='cuda:0')
TARGETS AFTER SUB:  tensor([51, 51, 79, 79, 79, 79, 79, 51, 79, 51, 79, 51, 79, 51, 51, 79, 79, 51,
        79, 79])
[51, 79]
TARGETS AFTER CLEANER:  tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1])
1000
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 1000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               None
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([19, 29,  0, 11,  1, 86, 90, 28, 23, 31, 39, 96, 82, 17, 71, 39,  8, 97,
        80, 71], device='cuda:0')
TARGETS AFTER SUB:  tensor([51, 51, 79, 79, 79, 79, 79, 51, 79, 51, 79, 51, 79, 51, 51, 79, 79, 51,
        79, 79])
[51, 79]
TARGETS AFTER CLEANER:  tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1])
1000
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 1000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
------------------------
INDICES:  1000
IMAGE SIZE: torch.Size([10, 3, 32, 32])
SAVING FOLDER FOR UNSUP:  C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c
prev_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight', 'blocks.4.layer.weight'])
activations_sum.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight', 'blocks.4.layer.weight'])
DEPTH:  5
ACTIVATIONS SHAPE:  tensor([[  -23.0373,  -279.1423,  -141.0066,  -209.6236,   -85.0508,   364.2941,
           488.1330,   355.8702,    17.4723,  -365.7928,  -787.4995,  -769.4597,
          -825.3785,  -605.4977,  -312.0245,  -183.4158],
        [  347.1895,  -117.8937,   142.6992,   389.4991,   594.2186,   949.3868,
          1293.9343,  1274.2936,   982.1247,   273.6573,  -273.8251,  -532.1636,
          -697.2955,  -638.9539,  -406.8452,  -108.6880],
        [  393.6382,    69.1170,   353.4186,   640.0391,   974.4329,  1274.3064,
          1514.0942,  1661.7300,  1409.3489,   820.1495,   137.4788,  -132.4145,
          -562.5291,  -597.3792,  -342.7393,   -18.7283],
        [  162.7405,   144.7224,   286.2344,   553.3110,   929.3172,   943.4540,
          1097.3386,  1386.8857,  1153.2803,   803.7664,   368.8355,    38.3992,
          -289.3409,  -338.0632,  -361.8392,   -61.3079],
        [  198.3665,   287.9948,   332.1811,   432.9002,   511.0887,   397.7570,
           748.6777,  1101.1113,   707.7755,   508.4434,   304.5345,    71.0183,
           -71.5729,  -110.8795,  -274.7238,  -159.5063],
        [ -145.7140,   -78.0565,    96.5706,   103.0461,   113.2237,   142.5259,
           292.3257,   418.2959,   460.6892,   374.6380,   121.3001,   -91.0230,
           -82.1503,  -184.6602,  -185.4811,  -307.8022],
        [ -145.6151,  -220.8774,  -317.3618,  -343.3654,  -221.4389,   -96.9275,
           -10.7959,   136.9462,   208.9553,   184.5306,    43.1931,   -72.6943,
          -137.7820,    70.9601,     4.4100,     3.9593],
        [  -18.3666,  -285.2870,  -398.5425,  -433.9517,  -132.9398,   -98.0479,
           -10.5021,     6.3505,   120.7296,   104.2511,    -3.6391,  -110.0752,
            78.9291,   178.5308,    60.7561,   118.9019],
        [  -23.5529,  -400.5669,  -588.1041,  -680.1404,  -446.9119,  -295.9275,
          -251.3421,   -53.1744,  -179.7800,  -203.1687,  -273.3275,  -302.3158,
          -157.6981,   -41.8452,     7.9863,    63.2667],
        [   92.4785,  -245.0159,  -595.0135,  -929.8528,  -606.0313,  -207.2251,
          -210.9506,  -135.3951,  -403.0511,  -365.1501,  -215.8082,  -189.9307,
          -145.5639,  -278.2806,   -68.5345,  -125.7221],
        [  161.7544,  -181.3973,  -649.5100,  -836.2901,  -610.9354,  -227.3739,
           -11.6318,  -289.3524,  -668.6852,  -714.7185,  -343.8475,  -315.0294,
          -196.3028,  -242.2523,   -56.6102,  -234.6315],
        [  -11.2222,  -381.5101,  -722.4086, -1004.9901,  -916.1442,  -686.7306,
          -525.9764,  -636.9851,  -918.7231,  -715.3336,  -414.9447,  -483.8630,
          -418.0056,  -438.4796,    56.5746,  -178.3444],
        [  -30.6439,  -325.9833,  -425.7183,  -862.2522,  -938.2538,  -705.1100,
          -679.1274,  -499.1210,  -694.8861,  -680.7599,  -556.8692,  -771.3211,
          -484.8600,  -367.8014,   215.6497,    17.7862],
        [  -97.4750,  -241.0600,  -370.3403,  -702.2993, -1035.7642,  -778.6544,
          -538.1420,  -481.2838,  -849.0498,  -860.6447,  -873.0784, -1001.9021,
          -820.2889,  -433.9550,   147.8567,    -8.0553],
        [   14.4435,   -88.8696,  -359.2774,  -555.4017,  -688.5326,  -553.3596,
          -483.9885,  -320.8527,  -508.2038,  -659.4849,  -721.0524,  -823.6600,
          -537.5330,  -128.3310,   449.1870,   354.1080],
        [ -210.5041,  -362.8004,  -668.2365,  -790.2148,  -784.1625,  -779.8833,
          -758.7701,  -451.3533,  -750.7649,  -889.3889,  -962.5464, -1051.7493,
          -803.9193,  -580.3116,    11.5612,   -56.2230]], device='cuda:0')
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.topk_layer:  <class 'list'>
CHECK_THRESHOLD:  tensor(1, device='cuda:0', dtype=torch.uint8) tensor(-0.0006) tensor(-0.1692)
topk_mask  tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0.])
threshold_mask  tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0],
       dtype=torch.uint8)
final_mask tensor([1., 0., 0., 0., 1., 2., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 2., 0.,
        1., 1., 0., 0., 1., 1., 0., 2., 0., 1., 0., 1., 1., 0., 1., 2., 0., 2.,
        0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1.,
        1., 0., 1., 0., 1., 2., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 2., 1., 0., 1., 1., 1., 1., 2.,
        1., 1., 1., 1., 1., 0.])
lower_lr_mask  tensor([-0., -0., -0., -0., -0., -1., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -1., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0.,
        -0., -0., -0., -0., -0., -1., -0., -1., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -1., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0.,
        -0., -0., -0., -0., -0., -1., -0., -0., -0., -0., -0., -0.])
higher_lr_mask  tensor([0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.0000, 0.1500, 0.0000,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.0000, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500])
lr_mask  tensor([[[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]]])
torch.Size([96, 3, 5, 5])
self.lr tensor([[[0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066]],

        [[0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066]],

        [[0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066],
         [0.4066, 0.4066, 0.4066, 0.4066, 0.4066]]], device='cuda:0')
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.topk_layer:  <class 'list'>
CHECK_THRESHOLD:  tensor(0, device='cuda:0', dtype=torch.uint8) tensor(-0.0004) tensor(0.0326)
topk_mask  tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0.])
threshold_mask  tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0],
       dtype=torch.uint8)
final_mask tensor([0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 2., 1., 1., 0., 0., 0., 1., 2., 1., 2.,
        1., 0., 0., 1., 0., 2., 0., 0., 1., 0., 1., 0., 0., 1., 2., 0., 1., 1.,
        1., 0., 2., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 2., 1., 1., 1., 1.,
        1., 0., 1., 2., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 2., 0.,
        1., 0., 2., 0., 2., 1., 1., 0., 1., 2., 1., 0., 0., 1., 2., 1., 1., 1.,
        1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 2., 1., 1., 2., 0., 0., 0., 1.,
        1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1.,
        1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.,
        0., 2., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0.,
        2., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 2., 0., 1., 0., 0., 0., 1.,
        0., 1., 0., 1., 2., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0.,
        0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1.,
        0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0.,
        1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0.,
        1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1.,
        0., 1., 1., 1., 1., 0., 1., 1., 2., 1., 0., 0., 1., 1., 0., 1., 2., 0.,
        0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 2., 1., 0., 1., 0., 0., 1.,
        2., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 2., 1., 0., 0., 0., 0.,
        1., 0., 1., 2., 0., 0., 0., 2., 0., 1., 1., 0., 2., 1., 1., 1., 1., 1.,
        0., 0., 1., 0., 0., 2., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0.,
        0., 0., 0., 0., 2., 0.])
lower_lr_mask  tensor([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0.,
        -0., -0., -0., -0., -0., -1., -0., -1., -0., -0., -0., -0., -0., -1.,
        -0., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0., -0., -0., -0.,
        -1., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0.,
        -0., -0., -0., -0., -0., -1., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -1., -0., -0., -0., -1., -0., -1., -0., -0., -0.,
        -0., -1., -0., -0., -0., -0., -1., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -1., -0., -0., -1., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -1., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -1., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0., -0.,
        -0., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0., -0., -0.,
        -0., -0., -1., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -1., -0., -0., -0., -0., -0., -0., -0., -0., -1., -0., -0., -0., -1.,
        -0., -0., -0., -0., -1., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -1., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,
        -0., -0., -0., -0., -1., -0.])
higher_lr_mask  tensor([0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.0000,
        0.0000, 0.1500, 0.1500, 0.0000, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000,
        0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500,
        0.1500, 0.1500, 0.0000, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500,
        0.0000, 0.0000, 0.1500, 0.1500, 0.1500, 0.0000, 0.0000, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.0000, 0.0000, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.0000, 0.1500, 0.1500, 0.0000,
        0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.0000, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.0000, 0.0000, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.0000,
        0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.0000,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.0000, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.0000,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500,
        0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.0000, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500,
        0.1500, 0.0000, 0.1500, 0.0000, 0.0000, 0.1500, 0.0000, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,
        0.1500, 0.1500, 0.1500, 0.1500, 0.0000, 0.1500])
lr_mask  tensor([[[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]],


        [[[1.1500]]]])
torch.Size([384, 96, 3, 3])
self.lr tensor([[[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]],

        [[0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247],
         [0.0247, 0.0247, 0.0247]]], device='cuda:0')
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.topk_layer:  <class 'list'>
CHECK_THRESHOLD:  tensor(0, device='cuda:0', dtype=torch.uint8) tensor(2.8539e-05) tensor(0.0028)
topk_mask  tensor([0., 0., 0.,  ..., 0., 1., 1.])
threshold_mask  tensor([0, 1, 0,  ..., 0, 0, 0], dtype=torch.uint8)
final_mask tensor([0., 1., 0.,  ..., 0., 1., 1.])
lower_lr_mask  tensor([-0., -0., -0.,  ..., -0., -0., -0.])
higher_lr_mask  tensor([0.1500, 0.1500, 0.1500,  ..., 0.1500, 0.0000, 0.0000])
lr_mask  tensor([[[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        ...,


        [[[1.1500]]],


        [[[1.0000]]],


        [[[1.0000]]]])
torch.Size([1536, 384, 3, 3])
self.lr tensor([[[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]],

        [[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]],

        [[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]],

        ...,

        [[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]],

        [[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]],

        [[0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490],
         [0.0490, 0.0490, 0.0490]]], device='cuda:0')
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.topk_layer:  <class 'list'>
CHECK_THRESHOLD:  tensor(0, device='cuda:0', dtype=torch.uint8) tensor(0.0004) tensor(0.0093)
topk_mask  tensor([0., 1., 1.,  ..., 0., 0., 1.])
threshold_mask  tensor([0, 1, 1,  ..., 1, 1, 1], dtype=torch.uint8)
final_mask tensor([0., 2., 2.,  ..., 1., 1., 2.])
lower_lr_mask  tensor([-0., -1., -1.,  ..., -0., -0., -1.])
higher_lr_mask  tensor([0.1500, 0.0000, 0.0000,  ..., 0.1500, 0.1500, 0.0000])
lr_mask  tensor([[[[1.1500]]],


        [[[0.0000]]],


        [[[0.0000]]],


        ...,


        [[[1.1500]]],


        [[[1.1500]]],


        [[[0.0000]]]])
torch.Size([2560, 1536, 3, 3])
self.lr tensor([[[0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488]],

        [[0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488]],

        [[0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488]],

        ...,

        [[0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488]],

        [[0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488]],

        [[0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488],
         [0.0488, 0.0488, 0.0488]]], device='cuda:0')
self.avg_deltas_layer:  <class 'torch.Tensor'>
self.topk_layer:  <class 'list'>
CHECK_THRESHOLD:  tensor(1, device='cuda:0', dtype=torch.uint8) tensor(1.7890e-06) tensor(-0.0032)
topk_mask  tensor([0., 0., 0.,  ..., 1., 1., 0.])
threshold_mask  tensor([1, 1, 0,  ..., 0, 0, 1], dtype=torch.uint8)
final_mask tensor([1., 1., 0.,  ..., 1., 1., 1.])
lower_lr_mask  tensor([-0., -0., -0.,  ..., -0., -0., -0.])
higher_lr_mask  tensor([0.1500, 0.1500, 0.1500,  ..., 0.0000, 0.0000, 0.1500])
lr_mask  tensor([[[[1.1500]]],


        [[[1.1500]]],


        [[[1.1500]]],


        ...,


        [[[1.0000]]],


        [[[1.0000]]],


        [[[1.1500]]]])
torch.Size([2560, 2560, 3, 3])
self.lr tensor([[[0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491]],

        [[0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491]],

        [[0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491]],

        ...,

        [[0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491]],

        [[0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491]],

        [[0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491],
         [0.0491, 0.0491, 0.0491]]], device='cuda:0')
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight', 'blocks.4.layer.weight'])
curr_dict.keys():  dict_keys(['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight', 'blocks.4.layer.weight'])
activations_sum[k] 96
activations_sum[k]:  [25, 59, 30, 5, 69, 42, 55, 68, 16, 82]
topk_kernels len:  5
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3', 'conv4']
topk_kernels:  {'conv0': [25, 59, 30, 5, 69, 42, 55, 68, 16, 82, 77, 53, 14, 35, 89], 'conv1': [50, 75, 56, 206, 354, 336, 365, 94, 118, 180, 345, 41, 191, 382, 140, 317, 104, 121, 352, 324, 92, 99, 264, 190, 304, 233, 105, 100, 117, 88, 202, 67, 144, 357, 26, 355, 326, 163, 130, 137, 146, 62, 36, 6, 312, 68, 217, 219, 39, 35, 296, 349, 197, 134, 171, 131, 33, 193, 292], 'conv2': [761, 958, 937, 307, 176, 195, 1317, 622, 95, 856, 1486, 731, 1385, 544, 1436, 1297, 612, 636, 1274, 311, 673, 407, 1294, 490, 348, 700, 1278, 421, 1171, 450, 35, 921, 626, 405, 250, 1281, 733, 353, 14, 499, 93, 1292, 312, 395, 380, 427, 1046, 905, 208, 1235, 1452, 1086, 1006, 61, 799, 457, 932, 869, 56, 1307, 1034, 851, 484, 981, 548, 1123, 855, 12, 1360, 639, 359, 1508, 209, 580, 1159, 1442, 743, 1404, 635, 1236, 1412, 1345, 577, 144, 66, 361, 1291, 1359, 276, 796, 1534, 1450, 1061, 866, 1306, 783, 900, 289, 123, 846, 1203, 730, 838, 818, 164, 1388, 1125, 1193, 1227, 875, 1100, 881, 105, 520, 67, 287, 1049, 1113, 574, 1105, 1083, 1000, 683, 1071, 667, 1521, 1286, 1126, 11, 243, 916, 449, 618, 698, 1466, 735, 1272, 952, 1405, 896, 229, 670, 294, 794, 1232, 542, 460, 198, 924, 850, 141, 746, 614, 1371, 1400, 773, 1242, 810, 268, 1172, 973, 155, 534, 167, 1421, 524, 1480, 711, 754, 1434, 454, 1535, 246, 911, 109, 379, 560, 681, 1500, 434, 906, 341, 692, 261, 994, 726, 942, 801, 302, 728, 598, 1030, 1233, 100, 1441, 1265, 852, 1238, 408, 991, 805, 368, 217, 687, 1008, 1116, 1464, 854, 1386, 313, 1372, 523, 1230, 935, 423, 812, 329, 1033, 199, 1325, 1516, 309, 1180, 1314, 15, 1472, 1361, 1455, 461, 640, 1489], 'conv3': [499, 1755, 1447, 505, 1394, 507, 1567, 1720, 653, 2500, 861, 2300, 398, 421, 689, 2309, 1762, 1554, 1137, 2460, 2154, 1348, 1843, 1506, 28, 1403, 1873, 340, 652, 2448, 589, 1783, 1326, 1127, 878, 648, 2106, 468, 2203, 2181, 2285, 2340, 43, 2062, 1943, 1894, 2185, 938, 1831, 828, 1617, 1536, 419, 1500, 1179, 1558, 1226, 512, 2317, 1100, 533, 80, 935, 1723, 1162, 1845, 758, 481, 1887, 2000, 1938, 1440, 792, 2241, 2282, 1867, 646, 2502, 926, 2518, 1599, 1359, 284, 540, 555, 1187, 1375, 2515, 390, 1406, 1433, 1054, 1619, 607, 1457, 1968, 1405, 971, 225, 2008, 746, 1789, 1206, 1438, 96, 593, 2072, 2086, 1164, 2117, 1125, 810, 1339, 2523, 1608, 1713, 2095, 1794, 2494, 1080, 526, 2391, 425, 1638, 1868, 446, 1597, 819, 520, 2, 1561, 2112, 2223, 2078, 1616, 1555, 177, 564, 1088, 194, 1533, 1942, 1253, 103, 2545, 2162, 1109, 354, 1218, 1275, 1448, 2254, 1380, 1698, 1217, 2114, 339, 781, 2441, 1280, 151, 1324, 2037, 1677, 775, 1964, 280, 2081, 1872, 36, 724, 81, 509, 669, 1061, 1759, 1781, 2151, 193, 277, 2456, 1644, 761, 2442, 231, 1341, 2315, 1770, 2453, 1647, 2183, 907, 1248, 2474, 1516, 362, 1492, 1449, 454, 240, 1995, 1874, 393, 559, 508, 2271, 472, 624, 1543, 2170, 470, 687, 2511, 2110, 1319, 2559, 1363, 884, 546, 291, 1424, 1097, 990, 1958, 2367, 2479, 145, 45, 1656, 1548, 2060, 943, 1684, 2084, 129, 1776, 1824, 2050, 613, 1613, 2001, 252, 780, 1153, 655, 818, 867, 1436, 1689, 2270, 1651, 1992, 6, 1385, 1346, 2328, 1170, 1788, 2067, 2134, 450, 1038, 1694, 962, 934, 697, 1132, 2283, 615, 1409, 1439, 37, 1510, 1799, 2525, 484, 29, 480, 611, 1150, 195, 1283, 236, 2305, 1664, 272, 313, 2465, 632, 1303, 683, 1194, 665, 202, 660, 365, 674, 730, 737, 1497, 1465, 2352, 63, 1926, 2028, 1646, 548, 2555, 1378, 172, 720, 1443, 980, 2323, 2482, 299, 1660, 1360, 557, 921, 40, 939, 1862, 2098, 1267, 995, 832, 1, 1393, 423, 1808, 1495, 399, 356, 1446, 662, 487, 1224, 2013, 1469, 663, 2090, 1225, 1924, 409, 1340, 2103, 2302, 1830, 588, 2357, 2093, 2003, 2033, 477, 2046, 394, 1266, 616, 1758, 2188, 1119, 370, 360, 2295, 297, 1415, 1292, 1499, 991, 1381, 2414, 11, 2031, 1659, 1998, 1670, 431, 1844, 1431, 598, 1370, 2532, 2011, 1996], 'conv4': [1323, 433, 929, 593, 448, 671, 2211, 2271, 2518, 2285, 2385, 1620, 2268, 397, 1029, 1250, 1887, 2121, 1473, 966, 881, 1898, 499, 99, 1026, 1867, 2214, 91, 783, 546, 1572, 1629, 997, 514, 188, 784, 1784, 2038, 13, 1148, 2158, 624, 1996, 2046, 1989, 615, 969, 1167, 2039, 2356, 1908, 1149, 2395, 2519, 1654, 1376, 2528, 1741, 754, 603, 910, 673, 1519, 161, 1364, 139, 674, 608, 507, 372, 808, 1657, 1610, 2258, 1019, 1925, 2308, 1285, 1498, 2388, 1090, 2548, 1764, 721, 6, 2537, 137, 2015, 1356, 1822, 1131, 2461, 1791, 1708, 2276, 1617, 1067, 1730, 1267, 411, 2224, 348, 2253, 1712, 21, 696, 1261, 2006, 1522, 1942, 1485, 1706, 1064, 610, 1772, 1100, 218, 1710, 1007, 551, 1739, 1153, 1348, 1622, 567, 437, 3, 82, 1797, 1539, 633, 1990, 2012, 1366, 1184, 605, 1284, 1524, 978, 725, 183, 1830, 468, 2423, 1412, 67, 398, 607, 193, 2022, 1727, 34, 113, 774, 258, 1575, 788, 2307, 1471, 2551, 2320, 1218, 2326, 1360, 1777, 1424, 2476, 489, 1832, 1418, 1490, 1135, 2265, 843, 412, 1237, 2209, 1425, 1492, 2366, 1223, 391, 1929, 2145, 2210, 1169, 2557, 530, 2212, 29, 1789, 535, 761, 772, 147, 1095, 381, 426, 1583, 627, 1483, 1746, 2064, 865, 1246, 2327, 1513, 1724, 1897, 1819, 2227, 935, 2438, 990, 2558, 1193, 687, 1139, 1221, 926, 589, 945, 2355, 2544, 2321, 586, 1280, 1817, 2176, 494, 2471, 959, 1310, 340, 2135, 1774, 704, 413, 1080, 347, 181, 415, 702, 712, 1521, 2120, 366, 993, 1511, 1058, 1025, 2543, 1390, 1641, 24, 152, 312, 1801, 1795, 2552, 2415, 880, 1719, 2345, 1416, 28, 1375, 1904, 1354, 270, 2482, 2237, 1692, 1686, 1656, 2436, 1387, 806, 635, 1969, 1536, 918, 225, 142, 2510, 379, 1011, 395, 675, 963, 1063, 2256, 2066, 1214, 1691, 1846, 641, 1550, 1342, 1152, 2361, 329, 493, 2202, 1127, 339, 241, 2251, 666, 70, 126, 1966, 813, 2484, 830, 708, 85, 429, 637, 648, 315, 1400, 470, 656, 864, 2445, 2333, 1868, 1071, 2505, 2133, 1403, 2207, 2393, 1509, 16, 95, 716, 1036, 796, 975, 778, 1847, 2305, 1748, 1813, 2228, 949, 263, 1255, 2208, 1420, 1085, 1472, 802, 26, 2070, 2435, 558, 1053, 1434, 2138, 2238, 1216, 1144, 1683, 2538, 544, 2406, 477, 1119, 497, 1099, 625, 240, 455, 2334, 276, 863, 1892, 1234, 1596, 1578, 592, 1547]}
activations_sum[k] len:  96
activations_sum[k] 384
activations_sum[k]:  [304, 137, 75, 345, 349, 197, 121, 22, 365, 336]
topk_kernels len:  5
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3', 'conv4']
topk_kernels:  {'conv0': [25, 59, 30, 5, 69, 42, 55, 68, 16, 82, 77, 53, 14, 35, 89], 'conv1': [304, 137, 75, 345, 349, 197, 121, 22, 365, 336, 146, 202, 56, 100, 4, 67, 266, 163, 296, 354, 50, 159, 41, 347, 343, 180, 118, 190, 92, 94, 104, 76, 324, 62, 172, 191, 274, 200, 186, 217, 36, 105, 330, 128, 73, 21, 254, 357, 133, 169, 6, 326, 206, 193, 99, 380, 382, 88, 262], 'conv2': [761, 958, 937, 307, 176, 195, 1317, 622, 95, 856, 1486, 731, 1385, 544, 1436, 1297, 612, 636, 1274, 311, 673, 407, 1294, 490, 348, 700, 1278, 421, 1171, 450, 35, 921, 626, 405, 250, 1281, 733, 353, 14, 499, 93, 1292, 312, 395, 380, 427, 1046, 905, 208, 1235, 1452, 1086, 1006, 61, 799, 457, 932, 869, 56, 1307, 1034, 851, 484, 981, 548, 1123, 855, 12, 1360, 639, 359, 1508, 209, 580, 1159, 1442, 743, 1404, 635, 1236, 1412, 1345, 577, 144, 66, 361, 1291, 1359, 276, 796, 1534, 1450, 1061, 866, 1306, 783, 900, 289, 123, 846, 1203, 730, 838, 818, 164, 1388, 1125, 1193, 1227, 875, 1100, 881, 105, 520, 67, 287, 1049, 1113, 574, 1105, 1083, 1000, 683, 1071, 667, 1521, 1286, 1126, 11, 243, 916, 449, 618, 698, 1466, 735, 1272, 952, 1405, 896, 229, 670, 294, 794, 1232, 542, 460, 198, 924, 850, 141, 746, 614, 1371, 1400, 773, 1242, 810, 268, 1172, 973, 155, 534, 167, 1421, 524, 1480, 711, 754, 1434, 454, 1535, 246, 911, 109, 379, 560, 681, 1500, 434, 906, 341, 692, 261, 994, 726, 942, 801, 302, 728, 598, 1030, 1233, 100, 1441, 1265, 852, 1238, 408, 991, 805, 368, 217, 687, 1008, 1116, 1464, 854, 1386, 313, 1372, 523, 1230, 935, 423, 812, 329, 1033, 199, 1325, 1516, 309, 1180, 1314, 15, 1472, 1361, 1455, 461, 640, 1489], 'conv3': [499, 1755, 1447, 505, 1394, 507, 1567, 1720, 653, 2500, 861, 2300, 398, 421, 689, 2309, 1762, 1554, 1137, 2460, 2154, 1348, 1843, 1506, 28, 1403, 1873, 340, 652, 2448, 589, 1783, 1326, 1127, 878, 648, 2106, 468, 2203, 2181, 2285, 2340, 43, 2062, 1943, 1894, 2185, 938, 1831, 828, 1617, 1536, 419, 1500, 1179, 1558, 1226, 512, 2317, 1100, 533, 80, 935, 1723, 1162, 1845, 758, 481, 1887, 2000, 1938, 1440, 792, 2241, 2282, 1867, 646, 2502, 926, 2518, 1599, 1359, 284, 540, 555, 1187, 1375, 2515, 390, 1406, 1433, 1054, 1619, 607, 1457, 1968, 1405, 971, 225, 2008, 746, 1789, 1206, 1438, 96, 593, 2072, 2086, 1164, 2117, 1125, 810, 1339, 2523, 1608, 1713, 2095, 1794, 2494, 1080, 526, 2391, 425, 1638, 1868, 446, 1597, 819, 520, 2, 1561, 2112, 2223, 2078, 1616, 1555, 177, 564, 1088, 194, 1533, 1942, 1253, 103, 2545, 2162, 1109, 354, 1218, 1275, 1448, 2254, 1380, 1698, 1217, 2114, 339, 781, 2441, 1280, 151, 1324, 2037, 1677, 775, 1964, 280, 2081, 1872, 36, 724, 81, 509, 669, 1061, 1759, 1781, 2151, 193, 277, 2456, 1644, 761, 2442, 231, 1341, 2315, 1770, 2453, 1647, 2183, 907, 1248, 2474, 1516, 362, 1492, 1449, 454, 240, 1995, 1874, 393, 559, 508, 2271, 472, 624, 1543, 2170, 470, 687, 2511, 2110, 1319, 2559, 1363, 884, 546, 291, 1424, 1097, 990, 1958, 2367, 2479, 145, 45, 1656, 1548, 2060, 943, 1684, 2084, 129, 1776, 1824, 2050, 613, 1613, 2001, 252, 780, 1153, 655, 818, 867, 1436, 1689, 2270, 1651, 1992, 6, 1385, 1346, 2328, 1170, 1788, 2067, 2134, 450, 1038, 1694, 962, 934, 697, 1132, 2283, 615, 1409, 1439, 37, 1510, 1799, 2525, 484, 29, 480, 611, 1150, 195, 1283, 236, 2305, 1664, 272, 313, 2465, 632, 1303, 683, 1194, 665, 202, 660, 365, 674, 730, 737, 1497, 1465, 2352, 63, 1926, 2028, 1646, 548, 2555, 1378, 172, 720, 1443, 980, 2323, 2482, 299, 1660, 1360, 557, 921, 40, 939, 1862, 2098, 1267, 995, 832, 1, 1393, 423, 1808, 1495, 399, 356, 1446, 662, 487, 1224, 2013, 1469, 663, 2090, 1225, 1924, 409, 1340, 2103, 2302, 1830, 588, 2357, 2093, 2003, 2033, 477, 2046, 394, 1266, 616, 1758, 2188, 1119, 370, 360, 2295, 297, 1415, 1292, 1499, 991, 1381, 2414, 11, 2031, 1659, 1998, 1670, 431, 1844, 1431, 598, 1370, 2532, 2011, 1996], 'conv4': [1323, 433, 929, 593, 448, 671, 2211, 2271, 2518, 2285, 2385, 1620, 2268, 397, 1029, 1250, 1887, 2121, 1473, 966, 881, 1898, 499, 99, 1026, 1867, 2214, 91, 783, 546, 1572, 1629, 997, 514, 188, 784, 1784, 2038, 13, 1148, 2158, 624, 1996, 2046, 1989, 615, 969, 1167, 2039, 2356, 1908, 1149, 2395, 2519, 1654, 1376, 2528, 1741, 754, 603, 910, 673, 1519, 161, 1364, 139, 674, 608, 507, 372, 808, 1657, 1610, 2258, 1019, 1925, 2308, 1285, 1498, 2388, 1090, 2548, 1764, 721, 6, 2537, 137, 2015, 1356, 1822, 1131, 2461, 1791, 1708, 2276, 1617, 1067, 1730, 1267, 411, 2224, 348, 2253, 1712, 21, 696, 1261, 2006, 1522, 1942, 1485, 1706, 1064, 610, 1772, 1100, 218, 1710, 1007, 551, 1739, 1153, 1348, 1622, 567, 437, 3, 82, 1797, 1539, 633, 1990, 2012, 1366, 1184, 605, 1284, 1524, 978, 725, 183, 1830, 468, 2423, 1412, 67, 398, 607, 193, 2022, 1727, 34, 113, 774, 258, 1575, 788, 2307, 1471, 2551, 2320, 1218, 2326, 1360, 1777, 1424, 2476, 489, 1832, 1418, 1490, 1135, 2265, 843, 412, 1237, 2209, 1425, 1492, 2366, 1223, 391, 1929, 2145, 2210, 1169, 2557, 530, 2212, 29, 1789, 535, 761, 772, 147, 1095, 381, 426, 1583, 627, 1483, 1746, 2064, 865, 1246, 2327, 1513, 1724, 1897, 1819, 2227, 935, 2438, 990, 2558, 1193, 687, 1139, 1221, 926, 589, 945, 2355, 2544, 2321, 586, 1280, 1817, 2176, 494, 2471, 959, 1310, 340, 2135, 1774, 704, 413, 1080, 347, 181, 415, 702, 712, 1521, 2120, 366, 993, 1511, 1058, 1025, 2543, 1390, 1641, 24, 152, 312, 1801, 1795, 2552, 2415, 880, 1719, 2345, 1416, 28, 1375, 1904, 1354, 270, 2482, 2237, 1692, 1686, 1656, 2436, 1387, 806, 635, 1969, 1536, 918, 225, 142, 2510, 379, 1011, 395, 675, 963, 1063, 2256, 2066, 1214, 1691, 1846, 641, 1550, 1342, 1152, 2361, 329, 493, 2202, 1127, 339, 241, 2251, 666, 70, 126, 1966, 813, 2484, 830, 708, 85, 429, 637, 648, 315, 1400, 470, 656, 864, 2445, 2333, 1868, 1071, 2505, 2133, 1403, 2207, 2393, 1509, 16, 95, 716, 1036, 796, 975, 778, 1847, 2305, 1748, 1813, 2228, 949, 263, 1255, 2208, 1420, 1085, 1472, 802, 26, 2070, 2435, 558, 1053, 1434, 2138, 2238, 1216, 1144, 1683, 2538, 544, 2406, 477, 1119, 497, 1099, 625, 240, 455, 2334, 276, 863, 1892, 1234, 1596, 1578, 592, 1547]}
activations_sum[k] len:  384
activations_sum[k] 1536
activations_sum[k]:  [958, 1278, 761, 1436, 856, 95, 14, 1385, 733, 405]
topk_kernels len:  5
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3', 'conv4']
topk_kernels:  {'conv0': [25, 59, 30, 5, 69, 42, 55, 68, 16, 82, 77, 53, 14, 35, 89], 'conv1': [304, 137, 75, 345, 349, 197, 121, 22, 365, 336, 146, 202, 56, 100, 4, 67, 266, 163, 296, 354, 50, 159, 41, 347, 343, 180, 118, 190, 92, 94, 104, 76, 324, 62, 172, 191, 274, 200, 186, 217, 36, 105, 330, 128, 73, 21, 254, 357, 133, 169, 6, 326, 206, 193, 99, 380, 382, 88, 262], 'conv2': [958, 1278, 761, 1436, 856, 95, 14, 1385, 733, 405, 622, 12, 307, 499, 1227, 1274, 1345, 869, 348, 612, 1452, 176, 1532, 250, 632, 1113, 312, 915, 423, 707, 406, 1508, 1291, 542, 942, 1317, 70, 743, 1442, 75, 544, 311, 574, 904, 961, 616, 959, 35, 801, 937, 490, 670, 1297, 994, 790, 434, 1450, 523, 450, 698, 1232, 936, 850, 636, 93, 164, 243, 22, 114, 911, 1238, 626, 1388, 350, 74, 1282, 1281, 1203, 913, 580, 238, 597, 1159, 774, 77, 889, 1025, 1086, 1486, 347, 868, 461, 783, 373, 905, 675, 639, 1245, 67, 359, 100, 673, 281, 1430, 1206, 705, 1101, 1089, 174, 1429, 662, 458, 810, 835, 1006, 1172, 923, 61, 852, 1185, 462, 667, 599, 41, 754, 395, 360, 855, 290, 728, 309, 25, 439, 1171, 1292, 457, 56, 875, 135, 13, 629, 148, 1424, 1002, 138, 1497, 1125, 1401, 614, 538, 964, 824, 1302, 441, 1294, 809, 1437, 1325, 246, 1100, 1186, 745, 324, 313, 356, 731, 276, 167, 335, 105, 212, 456, 851, 649, 596, 1192, 294, 118, 566, 133, 1248, 326, 584, 436, 1116, 447, 15, 933, 102, 818, 1046, 732, 195, 132, 233, 519, 1001, 1522, 674, 1022, 425, 1448, 688, 891, 1494, 1033, 886, 650, 508, 1307, 66, 808, 563, 1324, 872, 134, 110, 413, 209, 119, 1516, 779, 1266, 846, 1178, 717, 955, 215, 799, 973, 1032], 'conv3': [499, 1755, 1447, 505, 1394, 507, 1567, 1720, 653, 2500, 861, 2300, 398, 421, 689, 2309, 1762, 1554, 1137, 2460, 2154, 1348, 1843, 1506, 28, 1403, 1873, 340, 652, 2448, 589, 1783, 1326, 1127, 878, 648, 2106, 468, 2203, 2181, 2285, 2340, 43, 2062, 1943, 1894, 2185, 938, 1831, 828, 1617, 1536, 419, 1500, 1179, 1558, 1226, 512, 2317, 1100, 533, 80, 935, 1723, 1162, 1845, 758, 481, 1887, 2000, 1938, 1440, 792, 2241, 2282, 1867, 646, 2502, 926, 2518, 1599, 1359, 284, 540, 555, 1187, 1375, 2515, 390, 1406, 1433, 1054, 1619, 607, 1457, 1968, 1405, 971, 225, 2008, 746, 1789, 1206, 1438, 96, 593, 2072, 2086, 1164, 2117, 1125, 810, 1339, 2523, 1608, 1713, 2095, 1794, 2494, 1080, 526, 2391, 425, 1638, 1868, 446, 1597, 819, 520, 2, 1561, 2112, 2223, 2078, 1616, 1555, 177, 564, 1088, 194, 1533, 1942, 1253, 103, 2545, 2162, 1109, 354, 1218, 1275, 1448, 2254, 1380, 1698, 1217, 2114, 339, 781, 2441, 1280, 151, 1324, 2037, 1677, 775, 1964, 280, 2081, 1872, 36, 724, 81, 509, 669, 1061, 1759, 1781, 2151, 193, 277, 2456, 1644, 761, 2442, 231, 1341, 2315, 1770, 2453, 1647, 2183, 907, 1248, 2474, 1516, 362, 1492, 1449, 454, 240, 1995, 1874, 393, 559, 508, 2271, 472, 624, 1543, 2170, 470, 687, 2511, 2110, 1319, 2559, 1363, 884, 546, 291, 1424, 1097, 990, 1958, 2367, 2479, 145, 45, 1656, 1548, 2060, 943, 1684, 2084, 129, 1776, 1824, 2050, 613, 1613, 2001, 252, 780, 1153, 655, 818, 867, 1436, 1689, 2270, 1651, 1992, 6, 1385, 1346, 2328, 1170, 1788, 2067, 2134, 450, 1038, 1694, 962, 934, 697, 1132, 2283, 615, 1409, 1439, 37, 1510, 1799, 2525, 484, 29, 480, 611, 1150, 195, 1283, 236, 2305, 1664, 272, 313, 2465, 632, 1303, 683, 1194, 665, 202, 660, 365, 674, 730, 737, 1497, 1465, 2352, 63, 1926, 2028, 1646, 548, 2555, 1378, 172, 720, 1443, 980, 2323, 2482, 299, 1660, 1360, 557, 921, 40, 939, 1862, 2098, 1267, 995, 832, 1, 1393, 423, 1808, 1495, 399, 356, 1446, 662, 487, 1224, 2013, 1469, 663, 2090, 1225, 1924, 409, 1340, 2103, 2302, 1830, 588, 2357, 2093, 2003, 2033, 477, 2046, 394, 1266, 616, 1758, 2188, 1119, 370, 360, 2295, 297, 1415, 1292, 1499, 991, 1381, 2414, 11, 2031, 1659, 1998, 1670, 431, 1844, 1431, 598, 1370, 2532, 2011, 1996], 'conv4': [1323, 433, 929, 593, 448, 671, 2211, 2271, 2518, 2285, 2385, 1620, 2268, 397, 1029, 1250, 1887, 2121, 1473, 966, 881, 1898, 499, 99, 1026, 1867, 2214, 91, 783, 546, 1572, 1629, 997, 514, 188, 784, 1784, 2038, 13, 1148, 2158, 624, 1996, 2046, 1989, 615, 969, 1167, 2039, 2356, 1908, 1149, 2395, 2519, 1654, 1376, 2528, 1741, 754, 603, 910, 673, 1519, 161, 1364, 139, 674, 608, 507, 372, 808, 1657, 1610, 2258, 1019, 1925, 2308, 1285, 1498, 2388, 1090, 2548, 1764, 721, 6, 2537, 137, 2015, 1356, 1822, 1131, 2461, 1791, 1708, 2276, 1617, 1067, 1730, 1267, 411, 2224, 348, 2253, 1712, 21, 696, 1261, 2006, 1522, 1942, 1485, 1706, 1064, 610, 1772, 1100, 218, 1710, 1007, 551, 1739, 1153, 1348, 1622, 567, 437, 3, 82, 1797, 1539, 633, 1990, 2012, 1366, 1184, 605, 1284, 1524, 978, 725, 183, 1830, 468, 2423, 1412, 67, 398, 607, 193, 2022, 1727, 34, 113, 774, 258, 1575, 788, 2307, 1471, 2551, 2320, 1218, 2326, 1360, 1777, 1424, 2476, 489, 1832, 1418, 1490, 1135, 2265, 843, 412, 1237, 2209, 1425, 1492, 2366, 1223, 391, 1929, 2145, 2210, 1169, 2557, 530, 2212, 29, 1789, 535, 761, 772, 147, 1095, 381, 426, 1583, 627, 1483, 1746, 2064, 865, 1246, 2327, 1513, 1724, 1897, 1819, 2227, 935, 2438, 990, 2558, 1193, 687, 1139, 1221, 926, 589, 945, 2355, 2544, 2321, 586, 1280, 1817, 2176, 494, 2471, 959, 1310, 340, 2135, 1774, 704, 413, 1080, 347, 181, 415, 702, 712, 1521, 2120, 366, 993, 1511, 1058, 1025, 2543, 1390, 1641, 24, 152, 312, 1801, 1795, 2552, 2415, 880, 1719, 2345, 1416, 28, 1375, 1904, 1354, 270, 2482, 2237, 1692, 1686, 1656, 2436, 1387, 806, 635, 1969, 1536, 918, 225, 142, 2510, 379, 1011, 395, 675, 963, 1063, 2256, 2066, 1214, 1691, 1846, 641, 1550, 1342, 1152, 2361, 329, 493, 2202, 1127, 339, 241, 2251, 666, 70, 126, 1966, 813, 2484, 830, 708, 85, 429, 637, 648, 315, 1400, 470, 656, 864, 2445, 2333, 1868, 1071, 2505, 2133, 1403, 2207, 2393, 1509, 16, 95, 716, 1036, 796, 975, 778, 1847, 2305, 1748, 1813, 2228, 949, 263, 1255, 2208, 1420, 1085, 1472, 802, 26, 2070, 2435, 558, 1053, 1434, 2138, 2238, 1216, 1144, 1683, 2538, 544, 2406, 477, 1119, 497, 1099, 625, 240, 455, 2334, 276, 863, 1892, 1234, 1596, 1578, 592, 1547]}
activations_sum[k] len:  1536
activations_sum[k] 2560
activations_sum[k]:  [505, 1755, 1843, 1403, 1783, 1554, 1938, 646, 421, 1447]
topk_kernels len:  5
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3', 'conv4']
topk_kernels:  {'conv0': [25, 59, 30, 5, 69, 42, 55, 68, 16, 82, 77, 53, 14, 35, 89], 'conv1': [304, 137, 75, 345, 349, 197, 121, 22, 365, 336, 146, 202, 56, 100, 4, 67, 266, 163, 296, 354, 50, 159, 41, 347, 343, 180, 118, 190, 92, 94, 104, 76, 324, 62, 172, 191, 274, 200, 186, 217, 36, 105, 330, 128, 73, 21, 254, 357, 133, 169, 6, 326, 206, 193, 99, 380, 382, 88, 262], 'conv2': [958, 1278, 761, 1436, 856, 95, 14, 1385, 733, 405, 622, 12, 307, 499, 1227, 1274, 1345, 869, 348, 612, 1452, 176, 1532, 250, 632, 1113, 312, 915, 423, 707, 406, 1508, 1291, 542, 942, 1317, 70, 743, 1442, 75, 544, 311, 574, 904, 961, 616, 959, 35, 801, 937, 490, 670, 1297, 994, 790, 434, 1450, 523, 450, 698, 1232, 936, 850, 636, 93, 164, 243, 22, 114, 911, 1238, 626, 1388, 350, 74, 1282, 1281, 1203, 913, 580, 238, 597, 1159, 774, 77, 889, 1025, 1086, 1486, 347, 868, 461, 783, 373, 905, 675, 639, 1245, 67, 359, 100, 673, 281, 1430, 1206, 705, 1101, 1089, 174, 1429, 662, 458, 810, 835, 1006, 1172, 923, 61, 852, 1185, 462, 667, 599, 41, 754, 395, 360, 855, 290, 728, 309, 25, 439, 1171, 1292, 457, 56, 875, 135, 13, 629, 148, 1424, 1002, 138, 1497, 1125, 1401, 614, 538, 964, 824, 1302, 441, 1294, 809, 1437, 1325, 246, 1100, 1186, 745, 324, 313, 356, 731, 276, 167, 335, 105, 212, 456, 851, 649, 596, 1192, 294, 118, 566, 133, 1248, 326, 584, 436, 1116, 447, 15, 933, 102, 818, 1046, 732, 195, 132, 233, 519, 1001, 1522, 674, 1022, 425, 1448, 688, 891, 1494, 1033, 886, 650, 508, 1307, 66, 808, 563, 1324, 872, 134, 110, 413, 209, 119, 1516, 779, 1266, 846, 1178, 717, 955, 215, 799, 973, 1032], 'conv3': [505, 1755, 1843, 1403, 1783, 1554, 1938, 646, 421, 1447, 792, 564, 2241, 1226, 2460, 472, 1424, 507, 2050, 1958, 1198, 419, 2414, 653, 2305, 195, 1720, 497, 2098, 129, 1789, 2428, 2, 534, 454, 1608, 1451, 96, 299, 398, 953, 2233, 1894, 1266, 1926, 878, 1698, 1448, 689, 304, 1303, 2453, 499, 1887, 305, 1385, 2093, 1548, 1406, 456, 1567, 404, 233, 943, 1753, 533, 935, 1162, 2161, 1271, 2154, 586, 2309, 922, 862, 2300, 658, 1873, 1506, 1394, 390, 1364, 861, 627, 2000, 1697, 772, 2185, 2051, 1992, 2162, 2511, 1831, 1677, 425, 1409, 370, 509, 2112, 2227, 561, 110, 1799, 2223, 2022, 199, 492, 1638, 1236, 1021, 2500, 2362, 1436, 1080, 774, 1304, 347, 555, 2254, 436, 1458, 2164, 2278, 2302, 2062, 1076, 2523, 1599, 1088, 657, 923, 1175, 2056, 2018, 2001, 1524, 708, 1647, 2265, 1612, 1051, 1109, 1839, 1227, 122, 2094, 2469, 674, 1796, 1446, 284, 331, 453, 1845, 2042, 1157, 720, 2057, 1375, 1457, 632, 2007, 2151, 249, 696, 2085, 964, 2245, 1405, 2070, 2114, 1074, 757, 51, 164, 1427, 431, 1811, 1510, 1325, 1138, 867, 402, 2420, 1248, 2181, 1564, 2456, 450, 68, 2270, 2315, 761, 1993, 340, 2106, 1809, 291, 80, 2252, 1578, 1781, 748, 2323, 102, 846, 262, 1337, 146, 1699, 2354, 912, 527, 895, 788, 2328, 746, 178, 638, 1221, 724, 2167, 1128, 1536, 735, 1003, 333, 240, 1985, 1253, 737, 715, 2021, 2240, 2179, 2317, 2342, 2044, 2011, 639, 1890, 1380, 780, 1045, 2004, 448, 1733, 231, 980, 1439, 283, 365, 944, 1646, 60, 2427, 1363, 1642, 1348, 1982, 1696, 605, 2341, 2152, 1998, 1341, 252, 726, 1920, 2160, 2250, 2340, 1968, 1867, 2095, 2352, 2248, 2249, 810, 2081, 1830, 723, 104, 987, 54, 1283, 1824, 1433, 1104, 2479, 1219, 2084, 378, 1340, 225, 1402, 2297, 2502, 1484, 1376, 339, 1220, 928, 626, 687, 2322, 616, 1545, 297, 1435, 758, 1676, 1555, 2514, 677, 742, 583, 694, 1689, 278, 903, 1690, 1013, 35, 927, 1362, 2236, 177, 1742, 1851, 156, 277, 346, 2173, 2221, 312, 52, 937, 69, 1874, 752, 1070, 2087, 1619, 2559, 1346, 1891, 1492, 969, 1002, 77, 1351, 2542, 753, 1267, 1056, 1210, 1187, 2174, 242, 1801, 2259, 46, 1670, 2079, 200, 1865, 13, 2242, 1353, 1928, 1044, 1411, 1614, 699, 1844, 1053, 72, 1645, 1356, 89, 884, 2551, 1287, 1961], 'conv4': [1323, 433, 929, 593, 448, 671, 2211, 2271, 2518, 2285, 2385, 1620, 2268, 397, 1029, 1250, 1887, 2121, 1473, 966, 881, 1898, 499, 99, 1026, 1867, 2214, 91, 783, 546, 1572, 1629, 997, 514, 188, 784, 1784, 2038, 13, 1148, 2158, 624, 1996, 2046, 1989, 615, 969, 1167, 2039, 2356, 1908, 1149, 2395, 2519, 1654, 1376, 2528, 1741, 754, 603, 910, 673, 1519, 161, 1364, 139, 674, 608, 507, 372, 808, 1657, 1610, 2258, 1019, 1925, 2308, 1285, 1498, 2388, 1090, 2548, 1764, 721, 6, 2537, 137, 2015, 1356, 1822, 1131, 2461, 1791, 1708, 2276, 1617, 1067, 1730, 1267, 411, 2224, 348, 2253, 1712, 21, 696, 1261, 2006, 1522, 1942, 1485, 1706, 1064, 610, 1772, 1100, 218, 1710, 1007, 551, 1739, 1153, 1348, 1622, 567, 437, 3, 82, 1797, 1539, 633, 1990, 2012, 1366, 1184, 605, 1284, 1524, 978, 725, 183, 1830, 468, 2423, 1412, 67, 398, 607, 193, 2022, 1727, 34, 113, 774, 258, 1575, 788, 2307, 1471, 2551, 2320, 1218, 2326, 1360, 1777, 1424, 2476, 489, 1832, 1418, 1490, 1135, 2265, 843, 412, 1237, 2209, 1425, 1492, 2366, 1223, 391, 1929, 2145, 2210, 1169, 2557, 530, 2212, 29, 1789, 535, 761, 772, 147, 1095, 381, 426, 1583, 627, 1483, 1746, 2064, 865, 1246, 2327, 1513, 1724, 1897, 1819, 2227, 935, 2438, 990, 2558, 1193, 687, 1139, 1221, 926, 589, 945, 2355, 2544, 2321, 586, 1280, 1817, 2176, 494, 2471, 959, 1310, 340, 2135, 1774, 704, 413, 1080, 347, 181, 415, 702, 712, 1521, 2120, 366, 993, 1511, 1058, 1025, 2543, 1390, 1641, 24, 152, 312, 1801, 1795, 2552, 2415, 880, 1719, 2345, 1416, 28, 1375, 1904, 1354, 270, 2482, 2237, 1692, 1686, 1656, 2436, 1387, 806, 635, 1969, 1536, 918, 225, 142, 2510, 379, 1011, 395, 675, 963, 1063, 2256, 2066, 1214, 1691, 1846, 641, 1550, 1342, 1152, 2361, 329, 493, 2202, 1127, 339, 241, 2251, 666, 70, 126, 1966, 813, 2484, 830, 708, 85, 429, 637, 648, 315, 1400, 470, 656, 864, 2445, 2333, 1868, 1071, 2505, 2133, 1403, 2207, 2393, 1509, 16, 95, 716, 1036, 796, 975, 778, 1847, 2305, 1748, 1813, 2228, 949, 263, 1255, 2208, 1420, 1085, 1472, 802, 26, 2070, 2435, 558, 1053, 1434, 2138, 2238, 1216, 1144, 1683, 2538, 544, 2406, 477, 1119, 497, 1099, 625, 240, 455, 2334, 276, 863, 1892, 1234, 1596, 1578, 592, 1547]}
activations_sum[k] len:  2560
activations_sum[k] 2560
activations_sum[k]:  [1118, 1323, 1657, 557, 888, 1143, 530, 966, 605, 91]
topk_kernels len:  5
topk_kernels keys:  ['conv0', 'conv1', 'conv2', 'conv3', 'conv4']
topk_kernels:  {'conv0': [25, 59, 30, 5, 69, 42, 55, 68, 16, 82, 77, 53, 14, 35, 89], 'conv1': [304, 137, 75, 345, 349, 197, 121, 22, 365, 336, 146, 202, 56, 100, 4, 67, 266, 163, 296, 354, 50, 159, 41, 347, 343, 180, 118, 190, 92, 94, 104, 76, 324, 62, 172, 191, 274, 200, 186, 217, 36, 105, 330, 128, 73, 21, 254, 357, 133, 169, 6, 326, 206, 193, 99, 380, 382, 88, 262], 'conv2': [958, 1278, 761, 1436, 856, 95, 14, 1385, 733, 405, 622, 12, 307, 499, 1227, 1274, 1345, 869, 348, 612, 1452, 176, 1532, 250, 632, 1113, 312, 915, 423, 707, 406, 1508, 1291, 542, 942, 1317, 70, 743, 1442, 75, 544, 311, 574, 904, 961, 616, 959, 35, 801, 937, 490, 670, 1297, 994, 790, 434, 1450, 523, 450, 698, 1232, 936, 850, 636, 93, 164, 243, 22, 114, 911, 1238, 626, 1388, 350, 74, 1282, 1281, 1203, 913, 580, 238, 597, 1159, 774, 77, 889, 1025, 1086, 1486, 347, 868, 461, 783, 373, 905, 675, 639, 1245, 67, 359, 100, 673, 281, 1430, 1206, 705, 1101, 1089, 174, 1429, 662, 458, 810, 835, 1006, 1172, 923, 61, 852, 1185, 462, 667, 599, 41, 754, 395, 360, 855, 290, 728, 309, 25, 439, 1171, 1292, 457, 56, 875, 135, 13, 629, 148, 1424, 1002, 138, 1497, 1125, 1401, 614, 538, 964, 824, 1302, 441, 1294, 809, 1437, 1325, 246, 1100, 1186, 745, 324, 313, 356, 731, 276, 167, 335, 105, 212, 456, 851, 649, 596, 1192, 294, 118, 566, 133, 1248, 326, 584, 436, 1116, 447, 15, 933, 102, 818, 1046, 732, 195, 132, 233, 519, 1001, 1522, 674, 1022, 425, 1448, 688, 891, 1494, 1033, 886, 650, 508, 1307, 66, 808, 563, 1324, 872, 134, 110, 413, 209, 119, 1516, 779, 1266, 846, 1178, 717, 955, 215, 799, 973, 1032], 'conv3': [505, 1755, 1843, 1403, 1783, 1554, 1938, 646, 421, 1447, 792, 564, 2241, 1226, 2460, 472, 1424, 507, 2050, 1958, 1198, 419, 2414, 653, 2305, 195, 1720, 497, 2098, 129, 1789, 2428, 2, 534, 454, 1608, 1451, 96, 299, 398, 953, 2233, 1894, 1266, 1926, 878, 1698, 1448, 689, 304, 1303, 2453, 499, 1887, 305, 1385, 2093, 1548, 1406, 456, 1567, 404, 233, 943, 1753, 533, 935, 1162, 2161, 1271, 2154, 586, 2309, 922, 862, 2300, 658, 1873, 1506, 1394, 390, 1364, 861, 627, 2000, 1697, 772, 2185, 2051, 1992, 2162, 2511, 1831, 1677, 425, 1409, 370, 509, 2112, 2227, 561, 110, 1799, 2223, 2022, 199, 492, 1638, 1236, 1021, 2500, 2362, 1436, 1080, 774, 1304, 347, 555, 2254, 436, 1458, 2164, 2278, 2302, 2062, 1076, 2523, 1599, 1088, 657, 923, 1175, 2056, 2018, 2001, 1524, 708, 1647, 2265, 1612, 1051, 1109, 1839, 1227, 122, 2094, 2469, 674, 1796, 1446, 284, 331, 453, 1845, 2042, 1157, 720, 2057, 1375, 1457, 632, 2007, 2151, 249, 696, 2085, 964, 2245, 1405, 2070, 2114, 1074, 757, 51, 164, 1427, 431, 1811, 1510, 1325, 1138, 867, 402, 2420, 1248, 2181, 1564, 2456, 450, 68, 2270, 2315, 761, 1993, 340, 2106, 1809, 291, 80, 2252, 1578, 1781, 748, 2323, 102, 846, 262, 1337, 146, 1699, 2354, 912, 527, 895, 788, 2328, 746, 178, 638, 1221, 724, 2167, 1128, 1536, 735, 1003, 333, 240, 1985, 1253, 737, 715, 2021, 2240, 2179, 2317, 2342, 2044, 2011, 639, 1890, 1380, 780, 1045, 2004, 448, 1733, 231, 980, 1439, 283, 365, 944, 1646, 60, 2427, 1363, 1642, 1348, 1982, 1696, 605, 2341, 2152, 1998, 1341, 252, 726, 1920, 2160, 2250, 2340, 1968, 1867, 2095, 2352, 2248, 2249, 810, 2081, 1830, 723, 104, 987, 54, 1283, 1824, 1433, 1104, 2479, 1219, 2084, 378, 1340, 225, 1402, 2297, 2502, 1484, 1376, 339, 1220, 928, 626, 687, 2322, 616, 1545, 297, 1435, 758, 1676, 1555, 2514, 677, 742, 583, 694, 1689, 278, 903, 1690, 1013, 35, 927, 1362, 2236, 177, 1742, 1851, 156, 277, 346, 2173, 2221, 312, 52, 937, 69, 1874, 752, 1070, 2087, 1619, 2559, 1346, 1891, 1492, 969, 1002, 77, 1351, 2542, 753, 1267, 1056, 1210, 1187, 2174, 242, 1801, 2259, 46, 1670, 2079, 200, 1865, 13, 2242, 1353, 1928, 1044, 1411, 1614, 699, 1844, 1053, 72, 1645, 1356, 89, 884, 2551, 1287, 1961], 'conv4': [1118, 1323, 1657, 557, 888, 1143, 530, 966, 605, 91, 1504, 426, 1167, 2265, 2388, 1233, 1957, 99, 1366, 1970, 2385, 2531, 93, 382, 2393, 1706, 2457, 2559, 674, 2121, 1869, 402, 1250, 770, 6, 1883, 1406, 778, 30, 2335, 172, 315, 1063, 883, 386, 1713, 2218, 70, 2211, 1375, 455, 499, 2060, 2440, 1428, 161, 2415, 757, 703, 654, 139, 2171, 2053, 1066, 1064, 326, 1099, 1471, 15, 1387, 140, 621, 2034, 1373, 607, 534, 1602, 1750, 1090, 1289, 774, 2510, 1999, 2195, 270, 1731, 1273, 1928, 2143, 2250, 1787, 400, 2532, 1556, 1912, 2475, 1513, 1704, 1284, 448, 625, 1676, 1730, 1434, 1607, 3, 612, 1034, 193, 1442, 208, 843, 1241, 1489, 2268, 825, 1384, 1981, 296, 1877, 177, 1298, 1880, 2141, 969, 7, 1477, 1313, 806, 798, 2015, 2337, 1622, 2395, 905, 2286, 1402, 412, 1908, 1084, 2138, 328, 1684, 556, 34, 2410, 2285, 2025, 663, 150, 1252, 0, 549, 2066, 792, 8, 366, 112, 1201, 2070, 704, 2099, 2302, 548, 482, 1982, 1397, 1640, 889, 2191, 2189, 1306, 1326, 2558, 503, 263, 1071, 2436, 1246, 2226, 789, 666, 2081, 593, 718, 544, 568, 1080, 364, 2485, 615, 1618, 1597, 1035, 361, 1700, 233, 24, 2327, 765, 1987, 861, 2021, 628, 1571, 1444, 1580, 2487, 2346, 2452, 2453, 54, 2158, 1452, 2343, 309, 1819, 1135, 1810, 1285, 1892, 2188, 1969, 2326, 1169, 494, 768, 1338, 5, 2228, 182, 1796, 1644, 924, 2432, 2112, 120, 815, 1583, 2207, 2480, 2205, 85, 1012, 1719, 2276, 904, 105, 1723, 1995, 2277, 2172, 2239, 1357, 637, 2085, 1040, 1922, 2512, 1230, 225, 658, 738, 1065, 242, 1468, 1639, 1734, 1210, 689, 788, 108, 1134, 2546, 207, 2413, 1523, 257, 1424, 2516, 671, 1124, 2134, 558, 335, 2224, 332, 1619, 1239, 926, 951, 891, 1445, 239, 493, 1609, 424, 449, 1732, 2311, 1852, 1759, 2254, 340, 1535, 2509, 1634, 447, 1572, 802, 1770, 395, 1030, 1411, 1645, 2212, 562, 311, 1420, 531, 1575, 1996, 502, 259, 202, 670, 2045, 682, 1390, 132, 2518, 565, 2356, 560, 190, 2220, 2149, 1418, 2079, 1355, 1109, 1113, 553, 2540, 2249, 1015, 2538, 564, 705, 214, 2525, 1993, 700, 341, 252, 430, 1937, 1140, 1878, 2011, 783, 1198, 86, 2507, 1275, 603, 1372, 1849, 1221, 2163, 604, 1522, 1339, 1558, 2517, 2535, 1496, 1204, 874, 1641, 1432, 895, 2492, 2271, 1467]}
activations_sum[k] len:  2560
['conv0', 'conv1', 'conv2', 'conv3', 'conv4']
delta_weights INFO: 
NUM OF TRACKED COV LAYERS:  5
NUM OF TRACKED WEIGHTS CHANGES PER LAYER:  2
avg_deltas INFO:  <class 'dict'>
avg_deltas keys:  ['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight', 'blocks.3.layer.weight', 'blocks.4.layer.weight']
################################################
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 4.77e-02	time: 00:00:20	Acc_train 0.00	Acc_test 0.00	convergence: 2.26e+01	R1: 0	Info MB:0.000e+00/SB:0.000e+00/MW:3.813e-06/SW:1.568e-01/MR:2.359e+01/SR:3.156e+00/MeD:1.986e+00/MaD:2.256e+01/MW:0.611/MAW:0.389
|        0 |        1 |        2 |       3 |        4 |        5 |        6 |       7 |       8 |        9 |       10 |       11 |       12 |       13 |      14 |       15 |       16 |       17 |       18 |       19 |       20 |       21 |       22 |      23 |       24 |       25 |       26 |       27 |       28 |       29 |
|----------+----------+----------+---------+----------+----------+----------+---------+---------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------+----------+----------|
|   0.0508 |   0.0497 |   0.0504 |   0.049 |   0.0401 |   0.0405 |   0.0468 |   0.046 |   0.044 |   0.0415 |   0.0479 |   0.0472 |   0.0484 |   0.0495 |   0.043 |   0.0458 |   0.0469 |   0.0484 |   0.0491 |   0.0493 |   0.0481 |   0.0478 |   0.0467 |   0.047 |   0.0496 |   0.0489 |   0.0481 |   0.0496 |   0.0482 |   0.0501 |
|  26.76   |  25.75   |  26.44   |  25.04  |  17.11   |  17.44   |  22.91   |  22.18  |  20.32  |  18.24   |  23.95   |  23.31   |  24.4    |  25.5    |  19.53  |  21.98   |  23.04   |  24.4    |  25.1    |  25.3    |  24.15   |  23.83   |  22.78   |  23.05  |  25.58   |  24.96   |  24.09   |  25.61   |  24.24   |  26.07   |
|   0.05   |   0.03   |   0.01   |   0.01  |   0.12   |   0.09   |   0.03   |   0.16  |   0.1   |   0.05   |   0.01   |   0.01   |   0.01   |   0.13   |   0.05  |   0.07   |   0.05   |   0.03   |   0.01   |   0      |   0.03   |   0.08   |   0.04   |   0.03  |   0.01   |   0      |   0.01   |   0      |   0.02   |   0      |
| nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan     | nan     | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan     | nan     | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan     | nan     | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:-1.642e-05/SW:1.986e-01/MR:2.318e+01/SR:2.852e+00/MeD:2.068e+00/MaD:2.171e+01/MW:0.541/MAW:0.459
|        0 |        1 |        2 |        3 |        4 |        5 |       6 |        7 |        8 |        9 |       10 |       11 |       12 |       13 |       14 |       15 |       16 |       17 |       18 |       19 |       20 |       21 |       22 |       23 |       24 |       25 |      26 |       27 |       28 |       29 |
|----------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------|
|   0.0425 |   0.0496 |   0.0488 |   0.0475 |   0.0485 |   0.0433 |   0.049 |   0.0481 |   0.0313 |   0.0497 |   0.0465 |   0.0465 |   0.0487 |   0.0494 |   0.0474 |   0.0506 |   0.0493 |   0.0487 |   0.0481 |   0.0503 |   0.0486 |   0.0452 |   0.0493 |   0.0476 |   0.0464 |   0.0425 |   0.049 |   0.0492 |   0.0486 |   0.0478 |
|  19.05   |  25.56   |  24.78   |  23.55   |  24.48   |  19.79   |  25.05  |  24.17   |  10.8    |  25.68   |  22.62   |  22.59   |  24.67   |  25.36   |  23.49   |  26.58   |  25.35   |  24.7    |  24.12   |  26.26   |  24.66   |  21.4    |  25.31   |  23.65   |  22.57   |  19.07   |  24.99  |  25.2    |  24.64   |  23.87   |
|   0.05   |   0.13   |   0.05   |   0.03   |   0.02   |   0.04   |   0.04  |   0.01   |   0.03   |   0      |   0.02   |   0.06   |   0.02   |   0.03   |   0.02   |   0.02   |   0.02   |   0.01   |   0.01   |   0.02   |   0.01   |   0.07   |   0.01   |   0.01   |   0.01   |   0.02   |   0.02  |   0.01   |   0.03   |   0.03   |
| nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:8.349e-05/SW:4.075e-01/MR:2.382e+01/SR:2.614e+00/MeD:2.061e+00/MaD:1.030e+01/MW:0.503/MAW:0.497
|        0 |        1 |       2 |        3 |        4 |        5 |        6 |        7 |        8 |        9 |       10 |       11 |      12 |       13 |      14 |       15 |       16 |       17 |       18 |       19 |       20 |       21 |       22 |      23 |       24 |       25 |       26 |      27 |       28 |       29 |
|----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------+---------+----------+----------|
|   0.0494 |   0.0487 |   0.048 |   0.0459 |   0.0443 |   0.0485 |   0.0501 |   0.0427 |   0.0492 |   0.0494 |   0.0485 |   0.0488 |   0.051 |   0.0404 |   0.053 |   0.0475 |   0.0436 |   0.0495 |   0.0463 |   0.0495 |   0.0421 |   0.0486 |   0.0487 |   0.048 |   0.0478 |   0.0482 |   0.0492 |   0.045 |   0.0424 |   0.0481 |
|  25.41   |  24.76   |  24.06  |  22.07   |  20.67   |  24.48   |  26.06   |  19.2    |  25.25   |  25.36   |  24.56   |  24.81   |  27.03  |  17.33   |  29.1   |  23.59   |  19.98   |  25.55   |  22.41   |  25.54   |  18.76   |  24.63   |  24.67   |  24.04  |  23.83   |  24.23   |  25.18   |  21.28  |  18.95   |  24.14   |
|   0      |   0.02   |   0.06  |   0.04   |   0.05   |   0.04   |   0.01   |   0.05   |   0.05   |   0      |   0.03   |   0.08   |   0.09  |   0.11   |   0.46  |   0.05   |   0.11   |   0.01   |   0.02   |   0.01   |   0.08   |   0.05   |   0.2    |   0.02  |   0.03   |   0.07   |   0.03   |   0.03  |   0.04   |   0.02   |
| nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan     | nan      | nan      |
| nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan     | nan      | nan      |
| nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan     | nan      | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:-3.140e-03/SW:7.398e-01/MR:2.156e+01/SR:2.806e+00/MeD:2.312e+00/MaD:8.527e+00/MW:0.760/MAW:0.240
|        0 |        1 |        2 |        3 |        4 |        5 |        6 |        7 |        8 |        9 |       10 |       11 |       12 |       13 |       14 |       15 |       16 |       17 |       18 |       19 |       20 |       21 |       22 |       23 |       24 |       25 |       26 |       27 |       28 |       29 |
|----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------|
|   0.0229 |   0.0243 |   0.0241 |   0.0239 |   0.0183 |   0.0212 |   0.0238 |   0.0235 |   0.0239 |   0.0222 |   0.0247 |   0.0233 |   0.0239 |   0.0235 |   0.0242 |   0.0236 |   0.0232 |   0.0207 |   0.0224 |   0.0221 |   0.0222 |   0.0248 |   0.0226 |   0.0222 |   0.0231 |   0.0228 |   0.0212 |   0.0229 |   0.0218 |   0.0246 |
|  22.01   |  24.58   |  24.25   |  23.88   |  14.33   |  18.96   |  23.68   |  23.13   |  23.84   |  20.77   |  25.39   |  22.69   |  23.91   |  23.07   |  24.35   |  23.19   |  22.49   |  18.09   |  21.11   |  20.55   |  20.64   |  25.53   |  21.44   |  20.65   |  22.27   |  21.87   |  19.01   |  21.97   |  20.01   |  25.21   |
|   0.13   |   0.07   |   0.02   |   0.04   |   0.47   |   0.12   |   0.14   |   0.2    |   0.06   |   0.24   |   0.03   |   0.1    |   0.14   |   0.21   |   0.04   |   0.12   |   0.1    |   0.28   |   0.19   |   0.29   |   0.15   |   0.04   |   0.42   |   0.27   |   0.11   |   0.26   |   0.57   |   0.18   |   0.24   |   0.1    |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |

 ************************************************************** 
MB:0.000e+00/SB:0.000e+00/MW:-6.297e-03/SW:8.852e-01/MR:7.535e+00/SR:1.422e+00/MeD:1.170e+00/MaD:4.182e+00/MW:0.564/MAW:0.436
|       0 |       1 |       2 |       3 |       4 |       5 |       6 |       7 |       8 |       9 |      10 |      11 |      12 |      13 |      14 |      15 |      16 |      17 |      18 |     19 |      20 |      21 |     22 |      23 |      24 |      25 |      26 |      27 |      28 |      29 |
|---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------|
|   0.179 |   0.239 |   0.228 |   0.225 |   0.182 |   0.173 |   0.196 |   0.244 |   0.185 |   0.262 |   0.185 |   0.191 |   0.207 |   0.219 |   0.194 |   0.227 |   0.164 |   0.195 |   0.206 |   0.2  |   0.219 |   0.199 |   0.21 |   0.239 |   0.226 |   0.213 |   0.214 |   0.209 |   0.194 |   0.241 |
|   6     |   9.93  |   9.1   |   8.89  |   6.19  |   5.68  |   7     |  10.32  |   6.36  |  11.72  |   6.37  |   6.7   |   7.67  |   8.5   |   6.88  |   9.04  |   5.23  |   6.95  |   7.66  |   7.23 |   8.51  |   7.2   |   7.91 |   9.91  |   8.99  |   8.06  |   8.16  |   7.84  |   6.86  |  10.09  |
|   0.51  |   0.39  |   0.29  |   0.42  |   1.36  |   3.53  |   0.35  |   0.51  |   0.97  |   0.28  |   0.6   |   0.42  |   0.46  |   0.28  |   0.94  |   0.42  |   3.63  |   0.38  |   0.62  |   1    |   0.52  |   0.63  |   0.37 |   0.41  |   0.41  |  20.09  |   0.29  |   0.61  |   0.34  |   0.37  |
| nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     |
| nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     |
| nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c/models
SAVED HEADS THRESHOLD:  0.91625
SAVED model.topk_kernels {'conv0': [25, 59, 30, 5, 69, 42, 55, 68, 16, 82, 77, 53, 14, 35, 89], 'conv1': [304, 137, 75, 345, 349, 197, 121, 22, 365, 336, 146, 202, 56, 100, 4, 67, 266, 163, 296, 354, 50, 159, 41, 347, 343, 180, 118, 190, 92, 94, 104, 76, 324, 62, 172, 191, 274, 200, 186, 217, 36, 105, 330, 128, 73, 21, 254, 357, 133, 169, 6, 326, 206, 193, 99, 380, 382, 88, 262], 'conv2': [958, 1278, 761, 1436, 856, 95, 14, 1385, 733, 405, 622, 12, 307, 499, 1227, 1274, 1345, 869, 348, 612, 1452, 176, 1532, 250, 632, 1113, 312, 915, 423, 707, 406, 1508, 1291, 542, 942, 1317, 70, 743, 1442, 75, 544, 311, 574, 904, 961, 616, 959, 35, 801, 937, 490, 670, 1297, 994, 790, 434, 1450, 523, 450, 698, 1232, 936, 850, 636, 93, 164, 243, 22, 114, 911, 1238, 626, 1388, 350, 74, 1282, 1281, 1203, 913, 580, 238, 597, 1159, 774, 77, 889, 1025, 1086, 1486, 347, 868, 461, 783, 373, 905, 675, 639, 1245, 67, 359, 100, 673, 281, 1430, 1206, 705, 1101, 1089, 174, 1429, 662, 458, 810, 835, 1006, 1172, 923, 61, 852, 1185, 462, 667, 599, 41, 754, 395, 360, 855, 290, 728, 309, 25, 439, 1171, 1292, 457, 56, 875, 135, 13, 629, 148, 1424, 1002, 138, 1497, 1125, 1401, 614, 538, 964, 824, 1302, 441, 1294, 809, 1437, 1325, 246, 1100, 1186, 745, 324, 313, 356, 731, 276, 167, 335, 105, 212, 456, 851, 649, 596, 1192, 294, 118, 566, 133, 1248, 326, 584, 436, 1116, 447, 15, 933, 102, 818, 1046, 732, 195, 132, 233, 519, 1001, 1522, 674, 1022, 425, 1448, 688, 891, 1494, 1033, 886, 650, 508, 1307, 66, 808, 563, 1324, 872, 134, 110, 413, 209, 119, 1516, 779, 1266, 846, 1178, 717, 955, 215, 799, 973, 1032], 'conv3': [505, 1755, 1843, 1403, 1783, 1554, 1938, 646, 421, 1447, 792, 564, 2241, 1226, 2460, 472, 1424, 507, 2050, 1958, 1198, 419, 2414, 653, 2305, 195, 1720, 497, 2098, 129, 1789, 2428, 2, 534, 454, 1608, 1451, 96, 299, 398, 953, 2233, 1894, 1266, 1926, 878, 1698, 1448, 689, 304, 1303, 2453, 499, 1887, 305, 1385, 2093, 1548, 1406, 456, 1567, 404, 233, 943, 1753, 533, 935, 1162, 2161, 1271, 2154, 586, 2309, 922, 862, 2300, 658, 1873, 1506, 1394, 390, 1364, 861, 627, 2000, 1697, 772, 2185, 2051, 1992, 2162, 2511, 1831, 1677, 425, 1409, 370, 509, 2112, 2227, 561, 110, 1799, 2223, 2022, 199, 492, 1638, 1236, 1021, 2500, 2362, 1436, 1080, 774, 1304, 347, 555, 2254, 436, 1458, 2164, 2278, 2302, 2062, 1076, 2523, 1599, 1088, 657, 923, 1175, 2056, 2018, 2001, 1524, 708, 1647, 2265, 1612, 1051, 1109, 1839, 1227, 122, 2094, 2469, 674, 1796, 1446, 284, 331, 453, 1845, 2042, 1157, 720, 2057, 1375, 1457, 632, 2007, 2151, 249, 696, 2085, 964, 2245, 1405, 2070, 2114, 1074, 757, 51, 164, 1427, 431, 1811, 1510, 1325, 1138, 867, 402, 2420, 1248, 2181, 1564, 2456, 450, 68, 2270, 2315, 761, 1993, 340, 2106, 1809, 291, 80, 2252, 1578, 1781, 748, 2323, 102, 846, 262, 1337, 146, 1699, 2354, 912, 527, 895, 788, 2328, 746, 178, 638, 1221, 724, 2167, 1128, 1536, 735, 1003, 333, 240, 1985, 1253, 737, 715, 2021, 2240, 2179, 2317, 2342, 2044, 2011, 639, 1890, 1380, 780, 1045, 2004, 448, 1733, 231, 980, 1439, 283, 365, 944, 1646, 60, 2427, 1363, 1642, 1348, 1982, 1696, 605, 2341, 2152, 1998, 1341, 252, 726, 1920, 2160, 2250, 2340, 1968, 1867, 2095, 2352, 2248, 2249, 810, 2081, 1830, 723, 104, 987, 54, 1283, 1824, 1433, 1104, 2479, 1219, 2084, 378, 1340, 225, 1402, 2297, 2502, 1484, 1376, 339, 1220, 928, 626, 687, 2322, 616, 1545, 297, 1435, 758, 1676, 1555, 2514, 677, 742, 583, 694, 1689, 278, 903, 1690, 1013, 35, 927, 1362, 2236, 177, 1742, 1851, 156, 277, 346, 2173, 2221, 312, 52, 937, 69, 1874, 752, 1070, 2087, 1619, 2559, 1346, 1891, 1492, 969, 1002, 77, 1351, 2542, 753, 1267, 1056, 1210, 1187, 2174, 242, 1801, 2259, 46, 1670, 2079, 200, 1865, 13, 2242, 1353, 1928, 1044, 1411, 1614, 699, 1844, 1053, 72, 1645, 1356, 89, 884, 2551, 1287, 1961], 'conv4': [1118, 1323, 1657, 557, 888, 1143, 530, 966, 605, 91, 1504, 426, 1167, 2265, 2388, 1233, 1957, 99, 1366, 1970, 2385, 2531, 93, 382, 2393, 1706, 2457, 2559, 674, 2121, 1869, 402, 1250, 770, 6, 1883, 1406, 778, 30, 2335, 172, 315, 1063, 883, 386, 1713, 2218, 70, 2211, 1375, 455, 499, 2060, 2440, 1428, 161, 2415, 757, 703, 654, 139, 2171, 2053, 1066, 1064, 326, 1099, 1471, 15, 1387, 140, 621, 2034, 1373, 607, 534, 1602, 1750, 1090, 1289, 774, 2510, 1999, 2195, 270, 1731, 1273, 1928, 2143, 2250, 1787, 400, 2532, 1556, 1912, 2475, 1513, 1704, 1284, 448, 625, 1676, 1730, 1434, 1607, 3, 612, 1034, 193, 1442, 208, 843, 1241, 1489, 2268, 825, 1384, 1981, 296, 1877, 177, 1298, 1880, 2141, 969, 7, 1477, 1313, 806, 798, 2015, 2337, 1622, 2395, 905, 2286, 1402, 412, 1908, 1084, 2138, 328, 1684, 556, 34, 2410, 2285, 2025, 663, 150, 1252, 0, 549, 2066, 792, 8, 366, 112, 1201, 2070, 704, 2099, 2302, 548, 482, 1982, 1397, 1640, 889, 2191, 2189, 1306, 1326, 2558, 503, 263, 1071, 2436, 1246, 2226, 789, 666, 2081, 593, 718, 544, 568, 1080, 364, 2485, 615, 1618, 1597, 1035, 361, 1700, 233, 24, 2327, 765, 1987, 861, 2021, 628, 1571, 1444, 1580, 2487, 2346, 2452, 2453, 54, 2158, 1452, 2343, 309, 1819, 1135, 1810, 1285, 1892, 2188, 1969, 2326, 1169, 494, 768, 1338, 5, 2228, 182, 1796, 1644, 924, 2432, 2112, 120, 815, 1583, 2207, 2480, 2205, 85, 1012, 1719, 2276, 904, 105, 1723, 1995, 2277, 2172, 2239, 1357, 637, 2085, 1040, 1922, 2512, 1230, 225, 658, 738, 1065, 242, 1468, 1639, 1734, 1210, 689, 788, 108, 1134, 2546, 207, 2413, 1523, 257, 1424, 2516, 671, 1124, 2134, 558, 335, 2224, 332, 1619, 1239, 926, 951, 891, 1445, 239, 493, 1609, 424, 449, 1732, 2311, 1852, 1759, 2254, 340, 1535, 2509, 1634, 447, 1572, 802, 1770, 395, 1030, 1411, 1645, 2212, 562, 311, 1420, 531, 1575, 1996, 502, 259, 202, 670, 2045, 682, 1390, 132, 2518, 565, 2356, 560, 190, 2220, 2149, 1418, 2079, 1355, 1109, 1113, 553, 2540, 2249, 1015, 2538, 564, 705, 214, 2525, 1993, 700, 341, 252, 430, 1937, 1140, 1878, 2011, 783, 1198, 86, 2507, 1275, 603, 1372, 1849, 1221, 2163, 604, 1522, 1339, 1558, 2517, 2535, 1496, 1204, 874, 1641, 1432, 895, 2492, 2271, 1467]}
SAVED model.heads_thresh 0.91625
CONFIG MODE:  supervised

 ********** Supervised learning of blocks [5] **********
SAVING FOLDER FOR SUP:  C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([49, 33, 72, 51, 71, 92, 15, 14, 23,  0, 71, 75, 81, 69, 40, 43, 92, 97,
        70, 53], device='cuda:0')
TARGETS AFTER SUB:  tensor([51, 51, 51, 79, 51, 79, 51, 79, 79, 79, 79, 51, 51, 51, 51, 51, 51, 51,
        79, 51])
[51, 79]
TARGETS AFTER CLEANER:  tensor([0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0])
200
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 200
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               None
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([19, 29,  0, 11,  1, 86, 90, 28, 23, 31, 39, 96, 82, 17, 71, 39,  8, 97,
        80, 71], device='cuda:0')
TARGETS AFTER SUB:  tensor([51, 51, 79, 79, 79, 79, 79, 51, 79, 51, 79, 51, 79, 51, 51, 79, 79, 51,
        79, 79])
[51, 79]
TARGETS AFTER CLEANER:  tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1])
1000
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 1000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               None
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([19, 29,  0, 11,  1, 86, 90, 28, 23, 31, 39, 96, 82, 17, 71, 39,  8, 97,
        80, 71], device='cuda:0')
TARGETS AFTER SUB:  tensor([51, 51, 79, 79, 79, 79, 79, 51, 79, 51, 79, 51, 79, 51, 51, 79, 79, 51,
        79, 79])
[51, 79]
TARGETS AFTER CLEANER:  tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1])
1000
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 1000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
------------------------
INDICES:  1000
IMAGE SIZE: torch.Size([64, 3, 32, 32])
num_blocks:  6
Epoch: [1/50]	lr: 1.00e-03	time: 00:00:30	Loss_train 0.07942	Acc_train 75.80	/	Loss_test 0.00993	Acc_test 83.50
BLOCKS IN SUP:  [5]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c/models
SAVED HEADS THRESHOLD:  0.91625
SAVED model.topk_kernels {'conv0': [25, 59, 30, 5, 69, 42, 55, 68, 16, 82, 77, 53, 14, 35, 89], 'conv1': [304, 137, 75, 345, 349, 197, 121, 22, 365, 336, 146, 202, 56, 100, 4, 67, 266, 163, 296, 354, 50, 159, 41, 347, 343, 180, 118, 190, 92, 94, 104, 76, 324, 62, 172, 191, 274, 200, 186, 217, 36, 105, 330, 128, 73, 21, 254, 357, 133, 169, 6, 326, 206, 193, 99, 380, 382, 88, 262], 'conv2': [958, 1278, 761, 1436, 856, 95, 14, 1385, 733, 405, 622, 12, 307, 499, 1227, 1274, 1345, 869, 348, 612, 1452, 176, 1532, 250, 632, 1113, 312, 915, 423, 707, 406, 1508, 1291, 542, 942, 1317, 70, 743, 1442, 75, 544, 311, 574, 904, 961, 616, 959, 35, 801, 937, 490, 670, 1297, 994, 790, 434, 1450, 523, 450, 698, 1232, 936, 850, 636, 93, 164, 243, 22, 114, 911, 1238, 626, 1388, 350, 74, 1282, 1281, 1203, 913, 580, 238, 597, 1159, 774, 77, 889, 1025, 1086, 1486, 347, 868, 461, 783, 373, 905, 675, 639, 1245, 67, 359, 100, 673, 281, 1430, 1206, 705, 1101, 1089, 174, 1429, 662, 458, 810, 835, 1006, 1172, 923, 61, 852, 1185, 462, 667, 599, 41, 754, 395, 360, 855, 290, 728, 309, 25, 439, 1171, 1292, 457, 56, 875, 135, 13, 629, 148, 1424, 1002, 138, 1497, 1125, 1401, 614, 538, 964, 824, 1302, 441, 1294, 809, 1437, 1325, 246, 1100, 1186, 745, 324, 313, 356, 731, 276, 167, 335, 105, 212, 456, 851, 649, 596, 1192, 294, 118, 566, 133, 1248, 326, 584, 436, 1116, 447, 15, 933, 102, 818, 1046, 732, 195, 132, 233, 519, 1001, 1522, 674, 1022, 425, 1448, 688, 891, 1494, 1033, 886, 650, 508, 1307, 66, 808, 563, 1324, 872, 134, 110, 413, 209, 119, 1516, 779, 1266, 846, 1178, 717, 955, 215, 799, 973, 1032], 'conv3': [505, 1755, 1843, 1403, 1783, 1554, 1938, 646, 421, 1447, 792, 564, 2241, 1226, 2460, 472, 1424, 507, 2050, 1958, 1198, 419, 2414, 653, 2305, 195, 1720, 497, 2098, 129, 1789, 2428, 2, 534, 454, 1608, 1451, 96, 299, 398, 953, 2233, 1894, 1266, 1926, 878, 1698, 1448, 689, 304, 1303, 2453, 499, 1887, 305, 1385, 2093, 1548, 1406, 456, 1567, 404, 233, 943, 1753, 533, 935, 1162, 2161, 1271, 2154, 586, 2309, 922, 862, 2300, 658, 1873, 1506, 1394, 390, 1364, 861, 627, 2000, 1697, 772, 2185, 2051, 1992, 2162, 2511, 1831, 1677, 425, 1409, 370, 509, 2112, 2227, 561, 110, 1799, 2223, 2022, 199, 492, 1638, 1236, 1021, 2500, 2362, 1436, 1080, 774, 1304, 347, 555, 2254, 436, 1458, 2164, 2278, 2302, 2062, 1076, 2523, 1599, 1088, 657, 923, 1175, 2056, 2018, 2001, 1524, 708, 1647, 2265, 1612, 1051, 1109, 1839, 1227, 122, 2094, 2469, 674, 1796, 1446, 284, 331, 453, 1845, 2042, 1157, 720, 2057, 1375, 1457, 632, 2007, 2151, 249, 696, 2085, 964, 2245, 1405, 2070, 2114, 1074, 757, 51, 164, 1427, 431, 1811, 1510, 1325, 1138, 867, 402, 2420, 1248, 2181, 1564, 2456, 450, 68, 2270, 2315, 761, 1993, 340, 2106, 1809, 291, 80, 2252, 1578, 1781, 748, 2323, 102, 846, 262, 1337, 146, 1699, 2354, 912, 527, 895, 788, 2328, 746, 178, 638, 1221, 724, 2167, 1128, 1536, 735, 1003, 333, 240, 1985, 1253, 737, 715, 2021, 2240, 2179, 2317, 2342, 2044, 2011, 639, 1890, 1380, 780, 1045, 2004, 448, 1733, 231, 980, 1439, 283, 365, 944, 1646, 60, 2427, 1363, 1642, 1348, 1982, 1696, 605, 2341, 2152, 1998, 1341, 252, 726, 1920, 2160, 2250, 2340, 1968, 1867, 2095, 2352, 2248, 2249, 810, 2081, 1830, 723, 104, 987, 54, 1283, 1824, 1433, 1104, 2479, 1219, 2084, 378, 1340, 225, 1402, 2297, 2502, 1484, 1376, 339, 1220, 928, 626, 687, 2322, 616, 1545, 297, 1435, 758, 1676, 1555, 2514, 677, 742, 583, 694, 1689, 278, 903, 1690, 1013, 35, 927, 1362, 2236, 177, 1742, 1851, 156, 277, 346, 2173, 2221, 312, 52, 937, 69, 1874, 752, 1070, 2087, 1619, 2559, 1346, 1891, 1492, 969, 1002, 77, 1351, 2542, 753, 1267, 1056, 1210, 1187, 2174, 242, 1801, 2259, 46, 1670, 2079, 200, 1865, 13, 2242, 1353, 1928, 1044, 1411, 1614, 699, 1844, 1053, 72, 1645, 1356, 89, 884, 2551, 1287, 1961], 'conv4': [1118, 1323, 1657, 557, 888, 1143, 530, 966, 605, 91, 1504, 426, 1167, 2265, 2388, 1233, 1957, 99, 1366, 1970, 2385, 2531, 93, 382, 2393, 1706, 2457, 2559, 674, 2121, 1869, 402, 1250, 770, 6, 1883, 1406, 778, 30, 2335, 172, 315, 1063, 883, 386, 1713, 2218, 70, 2211, 1375, 455, 499, 2060, 2440, 1428, 161, 2415, 757, 703, 654, 139, 2171, 2053, 1066, 1064, 326, 1099, 1471, 15, 1387, 140, 621, 2034, 1373, 607, 534, 1602, 1750, 1090, 1289, 774, 2510, 1999, 2195, 270, 1731, 1273, 1928, 2143, 2250, 1787, 400, 2532, 1556, 1912, 2475, 1513, 1704, 1284, 448, 625, 1676, 1730, 1434, 1607, 3, 612, 1034, 193, 1442, 208, 843, 1241, 1489, 2268, 825, 1384, 1981, 296, 1877, 177, 1298, 1880, 2141, 969, 7, 1477, 1313, 806, 798, 2015, 2337, 1622, 2395, 905, 2286, 1402, 412, 1908, 1084, 2138, 328, 1684, 556, 34, 2410, 2285, 2025, 663, 150, 1252, 0, 549, 2066, 792, 8, 366, 112, 1201, 2070, 704, 2099, 2302, 548, 482, 1982, 1397, 1640, 889, 2191, 2189, 1306, 1326, 2558, 503, 263, 1071, 2436, 1246, 2226, 789, 666, 2081, 593, 718, 544, 568, 1080, 364, 2485, 615, 1618, 1597, 1035, 361, 1700, 233, 24, 2327, 765, 1987, 861, 2021, 628, 1571, 1444, 1580, 2487, 2346, 2452, 2453, 54, 2158, 1452, 2343, 309, 1819, 1135, 1810, 1285, 1892, 2188, 1969, 2326, 1169, 494, 768, 1338, 5, 2228, 182, 1796, 1644, 924, 2432, 2112, 120, 815, 1583, 2207, 2480, 2205, 85, 1012, 1719, 2276, 904, 105, 1723, 1995, 2277, 2172, 2239, 1357, 637, 2085, 1040, 1922, 2512, 1230, 225, 658, 738, 1065, 242, 1468, 1639, 1734, 1210, 689, 788, 108, 1134, 2546, 207, 2413, 1523, 257, 1424, 2516, 671, 1124, 2134, 558, 335, 2224, 332, 1619, 1239, 926, 951, 891, 1445, 239, 493, 1609, 424, 449, 1732, 2311, 1852, 1759, 2254, 340, 1535, 2509, 1634, 447, 1572, 802, 1770, 395, 1030, 1411, 1645, 2212, 562, 311, 1420, 531, 1575, 1996, 502, 259, 202, 670, 2045, 682, 1390, 132, 2518, 565, 2356, 560, 190, 2220, 2149, 1418, 2079, 1355, 1109, 1113, 553, 2540, 2249, 1015, 2538, 564, 705, 214, 2525, 1993, 700, 341, 252, 430, 1937, 1140, 1878, 2011, 783, 1198, 86, 2507, 1275, 603, 1372, 1849, 1221, 2163, 604, 1522, 1339, 1558, 2517, 2535, 1496, 1204, 874, 1641, 1432, 895, 2492, 2271, 1467]}
SAVED model.heads_thresh 0.91625
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
Epoch: [10/50]	lr: 1.00e-03	time: 00:00:32	Loss_train 0.03255	Acc_train 85.67	/	Loss_test 0.00750	Acc_test 88.50
BLOCKS IN SUP:  [5]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c/models
SAVED HEADS THRESHOLD:  0.91625
SAVED model.topk_kernels {'conv0': [25, 59, 30, 5, 69, 42, 55, 68, 16, 82, 77, 53, 14, 35, 89], 'conv1': [304, 137, 75, 345, 349, 197, 121, 22, 365, 336, 146, 202, 56, 100, 4, 67, 266, 163, 296, 354, 50, 159, 41, 347, 343, 180, 118, 190, 92, 94, 104, 76, 324, 62, 172, 191, 274, 200, 186, 217, 36, 105, 330, 128, 73, 21, 254, 357, 133, 169, 6, 326, 206, 193, 99, 380, 382, 88, 262], 'conv2': [958, 1278, 761, 1436, 856, 95, 14, 1385, 733, 405, 622, 12, 307, 499, 1227, 1274, 1345, 869, 348, 612, 1452, 176, 1532, 250, 632, 1113, 312, 915, 423, 707, 406, 1508, 1291, 542, 942, 1317, 70, 743, 1442, 75, 544, 311, 574, 904, 961, 616, 959, 35, 801, 937, 490, 670, 1297, 994, 790, 434, 1450, 523, 450, 698, 1232, 936, 850, 636, 93, 164, 243, 22, 114, 911, 1238, 626, 1388, 350, 74, 1282, 1281, 1203, 913, 580, 238, 597, 1159, 774, 77, 889, 1025, 1086, 1486, 347, 868, 461, 783, 373, 905, 675, 639, 1245, 67, 359, 100, 673, 281, 1430, 1206, 705, 1101, 1089, 174, 1429, 662, 458, 810, 835, 1006, 1172, 923, 61, 852, 1185, 462, 667, 599, 41, 754, 395, 360, 855, 290, 728, 309, 25, 439, 1171, 1292, 457, 56, 875, 135, 13, 629, 148, 1424, 1002, 138, 1497, 1125, 1401, 614, 538, 964, 824, 1302, 441, 1294, 809, 1437, 1325, 246, 1100, 1186, 745, 324, 313, 356, 731, 276, 167, 335, 105, 212, 456, 851, 649, 596, 1192, 294, 118, 566, 133, 1248, 326, 584, 436, 1116, 447, 15, 933, 102, 818, 1046, 732, 195, 132, 233, 519, 1001, 1522, 674, 1022, 425, 1448, 688, 891, 1494, 1033, 886, 650, 508, 1307, 66, 808, 563, 1324, 872, 134, 110, 413, 209, 119, 1516, 779, 1266, 846, 1178, 717, 955, 215, 799, 973, 1032], 'conv3': [505, 1755, 1843, 1403, 1783, 1554, 1938, 646, 421, 1447, 792, 564, 2241, 1226, 2460, 472, 1424, 507, 2050, 1958, 1198, 419, 2414, 653, 2305, 195, 1720, 497, 2098, 129, 1789, 2428, 2, 534, 454, 1608, 1451, 96, 299, 398, 953, 2233, 1894, 1266, 1926, 878, 1698, 1448, 689, 304, 1303, 2453, 499, 1887, 305, 1385, 2093, 1548, 1406, 456, 1567, 404, 233, 943, 1753, 533, 935, 1162, 2161, 1271, 2154, 586, 2309, 922, 862, 2300, 658, 1873, 1506, 1394, 390, 1364, 861, 627, 2000, 1697, 772, 2185, 2051, 1992, 2162, 2511, 1831, 1677, 425, 1409, 370, 509, 2112, 2227, 561, 110, 1799, 2223, 2022, 199, 492, 1638, 1236, 1021, 2500, 2362, 1436, 1080, 774, 1304, 347, 555, 2254, 436, 1458, 2164, 2278, 2302, 2062, 1076, 2523, 1599, 1088, 657, 923, 1175, 2056, 2018, 2001, 1524, 708, 1647, 2265, 1612, 1051, 1109, 1839, 1227, 122, 2094, 2469, 674, 1796, 1446, 284, 331, 453, 1845, 2042, 1157, 720, 2057, 1375, 1457, 632, 2007, 2151, 249, 696, 2085, 964, 2245, 1405, 2070, 2114, 1074, 757, 51, 164, 1427, 431, 1811, 1510, 1325, 1138, 867, 402, 2420, 1248, 2181, 1564, 2456, 450, 68, 2270, 2315, 761, 1993, 340, 2106, 1809, 291, 80, 2252, 1578, 1781, 748, 2323, 102, 846, 262, 1337, 146, 1699, 2354, 912, 527, 895, 788, 2328, 746, 178, 638, 1221, 724, 2167, 1128, 1536, 735, 1003, 333, 240, 1985, 1253, 737, 715, 2021, 2240, 2179, 2317, 2342, 2044, 2011, 639, 1890, 1380, 780, 1045, 2004, 448, 1733, 231, 980, 1439, 283, 365, 944, 1646, 60, 2427, 1363, 1642, 1348, 1982, 1696, 605, 2341, 2152, 1998, 1341, 252, 726, 1920, 2160, 2250, 2340, 1968, 1867, 2095, 2352, 2248, 2249, 810, 2081, 1830, 723, 104, 987, 54, 1283, 1824, 1433, 1104, 2479, 1219, 2084, 378, 1340, 225, 1402, 2297, 2502, 1484, 1376, 339, 1220, 928, 626, 687, 2322, 616, 1545, 297, 1435, 758, 1676, 1555, 2514, 677, 742, 583, 694, 1689, 278, 903, 1690, 1013, 35, 927, 1362, 2236, 177, 1742, 1851, 156, 277, 346, 2173, 2221, 312, 52, 937, 69, 1874, 752, 1070, 2087, 1619, 2559, 1346, 1891, 1492, 969, 1002, 77, 1351, 2542, 753, 1267, 1056, 1210, 1187, 2174, 242, 1801, 2259, 46, 1670, 2079, 200, 1865, 13, 2242, 1353, 1928, 1044, 1411, 1614, 699, 1844, 1053, 72, 1645, 1356, 89, 884, 2551, 1287, 1961], 'conv4': [1118, 1323, 1657, 557, 888, 1143, 530, 966, 605, 91, 1504, 426, 1167, 2265, 2388, 1233, 1957, 99, 1366, 1970, 2385, 2531, 93, 382, 2393, 1706, 2457, 2559, 674, 2121, 1869, 402, 1250, 770, 6, 1883, 1406, 778, 30, 2335, 172, 315, 1063, 883, 386, 1713, 2218, 70, 2211, 1375, 455, 499, 2060, 2440, 1428, 161, 2415, 757, 703, 654, 139, 2171, 2053, 1066, 1064, 326, 1099, 1471, 15, 1387, 140, 621, 2034, 1373, 607, 534, 1602, 1750, 1090, 1289, 774, 2510, 1999, 2195, 270, 1731, 1273, 1928, 2143, 2250, 1787, 400, 2532, 1556, 1912, 2475, 1513, 1704, 1284, 448, 625, 1676, 1730, 1434, 1607, 3, 612, 1034, 193, 1442, 208, 843, 1241, 1489, 2268, 825, 1384, 1981, 296, 1877, 177, 1298, 1880, 2141, 969, 7, 1477, 1313, 806, 798, 2015, 2337, 1622, 2395, 905, 2286, 1402, 412, 1908, 1084, 2138, 328, 1684, 556, 34, 2410, 2285, 2025, 663, 150, 1252, 0, 549, 2066, 792, 8, 366, 112, 1201, 2070, 704, 2099, 2302, 548, 482, 1982, 1397, 1640, 889, 2191, 2189, 1306, 1326, 2558, 503, 263, 1071, 2436, 1246, 2226, 789, 666, 2081, 593, 718, 544, 568, 1080, 364, 2485, 615, 1618, 1597, 1035, 361, 1700, 233, 24, 2327, 765, 1987, 861, 2021, 628, 1571, 1444, 1580, 2487, 2346, 2452, 2453, 54, 2158, 1452, 2343, 309, 1819, 1135, 1810, 1285, 1892, 2188, 1969, 2326, 1169, 494, 768, 1338, 5, 2228, 182, 1796, 1644, 924, 2432, 2112, 120, 815, 1583, 2207, 2480, 2205, 85, 1012, 1719, 2276, 904, 105, 1723, 1995, 2277, 2172, 2239, 1357, 637, 2085, 1040, 1922, 2512, 1230, 225, 658, 738, 1065, 242, 1468, 1639, 1734, 1210, 689, 788, 108, 1134, 2546, 207, 2413, 1523, 257, 1424, 2516, 671, 1124, 2134, 558, 335, 2224, 332, 1619, 1239, 926, 951, 891, 1445, 239, 493, 1609, 424, 449, 1732, 2311, 1852, 1759, 2254, 340, 1535, 2509, 1634, 447, 1572, 802, 1770, 395, 1030, 1411, 1645, 2212, 562, 311, 1420, 531, 1575, 1996, 502, 259, 202, 670, 2045, 682, 1390, 132, 2518, 565, 2356, 560, 190, 2220, 2149, 1418, 2079, 1355, 1109, 1113, 553, 2540, 2249, 1015, 2538, 564, 705, 214, 2525, 1993, 700, 341, 252, 430, 1937, 1140, 1878, 2011, 783, 1198, 86, 2507, 1275, 603, 1372, 1849, 1221, 2163, 604, 1522, 1339, 1558, 2517, 2535, 1496, 1204, 874, 1641, 1432, 895, 2492, 2271, 1467]}
SAVED model.heads_thresh 0.91625
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
Epoch: [20/50]	lr: 2.50e-04	time: 00:00:34	Loss_train 0.01605	Acc_train 90.58	/	Loss_test 0.00638	Acc_test 87.50
BLOCKS IN SUP:  [5]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c/models
SAVED HEADS THRESHOLD:  0.91625
SAVED model.topk_kernels {'conv0': [25, 59, 30, 5, 69, 42, 55, 68, 16, 82, 77, 53, 14, 35, 89], 'conv1': [304, 137, 75, 345, 349, 197, 121, 22, 365, 336, 146, 202, 56, 100, 4, 67, 266, 163, 296, 354, 50, 159, 41, 347, 343, 180, 118, 190, 92, 94, 104, 76, 324, 62, 172, 191, 274, 200, 186, 217, 36, 105, 330, 128, 73, 21, 254, 357, 133, 169, 6, 326, 206, 193, 99, 380, 382, 88, 262], 'conv2': [958, 1278, 761, 1436, 856, 95, 14, 1385, 733, 405, 622, 12, 307, 499, 1227, 1274, 1345, 869, 348, 612, 1452, 176, 1532, 250, 632, 1113, 312, 915, 423, 707, 406, 1508, 1291, 542, 942, 1317, 70, 743, 1442, 75, 544, 311, 574, 904, 961, 616, 959, 35, 801, 937, 490, 670, 1297, 994, 790, 434, 1450, 523, 450, 698, 1232, 936, 850, 636, 93, 164, 243, 22, 114, 911, 1238, 626, 1388, 350, 74, 1282, 1281, 1203, 913, 580, 238, 597, 1159, 774, 77, 889, 1025, 1086, 1486, 347, 868, 461, 783, 373, 905, 675, 639, 1245, 67, 359, 100, 673, 281, 1430, 1206, 705, 1101, 1089, 174, 1429, 662, 458, 810, 835, 1006, 1172, 923, 61, 852, 1185, 462, 667, 599, 41, 754, 395, 360, 855, 290, 728, 309, 25, 439, 1171, 1292, 457, 56, 875, 135, 13, 629, 148, 1424, 1002, 138, 1497, 1125, 1401, 614, 538, 964, 824, 1302, 441, 1294, 809, 1437, 1325, 246, 1100, 1186, 745, 324, 313, 356, 731, 276, 167, 335, 105, 212, 456, 851, 649, 596, 1192, 294, 118, 566, 133, 1248, 326, 584, 436, 1116, 447, 15, 933, 102, 818, 1046, 732, 195, 132, 233, 519, 1001, 1522, 674, 1022, 425, 1448, 688, 891, 1494, 1033, 886, 650, 508, 1307, 66, 808, 563, 1324, 872, 134, 110, 413, 209, 119, 1516, 779, 1266, 846, 1178, 717, 955, 215, 799, 973, 1032], 'conv3': [505, 1755, 1843, 1403, 1783, 1554, 1938, 646, 421, 1447, 792, 564, 2241, 1226, 2460, 472, 1424, 507, 2050, 1958, 1198, 419, 2414, 653, 2305, 195, 1720, 497, 2098, 129, 1789, 2428, 2, 534, 454, 1608, 1451, 96, 299, 398, 953, 2233, 1894, 1266, 1926, 878, 1698, 1448, 689, 304, 1303, 2453, 499, 1887, 305, 1385, 2093, 1548, 1406, 456, 1567, 404, 233, 943, 1753, 533, 935, 1162, 2161, 1271, 2154, 586, 2309, 922, 862, 2300, 658, 1873, 1506, 1394, 390, 1364, 861, 627, 2000, 1697, 772, 2185, 2051, 1992, 2162, 2511, 1831, 1677, 425, 1409, 370, 509, 2112, 2227, 561, 110, 1799, 2223, 2022, 199, 492, 1638, 1236, 1021, 2500, 2362, 1436, 1080, 774, 1304, 347, 555, 2254, 436, 1458, 2164, 2278, 2302, 2062, 1076, 2523, 1599, 1088, 657, 923, 1175, 2056, 2018, 2001, 1524, 708, 1647, 2265, 1612, 1051, 1109, 1839, 1227, 122, 2094, 2469, 674, 1796, 1446, 284, 331, 453, 1845, 2042, 1157, 720, 2057, 1375, 1457, 632, 2007, 2151, 249, 696, 2085, 964, 2245, 1405, 2070, 2114, 1074, 757, 51, 164, 1427, 431, 1811, 1510, 1325, 1138, 867, 402, 2420, 1248, 2181, 1564, 2456, 450, 68, 2270, 2315, 761, 1993, 340, 2106, 1809, 291, 80, 2252, 1578, 1781, 748, 2323, 102, 846, 262, 1337, 146, 1699, 2354, 912, 527, 895, 788, 2328, 746, 178, 638, 1221, 724, 2167, 1128, 1536, 735, 1003, 333, 240, 1985, 1253, 737, 715, 2021, 2240, 2179, 2317, 2342, 2044, 2011, 639, 1890, 1380, 780, 1045, 2004, 448, 1733, 231, 980, 1439, 283, 365, 944, 1646, 60, 2427, 1363, 1642, 1348, 1982, 1696, 605, 2341, 2152, 1998, 1341, 252, 726, 1920, 2160, 2250, 2340, 1968, 1867, 2095, 2352, 2248, 2249, 810, 2081, 1830, 723, 104, 987, 54, 1283, 1824, 1433, 1104, 2479, 1219, 2084, 378, 1340, 225, 1402, 2297, 2502, 1484, 1376, 339, 1220, 928, 626, 687, 2322, 616, 1545, 297, 1435, 758, 1676, 1555, 2514, 677, 742, 583, 694, 1689, 278, 903, 1690, 1013, 35, 927, 1362, 2236, 177, 1742, 1851, 156, 277, 346, 2173, 2221, 312, 52, 937, 69, 1874, 752, 1070, 2087, 1619, 2559, 1346, 1891, 1492, 969, 1002, 77, 1351, 2542, 753, 1267, 1056, 1210, 1187, 2174, 242, 1801, 2259, 46, 1670, 2079, 200, 1865, 13, 2242, 1353, 1928, 1044, 1411, 1614, 699, 1844, 1053, 72, 1645, 1356, 89, 884, 2551, 1287, 1961], 'conv4': [1118, 1323, 1657, 557, 888, 1143, 530, 966, 605, 91, 1504, 426, 1167, 2265, 2388, 1233, 1957, 99, 1366, 1970, 2385, 2531, 93, 382, 2393, 1706, 2457, 2559, 674, 2121, 1869, 402, 1250, 770, 6, 1883, 1406, 778, 30, 2335, 172, 315, 1063, 883, 386, 1713, 2218, 70, 2211, 1375, 455, 499, 2060, 2440, 1428, 161, 2415, 757, 703, 654, 139, 2171, 2053, 1066, 1064, 326, 1099, 1471, 15, 1387, 140, 621, 2034, 1373, 607, 534, 1602, 1750, 1090, 1289, 774, 2510, 1999, 2195, 270, 1731, 1273, 1928, 2143, 2250, 1787, 400, 2532, 1556, 1912, 2475, 1513, 1704, 1284, 448, 625, 1676, 1730, 1434, 1607, 3, 612, 1034, 193, 1442, 208, 843, 1241, 1489, 2268, 825, 1384, 1981, 296, 1877, 177, 1298, 1880, 2141, 969, 7, 1477, 1313, 806, 798, 2015, 2337, 1622, 2395, 905, 2286, 1402, 412, 1908, 1084, 2138, 328, 1684, 556, 34, 2410, 2285, 2025, 663, 150, 1252, 0, 549, 2066, 792, 8, 366, 112, 1201, 2070, 704, 2099, 2302, 548, 482, 1982, 1397, 1640, 889, 2191, 2189, 1306, 1326, 2558, 503, 263, 1071, 2436, 1246, 2226, 789, 666, 2081, 593, 718, 544, 568, 1080, 364, 2485, 615, 1618, 1597, 1035, 361, 1700, 233, 24, 2327, 765, 1987, 861, 2021, 628, 1571, 1444, 1580, 2487, 2346, 2452, 2453, 54, 2158, 1452, 2343, 309, 1819, 1135, 1810, 1285, 1892, 2188, 1969, 2326, 1169, 494, 768, 1338, 5, 2228, 182, 1796, 1644, 924, 2432, 2112, 120, 815, 1583, 2207, 2480, 2205, 85, 1012, 1719, 2276, 904, 105, 1723, 1995, 2277, 2172, 2239, 1357, 637, 2085, 1040, 1922, 2512, 1230, 225, 658, 738, 1065, 242, 1468, 1639, 1734, 1210, 689, 788, 108, 1134, 2546, 207, 2413, 1523, 257, 1424, 2516, 671, 1124, 2134, 558, 335, 2224, 332, 1619, 1239, 926, 951, 891, 1445, 239, 493, 1609, 424, 449, 1732, 2311, 1852, 1759, 2254, 340, 1535, 2509, 1634, 447, 1572, 802, 1770, 395, 1030, 1411, 1645, 2212, 562, 311, 1420, 531, 1575, 1996, 502, 259, 202, 670, 2045, 682, 1390, 132, 2518, 565, 2356, 560, 190, 2220, 2149, 1418, 2079, 1355, 1109, 1113, 553, 2540, 2249, 1015, 2538, 564, 705, 214, 2525, 1993, 700, 341, 252, 430, 1937, 1140, 1878, 2011, 783, 1198, 86, 2507, 1275, 603, 1372, 1849, 1221, 2163, 604, 1522, 1339, 1558, 2517, 2535, 1496, 1204, 874, 1641, 1432, 895, 2492, 2271, 1467]}
SAVED model.heads_thresh 0.91625
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
Epoch: [30/50]	lr: 1.25e-04	time: 00:00:36	Loss_train 0.01323	Acc_train 91.52	/	Loss_test 0.00624	Acc_test 89.00
BLOCKS IN SUP:  [5]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c/models
SAVED HEADS THRESHOLD:  0.91625
SAVED model.topk_kernels {'conv0': [25, 59, 30, 5, 69, 42, 55, 68, 16, 82, 77, 53, 14, 35, 89], 'conv1': [304, 137, 75, 345, 349, 197, 121, 22, 365, 336, 146, 202, 56, 100, 4, 67, 266, 163, 296, 354, 50, 159, 41, 347, 343, 180, 118, 190, 92, 94, 104, 76, 324, 62, 172, 191, 274, 200, 186, 217, 36, 105, 330, 128, 73, 21, 254, 357, 133, 169, 6, 326, 206, 193, 99, 380, 382, 88, 262], 'conv2': [958, 1278, 761, 1436, 856, 95, 14, 1385, 733, 405, 622, 12, 307, 499, 1227, 1274, 1345, 869, 348, 612, 1452, 176, 1532, 250, 632, 1113, 312, 915, 423, 707, 406, 1508, 1291, 542, 942, 1317, 70, 743, 1442, 75, 544, 311, 574, 904, 961, 616, 959, 35, 801, 937, 490, 670, 1297, 994, 790, 434, 1450, 523, 450, 698, 1232, 936, 850, 636, 93, 164, 243, 22, 114, 911, 1238, 626, 1388, 350, 74, 1282, 1281, 1203, 913, 580, 238, 597, 1159, 774, 77, 889, 1025, 1086, 1486, 347, 868, 461, 783, 373, 905, 675, 639, 1245, 67, 359, 100, 673, 281, 1430, 1206, 705, 1101, 1089, 174, 1429, 662, 458, 810, 835, 1006, 1172, 923, 61, 852, 1185, 462, 667, 599, 41, 754, 395, 360, 855, 290, 728, 309, 25, 439, 1171, 1292, 457, 56, 875, 135, 13, 629, 148, 1424, 1002, 138, 1497, 1125, 1401, 614, 538, 964, 824, 1302, 441, 1294, 809, 1437, 1325, 246, 1100, 1186, 745, 324, 313, 356, 731, 276, 167, 335, 105, 212, 456, 851, 649, 596, 1192, 294, 118, 566, 133, 1248, 326, 584, 436, 1116, 447, 15, 933, 102, 818, 1046, 732, 195, 132, 233, 519, 1001, 1522, 674, 1022, 425, 1448, 688, 891, 1494, 1033, 886, 650, 508, 1307, 66, 808, 563, 1324, 872, 134, 110, 413, 209, 119, 1516, 779, 1266, 846, 1178, 717, 955, 215, 799, 973, 1032], 'conv3': [505, 1755, 1843, 1403, 1783, 1554, 1938, 646, 421, 1447, 792, 564, 2241, 1226, 2460, 472, 1424, 507, 2050, 1958, 1198, 419, 2414, 653, 2305, 195, 1720, 497, 2098, 129, 1789, 2428, 2, 534, 454, 1608, 1451, 96, 299, 398, 953, 2233, 1894, 1266, 1926, 878, 1698, 1448, 689, 304, 1303, 2453, 499, 1887, 305, 1385, 2093, 1548, 1406, 456, 1567, 404, 233, 943, 1753, 533, 935, 1162, 2161, 1271, 2154, 586, 2309, 922, 862, 2300, 658, 1873, 1506, 1394, 390, 1364, 861, 627, 2000, 1697, 772, 2185, 2051, 1992, 2162, 2511, 1831, 1677, 425, 1409, 370, 509, 2112, 2227, 561, 110, 1799, 2223, 2022, 199, 492, 1638, 1236, 1021, 2500, 2362, 1436, 1080, 774, 1304, 347, 555, 2254, 436, 1458, 2164, 2278, 2302, 2062, 1076, 2523, 1599, 1088, 657, 923, 1175, 2056, 2018, 2001, 1524, 708, 1647, 2265, 1612, 1051, 1109, 1839, 1227, 122, 2094, 2469, 674, 1796, 1446, 284, 331, 453, 1845, 2042, 1157, 720, 2057, 1375, 1457, 632, 2007, 2151, 249, 696, 2085, 964, 2245, 1405, 2070, 2114, 1074, 757, 51, 164, 1427, 431, 1811, 1510, 1325, 1138, 867, 402, 2420, 1248, 2181, 1564, 2456, 450, 68, 2270, 2315, 761, 1993, 340, 2106, 1809, 291, 80, 2252, 1578, 1781, 748, 2323, 102, 846, 262, 1337, 146, 1699, 2354, 912, 527, 895, 788, 2328, 746, 178, 638, 1221, 724, 2167, 1128, 1536, 735, 1003, 333, 240, 1985, 1253, 737, 715, 2021, 2240, 2179, 2317, 2342, 2044, 2011, 639, 1890, 1380, 780, 1045, 2004, 448, 1733, 231, 980, 1439, 283, 365, 944, 1646, 60, 2427, 1363, 1642, 1348, 1982, 1696, 605, 2341, 2152, 1998, 1341, 252, 726, 1920, 2160, 2250, 2340, 1968, 1867, 2095, 2352, 2248, 2249, 810, 2081, 1830, 723, 104, 987, 54, 1283, 1824, 1433, 1104, 2479, 1219, 2084, 378, 1340, 225, 1402, 2297, 2502, 1484, 1376, 339, 1220, 928, 626, 687, 2322, 616, 1545, 297, 1435, 758, 1676, 1555, 2514, 677, 742, 583, 694, 1689, 278, 903, 1690, 1013, 35, 927, 1362, 2236, 177, 1742, 1851, 156, 277, 346, 2173, 2221, 312, 52, 937, 69, 1874, 752, 1070, 2087, 1619, 2559, 1346, 1891, 1492, 969, 1002, 77, 1351, 2542, 753, 1267, 1056, 1210, 1187, 2174, 242, 1801, 2259, 46, 1670, 2079, 200, 1865, 13, 2242, 1353, 1928, 1044, 1411, 1614, 699, 1844, 1053, 72, 1645, 1356, 89, 884, 2551, 1287, 1961], 'conv4': [1118, 1323, 1657, 557, 888, 1143, 530, 966, 605, 91, 1504, 426, 1167, 2265, 2388, 1233, 1957, 99, 1366, 1970, 2385, 2531, 93, 382, 2393, 1706, 2457, 2559, 674, 2121, 1869, 402, 1250, 770, 6, 1883, 1406, 778, 30, 2335, 172, 315, 1063, 883, 386, 1713, 2218, 70, 2211, 1375, 455, 499, 2060, 2440, 1428, 161, 2415, 757, 703, 654, 139, 2171, 2053, 1066, 1064, 326, 1099, 1471, 15, 1387, 140, 621, 2034, 1373, 607, 534, 1602, 1750, 1090, 1289, 774, 2510, 1999, 2195, 270, 1731, 1273, 1928, 2143, 2250, 1787, 400, 2532, 1556, 1912, 2475, 1513, 1704, 1284, 448, 625, 1676, 1730, 1434, 1607, 3, 612, 1034, 193, 1442, 208, 843, 1241, 1489, 2268, 825, 1384, 1981, 296, 1877, 177, 1298, 1880, 2141, 969, 7, 1477, 1313, 806, 798, 2015, 2337, 1622, 2395, 905, 2286, 1402, 412, 1908, 1084, 2138, 328, 1684, 556, 34, 2410, 2285, 2025, 663, 150, 1252, 0, 549, 2066, 792, 8, 366, 112, 1201, 2070, 704, 2099, 2302, 548, 482, 1982, 1397, 1640, 889, 2191, 2189, 1306, 1326, 2558, 503, 263, 1071, 2436, 1246, 2226, 789, 666, 2081, 593, 718, 544, 568, 1080, 364, 2485, 615, 1618, 1597, 1035, 361, 1700, 233, 24, 2327, 765, 1987, 861, 2021, 628, 1571, 1444, 1580, 2487, 2346, 2452, 2453, 54, 2158, 1452, 2343, 309, 1819, 1135, 1810, 1285, 1892, 2188, 1969, 2326, 1169, 494, 768, 1338, 5, 2228, 182, 1796, 1644, 924, 2432, 2112, 120, 815, 1583, 2207, 2480, 2205, 85, 1012, 1719, 2276, 904, 105, 1723, 1995, 2277, 2172, 2239, 1357, 637, 2085, 1040, 1922, 2512, 1230, 225, 658, 738, 1065, 242, 1468, 1639, 1734, 1210, 689, 788, 108, 1134, 2546, 207, 2413, 1523, 257, 1424, 2516, 671, 1124, 2134, 558, 335, 2224, 332, 1619, 1239, 926, 951, 891, 1445, 239, 493, 1609, 424, 449, 1732, 2311, 1852, 1759, 2254, 340, 1535, 2509, 1634, 447, 1572, 802, 1770, 395, 1030, 1411, 1645, 2212, 562, 311, 1420, 531, 1575, 1996, 502, 259, 202, 670, 2045, 682, 1390, 132, 2518, 565, 2356, 560, 190, 2220, 2149, 1418, 2079, 1355, 1109, 1113, 553, 2540, 2249, 1015, 2538, 564, 705, 214, 2525, 1993, 700, 341, 252, 430, 1937, 1140, 1878, 2011, 783, 1198, 86, 2507, 1275, 603, 1372, 1849, 1221, 2163, 604, 1522, 1339, 1558, 2517, 2535, 1496, 1204, 874, 1641, 1432, 895, 2492, 2271, 1467]}
SAVED model.heads_thresh 0.91625
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
Epoch: [40/50]	lr: 3.13e-05	time: 00:00:37	Loss_train 0.01088	Acc_train 92.06	/	Loss_test 0.00635	Acc_test 88.00
BLOCKS IN SUP:  [5]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c/models
SAVED HEADS THRESHOLD:  0.91625
SAVED model.topk_kernels {'conv0': [25, 59, 30, 5, 69, 42, 55, 68, 16, 82, 77, 53, 14, 35, 89], 'conv1': [304, 137, 75, 345, 349, 197, 121, 22, 365, 336, 146, 202, 56, 100, 4, 67, 266, 163, 296, 354, 50, 159, 41, 347, 343, 180, 118, 190, 92, 94, 104, 76, 324, 62, 172, 191, 274, 200, 186, 217, 36, 105, 330, 128, 73, 21, 254, 357, 133, 169, 6, 326, 206, 193, 99, 380, 382, 88, 262], 'conv2': [958, 1278, 761, 1436, 856, 95, 14, 1385, 733, 405, 622, 12, 307, 499, 1227, 1274, 1345, 869, 348, 612, 1452, 176, 1532, 250, 632, 1113, 312, 915, 423, 707, 406, 1508, 1291, 542, 942, 1317, 70, 743, 1442, 75, 544, 311, 574, 904, 961, 616, 959, 35, 801, 937, 490, 670, 1297, 994, 790, 434, 1450, 523, 450, 698, 1232, 936, 850, 636, 93, 164, 243, 22, 114, 911, 1238, 626, 1388, 350, 74, 1282, 1281, 1203, 913, 580, 238, 597, 1159, 774, 77, 889, 1025, 1086, 1486, 347, 868, 461, 783, 373, 905, 675, 639, 1245, 67, 359, 100, 673, 281, 1430, 1206, 705, 1101, 1089, 174, 1429, 662, 458, 810, 835, 1006, 1172, 923, 61, 852, 1185, 462, 667, 599, 41, 754, 395, 360, 855, 290, 728, 309, 25, 439, 1171, 1292, 457, 56, 875, 135, 13, 629, 148, 1424, 1002, 138, 1497, 1125, 1401, 614, 538, 964, 824, 1302, 441, 1294, 809, 1437, 1325, 246, 1100, 1186, 745, 324, 313, 356, 731, 276, 167, 335, 105, 212, 456, 851, 649, 596, 1192, 294, 118, 566, 133, 1248, 326, 584, 436, 1116, 447, 15, 933, 102, 818, 1046, 732, 195, 132, 233, 519, 1001, 1522, 674, 1022, 425, 1448, 688, 891, 1494, 1033, 886, 650, 508, 1307, 66, 808, 563, 1324, 872, 134, 110, 413, 209, 119, 1516, 779, 1266, 846, 1178, 717, 955, 215, 799, 973, 1032], 'conv3': [505, 1755, 1843, 1403, 1783, 1554, 1938, 646, 421, 1447, 792, 564, 2241, 1226, 2460, 472, 1424, 507, 2050, 1958, 1198, 419, 2414, 653, 2305, 195, 1720, 497, 2098, 129, 1789, 2428, 2, 534, 454, 1608, 1451, 96, 299, 398, 953, 2233, 1894, 1266, 1926, 878, 1698, 1448, 689, 304, 1303, 2453, 499, 1887, 305, 1385, 2093, 1548, 1406, 456, 1567, 404, 233, 943, 1753, 533, 935, 1162, 2161, 1271, 2154, 586, 2309, 922, 862, 2300, 658, 1873, 1506, 1394, 390, 1364, 861, 627, 2000, 1697, 772, 2185, 2051, 1992, 2162, 2511, 1831, 1677, 425, 1409, 370, 509, 2112, 2227, 561, 110, 1799, 2223, 2022, 199, 492, 1638, 1236, 1021, 2500, 2362, 1436, 1080, 774, 1304, 347, 555, 2254, 436, 1458, 2164, 2278, 2302, 2062, 1076, 2523, 1599, 1088, 657, 923, 1175, 2056, 2018, 2001, 1524, 708, 1647, 2265, 1612, 1051, 1109, 1839, 1227, 122, 2094, 2469, 674, 1796, 1446, 284, 331, 453, 1845, 2042, 1157, 720, 2057, 1375, 1457, 632, 2007, 2151, 249, 696, 2085, 964, 2245, 1405, 2070, 2114, 1074, 757, 51, 164, 1427, 431, 1811, 1510, 1325, 1138, 867, 402, 2420, 1248, 2181, 1564, 2456, 450, 68, 2270, 2315, 761, 1993, 340, 2106, 1809, 291, 80, 2252, 1578, 1781, 748, 2323, 102, 846, 262, 1337, 146, 1699, 2354, 912, 527, 895, 788, 2328, 746, 178, 638, 1221, 724, 2167, 1128, 1536, 735, 1003, 333, 240, 1985, 1253, 737, 715, 2021, 2240, 2179, 2317, 2342, 2044, 2011, 639, 1890, 1380, 780, 1045, 2004, 448, 1733, 231, 980, 1439, 283, 365, 944, 1646, 60, 2427, 1363, 1642, 1348, 1982, 1696, 605, 2341, 2152, 1998, 1341, 252, 726, 1920, 2160, 2250, 2340, 1968, 1867, 2095, 2352, 2248, 2249, 810, 2081, 1830, 723, 104, 987, 54, 1283, 1824, 1433, 1104, 2479, 1219, 2084, 378, 1340, 225, 1402, 2297, 2502, 1484, 1376, 339, 1220, 928, 626, 687, 2322, 616, 1545, 297, 1435, 758, 1676, 1555, 2514, 677, 742, 583, 694, 1689, 278, 903, 1690, 1013, 35, 927, 1362, 2236, 177, 1742, 1851, 156, 277, 346, 2173, 2221, 312, 52, 937, 69, 1874, 752, 1070, 2087, 1619, 2559, 1346, 1891, 1492, 969, 1002, 77, 1351, 2542, 753, 1267, 1056, 1210, 1187, 2174, 242, 1801, 2259, 46, 1670, 2079, 200, 1865, 13, 2242, 1353, 1928, 1044, 1411, 1614, 699, 1844, 1053, 72, 1645, 1356, 89, 884, 2551, 1287, 1961], 'conv4': [1118, 1323, 1657, 557, 888, 1143, 530, 966, 605, 91, 1504, 426, 1167, 2265, 2388, 1233, 1957, 99, 1366, 1970, 2385, 2531, 93, 382, 2393, 1706, 2457, 2559, 674, 2121, 1869, 402, 1250, 770, 6, 1883, 1406, 778, 30, 2335, 172, 315, 1063, 883, 386, 1713, 2218, 70, 2211, 1375, 455, 499, 2060, 2440, 1428, 161, 2415, 757, 703, 654, 139, 2171, 2053, 1066, 1064, 326, 1099, 1471, 15, 1387, 140, 621, 2034, 1373, 607, 534, 1602, 1750, 1090, 1289, 774, 2510, 1999, 2195, 270, 1731, 1273, 1928, 2143, 2250, 1787, 400, 2532, 1556, 1912, 2475, 1513, 1704, 1284, 448, 625, 1676, 1730, 1434, 1607, 3, 612, 1034, 193, 1442, 208, 843, 1241, 1489, 2268, 825, 1384, 1981, 296, 1877, 177, 1298, 1880, 2141, 969, 7, 1477, 1313, 806, 798, 2015, 2337, 1622, 2395, 905, 2286, 1402, 412, 1908, 1084, 2138, 328, 1684, 556, 34, 2410, 2285, 2025, 663, 150, 1252, 0, 549, 2066, 792, 8, 366, 112, 1201, 2070, 704, 2099, 2302, 548, 482, 1982, 1397, 1640, 889, 2191, 2189, 1306, 1326, 2558, 503, 263, 1071, 2436, 1246, 2226, 789, 666, 2081, 593, 718, 544, 568, 1080, 364, 2485, 615, 1618, 1597, 1035, 361, 1700, 233, 24, 2327, 765, 1987, 861, 2021, 628, 1571, 1444, 1580, 2487, 2346, 2452, 2453, 54, 2158, 1452, 2343, 309, 1819, 1135, 1810, 1285, 1892, 2188, 1969, 2326, 1169, 494, 768, 1338, 5, 2228, 182, 1796, 1644, 924, 2432, 2112, 120, 815, 1583, 2207, 2480, 2205, 85, 1012, 1719, 2276, 904, 105, 1723, 1995, 2277, 2172, 2239, 1357, 637, 2085, 1040, 1922, 2512, 1230, 225, 658, 738, 1065, 242, 1468, 1639, 1734, 1210, 689, 788, 108, 1134, 2546, 207, 2413, 1523, 257, 1424, 2516, 671, 1124, 2134, 558, 335, 2224, 332, 1619, 1239, 926, 951, 891, 1445, 239, 493, 1609, 424, 449, 1732, 2311, 1852, 1759, 2254, 340, 1535, 2509, 1634, 447, 1572, 802, 1770, 395, 1030, 1411, 1645, 2212, 562, 311, 1420, 531, 1575, 1996, 502, 259, 202, 670, 2045, 682, 1390, 132, 2518, 565, 2356, 560, 190, 2220, 2149, 1418, 2079, 1355, 1109, 1113, 553, 2540, 2249, 1015, 2538, 564, 705, 214, 2525, 1993, 700, 341, 252, 430, 1937, 1140, 1878, 2011, 783, 1198, 86, 2507, 1275, 603, 1372, 1849, 1221, 2163, 604, 1522, 1339, 1558, 2517, 2535, 1496, 1204, 874, 1641, 1432, 895, 2492, 2271, 1467]}
SAVED model.heads_thresh 0.91625
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
num_blocks:  6
Epoch: [50/50]	lr: 7.81e-06	time: 00:00:39	Loss_train 0.01048	Acc_train 92.35	/	Loss_test 0.00633	Acc_test 89.00
new_head:  {'blocks.5.layer.bias': tensor([ 0.0052, -0.0217], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0350,  0.0053, -0.0105,  ...,  0.0090, -0.0074,  0.0153],
        [-0.0067,  0.0172,  0.0149,  ..., -0.0063,  0.0286, -0.0075]],
       device='cuda:0')}
BLOCKS IN SUP:  [5]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c/models
SAVED HEADS THRESHOLD:  0.9109999999999999
SAVED model.topk_kernels {'conv0': [25, 59, 30, 5, 69, 42, 55, 68, 16, 82, 77, 53, 14, 35, 89], 'conv1': [304, 137, 75, 345, 349, 197, 121, 22, 365, 336, 146, 202, 56, 100, 4, 67, 266, 163, 296, 354, 50, 159, 41, 347, 343, 180, 118, 190, 92, 94, 104, 76, 324, 62, 172, 191, 274, 200, 186, 217, 36, 105, 330, 128, 73, 21, 254, 357, 133, 169, 6, 326, 206, 193, 99, 380, 382, 88, 262], 'conv2': [958, 1278, 761, 1436, 856, 95, 14, 1385, 733, 405, 622, 12, 307, 499, 1227, 1274, 1345, 869, 348, 612, 1452, 176, 1532, 250, 632, 1113, 312, 915, 423, 707, 406, 1508, 1291, 542, 942, 1317, 70, 743, 1442, 75, 544, 311, 574, 904, 961, 616, 959, 35, 801, 937, 490, 670, 1297, 994, 790, 434, 1450, 523, 450, 698, 1232, 936, 850, 636, 93, 164, 243, 22, 114, 911, 1238, 626, 1388, 350, 74, 1282, 1281, 1203, 913, 580, 238, 597, 1159, 774, 77, 889, 1025, 1086, 1486, 347, 868, 461, 783, 373, 905, 675, 639, 1245, 67, 359, 100, 673, 281, 1430, 1206, 705, 1101, 1089, 174, 1429, 662, 458, 810, 835, 1006, 1172, 923, 61, 852, 1185, 462, 667, 599, 41, 754, 395, 360, 855, 290, 728, 309, 25, 439, 1171, 1292, 457, 56, 875, 135, 13, 629, 148, 1424, 1002, 138, 1497, 1125, 1401, 614, 538, 964, 824, 1302, 441, 1294, 809, 1437, 1325, 246, 1100, 1186, 745, 324, 313, 356, 731, 276, 167, 335, 105, 212, 456, 851, 649, 596, 1192, 294, 118, 566, 133, 1248, 326, 584, 436, 1116, 447, 15, 933, 102, 818, 1046, 732, 195, 132, 233, 519, 1001, 1522, 674, 1022, 425, 1448, 688, 891, 1494, 1033, 886, 650, 508, 1307, 66, 808, 563, 1324, 872, 134, 110, 413, 209, 119, 1516, 779, 1266, 846, 1178, 717, 955, 215, 799, 973, 1032], 'conv3': [505, 1755, 1843, 1403, 1783, 1554, 1938, 646, 421, 1447, 792, 564, 2241, 1226, 2460, 472, 1424, 507, 2050, 1958, 1198, 419, 2414, 653, 2305, 195, 1720, 497, 2098, 129, 1789, 2428, 2, 534, 454, 1608, 1451, 96, 299, 398, 953, 2233, 1894, 1266, 1926, 878, 1698, 1448, 689, 304, 1303, 2453, 499, 1887, 305, 1385, 2093, 1548, 1406, 456, 1567, 404, 233, 943, 1753, 533, 935, 1162, 2161, 1271, 2154, 586, 2309, 922, 862, 2300, 658, 1873, 1506, 1394, 390, 1364, 861, 627, 2000, 1697, 772, 2185, 2051, 1992, 2162, 2511, 1831, 1677, 425, 1409, 370, 509, 2112, 2227, 561, 110, 1799, 2223, 2022, 199, 492, 1638, 1236, 1021, 2500, 2362, 1436, 1080, 774, 1304, 347, 555, 2254, 436, 1458, 2164, 2278, 2302, 2062, 1076, 2523, 1599, 1088, 657, 923, 1175, 2056, 2018, 2001, 1524, 708, 1647, 2265, 1612, 1051, 1109, 1839, 1227, 122, 2094, 2469, 674, 1796, 1446, 284, 331, 453, 1845, 2042, 1157, 720, 2057, 1375, 1457, 632, 2007, 2151, 249, 696, 2085, 964, 2245, 1405, 2070, 2114, 1074, 757, 51, 164, 1427, 431, 1811, 1510, 1325, 1138, 867, 402, 2420, 1248, 2181, 1564, 2456, 450, 68, 2270, 2315, 761, 1993, 340, 2106, 1809, 291, 80, 2252, 1578, 1781, 748, 2323, 102, 846, 262, 1337, 146, 1699, 2354, 912, 527, 895, 788, 2328, 746, 178, 638, 1221, 724, 2167, 1128, 1536, 735, 1003, 333, 240, 1985, 1253, 737, 715, 2021, 2240, 2179, 2317, 2342, 2044, 2011, 639, 1890, 1380, 780, 1045, 2004, 448, 1733, 231, 980, 1439, 283, 365, 944, 1646, 60, 2427, 1363, 1642, 1348, 1982, 1696, 605, 2341, 2152, 1998, 1341, 252, 726, 1920, 2160, 2250, 2340, 1968, 1867, 2095, 2352, 2248, 2249, 810, 2081, 1830, 723, 104, 987, 54, 1283, 1824, 1433, 1104, 2479, 1219, 2084, 378, 1340, 225, 1402, 2297, 2502, 1484, 1376, 339, 1220, 928, 626, 687, 2322, 616, 1545, 297, 1435, 758, 1676, 1555, 2514, 677, 742, 583, 694, 1689, 278, 903, 1690, 1013, 35, 927, 1362, 2236, 177, 1742, 1851, 156, 277, 346, 2173, 2221, 312, 52, 937, 69, 1874, 752, 1070, 2087, 1619, 2559, 1346, 1891, 1492, 969, 1002, 77, 1351, 2542, 753, 1267, 1056, 1210, 1187, 2174, 242, 1801, 2259, 46, 1670, 2079, 200, 1865, 13, 2242, 1353, 1928, 1044, 1411, 1614, 699, 1844, 1053, 72, 1645, 1356, 89, 884, 2551, 1287, 1961], 'conv4': [1118, 1323, 1657, 557, 888, 1143, 530, 966, 605, 91, 1504, 426, 1167, 2265, 2388, 1233, 1957, 99, 1366, 1970, 2385, 2531, 93, 382, 2393, 1706, 2457, 2559, 674, 2121, 1869, 402, 1250, 770, 6, 1883, 1406, 778, 30, 2335, 172, 315, 1063, 883, 386, 1713, 2218, 70, 2211, 1375, 455, 499, 2060, 2440, 1428, 161, 2415, 757, 703, 654, 139, 2171, 2053, 1066, 1064, 326, 1099, 1471, 15, 1387, 140, 621, 2034, 1373, 607, 534, 1602, 1750, 1090, 1289, 774, 2510, 1999, 2195, 270, 1731, 1273, 1928, 2143, 2250, 1787, 400, 2532, 1556, 1912, 2475, 1513, 1704, 1284, 448, 625, 1676, 1730, 1434, 1607, 3, 612, 1034, 193, 1442, 208, 843, 1241, 1489, 2268, 825, 1384, 1981, 296, 1877, 177, 1298, 1880, 2141, 969, 7, 1477, 1313, 806, 798, 2015, 2337, 1622, 2395, 905, 2286, 1402, 412, 1908, 1084, 2138, 328, 1684, 556, 34, 2410, 2285, 2025, 663, 150, 1252, 0, 549, 2066, 792, 8, 366, 112, 1201, 2070, 704, 2099, 2302, 548, 482, 1982, 1397, 1640, 889, 2191, 2189, 1306, 1326, 2558, 503, 263, 1071, 2436, 1246, 2226, 789, 666, 2081, 593, 718, 544, 568, 1080, 364, 2485, 615, 1618, 1597, 1035, 361, 1700, 233, 24, 2327, 765, 1987, 861, 2021, 628, 1571, 1444, 1580, 2487, 2346, 2452, 2453, 54, 2158, 1452, 2343, 309, 1819, 1135, 1810, 1285, 1892, 2188, 1969, 2326, 1169, 494, 768, 1338, 5, 2228, 182, 1796, 1644, 924, 2432, 2112, 120, 815, 1583, 2207, 2480, 2205, 85, 1012, 1719, 2276, 904, 105, 1723, 1995, 2277, 2172, 2239, 1357, 637, 2085, 1040, 1922, 2512, 1230, 225, 658, 738, 1065, 242, 1468, 1639, 1734, 1210, 689, 788, 108, 1134, 2546, 207, 2413, 1523, 257, 1424, 2516, 671, 1124, 2134, 558, 335, 2224, 332, 1619, 1239, 926, 951, 891, 1445, 239, 493, 1609, 424, 449, 1732, 2311, 1852, 1759, 2254, 340, 1535, 2509, 1634, 447, 1572, 802, 1770, 395, 1030, 1411, 1645, 2212, 562, 311, 1420, 531, 1575, 1996, 502, 259, 202, 670, 2045, 682, 1390, 132, 2518, 565, 2356, 560, 190, 2220, 2149, 1418, 2079, 1355, 1109, 1113, 553, 2540, 2249, 1015, 2538, 564, 705, 214, 2525, 1993, 700, 341, 252, 430, 1937, 1140, 1878, 2011, 783, 1198, 86, 2507, 1275, 603, 1372, 1849, 1221, 2163, 604, 1522, 1339, 1558, 2517, 2535, 1496, 1204, 874, 1641, 1432, 895, 2492, 2271, 1467]}
SAVED model.heads_thresh 0.9109999999999999
RESULT:  {'train_loss': 0.010478164069354534, 'train_acc': 92.35000014305115, 'test_loss': 0.006333355326205492, 'test_acc': 89.0, 'convergence': 22.592145919799805, 'R1': 0, 'dataset_sup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [51, 79]}, 'dataset_unsup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 50, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [51, 79]}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3, 4], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 50, 'lr': 0.005}, 't1': {'blocks': [5], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}}
IN R4:  {'count': 4, 'R0': {'train_loss': 0.014464263804256916, 'train_acc': 84.64999794960022, 'test_loss': 0.005055953748524189, 'test_acc': 82.0, 'convergence': 23.65986442565918, 'R1': 0, 'dataset_sup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [32, 93]}, 'dataset_unsup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 50, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [32, 93]}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3, 4], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 50, 'lr': 0.005}, 't1': {'blocks': [5], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}}, 'model_config': {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}, 'R1': {'train_loss': 0.0061482023447752, 'train_acc': 94.37000155448914, 'test_loss': 0.007179258391261101, 'test_acc': 88.0, 'convergence': 23.331533432006836, 'R1': 0, 'dataset_sup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [35, 92]}, 'dataset_unsup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 50, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [35, 92]}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3, 4], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 50, 'lr': 0.005}, 't1': {'blocks': [5], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}}, 'R2': {'train_loss': 0.006392252631485462, 'train_acc': 94.67999935150146, 'test_loss': 0.003225994296371937, 'test_acc': 92.5, 'convergence': 23.05847930908203, 'R1': 0, 'dataset_sup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [16, 41]}, 'dataset_unsup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 50, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [16, 41]}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3, 4], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 50, 'lr': 0.005}, 't1': {'blocks': [5], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}}, 'R3': {'train_loss': 0.005003861151635647, 'train_acc': 96.07999920845032, 'test_loss': 0.0020557385869324207, 'test_acc': 96.0, 'convergence': 22.7949275970459, 'R1': 0, 'dataset_sup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [2, 56]}, 'dataset_unsup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 50, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [2, 56]}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3, 4], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 50, 'lr': 0.005}, 't1': {'blocks': [5], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}}, 'R4': {'train_loss': 0.010478164069354534, 'train_acc': 92.35000014305115, 'test_loss': 0.006333355326205492, 'test_acc': 89.0, 'convergence': 22.592145919799805, 'R1': 0, 'dataset_sup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [51, 79]}, 'dataset_unsup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 50, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [51, 79]}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3, 4], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 50, 'lr': 0.005}, 't1': {'blocks': [5], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}}}
################################## EVALUATION OF TASK 0 ############################################
SEED:  0
block 0, size : 96 16 16
96
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 2.886751345948129
block 1, size : 384 8 8
384
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.8505172717997146
block 2, size : 1536 4 4
1536
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.4252586358998573
block 3, size : 2560 2 2
2560
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.21262931794992865
block 4, size : 2560 1 1
2560
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.1647019614671031
NOWWWW
1 1
2
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
blocks['b%s' % 5] 2560 2560
range = 0.3423265984407288
train_layer_order: consecutive {'t0': {'blocks': [0, 1, 2, 3, 4], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 50, 'lr': 0.005}, 't1': {'blocks': [5], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}
The device used will be: 
True
cuda:0
PARAMSSSS IN LOAD:  {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
avg_deltas:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
topk_layer:  None
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.0.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
topk_layer inside model after retrieval:  [25, 59, 30, 5, 69, 42, 55, 68, 16, 82, 77, 53, 14, 35, 89]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.1.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
topk_layer inside model after retrieval:  [304, 137, 75, 345, 349, 197, 121, 22, 365, 336, 146, 202, 56, 100, 4, 67, 266, 163, 296, 354, 50, 159, 41, 347, 343, 180, 118, 190, 92, 94, 104, 76, 324, 62, 172, 191, 274, 200, 186, 217, 36, 105, 330, 128, 73, 21, 254, 357, 133, 169, 6, 326, 206, 193, 99, 380, 382, 88, 262]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.2.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
topk_layer inside model after retrieval:  [958, 1278, 761, 1436, 856, 95, 14, 1385, 733, 405, 622, 12, 307, 499, 1227, 1274, 1345, 869, 348, 612, 1452, 176, 1532, 250, 632, 1113, 312, 915, 423, 707, 406, 1508, 1291, 542, 942, 1317, 70, 743, 1442, 75, 544, 311, 574, 904, 961, 616, 959, 35, 801, 937, 490, 670, 1297, 994, 790, 434, 1450, 523, 450, 698, 1232, 936, 850, 636, 93, 164, 243, 22, 114, 911, 1238, 626, 1388, 350, 74, 1282, 1281, 1203, 913, 580, 238, 597, 1159, 774, 77, 889, 1025, 1086, 1486, 347, 868, 461, 783, 373, 905, 675, 639, 1245, 67, 359, 100, 673, 281, 1430, 1206, 705, 1101, 1089, 174, 1429, 662, 458, 810, 835, 1006, 1172, 923, 61, 852, 1185, 462, 667, 599, 41, 754, 395, 360, 855, 290, 728, 309, 25, 439, 1171, 1292, 457, 56, 875, 135, 13, 629, 148, 1424, 1002, 138, 1497, 1125, 1401, 614, 538, 964, 824, 1302, 441, 1294, 809, 1437, 1325, 246, 1100, 1186, 745, 324, 313, 356, 731, 276, 167, 335, 105, 212, 456, 851, 649, 596, 1192, 294, 118, 566, 133, 1248, 326, 584, 436, 1116, 447, 15, 933, 102, 818, 1046, 732, 195, 132, 233, 519, 1001, 1522, 674, 1022, 425, 1448, 688, 891, 1494, 1033, 886, 650, 508, 1307, 66, 808, 563, 1324, 872, 134, 110, 413, 209, 119, 1516, 779, 1266, 846, 1178, 717, 955, 215, 799, 973, 1032]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.3.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
topk_layer inside model after retrieval:  [505, 1755, 1843, 1403, 1783, 1554, 1938, 646, 421, 1447, 792, 564, 2241, 1226, 2460, 472, 1424, 507, 2050, 1958, 1198, 419, 2414, 653, 2305, 195, 1720, 497, 2098, 129, 1789, 2428, 2, 534, 454, 1608, 1451, 96, 299, 398, 953, 2233, 1894, 1266, 1926, 878, 1698, 1448, 689, 304, 1303, 2453, 499, 1887, 305, 1385, 2093, 1548, 1406, 456, 1567, 404, 233, 943, 1753, 533, 935, 1162, 2161, 1271, 2154, 586, 2309, 922, 862, 2300, 658, 1873, 1506, 1394, 390, 1364, 861, 627, 2000, 1697, 772, 2185, 2051, 1992, 2162, 2511, 1831, 1677, 425, 1409, 370, 509, 2112, 2227, 561, 110, 1799, 2223, 2022, 199, 492, 1638, 1236, 1021, 2500, 2362, 1436, 1080, 774, 1304, 347, 555, 2254, 436, 1458, 2164, 2278, 2302, 2062, 1076, 2523, 1599, 1088, 657, 923, 1175, 2056, 2018, 2001, 1524, 708, 1647, 2265, 1612, 1051, 1109, 1839, 1227, 122, 2094, 2469, 674, 1796, 1446, 284, 331, 453, 1845, 2042, 1157, 720, 2057, 1375, 1457, 632, 2007, 2151, 249, 696, 2085, 964, 2245, 1405, 2070, 2114, 1074, 757, 51, 164, 1427, 431, 1811, 1510, 1325, 1138, 867, 402, 2420, 1248, 2181, 1564, 2456, 450, 68, 2270, 2315, 761, 1993, 340, 2106, 1809, 291, 80, 2252, 1578, 1781, 748, 2323, 102, 846, 262, 1337, 146, 1699, 2354, 912, 527, 895, 788, 2328, 746, 178, 638, 1221, 724, 2167, 1128, 1536, 735, 1003, 333, 240, 1985, 1253, 737, 715, 2021, 2240, 2179, 2317, 2342, 2044, 2011, 639, 1890, 1380, 780, 1045, 2004, 448, 1733, 231, 980, 1439, 283, 365, 944, 1646, 60, 2427, 1363, 1642, 1348, 1982, 1696, 605, 2341, 2152, 1998, 1341, 252, 726, 1920, 2160, 2250, 2340, 1968, 1867, 2095, 2352, 2248, 2249, 810, 2081, 1830, 723, 104, 987, 54, 1283, 1824, 1433, 1104, 2479, 1219, 2084, 378, 1340, 225, 1402, 2297, 2502, 1484, 1376, 339, 1220, 928, 626, 687, 2322, 616, 1545, 297, 1435, 758, 1676, 1555, 2514, 677, 742, 583, 694, 1689, 278, 903, 1690, 1013, 35, 927, 1362, 2236, 177, 1742, 1851, 156, 277, 346, 2173, 2221, 312, 52, 937, 69, 1874, 752, 1070, 2087, 1619, 2559, 1346, 1891, 1492, 969, 1002, 77, 1351, 2542, 753, 1267, 1056, 1210, 1187, 2174, 242, 1801, 2259, 46, 1670, 2079, 200, 1865, 13, 2242, 1353, 1928, 1044, 1411, 1614, 699, 1844, 1053, 72, 1645, 1356, 89, 884, 2551, 1287, 1961]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.4.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
topk_layer inside model after retrieval:  [1118, 1323, 1657, 557, 888, 1143, 530, 966, 605, 91, 1504, 426, 1167, 2265, 2388, 1233, 1957, 99, 1366, 1970, 2385, 2531, 93, 382, 2393, 1706, 2457, 2559, 674, 2121, 1869, 402, 1250, 770, 6, 1883, 1406, 778, 30, 2335, 172, 315, 1063, 883, 386, 1713, 2218, 70, 2211, 1375, 455, 499, 2060, 2440, 1428, 161, 2415, 757, 703, 654, 139, 2171, 2053, 1066, 1064, 326, 1099, 1471, 15, 1387, 140, 621, 2034, 1373, 607, 534, 1602, 1750, 1090, 1289, 774, 2510, 1999, 2195, 270, 1731, 1273, 1928, 2143, 2250, 1787, 400, 2532, 1556, 1912, 2475, 1513, 1704, 1284, 448, 625, 1676, 1730, 1434, 1607, 3, 612, 1034, 193, 1442, 208, 843, 1241, 1489, 2268, 825, 1384, 1981, 296, 1877, 177, 1298, 1880, 2141, 969, 7, 1477, 1313, 806, 798, 2015, 2337, 1622, 2395, 905, 2286, 1402, 412, 1908, 1084, 2138, 328, 1684, 556, 34, 2410, 2285, 2025, 663, 150, 1252, 0, 549, 2066, 792, 8, 366, 112, 1201, 2070, 704, 2099, 2302, 548, 482, 1982, 1397, 1640, 889, 2191, 2189, 1306, 1326, 2558, 503, 263, 1071, 2436, 1246, 2226, 789, 666, 2081, 593, 718, 544, 568, 1080, 364, 2485, 615, 1618, 1597, 1035, 361, 1700, 233, 24, 2327, 765, 1987, 861, 2021, 628, 1571, 1444, 1580, 2487, 2346, 2452, 2453, 54, 2158, 1452, 2343, 309, 1819, 1135, 1810, 1285, 1892, 2188, 1969, 2326, 1169, 494, 768, 1338, 5, 2228, 182, 1796, 1644, 924, 2432, 2112, 120, 815, 1583, 2207, 2480, 2205, 85, 1012, 1719, 2276, 904, 105, 1723, 1995, 2277, 2172, 2239, 1357, 637, 2085, 1040, 1922, 2512, 1230, 225, 658, 738, 1065, 242, 1468, 1639, 1734, 1210, 689, 788, 108, 1134, 2546, 207, 2413, 1523, 257, 1424, 2516, 671, 1124, 2134, 558, 335, 2224, 332, 1619, 1239, 926, 951, 891, 1445, 239, 493, 1609, 424, 449, 1732, 2311, 1852, 1759, 2254, 340, 1535, 2509, 1634, 447, 1572, 802, 1770, 395, 1030, 1411, 1645, 2212, 562, 311, 1420, 531, 1575, 1996, 502, 259, 202, 670, 2045, 682, 1390, 132, 2518, 565, 2356, 560, 190, 2220, 2149, 1418, 2079, 1355, 1109, 1113, 553, 2540, 2249, 1015, 2538, 564, 705, 214, 2525, 1993, 700, 341, 252, 430, 1937, 1140, 1878, 2011, 783, 1198, 86, 2507, 1275, 603, 1372, 1849, 1221, 2163, 604, 1522, 1339, 1558, 2517, 2535, 1496, 1204, 874, 1641, 1432, 895, 2492, 2271, 1467]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
The device used will be: 
True
cuda:0
The device used will be: 
True
cuda:0
chosen_head:  {'blocks.5.layer.bias': tensor([ 0.0052, -0.0217], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0350,  0.0053, -0.0105,  ...,  0.0090, -0.0074,  0.0153],
        [-0.0067,  0.0172,  0.0149,  ..., -0.0063,  0.0286, -0.0075]],
       device='cuda:0')}

 Model C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c loaded successfuly with best perf



 ----- Architecture Block BatchNorm2dSK3962(5, 5)1.0reflect, number 0 -----
- BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK963842(3, 3)0.6499999761581421reflect, number 1 -----
- BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.4)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK38415362(3, 3)0.25reflect, number 2 -----
- BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block BatchNorm2dSK153625602(3, 3)0.25reflect, number 3 -----
- BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block BatchNorm2dSK256025602(3, 3)0.25reflect, number 4 -----
- BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 5 -----
- Flatten(start_dim=1, end_dim=-1)
- Dropout(p=0.5, inplace=False)
- Linear(in_features=2560, out_features=2, bias=True)
model.heads:  [{'blocks.5.layer.bias': tensor([ 1.5791e-05, -1.6506e-02], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0120,  0.0150,  0.0043,  ...,  0.0072,  0.0111, -0.0045],
        [ 0.0163,  0.0075,  0.0001,  ..., -0.0045,  0.0101,  0.0122]],
       device='cuda:0')}, {'blocks.5.layer.bias': tensor([ 0.0040, -0.0205], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 1.5765e-02,  1.0660e-02,  4.3432e-03,  ...,  1.0930e-02,
          1.1431e-02, -3.5607e-03],
        [ 1.2527e-02,  1.1907e-02,  8.8815e-05,  ..., -8.2031e-03,
          9.7858e-03,  1.1328e-02]], device='cuda:0')}, {'blocks.5.layer.bias': tensor([ 0.0005, -0.0170], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0349,  0.0030,  0.0162,  ..., -0.0045,  0.0023,  0.0014],
        [-0.0066,  0.0195, -0.0117,  ...,  0.0072,  0.0189,  0.0064]],
       device='cuda:0')}, {'blocks.5.layer.bias': tensor([-0.0005, -0.0160], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0531,  0.0097,  0.0127,  ...,  0.0032,  0.0010,  0.0103],
        [-0.0248,  0.0129, -0.0082,  ..., -0.0005,  0.0202, -0.0025]],
       device='cuda:0')}, {'blocks.5.layer.bias': tensor([ 0.0052, -0.0217], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0350,  0.0053, -0.0105,  ...,  0.0090, -0.0074,  0.0153],
        [-0.0067,  0.0172,  0.0149,  ..., -0.0063,  0.0286, -0.0075]],
       device='cuda:0')}]
[Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (5): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=2560, out_features=2, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)]
LAYER NAME:  Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (5): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=2560, out_features=2, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)
LAYER CHILDREN:  [BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=1.4)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
  )
  (layer): Linear(in_features=2560, out_features=2, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)]
subsubl NAME:  Sequential(
  (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=1.4)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): Flatten(start_dim=1, end_dim=-1)
)
subsubl NAME:  Linear(in_features=2560, out_features=2, bias=True)
subsubl NAME:  Dropout(p=0.5, inplace=False)
CONFIG MODE:  unsupervised
CONFIG MODE:  supervised
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([49, 33, 72, 51, 71, 92, 15, 14, 23,  0, 71, 75, 81, 69, 40, 43, 92, 97,
        70, 53], device='cuda:0')
TARGETS AFTER SUB:  tensor([93, 32, 32, 32, 32, 93, 32, 93, 32, 93, 32, 93, 93, 93, 32, 32, 93, 32,
        32, 93])
[32, 93]
TARGETS AFTER CLEANER:  tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1])
200
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 200
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               None
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([19, 29,  0, 11,  1, 86, 90, 28, 23, 31, 39, 96, 82, 17, 71, 39,  8, 97,
        80, 71], device='cuda:0')
TARGETS AFTER SUB:  tensor([93, 93, 32, 32, 32, 32, 32, 32, 32, 32, 93, 32, 93, 93, 93, 32, 32, 32,
        93, 32])
[32, 93]
TARGETS AFTER CLEANER:  tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0])
1000
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 1000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               None
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([19, 29,  0, 11,  1, 86, 90, 28, 23, 31, 39, 96, 82, 17, 71, 39,  8, 97,
        80, 71], device='cuda:0')
TARGETS AFTER SUB:  tensor([93, 93, 32, 32, 32, 32, 32, 32, 32, 32, 93, 32, 93, 93, 93, 32, 32, 32,
        93, 32])
[32, 93]
TARGETS AFTER CLEANER:  tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0])
1000
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 1000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
------------------------
INDICES:  1000
IMAGE SIZE: torch.Size([64, 3, 32, 32])
The device used will be: 
True
cuda:0
output.detach().clone():  tensor([[-1.0806e+01,  2.8342e+01],
        [-3.2427e+01,  5.0429e+01],
        [ 7.3954e+00, -9.3016e+00],
        [ 2.0760e+01,  2.3864e+01],
        [ 2.5430e+01, -5.2256e+00],
        [-2.9405e+01,  5.4425e+01],
        [-2.6617e+00,  9.7306e+00],
        [ 4.0657e+00,  6.5976e+00],
        [ 1.0077e+01,  3.9724e+00],
        [-1.5174e+01,  1.7886e+01],
        [ 1.0731e+01,  1.5732e+01],
        [-5.8756e+00,  8.9532e+00],
        [ 3.6152e+00,  7.2843e+00],
        [ 8.6044e-01,  4.2838e+00],
        [ 8.5449e+00, -6.7962e+00],
        [ 1.1726e+01,  5.7708e-02],
        [ 2.3723e+00, -7.5382e+00],
        [ 7.4679e+00,  4.1697e+00],
        [ 5.4487e-01,  1.9116e+01],
        [ 9.7697e+00, -1.9659e+00],
        [ 5.4772e+00,  1.9480e+00],
        [ 1.0355e+01, -2.8178e+00],
        [ 2.0355e+01, -1.5120e+01],
        [ 4.9493e+00,  9.6315e+00],
        [-1.1340e+01,  1.9996e+01],
        [ 2.7324e+00,  1.4099e+00],
        [-1.8016e+01,  4.1954e+01],
        [-4.5710e+00,  1.8519e+01],
        [ 1.3922e+01, -4.2097e+00],
        [ 8.5202e+00,  7.8863e+00],
        [-3.7682e+01,  5.1799e+01],
        [-3.1918e+00,  1.8681e+01],
        [ 1.9893e+01, -1.0317e+00],
        [ 1.3687e+01, -5.0453e+00],
        [ 2.3739e+00,  4.2630e+01],
        [ 6.7385e+00, -6.9873e+00],
        [-1.9627e-01,  7.0393e+00],
        [ 2.3351e+01, -3.5915e+01],
        [ 6.2840e-01, -1.6240e+00],
        [ 9.6153e-02,  3.5494e+00],
        [ 9.5409e+00, -1.1265e+01],
        [ 5.2495e+00,  6.9462e+00],
        [ 3.8149e+00,  1.1566e+01],
        [ 1.0379e+01,  7.3083e+00],
        [ 2.5320e+00,  4.8126e+00],
        [-2.8006e+00,  2.0564e+00],
        [ 1.3447e+00,  7.2055e+00],
        [ 4.3530e+00,  8.6680e-01],
        [ 1.0516e+01, -2.5166e+00],
        [ 2.7778e+00, -1.2612e-02],
        [-7.2702e-01,  7.1095e+00],
        [ 6.1896e+00,  8.6354e+00],
        [ 3.2113e+00,  9.9617e-01],
        [ 2.3582e+01,  6.1412e+00],
        [ 9.7268e+00, -7.7194e+00],
        [ 1.4208e+01, -7.6337e+00],
        [ 1.0900e+00,  1.4127e+01],
        [ 1.7944e+00,  1.2706e+01],
        [ 1.8544e+01, -1.6049e+01],
        [ 9.0359e+00,  1.0000e+00],
        [ 1.2369e+01, -7.8225e+00],
        [ 5.8693e+00,  2.0360e-01],
        [ 9.3432e+00,  1.1386e+01],
        [-1.0579e+01,  4.9127e+01],
        [ 1.1020e+01,  1.0035e+01],
        [-3.3376e+01,  4.9387e+01],
        [ 9.4958e+00,  1.3555e+01],
        [ 6.0256e+00,  5.8052e+00],
        [-9.5227e-01,  1.4996e+01],
        [ 2.8836e+00,  1.0028e+01],
        [ 3.6859e+00,  1.6718e+00],
        [ 6.8656e+00,  6.4819e+00],
        [ 1.4310e+01, -1.8456e-01],
        [-1.8737e+01,  3.6683e+01],
        [ 1.6778e+01, -2.0588e+01],
        [ 3.6395e+01, -2.6389e+01],
        [ 3.8441e+00, -1.9512e+00],
        [ 5.8733e+00,  5.3408e+00],
        [ 1.7249e+01,  5.1878e+00],
        [-9.8458e+00,  1.7198e+01],
        [ 1.7181e+01,  5.8722e+00],
        [-3.4289e-01,  5.4423e+00],
        [-1.9938e+01,  4.3862e+01],
        [-2.3531e+00,  2.9851e+01],
        [ 1.0377e+00, -9.1440e-02],
        [ 5.8432e+00, -1.1113e+01],
        [ 4.1146e-01,  6.8486e+00],
        [ 3.6113e+01, -3.3105e+01],
        [ 2.1675e+01, -2.5641e+00],
        [ 2.8052e+01, -1.1497e+01],
        [ 1.0096e+01,  1.5002e+00],
        [ 6.1412e+00,  6.1817e+00],
        [ 8.4576e+00,  2.0179e+01],
        [ 5.3874e+00, -4.4037e+00],
        [ 1.7977e+01, -2.3481e+01],
        [-1.0888e+01,  2.8078e+01],
        [ 9.2394e+00, -6.3313e-01],
        [ 4.6675e-01, -1.8260e+00],
        [ 8.8829e+00,  1.0125e+01],
        [ 3.0083e+01, -1.2715e+00],
        [ 1.0715e+01,  6.0998e+00],
        [ 1.7643e+01, -7.0910e+00],
        [ 1.1219e+01, -6.7576e+00],
        [ 7.6834e+00,  7.2657e+00],
        [ 5.8920e+00,  1.5688e+00],
        [ 1.8637e+01, -2.4218e+00],
        [-2.1047e+01,  5.9427e+01],
        [-5.1595e+00,  3.9884e+00],
        [ 2.5753e+01, -2.8813e+00],
        [ 6.3588e+00,  2.5145e+00],
        [ 6.2671e+00,  8.8676e+00],
        [ 9.4973e+00,  1.0706e+01],
        [ 1.1456e+01, -7.1646e+00],
        [ 5.8280e+00,  6.5211e+00],
        [-3.1797e+01,  3.6224e+01],
        [ 1.1339e+00,  3.1878e+00],
        [ 2.5386e+00,  1.5638e+01],
        [ 1.9840e+01,  1.3513e+01],
        [-3.4993e+00,  4.2405e+01],
        [-7.1988e+00,  2.3614e+01],
        [ 6.9872e+00,  3.8338e-01],
        [-4.1161e+00,  3.2286e+01],
        [ 1.9954e+00,  7.4717e+00],
        [ 1.8279e+01, -2.0162e+00],
        [ 1.4138e+01,  6.3761e-01],
        [ 1.2904e+01,  1.1363e+00],
        [-3.4592e+01,  7.0595e+01],
        [ 4.0052e+00,  1.2324e+01],
        [ 1.2377e+01, -2.2977e+01],
        [ 8.8515e+00, -1.5982e+01],
        [ 2.2797e+01,  5.4243e-02],
        [ 9.9057e+00, -6.0119e+00],
        [ 6.0616e+00,  1.4138e+01],
        [ 5.7985e-01,  1.2343e+01],
        [ 3.3716e+00,  4.4840e+00],
        [ 2.5208e+01, -6.7219e+00],
        [ 3.6029e-01,  1.8539e+01],
        [ 1.9743e+01, -9.2065e+00],
        [ 6.3744e+00,  7.3543e+00],
        [-6.5711e+00,  1.6939e+01],
        [-2.7654e+01,  5.0421e+01],
        [-1.4572e+00,  2.0573e+01],
        [ 2.2597e+00,  8.3683e+00],
        [ 3.2542e+00,  3.9214e+00],
        [ 1.0502e+01,  1.5009e+01],
        [-5.1240e+00,  1.6816e+00],
        [ 3.5049e+00,  1.3874e+01],
        [ 1.8539e+01,  3.5353e+00],
        [ 1.8458e+01, -8.0966e+00],
        [-4.8381e-01, -4.5578e+00],
        [ 1.4950e+01,  2.3146e+01],
        [ 3.2082e+01,  7.0096e+00],
        [ 1.1917e+01, -7.5435e+00],
        [-2.7277e+01,  4.6283e+01],
        [ 1.0112e+01,  1.2051e+00],
        [ 4.4262e+00, -1.2293e+00],
        [-7.2754e+00,  2.7338e+01],
        [ 2.1337e+00,  1.1116e+01],
        [ 3.1560e+00,  1.6909e+01],
        [-9.4047e+00,  2.6575e+01],
        [-7.9552e+00,  1.4104e+01],
        [ 2.4353e+00,  2.4987e+00],
        [ 1.5134e+01,  1.2344e+00],
        [ 1.1661e+01,  3.0036e+00],
        [-5.7710e+00,  5.3975e+00],
        [-7.1469e+00,  1.2134e+01],
        [ 4.0146e+00,  7.2132e+00],
        [ 1.1094e+01, -6.5869e+00],
        [ 7.5019e+00, -2.1030e+00],
        [-3.2903e+01,  5.6419e+01],
        [ 1.7227e+01, -1.3373e+00],
        [ 1.0044e+01, -9.5589e+00],
        [ 1.6301e+01,  1.3913e+01],
        [-5.1505e-01,  7.2359e+00],
        [ 1.9622e+00,  3.5314e+00],
        [ 1.6330e+01, -6.1932e+00],
        [ 2.0719e+01, -1.2363e+01],
        [ 6.0676e+00,  2.1991e+00],
        [-3.5478e+00,  2.1656e+01],
        [ 3.7372e+00,  9.5322e+00],
        [ 7.8955e+00,  3.3552e+00],
        [-2.8052e+01,  4.3501e+01],
        [ 1.8034e+01,  6.0599e+00],
        [ 1.4121e+01,  7.1406e+00],
        [ 9.3366e+00,  3.8195e+00],
        [-1.5921e+00,  3.5209e+00],
        [ 4.6174e+00,  1.3663e+01],
        [-3.6891e+00,  2.0933e+01],
        [-5.7306e+00,  2.4536e+01],
        [-3.8537e+01,  7.2766e+01],
        [-9.1185e+00,  3.4226e+00],
        [ 1.3314e+01,  5.3419e-02],
        [ 2.2787e+01, -1.6645e+01],
        [ 7.7560e+00, -7.6688e+00],
        [ 2.1529e+01, -3.6046e+00],
        [ 8.9227e-01,  9.9157e+00],
        [ 2.0687e+01,  1.6816e+01],
        [ 6.0218e+00,  6.5660e+00],
        [ 8.1713e+00,  1.5923e+00],
        [-1.0540e+01,  5.0166e-01]], device='cuda:0')
output.data.max(1)[1]:  tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1], device='cuda:0')
tot_sum:  tensor(89.8554, device='cuda:0') 0
The device used will be: 
True
cuda:0
output.detach().clone():  tensor([[-1.0541e+00,  5.5608e+00],
        [ 6.8661e+00,  7.0817e+00],
        [ 1.8549e+01,  1.6538e+00],
        [ 1.2054e+01, -2.2907e+00],
        [ 1.3052e+01,  1.5289e+00],
        [ 1.4142e+01, -1.2213e+01],
        [ 7.1460e+00, -2.0467e+00],
        [ 2.0262e+00,  2.2993e+01],
        [ 2.2609e+00, -7.9569e+00],
        [-5.0125e-01,  3.2122e+00],
        [ 1.7145e-01,  8.7018e+00],
        [ 1.9230e+01,  9.4069e+00],
        [ 1.2128e+01, -7.8061e+00],
        [ 1.2852e+01,  1.5318e+01],
        [-1.0605e+01,  1.5032e+01],
        [ 1.3910e+01, -9.9904e-01],
        [-1.6752e-01,  8.5227e+00],
        [ 3.4118e+00,  1.2803e+01],
        [ 8.4682e+00, -2.3954e+00],
        [ 1.3444e+01,  6.6186e+00],
        [ 1.1966e+01, -4.6213e+00],
        [ 1.6008e+01, -1.2931e+01],
        [ 9.6278e+00,  7.9085e+00],
        [ 8.9779e+00,  1.3873e+01],
        [ 1.4952e+01,  4.0549e+00],
        [ 3.5740e+00,  1.4350e+01],
        [ 1.0086e+01,  9.0245e+00],
        [ 7.4308e+00,  6.6093e+00],
        [ 8.6147e+00, -4.4725e+00],
        [ 5.6215e-02,  1.0305e+01],
        [ 1.2342e+01, -1.4666e-01],
        [-1.7879e-01,  7.0217e+00],
        [ 1.1314e+01, -4.9313e+00],
        [ 3.9751e+01, -2.2483e+00],
        [-8.4550e+00,  2.7460e+01],
        [ 3.9136e+00,  6.6228e+00],
        [ 8.4769e+00,  2.8396e+00],
        [ 1.3819e+01,  5.8421e+00],
        [-3.3052e+01,  4.3058e+01],
        [ 1.2265e+01, -1.3655e+00],
        [-2.7350e+00,  2.8221e+00],
        [ 2.8184e+00,  1.9949e+01],
        [ 2.1885e+01,  5.5160e-01],
        [ 1.0196e+01,  2.1528e+00],
        [ 8.2475e+00,  4.6751e+00],
        [ 9.1774e+00,  8.5094e+00],
        [ 3.1624e+00, -1.1551e+00],
        [ 1.8583e+01,  1.4822e+00],
        [ 6.6618e+00, -1.5177e+00],
        [ 2.5388e+01,  4.8253e+00],
        [ 1.3726e+01, -3.0980e+00],
        [-7.9008e+00,  7.7054e-01],
        [ 1.0862e+01,  7.4180e+00],
        [ 7.6211e+00, -8.7923e+00],
        [ 1.5898e+01, -4.0845e-01],
        [-6.6322e+00,  1.1867e+01],
        [ 9.8645e+00,  4.1851e+00],
        [ 1.3485e+01,  4.6915e+00],
        [ 1.4972e+01,  2.9740e+00],
        [ 1.3092e+01,  1.6632e+01],
        [ 7.2018e+00,  4.4356e+00],
        [-7.9293e-01,  3.6796e+01],
        [ 1.1726e+01,  5.0893e+00],
        [ 1.1542e+01,  4.8647e+00],
        [ 1.3762e+01, -3.7260e+00],
        [ 2.3887e+00,  1.7816e+01],
        [ 4.4913e+00, -3.5451e+00],
        [ 6.1313e+00,  1.0131e+01],
        [ 1.2796e+01, -1.6238e+01],
        [-5.0805e+00,  3.3560e+00],
        [ 2.1331e+00,  1.5975e+01],
        [-4.1411e+00,  1.0715e+01],
        [-4.6284e+00, -6.4194e-01],
        [ 1.3802e+01, -1.4789e+00],
        [ 1.2974e+01, -4.3681e+00],
        [-8.5197e+00,  2.4531e+01],
        [-1.1778e-01,  1.6672e+01],
        [ 2.0691e-01, -1.0246e+01],
        [ 9.1816e+00, -7.2889e+00],
        [-2.3018e+01,  1.7514e+01],
        [-1.5426e+00,  3.5771e+01],
        [ 8.7908e+00,  2.0170e+00],
        [ 1.3103e+01,  5.2720e-02],
        [ 6.4099e+00, -1.4760e+00],
        [ 6.4838e+00, -7.2282e+00],
        [ 1.0336e+01, -5.3488e+00],
        [-3.2847e+00,  2.6801e+01],
        [-9.0641e-01, -2.9030e+00],
        [ 7.9116e+00, -6.1631e+00],
        [ 1.6231e+01, -1.3035e+01],
        [ 4.2023e+00,  1.2166e+01],
        [-6.6480e+00,  1.0542e+01],
        [ 7.3073e+00,  6.3183e-02],
        [-1.3341e+01,  8.1751e+00],
        [ 4.2650e+00,  3.1601e+00],
        [ 9.0938e+00, -1.0453e+01],
        [ 1.4501e+01, -1.4750e+01],
        [-2.4921e+00,  8.6336e+00],
        [ 1.9158e+01, -4.3332e+00],
        [-1.6543e+00,  7.8028e+00],
        [ 1.5004e+01, -1.2239e+01],
        [ 9.5640e-01,  4.2633e+00],
        [-2.3608e+01,  1.1044e+01],
        [ 7.2919e-02,  4.1163e-01],
        [ 7.4177e+00,  7.5311e+00],
        [-1.1516e+00,  1.4421e+01],
        [ 1.1220e+01,  1.2873e+01],
        [ 5.0599e+00,  1.2319e+01],
        [ 2.5693e+01, -9.3638e+00],
        [ 1.5225e+01,  3.6364e+00],
        [ 2.0101e+01, -6.3726e+00],
        [ 1.0429e+01,  3.6149e+00],
        [ 1.5259e+01,  3.5468e+00],
        [ 1.6397e+00, -2.6355e+00],
        [-5.7675e+00,  1.0314e+01],
        [ 2.2501e+01,  2.2123e+01],
        [ 9.6568e+00, -8.6732e+00],
        [ 1.4512e+01,  1.1951e+01],
        [ 1.2424e+01,  9.6507e+00],
        [ 1.2300e+00,  1.6014e+01],
        [ 5.8614e+00,  2.2950e+01],
        [ 8.7745e+00, -1.4220e+00],
        [ 1.4271e+01, -6.4159e+00],
        [-2.3836e+00,  1.7832e+01],
        [ 6.8420e-01,  1.4696e+01],
        [ 2.7262e+01,  1.0833e+01],
        [ 3.9527e+00,  1.3236e+01],
        [ 9.6621e+00, -1.0059e+00],
        [ 1.4047e+00,  3.6975e+01],
        [ 1.7758e+01,  1.5595e+01],
        [ 1.2371e+01,  1.7548e+00],
        [ 1.1591e+01, -1.8780e+00],
        [ 1.8887e+01, -4.3869e+00],
        [ 4.9242e+00,  2.5365e+00],
        [ 7.8751e-01,  1.3330e+01],
        [-5.1599e-01,  1.5292e+01],
        [-1.5959e+00,  1.2223e+00],
        [ 2.0933e+01, -7.6835e+00],
        [ 1.0789e+01,  4.3839e-01],
        [ 1.3764e+01,  2.5142e+01],
        [ 1.3055e+01, -8.7632e+00],
        [ 1.0231e+01, -5.8573e+00],
        [ 5.4721e+00,  1.4727e+01],
        [ 1.3579e+01,  2.3101e+00],
        [ 1.1801e+01, -1.7811e-02],
        [ 2.0786e+01,  2.2674e+00],
        [ 4.0969e+00, -4.5147e-01],
        [-1.5233e+00,  8.2441e+00],
        [ 1.2972e+01,  6.1431e+00],
        [-2.3534e+01,  2.6542e+01],
        [ 7.3945e+00,  2.9735e+00],
        [-6.0667e+00,  4.1604e+00],
        [ 1.7180e+01, -2.5152e+00],
        [-3.4929e+00,  2.1494e+01],
        [ 1.3951e+01, -3.2876e+00],
        [ 7.1691e+00,  1.6754e+01],
        [ 1.3857e+00,  6.6134e+00],
        [ 2.0422e+01, -1.5230e+00],
        [ 6.7604e+00,  8.4561e+00],
        [ 1.0183e+01,  1.1079e+01],
        [ 1.3911e+01, -2.3158e+00],
        [ 7.9159e+00, -2.5583e+00],
        [ 1.1236e+01, -1.7691e+00],
        [ 1.2497e+01,  1.5000e+01],
        [ 6.4737e+00,  1.3300e+00],
        [-8.0711e-01,  1.0944e+01],
        [ 3.9679e+00,  1.2447e+01],
        [ 3.8648e+00,  9.4825e+00],
        [-3.9611e+01,  4.8253e+01],
        [ 4.6648e+00,  2.5951e+00],
        [ 1.5324e+01, -4.7715e+00],
        [ 9.4448e+00,  2.3859e+00],
        [ 5.4699e+00,  1.5259e+01],
        [ 1.2106e+01,  8.9486e+00],
        [ 3.4488e+00,  9.1389e+00],
        [ 8.6992e+00, -1.6304e+00],
        [ 8.9324e+00,  6.2022e+00],
        [-4.0324e+00, -1.0093e+00],
        [ 5.2880e+00,  1.8650e+01],
        [ 1.0433e+01,  2.8659e+01],
        [-2.1091e+01,  1.0490e+01],
        [ 1.0996e+01,  1.4514e+01],
        [ 1.0472e+01,  2.8077e+01],
        [ 1.2796e+01, -7.3969e+00],
        [ 5.8120e+00, -1.6046e+00],
        [ 4.2619e+00,  2.9136e+00],
        [ 9.8962e+00, -1.3460e+00],
        [-1.6179e+01,  3.3349e+01],
        [ 1.0664e+01, -3.1269e+00],
        [ 7.4791e+00,  1.5393e+01],
        [ 2.3597e+01, -1.2346e+01],
        [ 5.0145e+00,  1.8036e+01],
        [-1.4179e+01,  1.6673e+01],
        [-1.3992e+01,  1.8454e+01],
        [ 2.1533e+01, -1.0319e+01],
        [ 8.1203e+00, -2.6269e+00],
        [ 1.0536e+01,  2.8314e+00],
        [ 4.3558e+00,  1.4130e+01],
        [ 6.9006e+00,  3.8103e+01],
        [ 6.9527e+00,  1.3139e+00]], device='cuda:0')
output.data.max(1)[1]:  tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0], device='cuda:0')
tot_sum:  tensor(96.8784, device='cuda:0') 1
The device used will be: 
True
cuda:0
output.detach().clone():  tensor([[ -2.1713,  17.3058],
        [ 28.2236,   0.4128],
        [ 30.8339, -20.2819],
        [  0.7635,   4.1703],
        [  8.9796,  19.8314],
        [ 16.3622,  -7.4891],
        [  4.3758,  34.7159],
        [  8.5187,   2.1090],
        [ -5.2179,  31.6813],
        [ 20.8965, -16.5228],
        [ 20.6866, -16.5445],
        [  0.6111,   6.5643],
        [  3.2832,  -0.5723],
        [  0.3233,  18.6817],
        [ 10.6088,   6.2060],
        [ 16.2795, -21.4457],
        [ 30.0447, -18.8308],
        [  8.2076,  -4.5622],
        [ 10.8968,  -6.3504],
        [ 17.2361, -21.0456],
        [ 36.3586, -12.4205],
        [-15.2210,  21.3692],
        [ 38.5316, -10.3621],
        [ 17.8356,  -3.8878],
        [  7.6310,   7.0333],
        [ 18.5280, -17.5445],
        [ 14.5394,  -9.4402],
        [ 13.1748,   9.5923],
        [  8.8728,   4.3763],
        [ 22.2726,  11.9556],
        [-19.2768,  23.4840],
        [-23.5538,  17.8577],
        [ 29.0404, -14.5402],
        [ 19.5359,  -1.2556],
        [  4.8465,  11.0426],
        [ -5.1047,  38.4575],
        [ -5.5263,   0.2559],
        [ 11.5199,   8.6792],
        [  4.0531,  18.7976],
        [ 20.9980,  -6.4175],
        [  1.4441,  -2.6154],
        [  5.5707,   1.0031],
        [ 11.0129,  -3.5879],
        [  8.8455,  -3.8583],
        [  9.0868,  -2.9454],
        [ 19.8023,  -8.0189],
        [ -8.0635,  24.4314],
        [ 31.4785,  -7.5552],
        [  9.9426,  -0.4757],
        [ 18.6009,  11.6127],
        [ -2.0341,  10.3005],
        [  7.5363, -12.5781],
        [ -1.0566,   6.2762],
        [ 13.6362,   9.4139],
        [ 18.0211, -11.6388],
        [ 11.2372,   5.1774],
        [  7.9290,  -0.8604],
        [ -2.3797,   7.6139],
        [  8.2456,  -4.3521],
        [ -2.7064,  23.9682],
        [ 60.5174, -15.5142],
        [ 14.1752,  -4.4119],
        [ 13.3239,   3.9197],
        [ -2.3735,   6.6648],
        [ 10.7623,   6.9244],
        [ -6.3715,   2.9289],
        [  5.9083,   2.6978],
        [ 17.2630,   0.2732],
        [ -0.3820,  20.5862],
        [ 12.7060,  -2.3381],
        [ 26.1752,  -6.1131],
        [ 15.7570, -10.6130],
        [ 20.2454,  17.2568],
        [ 21.5336,  -8.1863],
        [ 27.4900,  -9.4886],
        [ 33.5140, -14.5068],
        [ 19.5374,   3.9784],
        [ 21.0848,  -1.0200],
        [ -7.0032,  17.9025],
        [ 15.0511,   0.4378],
        [  1.8987,   5.9049],
        [ -1.6358,   9.1728],
        [ -4.8594,   4.9465],
        [-11.3846,  19.9346],
        [ 40.3464,  -1.4407],
        [ 20.7764,  -4.7656],
        [-22.1297,  12.0908],
        [  8.5309,   9.9546],
        [  3.6367, -14.2370],
        [ 58.3824, -19.8341],
        [ 16.1432,  -6.4306],
        [  7.7170,   3.5995],
        [ -2.4611,  17.8412],
        [ -1.1119,   8.3717],
        [  6.4048,  10.0014],
        [ 12.5172,  11.5759],
        [ 14.9766,  -0.9331],
        [ 23.8549, -10.9436],
        [  7.8891,   5.0335],
        [ 12.4144,   2.5342],
        [  3.9231,  15.1920],
        [ 30.1713, -17.0155],
        [-28.5428,  29.0272],
        [  9.0560,   0.9495],
        [ 18.7355, -16.8070],
        [  3.0969,  -2.1509],
        [ 13.8201,  -0.4526],
        [ -1.9201,  20.7817],
        [ 25.1782, -21.9815],
        [ 15.6108, -17.5172],
        [ 11.0213,  11.4151],
        [ 18.9739,  -4.1985],
        [ 32.1363, -14.1906],
        [ 10.1625,  -1.5210],
        [-11.5633,  23.9122],
        [ 26.2851, -10.8365],
        [ 25.5413, -13.2187],
        [ 19.1647,  10.5586],
        [ 12.6827,   4.6959],
        [  7.9016,  -2.5441],
        [ 16.4450,  -4.8495],
        [  6.7022,  16.1694],
        [  3.3273,  12.8876],
        [  1.6024,  -2.3469],
        [  1.0517,  -2.4111],
        [ 13.8131,   5.2974],
        [ 26.6884, -16.3277],
        [-13.7116,  41.2090],
        [ 13.4356,  24.9442],
        [ 13.2180,  -7.8193],
        [  4.0115,  12.3170],
        [ 15.9162,  -8.0609],
        [ -0.9000,  12.1275],
        [ 14.2049,  -2.9544],
        [ 13.7781,  -5.7791],
        [ 26.7258,  -1.2160],
        [ 19.8658, -11.2097],
        [  6.0982,  -3.3332],
        [  6.4815,  -4.7331],
        [ 17.0919,  -4.5043],
        [  3.9943,  11.2221],
        [ -9.7818,  23.8312],
        [ 24.5944, -17.2420],
        [ 17.5427,   2.6603],
        [ -6.8487,  25.0248],
        [ 30.6859, -13.4971],
        [ 13.2363,   5.6624],
        [ 10.4376,  -6.0110],
        [-10.4365,  15.9298],
        [  6.0570,   5.5804],
        [  1.0867,   9.5764],
        [  1.9220,   0.0851],
        [  7.1630,  -5.2703],
        [ -2.7084,   5.2022],
        [ 18.1485,  26.4751],
        [  3.0274,  11.7973],
        [ 15.4689,  -5.4332],
        [ -1.8746,   8.5952],
        [ 32.9810, -15.0567],
        [ 17.9209, -14.8435],
        [ -3.9759,  16.1715],
        [ 12.5350,  10.5183],
        [  0.6059,  20.1230],
        [ 11.6085,   4.6540],
        [ -0.0994,   6.1721],
        [ 20.9676,  -7.2391],
        [ -6.4595,   4.7349],
        [ 19.5765, -19.8255],
        [ 13.8870,  -5.5320],
        [  0.9268,  20.1281],
        [ -9.0911,   8.7172],
        [ -2.3869,   1.3911],
        [ 12.8722,   1.2533],
        [  8.9459,   2.8847],
        [  0.5318,   9.6050],
        [  3.6040,   9.6650],
        [ -3.0349,  -2.4691],
        [ 24.4703,   0.5488],
        [  4.9785,  14.6825],
        [ 12.9293,  25.1663],
        [  9.5356,   4.5813],
        [  1.7563,  -8.8867],
        [ -4.2887,  15.0963],
        [-11.1077,  18.5682],
        [ 15.8764, -11.3698],
        [ 21.3808,  -2.5756],
        [  3.1760,   1.2857],
        [ 18.0826,   3.9918],
        [  1.5635,   5.8068],
        [ -3.2598,  19.8142],
        [ -8.5626,  22.6026],
        [-12.4411,  29.6108],
        [  9.7829,   8.3250],
        [ 10.0530,   0.4834],
        [  3.3699, -15.9335],
        [  9.8449,  -3.0021],
        [  3.8706,  -0.8631],
        [ 14.7319, -10.4104],
        [ 24.6790,  11.3241],
        [  9.7869,  -2.4425]], device='cuda:0')
output.data.max(1)[1]:  tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')
tot_sum:  tensor(92.1462, device='cuda:0') 2
The device used will be: 
True
cuda:0
output.detach().clone():  tensor([[ 1.2927e+01,  2.9617e+00],
        [ 2.7561e+00,  2.6012e+00],
        [ 2.7308e+00,  1.4513e+01],
        [-8.6129e+00,  2.1882e+01],
        [ 1.6947e+01, -1.2573e+01],
        [ 1.0572e+01,  9.4927e+00],
        [-1.0229e+00,  8.3751e+00],
        [ 2.3420e+00,  1.1707e+01],
        [ 2.3993e+01, -7.7308e+00],
        [ 1.7418e+00,  1.7120e+01],
        [-6.2666e-01,  9.4997e+00],
        [ 5.6433e+00,  3.2905e+01],
        [-6.3463e+00,  1.1566e+01],
        [ 2.1206e+01, -1.2564e+01],
        [-4.6695e+00,  1.6865e+01],
        [ 1.9102e+01, -7.8512e+00],
        [ 5.1338e+00, -3.2413e+00],
        [ 8.8853e+00,  1.2376e+01],
        [ 1.0219e+01, -3.8372e+00],
        [-2.2076e+00, -1.2351e+00],
        [ 1.2324e+01, -1.7490e+01],
        [-5.1193e+01,  6.6641e+01],
        [ 3.9093e+00, -4.2832e+00],
        [ 3.0792e+01, -2.7784e+01],
        [ 1.8075e+01, -3.8853e-01],
        [-6.5415e+00,  1.9464e+01],
        [-1.6533e+01,  1.7017e+01],
        [ 2.8497e+01,  9.5987e+00],
        [ 1.2727e+01, -7.4926e+00],
        [-7.5565e+01,  9.1576e+01],
        [-9.4450e+00,  2.0344e+01],
        [-1.8128e+00,  1.6637e+01],
        [ 1.4472e+01,  4.0137e+00],
        [ 2.0467e+01, -1.3893e+01],
        [ 2.2109e+00, -1.2275e+00],
        [ 1.3694e+01,  8.0617e-01],
        [-1.2602e+01,  1.5367e+01],
        [-8.4916e+00,  1.5952e+01],
        [-1.1964e+01,  1.6171e+01],
        [ 8.9485e+00, -4.0148e+00],
        [-1.7201e+01,  1.1505e+01],
        [-1.9805e+01,  3.0173e+01],
        [ 3.1922e+01, -9.8480e+00],
        [-1.8690e+01,  3.1939e+01],
        [-3.1214e+01,  4.9390e+01],
        [-9.7229e+00,  1.6983e+01],
        [-8.8151e+00,  1.4308e+01],
        [ 2.9662e+01, -8.9334e+00],
        [-1.2078e+01,  2.6118e+01],
        [ 1.7627e+01,  4.8089e+00],
        [ 3.3310e+00,  1.1755e+00],
        [ 2.2563e+01,  3.0860e-01],
        [ 5.3354e+00,  6.4479e+00],
        [ 8.4268e+00,  1.1772e+01],
        [-5.5213e+01,  6.9330e+01],
        [ 2.3053e+01, -2.8557e+01],
        [-2.1251e+01,  2.8595e+01],
        [ 9.8489e+00,  1.4674e+00],
        [ 1.3111e+01,  1.6640e+00],
        [-1.5962e+01,  1.5217e+01],
        [-3.8348e-01,  1.1011e+01],
        [-3.9050e-01,  3.5871e+00],
        [ 1.7135e+01, -3.9791e+00],
        [ 1.5984e+01, -2.6584e+01],
        [-4.7611e+01,  6.6615e+01],
        [-1.5189e+01,  1.3830e+01],
        [ 8.3341e+00, -4.1921e+00],
        [ 2.3958e+01, -2.5864e+01],
        [ 2.0126e+01, -1.0413e+01],
        [ 4.6702e+00, -1.1801e+01],
        [-5.3901e+01,  7.6668e+01],
        [ 1.1016e+01,  5.3522e+00],
        [ 1.7257e+01, -4.9340e+00],
        [ 3.5964e-01,  1.4857e+01],
        [-8.6291e+00,  1.6166e+01],
        [ 4.0469e+00,  5.4198e+00],
        [ 2.2743e+01,  1.4759e+01],
        [ 6.5484e+00,  8.4001e+00],
        [ 2.0987e+01, -9.3501e+00],
        [ 3.0136e+01, -1.3581e+01],
        [ 1.4968e+01, -6.9687e+00],
        [ 1.1043e+01,  9.1609e+00],
        [ 2.7972e+01, -2.1830e+01],
        [-5.4365e+01,  7.9384e+01],
        [-1.9720e+00,  9.7755e+00],
        [-5.7653e+01,  6.2080e+01],
        [-2.5323e+00,  1.3340e+01],
        [ 1.2293e+01,  1.5877e+01],
        [-3.5465e+00, -1.7241e+00],
        [-4.1604e+00,  2.9670e+01],
        [-1.2296e+01,  2.0902e+01],
        [-1.2539e+01,  2.4369e+01],
        [-4.5556e+00,  2.3671e+01],
        [ 1.9890e+01, -6.5229e+00],
        [-1.4725e+01,  3.2832e+01],
        [ 8.4040e+00,  2.1480e+00],
        [ 1.0747e+01,  4.6332e+00],
        [-1.3055e+01,  1.5062e+01],
        [ 1.8494e+01,  6.1656e-01],
        [ 1.2891e+00,  5.5535e+00],
        [ 4.6323e+00, -4.5453e+00],
        [ 2.9313e+00,  1.1194e+01],
        [-1.9716e+01,  9.6770e+00],
        [ 1.1792e+01, -1.2554e+00],
        [-3.1294e+01,  7.0200e+01],
        [ 9.9033e+00,  1.3147e+01],
        [ 1.8107e+01, -7.9702e+00],
        [ 1.9701e+00,  1.3164e+01],
        [ 9.7134e+00, -2.2884e+00],
        [-1.7475e+00,  1.8162e+01],
        [-5.1539e+00,  1.5189e+01],
        [ 8.2301e+00,  2.9973e+00],
        [ 5.3371e+00, -2.8434e+00],
        [-6.0648e+00,  2.2471e+01],
        [ 1.7384e+01,  1.2829e+01],
        [ 3.1996e+01, -2.1990e+01],
        [ 3.2354e+00,  9.3521e+00],
        [-1.1888e-01,  7.2941e+00],
        [-2.0888e+01,  2.5209e+01],
        [-6.4008e+00,  2.0444e+01],
        [-1.1730e+01,  1.9100e+01],
        [-1.5099e+01,  2.7447e+01],
        [-1.4462e+01,  2.1530e+01],
        [ 1.4074e-01,  1.4440e+01],
        [ 3.8646e+01,  4.4523e-01],
        [ 6.2777e-01,  2.6869e+01],
        [ 5.7800e+00,  8.1676e+00],
        [-2.6894e+01,  2.9605e+01],
        [ 1.5646e+01,  5.4091e+00],
        [ 2.6767e+01, -1.5554e+01],
        [ 2.2749e+01,  6.0619e+00],
        [-1.0930e+01,  2.7259e+01],
        [-2.1557e+00,  2.1817e+01],
        [ 5.5355e+00, -1.2443e+00],
        [ 1.3779e+01,  4.1452e+00],
        [ 1.1029e+00,  4.0409e+00],
        [-2.9366e+01,  3.4353e+01],
        [-3.7788e+00,  1.2045e+01],
        [ 1.7600e+01,  8.8629e+00],
        [ 3.7573e+01,  7.0504e+00],
        [-2.9444e+00,  4.8729e+00],
        [ 8.7554e-01,  7.0379e-02],
        [-4.3454e+01,  6.1455e+01],
        [-7.8753e+00,  6.1506e+00],
        [ 3.6027e+01, -2.5667e+01],
        [ 1.1097e+01,  3.3906e+01],
        [ 1.7568e+01,  5.2822e+00],
        [-1.7588e+01,  1.6417e+01],
        [-2.3051e+01,  4.0996e+01],
        [-4.2096e+00,  8.6711e+00],
        [ 1.6185e+01, -1.1639e+01],
        [-2.2820e+01,  2.1824e+01],
        [-1.8150e+01,  2.4298e+01],
        [ 1.5146e+01, -1.1253e+01],
        [ 2.4051e+00,  1.5875e+01],
        [ 2.1018e+01, -3.6393e+00],
        [ 3.6152e-01,  8.1885e+00],
        [-1.8707e+01,  2.5427e+01],
        [-5.0176e+01,  7.3691e+01],
        [-2.5905e+01,  4.3441e+01],
        [-3.9579e+01,  6.3502e+01],
        [ 1.5865e-01,  3.4866e+00],
        [ 1.4462e+01,  2.0205e-01],
        [ 3.4387e+01, -4.6640e+00],
        [ 1.6470e+00,  3.4521e+00],
        [ 1.3804e+01,  6.3987e+00],
        [-1.6588e+01,  3.5393e+01],
        [-4.8505e+00,  1.0249e+01],
        [-1.4657e+01,  3.0145e+01],
        [-1.7300e+01,  4.1238e+01],
        [ 1.3970e+01, -2.4188e-01],
        [-5.0984e+01,  8.5212e+01],
        [-1.3146e+01,  3.2045e+01],
        [ 1.2617e+00,  4.8109e+00],
        [-4.5406e+01,  8.1409e+01],
        [ 1.7846e+01,  1.5507e+01],
        [ 1.9037e+00, -2.1528e+00],
        [-1.2988e+01,  4.1624e+01],
        [-2.1274e+01,  4.1336e+01],
        [ 2.2959e+01, -3.5523e+01],
        [-2.5420e+01,  4.2608e+01],
        [ 2.4552e+01, -5.5445e+00],
        [ 1.2372e+01,  9.7517e-01],
        [ 1.9639e+01, -9.8756e+00],
        [ 3.5188e+01, -2.6833e+01],
        [-2.1980e+01,  2.5057e+01],
        [ 1.8395e+00,  1.4375e+01],
        [-3.4569e+01,  4.3225e+01],
        [ 7.3275e+00, -5.5793e+00],
        [ 1.0851e+01,  7.4420e-01],
        [-2.3184e+00,  1.0174e+01],
        [-2.6795e+01,  6.5174e+01],
        [ 8.1596e+00,  2.5034e+00],
        [-3.4969e+01,  5.2139e+01],
        [ 9.0096e+00, -1.4052e+01],
        [ 1.4412e+01,  8.6407e+00],
        [ 2.2776e+01, -9.8644e+00],
        [ 2.4166e+01, -2.7976e+01],
        [ 1.1937e+01,  4.8772e+00],
        [ 9.9876e+00,  1.4105e+01]], device='cuda:0')
output.data.max(1)[1]:  tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1], device='cuda:0')
tot_sum:  tensor(80.2392, device='cuda:0') 3
The device used will be: 
True
cuda:0
output.detach().clone():  tensor([[ 11.2423,   3.8919],
        [-17.6397,  14.1968],
        [ 22.5246,   6.2862],
        [ -3.9016,   0.0919],
        [  8.7704,  -0.5042],
        [ 13.1064,   2.7825],
        [  9.6846,  18.9515],
        [  7.3823,   4.2548],
        [  0.2711,  10.2807],
        [  2.2586,  35.2433],
        [ -5.0997,  23.9981],
        [  1.5520,  12.1763],
        [-14.6930,  24.4562],
        [ -0.9955,  14.2444],
        [ 13.7087,  30.9146],
        [ -2.7836,   9.8520],
        [-16.2684,  35.2729],
        [ -6.3767,  13.0972],
        [ 22.8564, -12.4959],
        [ -1.9368,  19.1799],
        [  9.8384, -11.5632],
        [ -4.0085,  27.0615],
        [  9.3325,  -9.2456],
        [  7.8016,   5.5657],
        [ -6.8071,  11.9061],
        [ -7.9982,  25.1866],
        [ 20.2467,  -3.9844],
        [ 13.8545,  -5.4998],
        [-13.9033,  15.8317],
        [  1.7167,   7.9958],
        [  3.9496,  10.0903],
        [-10.7406,   0.7015],
        [ -6.9074,  51.9101],
        [ -5.6845,  22.4991],
        [ -7.5161,  22.2913],
        [ 19.6821,   0.5218],
        [  5.2204,  -5.5944],
        [ -1.5238,  19.4691],
        [-12.2651,  11.5203],
        [ -0.1647,  20.3675],
        [-12.3688,  15.5652],
        [ 22.8341,  -8.1700],
        [ 10.7423,  12.3076],
        [ -0.9334,   8.4702],
        [  9.1830,   7.2229],
        [ -3.2892,  14.0964],
        [ 17.7980,   5.0733],
        [ 13.8665,   5.2438],
        [  2.0187,   5.1564],
        [ 10.2991,   1.0171],
        [  2.6309,  15.5449],
        [ 21.7197, -11.5831],
        [-29.1171,  63.3447],
        [ -4.3075,  23.1124],
        [-29.5690,  45.0171],
        [  4.7928,   0.1408],
        [ 15.0733,  15.1401],
        [-14.4846,  25.7119],
        [ 14.4287,   2.1254],
        [  4.7288,  -4.2447],
        [  1.6852,   3.5342],
        [-31.6196,  47.6299],
        [ 17.5497,  -8.9084],
        [  9.6377,  -2.2676],
        [  5.7217,  -1.2153],
        [  1.3723,  16.3142],
        [ 17.1167,  20.9786],
        [-10.6655,  26.1542],
        [  4.0446,  20.0482],
        [-13.1162,  20.4682],
        [ 13.8583,  -4.9854],
        [  7.0493,   0.3756],
        [  1.6534,   1.3539],
        [ -0.7384,   8.0825],
        [ -7.4728,  12.6166],
        [ 26.1428,  -9.7750],
        [ 10.8825,   4.0659],
        [  8.5871,  12.4676],
        [ 10.1062,  -5.5600],
        [-10.9246,  16.2819],
        [ 19.3676, -13.2265],
        [ 10.1125, -12.0192],
        [-21.6183,  24.6955],
        [  5.1982,  21.2648],
        [ 18.1194, -11.5458],
        [  5.6494,  -0.1563],
        [ 12.6415, -10.6346],
        [ 13.3426,  20.0099],
        [  5.0907,   9.4896],
        [  4.0210,   9.9265],
        [ 12.4167,   0.9303],
        [ -3.8640,  16.0594],
        [  3.8850,  -1.9925],
        [-18.2943,  28.6619],
        [ 18.4096,  -1.9954],
        [ -2.8640,  -9.6999],
        [ -7.1682,  13.3160],
        [ 17.4775,  -0.0991],
        [-17.1292,  35.0531],
        [-22.4873,  26.1324],
        [  3.3510,   0.9702],
        [ 14.0312,   4.8302],
        [  4.6785,   3.1248],
        [ -2.4650,  16.9649],
        [ 10.1981,  28.3499],
        [  3.6758,   9.2353],
        [  6.0375, -13.1681],
        [ -3.4532,   6.2180],
        [-22.8878,  37.0043],
        [-26.2731,  62.2756],
        [ 19.2828, -16.7892],
        [-12.1108,  23.3611],
        [-30.3683,  53.1351],
        [  8.9849, -14.1513],
        [ 18.5716,   1.6272],
        [  5.8456,  16.2285],
        [ -1.3220,  15.3712],
        [ 16.7210,  -0.5063],
        [ -0.1862,  15.0107],
        [ 11.0841,  10.1775],
        [ -5.5268,  17.8492],
        [  4.3704, -14.9709],
        [ -1.9018,   6.0437],
        [ 14.7111, -13.7652],
        [ 20.5429, -12.5441],
        [ -1.2854,  12.1845],
        [ -4.5371,   5.5204],
        [  1.8263,  10.5223],
        [  4.7451, -10.4414],
        [  5.2242,  -0.7628],
        [ 27.6963,  -9.2111],
        [  2.4651,   3.6074],
        [ -2.4353,  17.8151],
        [  8.9400,   1.5961],
        [ -1.6738,  21.3346],
        [-25.0811,  43.0820],
        [ 14.8476,   4.2673],
        [-10.4927,  13.2033],
        [ 12.9525, -17.9945],
        [ 20.2472,   2.6032],
        [  8.0062,  -2.7723],
        [ 11.1464,   2.9788],
        [-19.3869,  24.3739],
        [ -7.2506,  31.1885],
        [-20.8946,  38.4305],
        [ 11.6281,   2.4151],
        [ 13.6144,  -5.0645],
        [-15.1088,  25.7363],
        [  1.4569,   0.2913],
        [ 11.0340,   0.1797],
        [  0.2288,  12.6936],
        [  0.1865,  -1.3580],
        [ 12.2902,   5.9898],
        [-19.4759,  44.4946],
        [ -9.2070,   8.2109],
        [-20.8521,  44.7750],
        [ -5.3271,  12.7875],
        [ -0.7779,  -4.7264],
        [-21.0246,  37.3529],
        [  9.1893,   7.9800],
        [  2.0033,  20.4328],
        [ -3.1493,  23.2111],
        [ -1.4608,   6.8592],
        [  9.9595,   3.3092],
        [  2.9812,  24.5158],
        [-24.7083,  33.3641],
        [-16.7521,  40.2673],
        [  8.0303,  12.0342],
        [  9.4224,   3.1650],
        [  3.1918,   9.9638],
        [  8.5630,   3.2673],
        [  4.9294,  33.4498],
        [ 17.2205, -12.9294],
        [ 14.8262, -15.0755],
        [ 10.1680, -15.4387],
        [ 21.5468, -11.5416],
        [-12.0499,  50.9553],
        [  6.2544,  -2.3612],
        [ -3.3177,   7.5248],
        [ 21.7847, -14.9422],
        [ 19.9624,  -8.1792],
        [ -3.8288,  12.4347],
        [ 16.3570,   4.3716],
        [-40.0614,  44.4874],
        [  8.4862,  -1.2266],
        [ 14.2437,   4.7633],
        [  9.5420,  20.1811],
        [ 28.4719,  10.6194],
        [ 12.7907,   5.3169],
        [ -4.4377,  14.4732],
        [  5.1940,  -0.8204],
        [  3.5144,   5.9523],
        [ -6.2795,   4.9199],
        [ 17.8165,   7.6931],
        [  1.2260,  13.9902],
        [  4.7774,   3.0778],
        [ 12.1128,  16.0564],
        [ -2.4640,  13.1269],
        [ -1.8131,  13.4083],
        [  5.2227,   1.1594]], device='cuda:0')
output.data.max(1)[1]:  tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0], device='cuda:0')
tot_sum:  tensor(92.7688, device='cuda:0') 4
max_key : 1
Accuracy of the network on the 1st dataset: 45.500 %
Test loss on the 1st dataset: 0.037
################################## EVALUATION OF TASK 1 ############################################
SEED:  0
block 0, size : 96 16 16
96
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 2.886751345948129
block 1, size : 384 8 8
384
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.8505172717997146
block 2, size : 1536 4 4
1536
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.4252586358998573
block 3, size : 2560 2 2
2560
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.21262931794992865
block 4, size : 2560 1 1
2560
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.1647019614671031
NOWWWW
1 1
2
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
blocks['b%s' % 5] 2560 2560
range = 0.3423265984407288
train_layer_order: consecutive {'t0': {'blocks': [0, 1, 2, 3, 4], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 50, 'lr': 0.005}, 't1': {'blocks': [5], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}
The device used will be: 
True
cuda:0
PARAMSSSS IN LOAD:  {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
avg_deltas:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
topk_layer:  None
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.0.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
topk_layer inside model after retrieval:  [25, 59, 30, 5, 69, 42, 55, 68, 16, 82, 77, 53, 14, 35, 89]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.1.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
topk_layer inside model after retrieval:  [304, 137, 75, 345, 349, 197, 121, 22, 365, 336, 146, 202, 56, 100, 4, 67, 266, 163, 296, 354, 50, 159, 41, 347, 343, 180, 118, 190, 92, 94, 104, 76, 324, 62, 172, 191, 274, 200, 186, 217, 36, 105, 330, 128, 73, 21, 254, 357, 133, 169, 6, 326, 206, 193, 99, 380, 382, 88, 262]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.2.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
topk_layer inside model after retrieval:  [958, 1278, 761, 1436, 856, 95, 14, 1385, 733, 405, 622, 12, 307, 499, 1227, 1274, 1345, 869, 348, 612, 1452, 176, 1532, 250, 632, 1113, 312, 915, 423, 707, 406, 1508, 1291, 542, 942, 1317, 70, 743, 1442, 75, 544, 311, 574, 904, 961, 616, 959, 35, 801, 937, 490, 670, 1297, 994, 790, 434, 1450, 523, 450, 698, 1232, 936, 850, 636, 93, 164, 243, 22, 114, 911, 1238, 626, 1388, 350, 74, 1282, 1281, 1203, 913, 580, 238, 597, 1159, 774, 77, 889, 1025, 1086, 1486, 347, 868, 461, 783, 373, 905, 675, 639, 1245, 67, 359, 100, 673, 281, 1430, 1206, 705, 1101, 1089, 174, 1429, 662, 458, 810, 835, 1006, 1172, 923, 61, 852, 1185, 462, 667, 599, 41, 754, 395, 360, 855, 290, 728, 309, 25, 439, 1171, 1292, 457, 56, 875, 135, 13, 629, 148, 1424, 1002, 138, 1497, 1125, 1401, 614, 538, 964, 824, 1302, 441, 1294, 809, 1437, 1325, 246, 1100, 1186, 745, 324, 313, 356, 731, 276, 167, 335, 105, 212, 456, 851, 649, 596, 1192, 294, 118, 566, 133, 1248, 326, 584, 436, 1116, 447, 15, 933, 102, 818, 1046, 732, 195, 132, 233, 519, 1001, 1522, 674, 1022, 425, 1448, 688, 891, 1494, 1033, 886, 650, 508, 1307, 66, 808, 563, 1324, 872, 134, 110, 413, 209, 119, 1516, 779, 1266, 846, 1178, 717, 955, 215, 799, 973, 1032]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.3.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
topk_layer inside model after retrieval:  [505, 1755, 1843, 1403, 1783, 1554, 1938, 646, 421, 1447, 792, 564, 2241, 1226, 2460, 472, 1424, 507, 2050, 1958, 1198, 419, 2414, 653, 2305, 195, 1720, 497, 2098, 129, 1789, 2428, 2, 534, 454, 1608, 1451, 96, 299, 398, 953, 2233, 1894, 1266, 1926, 878, 1698, 1448, 689, 304, 1303, 2453, 499, 1887, 305, 1385, 2093, 1548, 1406, 456, 1567, 404, 233, 943, 1753, 533, 935, 1162, 2161, 1271, 2154, 586, 2309, 922, 862, 2300, 658, 1873, 1506, 1394, 390, 1364, 861, 627, 2000, 1697, 772, 2185, 2051, 1992, 2162, 2511, 1831, 1677, 425, 1409, 370, 509, 2112, 2227, 561, 110, 1799, 2223, 2022, 199, 492, 1638, 1236, 1021, 2500, 2362, 1436, 1080, 774, 1304, 347, 555, 2254, 436, 1458, 2164, 2278, 2302, 2062, 1076, 2523, 1599, 1088, 657, 923, 1175, 2056, 2018, 2001, 1524, 708, 1647, 2265, 1612, 1051, 1109, 1839, 1227, 122, 2094, 2469, 674, 1796, 1446, 284, 331, 453, 1845, 2042, 1157, 720, 2057, 1375, 1457, 632, 2007, 2151, 249, 696, 2085, 964, 2245, 1405, 2070, 2114, 1074, 757, 51, 164, 1427, 431, 1811, 1510, 1325, 1138, 867, 402, 2420, 1248, 2181, 1564, 2456, 450, 68, 2270, 2315, 761, 1993, 340, 2106, 1809, 291, 80, 2252, 1578, 1781, 748, 2323, 102, 846, 262, 1337, 146, 1699, 2354, 912, 527, 895, 788, 2328, 746, 178, 638, 1221, 724, 2167, 1128, 1536, 735, 1003, 333, 240, 1985, 1253, 737, 715, 2021, 2240, 2179, 2317, 2342, 2044, 2011, 639, 1890, 1380, 780, 1045, 2004, 448, 1733, 231, 980, 1439, 283, 365, 944, 1646, 60, 2427, 1363, 1642, 1348, 1982, 1696, 605, 2341, 2152, 1998, 1341, 252, 726, 1920, 2160, 2250, 2340, 1968, 1867, 2095, 2352, 2248, 2249, 810, 2081, 1830, 723, 104, 987, 54, 1283, 1824, 1433, 1104, 2479, 1219, 2084, 378, 1340, 225, 1402, 2297, 2502, 1484, 1376, 339, 1220, 928, 626, 687, 2322, 616, 1545, 297, 1435, 758, 1676, 1555, 2514, 677, 742, 583, 694, 1689, 278, 903, 1690, 1013, 35, 927, 1362, 2236, 177, 1742, 1851, 156, 277, 346, 2173, 2221, 312, 52, 937, 69, 1874, 752, 1070, 2087, 1619, 2559, 1346, 1891, 1492, 969, 1002, 77, 1351, 2542, 753, 1267, 1056, 1210, 1187, 2174, 242, 1801, 2259, 46, 1670, 2079, 200, 1865, 13, 2242, 1353, 1928, 1044, 1411, 1614, 699, 1844, 1053, 72, 1645, 1356, 89, 884, 2551, 1287, 1961]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.4.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
topk_layer inside model after retrieval:  [1118, 1323, 1657, 557, 888, 1143, 530, 966, 605, 91, 1504, 426, 1167, 2265, 2388, 1233, 1957, 99, 1366, 1970, 2385, 2531, 93, 382, 2393, 1706, 2457, 2559, 674, 2121, 1869, 402, 1250, 770, 6, 1883, 1406, 778, 30, 2335, 172, 315, 1063, 883, 386, 1713, 2218, 70, 2211, 1375, 455, 499, 2060, 2440, 1428, 161, 2415, 757, 703, 654, 139, 2171, 2053, 1066, 1064, 326, 1099, 1471, 15, 1387, 140, 621, 2034, 1373, 607, 534, 1602, 1750, 1090, 1289, 774, 2510, 1999, 2195, 270, 1731, 1273, 1928, 2143, 2250, 1787, 400, 2532, 1556, 1912, 2475, 1513, 1704, 1284, 448, 625, 1676, 1730, 1434, 1607, 3, 612, 1034, 193, 1442, 208, 843, 1241, 1489, 2268, 825, 1384, 1981, 296, 1877, 177, 1298, 1880, 2141, 969, 7, 1477, 1313, 806, 798, 2015, 2337, 1622, 2395, 905, 2286, 1402, 412, 1908, 1084, 2138, 328, 1684, 556, 34, 2410, 2285, 2025, 663, 150, 1252, 0, 549, 2066, 792, 8, 366, 112, 1201, 2070, 704, 2099, 2302, 548, 482, 1982, 1397, 1640, 889, 2191, 2189, 1306, 1326, 2558, 503, 263, 1071, 2436, 1246, 2226, 789, 666, 2081, 593, 718, 544, 568, 1080, 364, 2485, 615, 1618, 1597, 1035, 361, 1700, 233, 24, 2327, 765, 1987, 861, 2021, 628, 1571, 1444, 1580, 2487, 2346, 2452, 2453, 54, 2158, 1452, 2343, 309, 1819, 1135, 1810, 1285, 1892, 2188, 1969, 2326, 1169, 494, 768, 1338, 5, 2228, 182, 1796, 1644, 924, 2432, 2112, 120, 815, 1583, 2207, 2480, 2205, 85, 1012, 1719, 2276, 904, 105, 1723, 1995, 2277, 2172, 2239, 1357, 637, 2085, 1040, 1922, 2512, 1230, 225, 658, 738, 1065, 242, 1468, 1639, 1734, 1210, 689, 788, 108, 1134, 2546, 207, 2413, 1523, 257, 1424, 2516, 671, 1124, 2134, 558, 335, 2224, 332, 1619, 1239, 926, 951, 891, 1445, 239, 493, 1609, 424, 449, 1732, 2311, 1852, 1759, 2254, 340, 1535, 2509, 1634, 447, 1572, 802, 1770, 395, 1030, 1411, 1645, 2212, 562, 311, 1420, 531, 1575, 1996, 502, 259, 202, 670, 2045, 682, 1390, 132, 2518, 565, 2356, 560, 190, 2220, 2149, 1418, 2079, 1355, 1109, 1113, 553, 2540, 2249, 1015, 2538, 564, 705, 214, 2525, 1993, 700, 341, 252, 430, 1937, 1140, 1878, 2011, 783, 1198, 86, 2507, 1275, 603, 1372, 1849, 1221, 2163, 604, 1522, 1339, 1558, 2517, 2535, 1496, 1204, 874, 1641, 1432, 895, 2492, 2271, 1467]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
The device used will be: 
True
cuda:0
The device used will be: 
True
cuda:0
chosen_head:  {'blocks.5.layer.bias': tensor([ 0.0052, -0.0217], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0350,  0.0053, -0.0105,  ...,  0.0090, -0.0074,  0.0153],
        [-0.0067,  0.0172,  0.0149,  ..., -0.0063,  0.0286, -0.0075]],
       device='cuda:0')}

 Model C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c loaded successfuly with best perf



 ----- Architecture Block BatchNorm2dSK3962(5, 5)1.0reflect, number 0 -----
- BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK963842(3, 3)0.6499999761581421reflect, number 1 -----
- BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.4)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK38415362(3, 3)0.25reflect, number 2 -----
- BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block BatchNorm2dSK153625602(3, 3)0.25reflect, number 3 -----
- BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block BatchNorm2dSK256025602(3, 3)0.25reflect, number 4 -----
- BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 5 -----
- Flatten(start_dim=1, end_dim=-1)
- Dropout(p=0.5, inplace=False)
- Linear(in_features=2560, out_features=2, bias=True)
model.heads:  [{'blocks.5.layer.bias': tensor([ 1.5791e-05, -1.6506e-02], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0120,  0.0150,  0.0043,  ...,  0.0072,  0.0111, -0.0045],
        [ 0.0163,  0.0075,  0.0001,  ..., -0.0045,  0.0101,  0.0122]],
       device='cuda:0')}, {'blocks.5.layer.bias': tensor([ 0.0040, -0.0205], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 1.5765e-02,  1.0660e-02,  4.3432e-03,  ...,  1.0930e-02,
          1.1431e-02, -3.5607e-03],
        [ 1.2527e-02,  1.1907e-02,  8.8815e-05,  ..., -8.2031e-03,
          9.7858e-03,  1.1328e-02]], device='cuda:0')}, {'blocks.5.layer.bias': tensor([ 0.0005, -0.0170], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0349,  0.0030,  0.0162,  ..., -0.0045,  0.0023,  0.0014],
        [-0.0066,  0.0195, -0.0117,  ...,  0.0072,  0.0189,  0.0064]],
       device='cuda:0')}, {'blocks.5.layer.bias': tensor([-0.0005, -0.0160], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0531,  0.0097,  0.0127,  ...,  0.0032,  0.0010,  0.0103],
        [-0.0248,  0.0129, -0.0082,  ..., -0.0005,  0.0202, -0.0025]],
       device='cuda:0')}, {'blocks.5.layer.bias': tensor([ 0.0052, -0.0217], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0350,  0.0053, -0.0105,  ...,  0.0090, -0.0074,  0.0153],
        [-0.0067,  0.0172,  0.0149,  ..., -0.0063,  0.0286, -0.0075]],
       device='cuda:0')}]
[Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (5): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=2560, out_features=2, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)]
LAYER NAME:  Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (5): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=2560, out_features=2, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)
LAYER CHILDREN:  [BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=1.4)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
  )
  (layer): Linear(in_features=2560, out_features=2, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)]
subsubl NAME:  Sequential(
  (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=1.4)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): Flatten(start_dim=1, end_dim=-1)
)
subsubl NAME:  Linear(in_features=2560, out_features=2, bias=True)
subsubl NAME:  Dropout(p=0.5, inplace=False)
CONFIG MODE:  unsupervised
CONFIG MODE:  supervised
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([49, 33, 72, 51, 71, 92, 15, 14, 23,  0, 71, 75, 81, 69, 40, 43, 92, 97,
        70, 53], device='cuda:0')
TARGETS AFTER SUB:  tensor([92, 92, 35, 92, 35, 92, 92, 92, 35, 35, 35, 35, 92, 92, 35, 35, 35, 35,
        92, 92])
[35, 92]
TARGETS AFTER CLEANER:  tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1])
200
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 200
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               None
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([19, 29,  0, 11,  1, 86, 90, 28, 23, 31, 39, 96, 82, 17, 71, 39,  8, 97,
        80, 71], device='cuda:0')
TARGETS AFTER SUB:  tensor([35, 92, 35, 35, 92, 35, 92, 35, 92, 35, 92, 92, 35, 35, 92, 35, 35, 92,
        35, 35])
[35, 92]
TARGETS AFTER CLEANER:  tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0])
1000
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 1000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               None
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([19, 29,  0, 11,  1, 86, 90, 28, 23, 31, 39, 96, 82, 17, 71, 39,  8, 97,
        80, 71], device='cuda:0')
TARGETS AFTER SUB:  tensor([35, 92, 35, 35, 92, 35, 92, 35, 92, 35, 92, 92, 35, 35, 92, 35, 35, 92,
        35, 35])
[35, 92]
TARGETS AFTER CLEANER:  tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0])
1000
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 1000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
------------------------
INDICES:  1000
IMAGE SIZE: torch.Size([64, 3, 32, 32])
The device used will be: 
True
cuda:0
output.detach().clone():  tensor([[ 10.0909,  -9.4039],
        [  6.2963,  10.0670],
        [ -2.5703,  -5.3536],
        [ 39.4494, -24.5744],
        [ 11.5106, -12.5590],
        [  8.3024, -14.5857],
        [ 22.6043,  -5.7670],
        [ 10.4970,  -1.7275],
        [ -5.4220,   8.9652],
        [ 14.0608,  -6.9180],
        [  6.2218, -10.4034],
        [ 11.6477,  -5.7212],
        [ 13.4273, -15.7044],
        [ 22.0457,  -6.1875],
        [ 13.6372, -13.6105],
        [ 11.8875,   4.4540],
        [  6.8424,   5.9525],
        [  7.9596,   2.3419],
        [ 17.1019,  16.5535],
        [  4.4034,  -5.9793],
        [  1.7202, -13.9479],
        [ 19.9042, -15.3669],
        [ 13.4473,  -1.7267],
        [ 12.8857,  -3.9677],
        [  9.6881,  -6.3827],
        [  7.3557,  -8.8037],
        [ 18.7070,  19.3835],
        [ 11.9009, -12.5760],
        [ 15.8071, -15.1595],
        [  4.3038,  -2.8020],
        [ 13.3878, -10.0034],
        [  8.2979,  -6.6194],
        [  8.2455, -10.2909],
        [  7.9134, -17.3862],
        [ 25.2508, -39.0822],
        [  8.0655,  -5.7110],
        [ 10.9935,  -1.4270],
        [  8.3280,   6.5264],
        [ 12.4570,  -7.5301],
        [  4.3359,  -4.4502],
        [ 11.9973,  -6.1851],
        [ 16.3162,  -4.8675],
        [ -5.4823,  -7.6506],
        [  8.9740,   9.0778],
        [ 22.4570, -13.1801],
        [ 18.8040,  -2.4458],
        [ 17.2320, -10.3347],
        [  3.1581,   6.4527],
        [ 16.0886,   0.6836],
        [ 18.6451,  -9.8620],
        [  6.5411,  -6.2409],
        [ 15.5508,  -9.3327],
        [ 29.6728,  -0.4668],
        [ 11.0839,  -7.5762],
        [ 24.4101, -21.1323],
        [ 14.3305, -12.1470],
        [ 31.1288, -24.6107],
        [ 21.5831, -17.5107],
        [ 14.4222, -17.6011],
        [ 18.3326, -23.8947],
        [ 17.5608,  -9.3838],
        [  9.7244,  -8.6286],
        [ 17.8337, -11.1985],
        [ 16.0919,   3.0166],
        [  7.0152,  -5.7851],
        [ 32.2666, -30.9087],
        [  8.7285,  -0.3811],
        [  6.5835,  -9.7173],
        [ 23.2532, -20.9089],
        [ 12.7057,   2.5794],
        [  3.3298,  -5.0488],
        [ 18.0483,   4.7155],
        [ 29.7115,  14.2433],
        [ 13.3580, -12.3796],
        [  7.5059,   8.7877],
        [ 11.2804,   8.8006],
        [ 11.0771,  10.7326],
        [ -2.9794,   4.9438],
        [ -2.5664,   2.7158],
        [  3.7402,   0.7910],
        [ 12.2150, -14.8612],
        [ 10.6156,  -2.9991],
        [ 20.0347, -25.6577],
        [  8.7376,  -4.8928],
        [ -1.9130,  -3.6860],
        [  1.5641,   5.3374],
        [ 10.8572, -17.8518],
        [  8.9869,   9.0204],
        [ -0.3349,  13.6797],
        [ 14.7351, -15.6442],
        [  1.1596,  22.1427],
        [  9.9129, -11.4626],
        [ -0.6874,  -1.2204],
        [  7.9750,  -6.2981],
        [ 19.5153, -19.6172],
        [ 16.7857,  -6.9650],
        [  4.0700, -11.4159],
        [  9.7250,   3.4779],
        [ 26.3690, -22.0046],
        [ 12.5694,  -1.6325],
        [ 33.3189, -26.5145],
        [ 25.9438,  -8.3767],
        [ 13.9523, -10.5948],
        [ 13.3263,  -9.3398],
        [ -2.6049,  11.6078],
        [  7.0417,  -4.6349],
        [  7.6755,   3.9477],
        [ 12.2411,   5.9474],
        [ 15.7439,   7.5348],
        [ 20.5682, -11.6387],
        [  6.7893,  -2.0808],
        [  8.8716,   7.3818],
        [ 19.9407,   1.8788],
        [ 15.7652,   6.2189],
        [  6.3213,  -7.7371],
        [ 16.5526, -12.6903],
        [ 10.3430,   2.7723],
        [  7.8989, -23.1952],
        [ 19.7312,  14.5834],
        [ 19.5872,  -1.8277],
        [ 13.6040,  -4.0479],
        [ 12.7613,  -2.1573],
        [ 21.8092, -18.6483],
        [ 20.4178,  21.3706],
        [  8.1957,  -8.3261],
        [ 16.4627, -19.1499],
        [ 16.4459,   0.7778],
        [  8.3828, -10.7846],
        [  1.9038,   8.2525],
        [ 11.0602,  -1.5877],
        [ 10.0372,  -9.5922],
        [  6.0489,   1.8077],
        [ 11.8754,  -0.4000],
        [ 29.0486, -17.8192],
        [ 14.2083,  -8.1144],
        [ 34.1421, -22.1460],
        [  4.2521,  14.3500],
        [  1.6798,   9.0009],
        [ 15.0460,  -9.1337],
        [  7.9687,  -2.3975],
        [  7.3664,  11.8808],
        [ 12.3385,   0.6417],
        [  7.1591, -11.7553],
        [ 12.3622, -16.5797],
        [  5.4019,  -9.3814],
        [ -1.2043, -10.4203],
        [  6.4818,  -4.9416],
        [ 21.3970, -12.4446],
        [ 11.2959, -15.6797],
        [ 18.4967,  -3.3342],
        [ 14.2040,  -4.6556],
        [ 20.1746, -20.9574],
        [ 14.0180,  13.4117],
        [  6.7849,  -5.7082],
        [ 14.9433, -23.0857],
        [ 14.5511,  -8.9990],
        [ 31.7650, -24.0184],
        [ 21.3778,   1.0823],
        [ 15.0561,  -2.6877],
        [  9.2120, -13.7861],
        [ 18.8293,   1.7548],
        [  8.0953,   6.0125],
        [ 13.0712,  12.9686],
        [ 24.2495, -23.2472],
        [  3.1037,  -1.5173],
        [ 21.3228, -17.7485],
        [  7.7829,   1.1332],
        [ 13.6487,  -5.6063],
        [  1.2093,  -2.8709],
        [ -1.4266,  -9.2837],
        [ 13.2350,  -5.4863],
        [  8.0578,  -4.3450],
        [ 16.0312,   5.7410],
        [  0.7682,   4.2931],
        [  2.5825, -20.2008],
        [ 13.8099,   8.9631],
        [  8.2746,  -4.9225],
        [ -0.9725,  -3.8393],
        [ 14.1533, -14.8915],
        [  4.5250,  -4.9010],
        [ 22.4229,  -8.4942],
        [ 18.1080,  -9.5937],
        [ 13.8302,  -3.0226],
        [  1.4453,  -0.2056],
        [ 32.6785, -23.2968],
        [ 27.8652, -22.8577],
        [  4.0140,   0.4502],
        [ 14.2416, -12.9809],
        [ 12.2032,  -9.5534],
        [ 29.3141, -29.2776],
        [ 19.6864, -25.5826],
        [  4.2375, -16.4870],
        [ 29.6808, -34.2817],
        [ 20.2095,   8.8879],
        [ 10.8610,  -9.0697],
        [  5.3116,  18.0222],
        [ 38.3480, -15.6596],
        [ 12.5762, -13.0083],
        [ 16.2690, -10.6697],
        [  8.0501,  -2.9517]], device='cuda:0')
output.data.max(1)[1]:  tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0], device='cuda:0')
tot_sum:  tensor(64.8241, device='cuda:0') 0
The device used will be: 
True
cuda:0
output.detach().clone():  tensor([[-1.0726e+01,  1.8768e+01],
        [-5.2535e+01,  5.1860e+01],
        [ 9.6922e+00,  6.5610e+00],
        [-5.1474e+00,  1.0747e+01],
        [ 1.5340e+01, -6.4226e+00],
        [-1.0532e+01,  1.5539e+01],
        [-2.6638e+01,  3.4254e+01],
        [ 5.3467e-01, -6.8181e+00],
        [-3.6942e+01,  3.1046e+01],
        [-5.2446e+01,  5.9589e+01],
        [-3.1343e+01,  4.0272e+01],
        [ 7.8950e+00, -9.8029e+00],
        [-1.1174e+01,  1.5036e+01],
        [ 1.2428e+01, -1.8237e+00],
        [-3.1254e+01,  2.9838e+01],
        [ 9.9571e+00,  5.3278e+00],
        [ 4.5104e+00, -1.1585e+00],
        [-3.4283e+01,  3.6690e+01],
        [-4.3722e+01,  4.4817e+01],
        [-5.0512e+01,  5.8258e+01],
        [-2.6104e+01,  3.5381e+01],
        [-6.1217e+00,  1.2048e+01],
        [-2.1713e+01,  2.2400e+01],
        [-3.7351e+00,  4.1800e+00],
        [-5.0179e+01,  5.4543e+01],
        [ 3.8249e+00, -2.0337e+00],
        [ 5.2726e+00,  8.0722e+00],
        [-4.8402e-01, -2.2033e+00],
        [-2.2682e+01,  2.1234e+01],
        [-5.5189e+00,  1.1350e+00],
        [-1.9346e+00,  1.3383e+01],
        [-4.2502e+01,  5.2068e+01],
        [-2.5304e+01,  2.5604e+01],
        [-1.8170e+01,  4.0859e+01],
        [ 2.8230e-01,  7.9426e-01],
        [-1.9466e+01,  3.0147e+01],
        [-5.2694e+00, -2.8732e+00],
        [ 9.1523e+00,  2.4503e+01],
        [ 7.3729e+00,  1.2708e+01],
        [-3.7840e+01,  3.5562e+01],
        [ 6.3046e+00,  2.2793e+01],
        [-2.8214e+01,  4.7461e+01],
        [-5.8793e+00,  6.0285e+00],
        [ 1.0688e+01,  1.1296e+01],
        [-2.8057e+01,  3.9286e+01],
        [-3.3904e-01,  1.8391e+01],
        [-4.9884e+01,  5.3161e+01],
        [ 3.2341e+00,  9.1342e+00],
        [ 7.9038e+00,  7.9542e+00],
        [ 1.7379e+01,  4.3934e+00],
        [ 5.4343e+00, -1.0031e+01],
        [ 1.0118e+01, -6.4567e-01],
        [ 1.5575e+01, -1.1111e+01],
        [-1.2031e+01,  3.0220e+01],
        [ 1.8702e-01,  1.4913e+00],
        [ 6.6621e+00,  5.0583e+00],
        [ 8.1275e+00, -4.5845e+00],
        [-2.1655e+01,  3.4770e+01],
        [-9.9281e+00,  1.0906e+01],
        [-8.4990e+00,  1.2007e+01],
        [-6.5401e+01,  7.5702e+01],
        [-5.2168e+00,  2.2440e+01],
        [-4.9321e+01,  5.6125e+01],
        [-2.6311e+01,  2.7813e+01],
        [-4.3540e+01,  3.7977e+01],
        [-8.5471e+00,  7.4986e+00],
        [-5.4004e+00, -1.9870e-01],
        [ 1.7877e+01,  2.3911e+01],
        [-1.7795e+01,  6.1703e+00],
        [-6.7190e-01,  6.4840e+00],
        [-8.7040e+00,  7.9657e+00],
        [ 1.1078e+00,  1.0756e+00],
        [ 4.1658e+00,  2.7355e+00],
        [-3.5288e+01,  3.3738e+01],
        [-2.9042e+01,  2.1696e+01],
        [-5.3698e+01,  5.5056e+01],
        [-4.0812e+01,  3.9902e+01],
        [-2.9052e+01,  3.4150e+01],
        [ 1.6083e+01,  5.7270e+00],
        [-2.4572e+01,  2.4470e+01],
        [-5.3033e+01,  5.3069e+01],
        [ 1.1424e+01,  1.1909e+01],
        [-3.6684e+01,  4.6065e+01],
        [ 2.4793e+01, -1.0686e+01],
        [ 1.5589e+00,  1.4799e+01],
        [-2.4967e+01,  2.8541e+01],
        [-3.0486e+01,  1.9776e+01],
        [ 5.6968e-01,  1.5724e+01],
        [-4.6666e+01,  4.6693e+01],
        [-3.0401e-01,  5.8560e+00],
        [ 2.1976e+01,  4.0641e+00],
        [ 1.4196e+01, -6.3394e+00],
        [-1.6700e+01,  2.6256e+01],
        [-5.0231e+00,  1.7818e+01],
        [-3.2092e+01,  1.9864e+01],
        [-1.4151e+00,  1.4618e+01],
        [-2.0698e+00,  4.4242e+00],
        [-3.7166e+01,  3.2565e+01],
        [ 4.9747e+00,  1.2433e+00],
        [ 1.6278e+01,  4.3059e+00],
        [-3.0063e+01,  3.8845e+01],
        [ 7.0831e+00,  2.5275e+00],
        [ 1.7536e+01, -2.6819e+00],
        [ 8.9439e+00, -5.2312e+00],
        [-4.0492e+01,  4.4479e+01],
        [-1.9835e+01,  1.9459e+01],
        [ 1.4174e+01, -3.3669e+00],
        [-3.7445e+00,  5.2846e+00],
        [-4.9352e+01,  4.6950e+01],
        [ 6.0816e+00, -8.1272e+00],
        [ 1.0213e+01, -4.3003e+00],
        [-3.1642e+01,  3.3986e+01],
        [ 8.8809e+00, -6.2313e+00],
        [-1.0643e+00,  5.9909e+00],
        [-7.3302e+00,  1.5507e+01],
        [-4.0671e+01,  5.5545e+01],
        [ 4.3179e+00, -2.6411e+00],
        [-2.6967e+01,  2.2785e+01],
        [-9.5290e+00,  1.8481e+01],
        [-3.7130e+01,  3.8390e+01],
        [-2.4907e+01,  3.5844e+01],
        [-3.8438e+00,  8.3749e+00],
        [-5.2331e+00,  1.1327e+01],
        [-4.3925e+01,  5.2439e+01],
        [-5.0304e+01,  3.7171e+01],
        [-4.6889e+01,  5.6437e+01],
        [-3.3164e-02,  9.8537e+00],
        [ 3.2999e+00,  5.4510e-03],
        [-2.9104e+01,  4.0727e+01],
        [-3.6795e+01,  2.1498e+01],
        [ 2.2752e+01,  2.1203e+01],
        [-5.1450e+01,  5.2097e+01],
        [-1.4666e+01,  1.8738e+01],
        [ 1.2149e+01, -3.1467e+00],
        [-3.5667e+01,  3.9051e+01],
        [ 9.6494e+00, -9.7799e+00],
        [-3.9423e+01,  4.1009e+01],
        [-5.8673e+00,  2.8327e+01],
        [-4.8706e+01,  5.7622e+01],
        [ 2.2919e+01,  1.1395e+01],
        [ 1.4289e+01,  7.5302e+00],
        [ 2.4463e+01,  2.9668e+00],
        [ 1.3869e+01, -2.3933e+00],
        [-5.2665e+01,  6.0413e+01],
        [ 4.2942e+00,  1.2047e+01],
        [-4.3109e+01,  4.0463e+01],
        [ 7.5523e+00, -7.6667e+00],
        [-1.0884e+01,  1.5945e+01],
        [-3.1317e+01,  4.4297e+01],
        [-8.0455e+00,  2.6053e+01],
        [-6.3516e+00,  1.1923e+01],
        [ 1.2296e+00, -9.1537e+00],
        [-3.7225e+01,  3.8227e+01],
        [ 1.1081e+01,  5.2818e+00],
        [ 1.0886e+01, -2.1171e+00],
        [-4.5021e+01,  3.9398e+01],
        [-2.6505e+01,  4.3277e+01],
        [ 2.1274e+01, -2.6725e+00],
        [-5.3157e+01,  5.9675e+01],
        [ 5.6904e+00, -4.4509e+00],
        [ 6.1210e+00,  1.7181e+01],
        [-3.7070e+01,  3.5351e+01],
        [-1.3387e+01,  1.6548e+01],
        [-1.4131e+01,  1.7976e+01],
        [ 8.8867e-01, -2.4647e+00],
        [ 1.2923e+01,  9.8496e+00],
        [-7.1549e+01,  8.9308e+01],
        [ 7.5344e+00,  1.5229e+01],
        [-4.4831e+01,  3.5358e+01],
        [-1.1537e+01,  4.5418e+00],
        [-1.1028e+00,  1.8670e+01],
        [-3.0027e+00, -1.3116e-01],
        [-2.6736e+01,  3.3371e+01],
        [ 1.7670e+00, -5.3705e-01],
        [-1.5963e+01,  1.5531e+01],
        [-1.5818e+01,  3.2655e+01],
        [ 6.8711e+00, -2.1627e+00],
        [-3.9873e+01,  5.5036e+01],
        [ 9.9545e+00,  2.8136e+01],
        [-3.7665e+01,  3.6882e+01],
        [-2.7866e+01,  3.8022e+01],
        [ 1.3955e+01, -1.7935e+01],
        [-8.8233e+00,  2.7932e+01],
        [-2.2399e+01,  2.0737e+01],
        [-5.0817e+00,  3.4287e+01],
        [-3.6281e+01,  3.2064e+01],
        [-6.4197e+01,  7.1094e+01],
        [-3.5703e+01,  3.1129e+01],
        [-5.9382e+00,  1.0475e+01],
        [ 1.3442e+01,  9.8365e+00],
        [-1.7409e+01,  3.1338e+01],
        [ 5.1892e+00,  3.1582e+00],
        [-2.2448e+01,  1.9269e+01],
        [ 4.5749e+00, -1.2175e+00],
        [ 4.5669e+00, -2.6026e+00],
        [-1.5242e+01, -2.3766e+00],
        [-2.9955e+01,  1.7705e+01],
        [-2.6609e+01,  3.8605e+01],
        [-5.1683e+01,  3.7852e+01],
        [ 5.2157e+00, -1.0028e+01]], device='cuda:0')
output.data.max(1)[1]:  tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0], device='cuda:0')
tot_sum:  tensor(57.8438, device='cuda:0') 1
The device used will be: 
True
cuda:0
output.detach().clone():  tensor([[ 1.2102e+01, -7.3939e+00],
        [-3.4450e+00,  1.5370e+00],
        [ 1.5697e+01,  1.8693e+00],
        [ 2.4218e+01, -1.0110e+01],
        [ 1.3946e+01, -3.0096e+00],
        [ 1.0161e+01, -1.2320e+00],
        [-5.2557e-01, -2.5753e-01],
        [ 1.3211e+01, -1.7808e+01],
        [ 1.2981e-01, -4.3117e+00],
        [ 1.5977e+01,  1.1453e+01],
        [ 4.6922e+00, -6.1405e+00],
        [ 8.8452e+00, -1.3063e+01],
        [ 7.9250e-01,  6.3499e+00],
        [ 2.6826e+01, -2.5749e+01],
        [ 9.4557e+00, -2.6518e+00],
        [-2.6930e+00,  1.5488e+01],
        [ 2.0688e+00, -1.0464e-01],
        [ 1.0933e+01, -1.1048e+01],
        [ 1.3744e+01, -5.5674e+00],
        [ 1.0989e+01,  5.3042e+00],
        [ 1.0472e+01,  2.7619e+01],
        [ 1.8546e+01,  2.0373e+00],
        [ 2.6314e+01, -1.5710e+01],
        [ 1.0057e+01, -1.0733e+01],
        [ 2.3327e+01, -2.2325e+01],
        [ 1.4941e+01, -1.3264e+01],
        [ 1.1831e+01, -4.2143e+00],
        [ 9.7553e+00,  9.4915e+00],
        [ 6.0666e+00,  1.6393e+01],
        [ 3.1788e-01, -2.8180e-01],
        [ 2.8295e+01,  9.1097e-01],
        [ 1.0666e+00, -6.9630e+00],
        [ 1.2997e+01, -8.9253e+00],
        [ 1.2091e+01, -7.6273e+00],
        [ 6.8890e+00,  8.5935e-01],
        [-2.1653e+01,  6.3565e+00],
        [ 1.7270e+01, -1.0369e+01],
        [ 2.2854e+01, -1.1379e+01],
        [-2.8840e-01,  7.3319e-01],
        [ 4.0843e+01, -3.1925e+01],
        [ 3.4035e+00,  1.4785e+01],
        [ 1.8788e+01, -1.6605e+01],
        [ 1.2729e+01, -2.4957e+01],
        [ 5.4517e+00, -1.8776e+00],
        [ 3.4973e-01, -4.9511e+00],
        [ 6.5069e+00,  9.8342e+00],
        [ 1.9721e+01,  6.3181e+00],
        [ 2.5972e+00, -8.2205e+00],
        [ 1.7464e+01, -1.4303e+01],
        [ 1.6614e+01,  5.1581e+00],
        [ 2.0545e+01, -2.5357e+01],
        [ 9.3266e+00,  5.8356e+00],
        [-7.4243e+00,  1.7035e+01],
        [ 2.3104e+01, -1.4756e+01],
        [ 1.7442e+00, -1.4442e+00],
        [ 4.7786e+00,  1.2980e+01],
        [ 2.3808e+01, -6.9713e+00],
        [ 1.9940e+01, -8.2199e+00],
        [ 1.4829e+01, -6.9728e+00],
        [ 1.0531e+01, -9.2911e+00],
        [ 5.6936e+00, -1.9525e+01],
        [ 3.2673e+01, -2.7074e+01],
        [ 4.2764e+00, -3.0160e+00],
        [ 1.8252e+01,  3.5676e+00],
        [ 1.9904e+01, -1.8526e+00],
        [ 5.2918e+00, -1.6917e+01],
        [ 6.8304e+00, -1.4177e+01],
        [ 5.4872e+00, -4.8005e+00],
        [ 6.5955e+00, -7.6441e+00],
        [ 1.9387e+00,  3.6323e+00],
        [ 3.7814e-01,  7.3680e+00],
        [ 3.5402e+01, -1.9545e+01],
        [ 2.1904e+01,  7.8378e-01],
        [ 2.0150e+01,  2.6132e+00],
        [ 1.4095e+01,  2.2681e+00],
        [ 2.5434e+01, -2.1070e+01],
        [ 1.0032e+01, -2.0743e+01],
        [ 2.1179e+01, -8.8111e+00],
        [ 1.4413e+01, -1.6690e+01],
        [ 1.6757e+01, -1.5079e+01],
        [ 1.0757e+01, -1.2333e+01],
        [ 2.2518e+01, -1.7981e+01],
        [ 9.4391e+00,  1.9658e+01],
        [ 5.0223e+00,  1.8747e+00],
        [ 3.0143e+01,  4.1712e+00],
        [ 2.0967e+01, -1.9609e+01],
        [-3.0753e+00,  8.1733e+00],
        [ 8.2627e+00,  3.7331e+00],
        [-1.2353e+01,  2.2509e+01],
        [ 1.9916e+01, -8.0743e-01],
        [ 2.3604e+01, -2.2957e+01],
        [ 1.8864e+01, -2.7006e+01],
        [-3.3456e+00, -9.7876e+00],
        [ 3.1088e+01, -3.8082e+01],
        [-2.0191e+00,  3.5207e+00],
        [ 1.7905e+01, -7.0980e+00],
        [ 6.4285e+00, -4.0845e+00],
        [ 9.3810e+00,  5.9038e+00],
        [ 3.9914e+01, -2.8684e+01],
        [-6.7390e+00,  1.0725e+01],
        [-5.2068e+00,  1.8187e+01],
        [ 3.6205e+00,  5.7608e+00],
        [ 2.7890e+00,  9.2350e-01],
        [-2.2209e+01,  4.2289e+01],
        [ 9.0814e+00, -4.0742e+00],
        [ 9.7227e+00, -1.5322e+01],
        [ 2.5164e+00, -1.4766e+01],
        [ 9.2539e+00, -1.1300e+01],
        [ 2.5928e+01, -2.0376e+01],
        [ 1.4932e+00, -9.4174e+00],
        [-9.6562e-02,  2.4575e-01],
        [ 2.0611e+01, -2.0742e+01],
        [ 1.1621e+01, -1.0643e+01],
        [ 3.4908e+00, -1.2964e+01],
        [ 3.4644e+01, -1.2660e+01],
        [ 2.3548e+00,  6.1592e+00],
        [-5.4634e-02, -1.4953e+00],
        [ 9.7516e+00, -6.2441e+00],
        [ 9.4861e+00, -7.9461e+00],
        [-4.9380e-01, -1.2255e+00],
        [-5.2979e+00,  2.8600e+01],
        [ 1.8726e+01,  4.5523e+00],
        [ 8.1704e+00, -5.7638e+00],
        [ 2.8702e+01, -1.2344e+01],
        [ 1.4336e+01, -1.1333e+00],
        [ 3.3882e+00,  9.9564e+00],
        [ 1.2574e+01, -1.6958e+01],
        [ 1.2896e+01, -9.0516e+00],
        [-5.2310e+00,  1.6854e+01],
        [-5.5237e+00,  3.8619e+00],
        [ 8.1062e+00, -1.0508e+01],
        [-2.0211e-01,  6.2956e+00],
        [ 5.2757e+00,  3.6401e+00],
        [ 1.6818e+01, -2.8892e+00],
        [ 5.7741e+00,  1.0998e+01],
        [ 2.5399e+01, -2.9379e+01],
        [ 1.0816e+01, -7.5105e+00],
        [-3.4098e+00,  1.2193e+01],
        [-1.7712e+00,  1.7976e+00],
        [ 2.0542e+01, -2.0974e+01],
        [ 3.1083e+01, -2.4565e+01],
        [ 1.2615e+01, -9.0722e+00],
        [ 1.3428e+01, -8.8974e+00],
        [-3.0402e+00,  1.9293e+01],
        [-2.4010e-01,  1.3355e+01],
        [ 2.5504e+01, -1.5684e+01],
        [ 1.9008e+01, -4.0616e-01],
        [-1.8488e+00,  4.3281e-01],
        [-1.1154e+01, -6.4647e+00],
        [ 2.0543e+00,  8.2468e+00],
        [ 1.4532e+01, -5.7623e+00],
        [-1.5310e-01,  3.4305e+00],
        [ 8.1158e+00,  1.3694e+01],
        [ 3.4429e-01, -3.5234e+00],
        [ 3.0740e+01, -1.5866e+01],
        [ 2.1146e+01, -1.4928e+01],
        [ 4.3218e+00, -9.8843e+00],
        [-4.3833e+00,  9.4444e+00],
        [ 1.6618e+01, -1.4827e+01],
        [ 4.6603e+00,  1.2659e+00],
        [-2.1706e+00,  1.3619e+01],
        [ 1.7462e+01, -2.0108e+01],
        [ 1.4882e+01, -8.2466e+00],
        [ 1.5183e+01,  2.6605e+01],
        [-1.2620e+01,  1.3715e+01],
        [ 1.2740e+01, -6.8278e+00],
        [ 2.4741e+01, -1.8929e+01],
        [ 2.6749e+01, -2.4395e+01],
        [ 1.1146e+01, -7.7945e+00],
        [ 1.8991e+01, -1.7761e+01],
        [-1.0427e+00,  2.6288e+00],
        [ 1.8718e+01, -1.3791e+01],
        [ 2.1791e+01,  2.2164e+01],
        [-3.2408e+00,  1.0681e-01],
        [ 2.9635e+01, -6.8624e+00],
        [ 5.6780e+00, -6.0541e+00],
        [ 1.0124e+01, -1.0226e+01],
        [ 3.6954e+00, -9.9789e+00],
        [-4.3727e-01,  3.4092e+01],
        [-3.9320e+00,  1.3480e+01],
        [ 1.4406e+01, -1.1022e+01],
        [ 3.5452e+01, -2.5980e+01],
        [-5.9296e+00,  2.9263e+01],
        [ 8.4698e+00,  5.3288e-01],
        [-6.0731e+00,  1.4115e+01],
        [-2.8685e+00,  5.5179e+00],
        [ 2.3110e+01, -1.9753e+01],
        [ 4.7176e+00,  4.2345e+00],
        [ 1.0836e+01, -1.2803e+00],
        [ 1.6277e+01, -1.7187e+01],
        [-1.3816e+00, -1.3058e+00],
        [-2.3726e+00, -2.2018e+00],
        [ 8.3653e+00, -9.1037e+00],
        [-4.1786e+00,  1.4859e+01],
        [ 1.9993e+01, -5.1387e+00],
        [-4.5068e+00,  1.4073e+01],
        [-3.2753e-02,  1.8040e+01],
        [ 3.8554e+00,  6.6869e-03],
        [ 1.5728e+01,  1.4954e+00],
        [ 2.1705e+01, -1.2429e+01]], device='cuda:0')
output.data.max(1)[1]:  tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0], device='cuda:0')
tot_sum:  tensor(76.2169, device='cuda:0') 2
The device used will be: 
True
cuda:0
output.detach().clone():  tensor([[ 3.1220e+01, -2.3472e+01],
        [ 1.9251e+01, -2.0970e+01],
        [ 4.3154e+01, -4.1894e+01],
        [ 1.1596e+01, -1.1972e+01],
        [ 2.0979e+01,  6.4504e+00],
        [ 2.9850e+01, -1.7482e+01],
        [ 1.1395e+01, -6.8640e+00],
        [ 6.4421e+00, -2.8993e+00],
        [ 3.6075e+01,  5.7129e+00],
        [ 2.7393e+01, -2.9439e+01],
        [ 2.8722e+01, -1.9793e+01],
        [ 9.9380e+00,  9.1701e+00],
        [-1.3538e+01,  2.3148e+01],
        [ 2.7306e+01, -3.6779e+01],
        [ 1.8525e+01, -7.0771e+00],
        [ 5.1035e+01, -3.7107e+01],
        [ 2.3152e+01, -1.3423e+00],
        [-3.6512e-02,  1.2757e+00],
        [ 8.0780e+00, -7.7781e+00],
        [ 1.7099e+01, -2.8724e+01],
        [ 5.5560e+00,  7.2386e+00],
        [ 2.9235e+01, -2.0721e+01],
        [ 2.0718e+01, -1.9132e+01],
        [ 3.1616e+01, -1.3609e+01],
        [ 3.8155e+01, -2.0103e+01],
        [ 4.6900e+01, -3.5671e+01],
        [ 1.5601e+01, -1.1888e+01],
        [ 3.6918e+01, -2.7370e+01],
        [ 1.9753e+01, -8.0332e+00],
        [ 4.1640e+01, -4.0283e+01],
        [ 3.4505e+01, -3.6782e+01],
        [ 1.3389e+01, -7.1710e+00],
        [ 2.3224e+01, -1.1228e+01],
        [ 1.5023e+01, -1.2840e+01],
        [ 2.0463e+01, -1.8787e+01],
        [ 4.6172e+01, -4.2100e+01],
        [ 2.6569e+01, -1.7786e+01],
        [-1.6465e+01,  2.5468e+01],
        [ 1.9685e+01,  9.5204e+00],
        [ 2.2388e+01, -8.2805e+00],
        [ 3.5122e+01, -4.1019e+01],
        [ 1.8012e+01, -1.2441e+01],
        [ 2.4514e+01, -1.5562e+01],
        [ 2.2090e+01,  3.7010e-01],
        [ 9.5611e+00,  3.5538e+00],
        [ 4.2659e+01, -4.9654e+01],
        [-2.4340e+00, -1.5185e+01],
        [ 3.0756e+01, -2.4121e+01],
        [ 1.6006e+01, -1.8693e+01],
        [ 7.5166e+00, -7.3675e+00],
        [ 2.2007e+01, -1.3965e+01],
        [ 4.6008e+01, -2.2730e+01],
        [ 2.3740e+01, -7.3992e+00],
        [ 1.0667e+01,  8.0774e-01],
        [ 2.9775e+01, -2.6391e+01],
        [ 3.8265e+01, -3.8367e+01],
        [ 3.5181e+01, -2.5904e+01],
        [ 3.5316e+01, -4.3459e+01],
        [ 2.3811e+01, -2.3942e+01],
        [ 2.8026e+01, -1.1668e+01],
        [ 3.3654e+01, -3.8251e+01],
        [ 2.6417e+01, -2.0866e+01],
        [ 6.1180e+01, -5.1799e+01],
        [-1.1905e+00,  1.1346e+01],
        [ 1.3210e+01, -1.2133e+01],
        [ 3.3557e+01, -2.0355e+01],
        [ 3.3917e+01, -3.5366e+01],
        [-5.2588e+00, -2.6655e+00],
        [ 4.2345e+01, -4.1698e+01],
        [ 3.3139e+01, -2.3667e+01],
        [ 2.0130e+01, -8.8293e-01],
        [ 1.9366e+01,  6.6737e+00],
        [ 2.2033e+01, -2.3583e+01],
        [ 4.8314e+01, -4.1797e+01],
        [ 3.4822e+01, -3.0285e+01],
        [ 3.5599e+01, -3.2438e+01],
        [ 4.0208e+01, -1.7520e+01],
        [ 2.7248e+01, -2.3262e+01],
        [ 5.1280e+00,  5.1729e+00],
        [ 1.5398e+01, -1.6308e+01],
        [ 3.0763e+01, -1.3991e+01],
        [ 3.2244e+01, -3.3292e+01],
        [ 5.4617e+01, -5.9219e+01],
        [ 2.1415e+01, -2.7698e+01],
        [ 1.2639e+01, -1.4215e+01],
        [ 2.5081e+01, -2.6497e+01],
        [ 9.6026e+00,  1.3731e+01],
        [ 2.9379e+01, -1.8776e+01],
        [ 1.3785e+00,  5.5227e+00],
        [ 2.1090e+01, -2.5069e+01],
        [ 1.3540e+01, -2.0887e+01],
        [ 1.8219e+00, -4.9560e+00],
        [-1.3568e+00,  1.4337e+01],
        [-4.3265e-01, -1.1817e+01],
        [ 2.1586e+01, -2.2324e+01],
        [ 4.4315e+01, -2.6749e+01],
        [-1.2078e+01, -1.0548e+00],
        [ 3.4410e+01, -3.1133e+01],
        [ 7.0630e+00,  6.2815e+00],
        [ 3.3380e+01, -2.3814e+01],
        [ 1.8904e+01,  1.0193e+01],
        [ 2.8830e+01,  1.5125e+01],
        [ 2.2235e+01, -1.7137e+01],
        [-6.7542e+00,  1.7435e+01],
        [ 4.1204e+01, -6.8898e+00],
        [ 2.8333e+01, -1.9985e+01],
        [ 3.1585e+01, -8.8121e+00],
        [ 9.7613e+00, -5.0531e+00],
        [ 1.4175e+01, -2.6404e+01],
        [ 2.3411e+01, -5.6516e+00],
        [ 3.4531e+01, -4.0094e+01],
        [ 1.0644e+01, -1.7284e+00],
        [ 4.4289e+01, -4.7469e+01],
        [ 4.2955e-01,  1.0720e+00],
        [ 1.2659e+01,  9.1125e+00],
        [ 1.4907e+01,  5.1738e+00],
        [ 4.7262e+01, -4.7694e+01],
        [ 3.3665e+01, -3.7882e+01],
        [ 3.1352e+00,  7.2677e-01],
        [ 5.2432e+01, -5.0088e+01],
        [ 3.2422e+01, -2.2867e+01],
        [ 3.2678e+01, -1.0694e+01],
        [ 2.2816e+01, -5.9796e+00],
        [ 3.2797e+01, -2.3879e+01],
        [ 3.6407e+01, -3.7190e+01],
        [ 3.1313e+01, -2.7469e+01],
        [ 3.5370e+01, -3.6045e+01],
        [ 4.4228e+01, -3.7086e+01],
        [ 1.6338e+01, -1.5108e+01],
        [ 1.8999e+00,  6.4070e-02],
        [ 2.4250e+01, -1.3314e+01],
        [ 1.9581e+01, -2.1983e+01],
        [ 3.9127e+01, -5.4721e+00],
        [ 4.0751e+01, -1.8932e+01],
        [ 2.1924e+01, -2.0133e+01],
        [ 1.1208e+01,  4.6497e+00],
        [ 3.6616e+01, -3.3042e+01],
        [ 6.1496e+00, -1.0962e+01],
        [ 1.3028e+01, -1.7210e+01],
        [ 4.2651e+01, -2.7777e+01],
        [ 2.1384e+01, -1.6376e+01],
        [ 1.3482e+01, -1.9082e+01],
        [ 1.5485e+01,  8.7832e-01],
        [ 2.4149e+01, -1.8337e+01],
        [ 1.0214e+01, -1.4598e+01],
        [ 3.4627e+01, -4.8459e+01],
        [ 2.6474e+00, -2.2028e+00],
        [ 1.5773e+01,  2.4157e+00],
        [ 2.9163e+01, -2.8185e+01],
        [ 3.8699e+01, -3.5341e+01],
        [ 1.7519e+01, -9.3424e+00],
        [-8.8828e+00,  1.3809e+01],
        [ 2.0030e+01,  5.5329e-01],
        [ 1.7259e+01, -9.4026e+00],
        [ 1.4496e+01, -1.0032e+01],
        [ 1.8451e+01, -1.6912e+01],
        [ 2.8663e+01, -2.1766e+01],
        [ 1.7381e+00,  3.3229e+00],
        [ 1.4247e+01, -2.4958e+01],
        [ 2.5805e+01, -2.5119e+01],
        [ 3.6768e+01, -4.2392e+01],
        [-8.7009e-01,  7.5547e-01],
        [ 4.4690e+01, -4.3688e+01],
        [ 1.9049e+01, -1.5542e+01],
        [ 3.1438e+01, -2.3822e+01],
        [ 2.7317e+01, -1.1064e+01],
        [ 2.2124e+01, -1.9474e+01],
        [ 1.4504e+00, -3.1124e+00],
        [ 2.5646e+01, -2.3968e+01],
        [ 2.5970e+01,  1.2120e+01],
        [ 1.1717e+01, -5.8048e+00],
        [ 4.8726e+01, -4.8690e+01],
        [ 8.8178e+00,  9.7840e+00],
        [ 1.7150e+01, -1.6055e+01],
        [ 3.7889e+01, -2.0666e+01],
        [ 2.1572e+00, -1.7454e+01],
        [ 2.5569e+01, -2.3215e+01],
        [-3.0821e+00,  1.1740e+00],
        [ 5.0333e+01, -4.2587e+01],
        [-2.4427e+00,  1.7297e+01],
        [ 2.1526e+01, -1.1705e+01],
        [ 5.6198e+01, -5.1834e+01],
        [ 3.7172e+01, -1.4408e+01],
        [ 4.3708e+01, -3.8109e+01],
        [ 2.3782e+01, -2.0430e+01],
        [-3.9786e+00,  9.9047e+00],
        [ 2.9158e+01, -2.6751e+01],
        [ 3.3516e+00, -4.6447e-02],
        [ 7.2727e+00, -7.2465e+00],
        [ 1.3818e+01,  9.4840e+00],
        [ 7.4869e+00, -1.3936e+00],
        [-1.3898e+01,  2.5521e+01],
        [ 2.9794e+01, -2.1025e+01],
        [ 1.6368e+01, -2.0942e+01],
        [ 1.1886e+01,  3.2758e+00],
        [ 3.7346e+01, -3.9993e+01],
        [ 7.1749e+00,  8.1097e+00],
        [ 2.2888e+00,  1.4004e+01],
        [ 6.2456e+01, -5.5652e+01],
        [ 2.9166e+01, -1.8359e+01]], device='cuda:0')
output.data.max(1)[1]:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0], device='cuda:0')
tot_sum:  tensor(51.7807, device='cuda:0') 3
The device used will be: 
True
cuda:0
output.detach().clone():  tensor([[-1.8695e+00,  6.5776e+00],
        [ 4.3080e+00, -1.5933e+01],
        [ 1.5505e+01, -4.5683e+00],
        [ 1.2962e+01,  3.3308e+00],
        [-1.4919e+01,  1.0107e+01],
        [-1.6002e+00,  9.3482e+00],
        [-5.2067e+00,  3.2985e+00],
        [ 3.6979e+00,  6.6028e+00],
        [ 1.7047e+01,  5.1883e-01],
        [ 3.2503e+01, -9.8157e+00],
        [-5.0422e-01,  1.9106e+01],
        [ 8.6212e+00, -2.7095e+00],
        [ 2.9099e+01, -2.3500e+01],
        [ 2.6910e+01, -4.4501e+00],
        [ 1.7875e+01, -3.0004e+00],
        [ 3.2264e+00,  1.3610e+01],
        [ 2.0965e+01, -1.9889e+01],
        [ 8.2187e+00, -3.1578e+00],
        [ 9.0724e-01, -5.2914e+00],
        [ 1.8948e+01, -1.7688e+01],
        [ 2.1595e+01, -1.5783e+01],
        [-7.4870e-01, -1.8982e+00],
        [ 2.1491e+01,  7.6064e+00],
        [ 5.5899e-01, -1.2809e+01],
        [ 1.4728e+01, -7.1116e+00],
        [ 1.2700e+01, -2.8797e+00],
        [ 2.2221e+01,  1.9567e+01],
        [ 9.9950e+00, -6.6435e+00],
        [ 1.1556e+01, -6.5490e+00],
        [ 1.5854e+01, -1.5207e+01],
        [ 8.3577e+00, -1.1045e+01],
        [ 2.7479e+01, -2.2381e+01],
        [ 1.4796e+01, -2.8628e+01],
        [ 1.9648e+01, -1.2844e+01],
        [ 9.9115e+00, -1.0042e+01],
        [ 2.2153e+01, -2.3202e+01],
        [ 1.5294e+01, -1.3708e+01],
        [ 2.4854e+01, -2.3876e+01],
        [ 3.3406e+01, -1.7048e+01],
        [ 4.7528e+00,  1.1500e+01],
        [ 4.5705e+00,  9.8126e-01],
        [ 1.9343e+01, -1.8341e+01],
        [ 3.2067e-01,  8.0264e+00],
        [-7.3825e+00,  1.1919e+01],
        [ 2.3897e+01, -2.2396e+01],
        [ 3.0517e+00,  2.0282e+01],
        [ 1.8277e+01,  5.0012e+00],
        [ 2.6431e-01,  1.3080e+01],
        [ 2.1809e+01, -2.6027e+01],
        [ 1.6098e+01, -2.4241e+01],
        [ 1.6243e+01, -3.1279e+00],
        [-1.3226e+01,  3.5998e+01],
        [ 3.5949e+01, -3.5913e+01],
        [ 1.5454e+00,  1.1039e+00],
        [ 1.9863e+01, -1.1350e+01],
        [ 2.4224e+00,  1.1685e+01],
        [-8.0167e-02,  2.1852e+01],
        [ 1.6200e+01, -7.2849e+00],
        [ 1.2422e+01, -1.3331e+01],
        [ 1.6737e+01, -1.3025e+01],
        [ 2.3280e+00,  7.2822e+00],
        [ 3.7125e+00, -2.3555e+00],
        [ 1.4954e+01, -2.4427e+01],
        [ 4.0682e+01, -3.1126e+01],
        [ 2.1627e+01, -1.3585e+01],
        [ 1.2113e+01,  5.9387e+00],
        [ 4.1241e+01, -3.1693e+01],
        [ 1.7643e+01, -1.5965e+01],
        [ 1.5591e+01, -4.7842e+00],
        [ 4.8315e+00, -3.0071e-01],
        [ 2.0541e+01, -1.1612e+01],
        [-7.7683e-02, -1.2151e+01],
        [ 2.8436e+01, -1.0429e+01],
        [ 1.2953e+01, -3.6765e+00],
        [ 2.6067e+01, -1.0209e+01],
        [ 4.2222e+00,  2.1817e+01],
        [ 1.8200e+01, -1.4214e+01],
        [ 4.3200e+00, -3.0903e+00],
        [ 3.9441e+00,  4.2324e+00],
        [ 8.4795e+00, -1.0199e+01],
        [ 7.9255e+00, -1.2527e+01],
        [-1.3989e+01,  6.0642e+00],
        [-4.3121e+00,  1.0238e+01],
        [ 2.5934e+01, -3.0116e+01],
        [ 2.9720e+01, -2.7537e+01],
        [ 4.0956e+00, -2.1714e+01],
        [ 2.7915e+01, -2.4638e+01],
        [ 3.1087e+00, -1.8406e+01],
        [ 1.4426e+00,  7.4750e+00],
        [ 2.5855e+01, -2.6531e+01],
        [ 3.0956e+01, -8.1932e+00],
        [ 1.1933e+00,  1.0255e+01],
        [ 4.2428e+00,  1.7566e+01],
        [ 1.6796e+01, -1.1225e+01],
        [ 1.0705e+01,  7.0542e+00],
        [ 6.3662e+00,  8.4877e+00],
        [ 6.7421e+00,  1.3841e+01],
        [ 1.0431e+01, -8.8908e+00],
        [ 1.4327e+01, -1.2536e+01],
        [ 2.0107e+00, -2.1254e+00],
        [ 1.0367e+01, -6.5049e+00],
        [ 1.2338e+01, -1.4384e+01],
        [ 1.2168e+01, -1.3744e+01],
        [ 1.9200e+01, -1.5128e+01],
        [ 1.7518e+01,  1.5899e+00],
        [ 2.6796e+01, -1.1512e+01],
        [ 1.8667e+01, -9.1945e+00],
        [ 2.6394e+01, -1.7611e+01],
        [ 2.3718e+01, -2.0334e+01],
        [ 2.2491e+01, -5.2677e+00],
        [ 3.0998e+01, -3.4178e+01],
        [ 3.9678e+01, -2.5750e+01],
        [ 1.9243e+01,  3.3710e-03],
        [ 2.6980e+00,  1.0096e+01],
        [ 7.1693e+00,  4.3056e+00],
        [ 1.3546e+01, -4.5942e+00],
        [ 8.2756e+00, -4.7329e+00],
        [ 2.3188e+01, -2.0782e+01],
        [-7.9577e+00,  1.4175e+01],
        [-4.0056e+00,  5.2447e+00],
        [ 9.2793e+00, -1.0829e+01],
        [ 1.6784e+01, -6.6285e+00],
        [ 2.8980e+01, -3.0428e+01],
        [-1.5417e+00, -4.0576e+00],
        [ 3.8471e+01, -2.1699e+01],
        [-3.4473e+00,  1.1696e+00],
        [ 1.2404e+01, -1.0728e+01],
        [ 3.3458e-01,  2.1649e+01],
        [ 1.9575e+01, -2.5472e+01],
        [ 2.5792e+00,  7.7780e-01],
        [ 4.2741e+01, -3.0746e+01],
        [ 2.8693e+01, -2.7598e+01],
        [ 3.4247e+00, -1.6558e+01],
        [ 1.6259e+01, -5.5785e+00],
        [ 1.4235e+01,  1.9420e+01],
        [ 1.4151e+01,  2.2119e+00],
        [ 1.7586e+01, -4.6065e+00],
        [ 2.6472e+01, -1.9330e+01],
        [-1.8853e-01,  1.5350e+01],
        [ 1.2203e+01, -1.1758e+01],
        [ 2.1420e+01, -9.6998e+00],
        [ 9.2351e+00,  3.4719e+01],
        [ 3.0908e+01, -2.7334e+01],
        [ 9.3785e+00,  2.8711e+01],
        [ 2.7996e+01, -2.7309e+01],
        [ 2.7652e+01, -2.5309e+01],
        [ 9.6813e+00, -2.7848e+00],
        [ 2.6075e+01, -3.0672e+01],
        [ 1.3846e+01, -1.3820e+01],
        [-3.9194e+00,  5.8833e+00],
        [-4.3481e+00,  1.5577e+01],
        [ 1.8217e+01, -2.9099e-02],
        [ 1.1941e+01, -7.4777e+00],
        [ 5.3033e+00, -1.1587e+01],
        [-1.8875e-01,  5.1150e+00],
        [ 2.0133e+01, -2.5757e+01],
        [-7.9227e+00,  1.6925e+01],
        [ 3.3069e+01, -3.3172e+01],
        [ 6.9080e+00, -9.3103e+00],
        [ 2.1884e+01, -2.6458e+01],
        [ 2.5255e+00, -2.3765e+00],
        [ 2.1282e+01, -1.3536e+01],
        [ 2.0587e+01, -2.2249e+01],
        [ 2.7583e+01, -2.7959e+01],
        [ 1.0066e+01, -6.2217e+00],
        [-1.3295e+00,  4.6345e+00],
        [ 2.0611e+00, -1.2772e+01],
        [ 3.2063e+00,  9.1618e+00],
        [ 2.0430e+01, -2.0863e+01],
        [ 3.4205e+01, -2.4825e+01],
        [ 1.6910e+01, -2.0044e+01],
        [ 3.7576e+00,  7.8651e+00],
        [ 1.2771e+01,  9.0485e+00],
        [ 6.2783e+00, -3.9242e+00],
        [-5.3473e+00,  1.2248e+01],
        [-1.3749e+00,  2.1455e+01],
        [ 1.7546e+01,  1.6767e+01],
        [ 1.2123e+01, -4.2673e+00],
        [ 2.7344e+01,  1.8611e+00],
        [ 4.0830e+01, -3.1264e+01],
        [ 1.0740e+01,  5.6005e+00],
        [ 3.2410e+00, -1.0587e+01],
        [ 1.4328e+01, -7.6931e+00],
        [ 1.0557e+01, -1.1974e+01],
        [ 1.7367e+01, -2.4362e+01],
        [ 5.8500e+00, -1.4864e+00],
        [ 1.1445e+01, -7.9379e+00],
        [ 3.3866e+01, -3.4650e+01],
        [ 1.0053e+01, -1.0792e+01],
        [-5.9249e-01, -4.9703e+00],
        [ 1.1686e+01,  1.5743e+01],
        [ 8.3534e+00, -5.1931e+00],
        [ 2.9924e+01, -1.6721e+01],
        [ 9.9450e+00, -1.3925e+01],
        [ 1.1485e+01, -4.9682e+00],
        [ 9.3863e+00, -3.2932e+00],
        [ 2.8461e+01, -1.7858e+01],
        [ 8.5440e+00,  2.2510e-01],
        [-8.1346e-01,  2.4115e+01],
        [ 2.0471e+01, -2.0171e+01]], device='cuda:0')
output.data.max(1)[1]:  tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0], device='cuda:0')
tot_sum:  tensor(69.5680, device='cuda:0') 4
max_key : 2
Accuracy of the network on the 1st dataset: 60.000 %
Test loss on the 1st dataset: 0.037
################################## EVALUATION OF TASK 2 ############################################
SEED:  0
block 0, size : 96 16 16
96
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 2.886751345948129
block 1, size : 384 8 8
384
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.8505172717997146
block 2, size : 1536 4 4
1536
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.4252586358998573
block 3, size : 2560 2 2
2560
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.21262931794992865
block 4, size : 2560 1 1
2560
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.1647019614671031
NOWWWW
1 1
2
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
blocks['b%s' % 5] 2560 2560
range = 0.3423265984407288
train_layer_order: consecutive {'t0': {'blocks': [0, 1, 2, 3, 4], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 50, 'lr': 0.005}, 't1': {'blocks': [5], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}
The device used will be: 
True
cuda:0
PARAMSSSS IN LOAD:  {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
avg_deltas:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
topk_layer:  None
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.0.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
topk_layer inside model after retrieval:  [25, 59, 30, 5, 69, 42, 55, 68, 16, 82, 77, 53, 14, 35, 89]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.1.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
topk_layer inside model after retrieval:  [304, 137, 75, 345, 349, 197, 121, 22, 365, 336, 146, 202, 56, 100, 4, 67, 266, 163, 296, 354, 50, 159, 41, 347, 343, 180, 118, 190, 92, 94, 104, 76, 324, 62, 172, 191, 274, 200, 186, 217, 36, 105, 330, 128, 73, 21, 254, 357, 133, 169, 6, 326, 206, 193, 99, 380, 382, 88, 262]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.2.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
topk_layer inside model after retrieval:  [958, 1278, 761, 1436, 856, 95, 14, 1385, 733, 405, 622, 12, 307, 499, 1227, 1274, 1345, 869, 348, 612, 1452, 176, 1532, 250, 632, 1113, 312, 915, 423, 707, 406, 1508, 1291, 542, 942, 1317, 70, 743, 1442, 75, 544, 311, 574, 904, 961, 616, 959, 35, 801, 937, 490, 670, 1297, 994, 790, 434, 1450, 523, 450, 698, 1232, 936, 850, 636, 93, 164, 243, 22, 114, 911, 1238, 626, 1388, 350, 74, 1282, 1281, 1203, 913, 580, 238, 597, 1159, 774, 77, 889, 1025, 1086, 1486, 347, 868, 461, 783, 373, 905, 675, 639, 1245, 67, 359, 100, 673, 281, 1430, 1206, 705, 1101, 1089, 174, 1429, 662, 458, 810, 835, 1006, 1172, 923, 61, 852, 1185, 462, 667, 599, 41, 754, 395, 360, 855, 290, 728, 309, 25, 439, 1171, 1292, 457, 56, 875, 135, 13, 629, 148, 1424, 1002, 138, 1497, 1125, 1401, 614, 538, 964, 824, 1302, 441, 1294, 809, 1437, 1325, 246, 1100, 1186, 745, 324, 313, 356, 731, 276, 167, 335, 105, 212, 456, 851, 649, 596, 1192, 294, 118, 566, 133, 1248, 326, 584, 436, 1116, 447, 15, 933, 102, 818, 1046, 732, 195, 132, 233, 519, 1001, 1522, 674, 1022, 425, 1448, 688, 891, 1494, 1033, 886, 650, 508, 1307, 66, 808, 563, 1324, 872, 134, 110, 413, 209, 119, 1516, 779, 1266, 846, 1178, 717, 955, 215, 799, 973, 1032]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.3.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
topk_layer inside model after retrieval:  [505, 1755, 1843, 1403, 1783, 1554, 1938, 646, 421, 1447, 792, 564, 2241, 1226, 2460, 472, 1424, 507, 2050, 1958, 1198, 419, 2414, 653, 2305, 195, 1720, 497, 2098, 129, 1789, 2428, 2, 534, 454, 1608, 1451, 96, 299, 398, 953, 2233, 1894, 1266, 1926, 878, 1698, 1448, 689, 304, 1303, 2453, 499, 1887, 305, 1385, 2093, 1548, 1406, 456, 1567, 404, 233, 943, 1753, 533, 935, 1162, 2161, 1271, 2154, 586, 2309, 922, 862, 2300, 658, 1873, 1506, 1394, 390, 1364, 861, 627, 2000, 1697, 772, 2185, 2051, 1992, 2162, 2511, 1831, 1677, 425, 1409, 370, 509, 2112, 2227, 561, 110, 1799, 2223, 2022, 199, 492, 1638, 1236, 1021, 2500, 2362, 1436, 1080, 774, 1304, 347, 555, 2254, 436, 1458, 2164, 2278, 2302, 2062, 1076, 2523, 1599, 1088, 657, 923, 1175, 2056, 2018, 2001, 1524, 708, 1647, 2265, 1612, 1051, 1109, 1839, 1227, 122, 2094, 2469, 674, 1796, 1446, 284, 331, 453, 1845, 2042, 1157, 720, 2057, 1375, 1457, 632, 2007, 2151, 249, 696, 2085, 964, 2245, 1405, 2070, 2114, 1074, 757, 51, 164, 1427, 431, 1811, 1510, 1325, 1138, 867, 402, 2420, 1248, 2181, 1564, 2456, 450, 68, 2270, 2315, 761, 1993, 340, 2106, 1809, 291, 80, 2252, 1578, 1781, 748, 2323, 102, 846, 262, 1337, 146, 1699, 2354, 912, 527, 895, 788, 2328, 746, 178, 638, 1221, 724, 2167, 1128, 1536, 735, 1003, 333, 240, 1985, 1253, 737, 715, 2021, 2240, 2179, 2317, 2342, 2044, 2011, 639, 1890, 1380, 780, 1045, 2004, 448, 1733, 231, 980, 1439, 283, 365, 944, 1646, 60, 2427, 1363, 1642, 1348, 1982, 1696, 605, 2341, 2152, 1998, 1341, 252, 726, 1920, 2160, 2250, 2340, 1968, 1867, 2095, 2352, 2248, 2249, 810, 2081, 1830, 723, 104, 987, 54, 1283, 1824, 1433, 1104, 2479, 1219, 2084, 378, 1340, 225, 1402, 2297, 2502, 1484, 1376, 339, 1220, 928, 626, 687, 2322, 616, 1545, 297, 1435, 758, 1676, 1555, 2514, 677, 742, 583, 694, 1689, 278, 903, 1690, 1013, 35, 927, 1362, 2236, 177, 1742, 1851, 156, 277, 346, 2173, 2221, 312, 52, 937, 69, 1874, 752, 1070, 2087, 1619, 2559, 1346, 1891, 1492, 969, 1002, 77, 1351, 2542, 753, 1267, 1056, 1210, 1187, 2174, 242, 1801, 2259, 46, 1670, 2079, 200, 1865, 13, 2242, 1353, 1928, 1044, 1411, 1614, 699, 1844, 1053, 72, 1645, 1356, 89, 884, 2551, 1287, 1961]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.4.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
topk_layer inside model after retrieval:  [1118, 1323, 1657, 557, 888, 1143, 530, 966, 605, 91, 1504, 426, 1167, 2265, 2388, 1233, 1957, 99, 1366, 1970, 2385, 2531, 93, 382, 2393, 1706, 2457, 2559, 674, 2121, 1869, 402, 1250, 770, 6, 1883, 1406, 778, 30, 2335, 172, 315, 1063, 883, 386, 1713, 2218, 70, 2211, 1375, 455, 499, 2060, 2440, 1428, 161, 2415, 757, 703, 654, 139, 2171, 2053, 1066, 1064, 326, 1099, 1471, 15, 1387, 140, 621, 2034, 1373, 607, 534, 1602, 1750, 1090, 1289, 774, 2510, 1999, 2195, 270, 1731, 1273, 1928, 2143, 2250, 1787, 400, 2532, 1556, 1912, 2475, 1513, 1704, 1284, 448, 625, 1676, 1730, 1434, 1607, 3, 612, 1034, 193, 1442, 208, 843, 1241, 1489, 2268, 825, 1384, 1981, 296, 1877, 177, 1298, 1880, 2141, 969, 7, 1477, 1313, 806, 798, 2015, 2337, 1622, 2395, 905, 2286, 1402, 412, 1908, 1084, 2138, 328, 1684, 556, 34, 2410, 2285, 2025, 663, 150, 1252, 0, 549, 2066, 792, 8, 366, 112, 1201, 2070, 704, 2099, 2302, 548, 482, 1982, 1397, 1640, 889, 2191, 2189, 1306, 1326, 2558, 503, 263, 1071, 2436, 1246, 2226, 789, 666, 2081, 593, 718, 544, 568, 1080, 364, 2485, 615, 1618, 1597, 1035, 361, 1700, 233, 24, 2327, 765, 1987, 861, 2021, 628, 1571, 1444, 1580, 2487, 2346, 2452, 2453, 54, 2158, 1452, 2343, 309, 1819, 1135, 1810, 1285, 1892, 2188, 1969, 2326, 1169, 494, 768, 1338, 5, 2228, 182, 1796, 1644, 924, 2432, 2112, 120, 815, 1583, 2207, 2480, 2205, 85, 1012, 1719, 2276, 904, 105, 1723, 1995, 2277, 2172, 2239, 1357, 637, 2085, 1040, 1922, 2512, 1230, 225, 658, 738, 1065, 242, 1468, 1639, 1734, 1210, 689, 788, 108, 1134, 2546, 207, 2413, 1523, 257, 1424, 2516, 671, 1124, 2134, 558, 335, 2224, 332, 1619, 1239, 926, 951, 891, 1445, 239, 493, 1609, 424, 449, 1732, 2311, 1852, 1759, 2254, 340, 1535, 2509, 1634, 447, 1572, 802, 1770, 395, 1030, 1411, 1645, 2212, 562, 311, 1420, 531, 1575, 1996, 502, 259, 202, 670, 2045, 682, 1390, 132, 2518, 565, 2356, 560, 190, 2220, 2149, 1418, 2079, 1355, 1109, 1113, 553, 2540, 2249, 1015, 2538, 564, 705, 214, 2525, 1993, 700, 341, 252, 430, 1937, 1140, 1878, 2011, 783, 1198, 86, 2507, 1275, 603, 1372, 1849, 1221, 2163, 604, 1522, 1339, 1558, 2517, 2535, 1496, 1204, 874, 1641, 1432, 895, 2492, 2271, 1467]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
The device used will be: 
True
cuda:0
The device used will be: 
True
cuda:0
chosen_head:  {'blocks.5.layer.bias': tensor([ 0.0052, -0.0217], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0350,  0.0053, -0.0105,  ...,  0.0090, -0.0074,  0.0153],
        [-0.0067,  0.0172,  0.0149,  ..., -0.0063,  0.0286, -0.0075]],
       device='cuda:0')}

 Model C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c loaded successfuly with best perf



 ----- Architecture Block BatchNorm2dSK3962(5, 5)1.0reflect, number 0 -----
- BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK963842(3, 3)0.6499999761581421reflect, number 1 -----
- BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.4)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK38415362(3, 3)0.25reflect, number 2 -----
- BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block BatchNorm2dSK153625602(3, 3)0.25reflect, number 3 -----
- BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block BatchNorm2dSK256025602(3, 3)0.25reflect, number 4 -----
- BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 5 -----
- Flatten(start_dim=1, end_dim=-1)
- Dropout(p=0.5, inplace=False)
- Linear(in_features=2560, out_features=2, bias=True)
model.heads:  [{'blocks.5.layer.bias': tensor([ 1.5791e-05, -1.6506e-02], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0120,  0.0150,  0.0043,  ...,  0.0072,  0.0111, -0.0045],
        [ 0.0163,  0.0075,  0.0001,  ..., -0.0045,  0.0101,  0.0122]],
       device='cuda:0')}, {'blocks.5.layer.bias': tensor([ 0.0040, -0.0205], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 1.5765e-02,  1.0660e-02,  4.3432e-03,  ...,  1.0930e-02,
          1.1431e-02, -3.5607e-03],
        [ 1.2527e-02,  1.1907e-02,  8.8815e-05,  ..., -8.2031e-03,
          9.7858e-03,  1.1328e-02]], device='cuda:0')}, {'blocks.5.layer.bias': tensor([ 0.0005, -0.0170], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0349,  0.0030,  0.0162,  ..., -0.0045,  0.0023,  0.0014],
        [-0.0066,  0.0195, -0.0117,  ...,  0.0072,  0.0189,  0.0064]],
       device='cuda:0')}, {'blocks.5.layer.bias': tensor([-0.0005, -0.0160], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0531,  0.0097,  0.0127,  ...,  0.0032,  0.0010,  0.0103],
        [-0.0248,  0.0129, -0.0082,  ..., -0.0005,  0.0202, -0.0025]],
       device='cuda:0')}, {'blocks.5.layer.bias': tensor([ 0.0052, -0.0217], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0350,  0.0053, -0.0105,  ...,  0.0090, -0.0074,  0.0153],
        [-0.0067,  0.0172,  0.0149,  ..., -0.0063,  0.0286, -0.0075]],
       device='cuda:0')}]
[Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (5): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=2560, out_features=2, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)]
LAYER NAME:  Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (5): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=2560, out_features=2, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)
LAYER CHILDREN:  [BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=1.4)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
  )
  (layer): Linear(in_features=2560, out_features=2, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)]
subsubl NAME:  Sequential(
  (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=1.4)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): Flatten(start_dim=1, end_dim=-1)
)
subsubl NAME:  Linear(in_features=2560, out_features=2, bias=True)
subsubl NAME:  Dropout(p=0.5, inplace=False)
CONFIG MODE:  unsupervised
CONFIG MODE:  supervised
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([49, 33, 72, 51, 71, 92, 15, 14, 23,  0, 71, 75, 81, 69, 40, 43, 92, 97,
        70, 53], device='cuda:0')
TARGETS AFTER SUB:  tensor([16, 41, 41, 16, 16, 16, 16, 41, 41, 16, 16, 41, 41, 41, 41, 41, 16, 16,
        41, 16])
[16, 41]
TARGETS AFTER CLEANER:  tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0])
200
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 200
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               None
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([19, 29,  0, 11,  1, 86, 90, 28, 23, 31, 39, 96, 82, 17, 71, 39,  8, 97,
        80, 71], device='cuda:0')
TARGETS AFTER SUB:  tensor([41, 41, 41, 41, 16, 16, 16, 41, 41, 16, 41, 41, 41, 16, 41, 41, 41, 41,
        16, 41])
[16, 41]
TARGETS AFTER CLEANER:  tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1])
1000
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 1000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               None
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([19, 29,  0, 11,  1, 86, 90, 28, 23, 31, 39, 96, 82, 17, 71, 39,  8, 97,
        80, 71], device='cuda:0')
TARGETS AFTER SUB:  tensor([41, 41, 41, 41, 16, 16, 16, 41, 41, 16, 41, 41, 41, 16, 41, 41, 41, 41,
        16, 41])
[16, 41]
TARGETS AFTER CLEANER:  tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1])
1000
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 1000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
------------------------
INDICES:  1000
IMAGE SIZE: torch.Size([64, 3, 32, 32])
The device used will be: 
True
cuda:0
output.detach().clone():  tensor([[ 11.8611,  22.9052],
        [  1.9152,  21.9060],
        [ 10.5054,  24.9094],
        [  7.4874,  13.2729],
        [ 18.0080,  -7.0590],
        [  5.2253,   2.4199],
        [  9.7907,  18.7558],
        [ 17.1959,  15.4745],
        [ 10.3461,  16.4121],
        [  0.2746,  13.5924],
        [  1.4347,   5.9466],
        [ 17.1504,  22.7612],
        [  4.4192,  -0.0758],
        [ 11.1872,   1.4808],
        [  1.4121,   1.2167],
        [ 12.0211,  10.1128],
        [-12.3987,   1.7881],
        [ 32.8019, -30.0409],
        [ 17.0648, -17.5440],
        [ 14.8263,  -8.9338],
        [  9.4428,  10.5035],
        [ 11.0597,  22.4378],
        [  9.9519,  -1.3783],
        [  7.8804,   0.8158],
        [ 11.0202,  -1.9228],
        [ 16.8024,  17.0980],
        [ 19.7274,  15.4896],
        [ 22.3617,  -4.9772],
        [  8.7964,  -2.8700],
        [ 11.9054,   3.7767],
        [ 12.4739,  -7.5736],
        [ 12.8410,   3.4624],
        [ 11.3292,   8.7009],
        [  9.5268,   2.2873],
        [ 11.3189, -13.7124],
        [ -4.4272,   9.9641],
        [ 13.0449,  -1.4129],
        [  9.9831,  -2.8512],
        [  9.2156,   8.4627],
        [  8.6883,  -1.1251],
        [  9.9388,  21.0351],
        [  2.8863,  12.8589],
        [  0.1555,   3.4274],
        [ 12.7924,  23.4409],
        [  2.6692,   1.5382],
        [  7.5254,   5.8410],
        [ -1.2674,  11.5881],
        [ 13.5879,  21.7986],
        [ 18.4063,  -1.5928],
        [ 12.3197, -10.3373],
        [  9.2902,   8.1735],
        [ 14.1677,   8.8558],
        [  9.6426,  19.0609],
        [  8.4100,  10.8071],
        [  2.3150,   7.7698],
        [ 19.8626,  16.3724],
        [ 21.4426,  -1.9422],
        [  9.2577,  -4.5962],
        [ 12.9260,  20.9947],
        [ 19.5989,  21.6227],
        [ 11.3851,   9.5728],
        [  4.4497,   7.4155],
        [ -3.1110,  -3.6694],
        [ 17.0583,   0.8228],
        [ 18.6556,  24.8104],
        [  6.2901, -22.1012],
        [ 15.1238,  -1.9614],
        [  4.1687,  -0.5068],
        [ 13.6598,  13.5868],
        [ -0.8820,  -0.1262],
        [ 18.3105,   3.1598],
        [  6.6489,  -2.3721],
        [ 14.1607,  24.1516],
        [ 13.5342, -11.5182],
        [ 11.1220,  -8.8773],
        [  3.0942,   3.8816],
        [  8.3708,  -4.5002],
        [ 10.5922,  27.5195],
        [  3.1606,   0.9573],
        [ 14.6761,  21.1111],
        [  0.4852,   5.6819],
        [  7.3213, -13.8363],
        [ 11.7038,   0.3257],
        [  3.4478,   4.5100],
        [ 10.7142,  12.6642],
        [ 14.9407,  17.1431],
        [ 14.4337,  -4.7742],
        [ -3.1628,   8.0092],
        [ 20.2469,  16.6707],
        [  8.6974,   3.6052],
        [  8.1031,  -0.3168],
        [ 11.0008,  18.5090],
        [ 13.5753, -14.9950],
        [  9.5053,   6.7488],
        [ 23.9666,  10.3359],
        [ 14.0902,   7.2097],
        [  5.3097,   3.2544],
        [  9.1790,  -5.1901],
        [  3.0735,  -0.8833],
        [ 13.3396, -10.2086],
        [ 13.1806,  -0.0439],
        [ 18.5413, -23.7862],
        [  5.7244,  -8.8148],
        [  9.0455,  -0.9629],
        [  2.0377,   1.3790],
        [  9.6651,  -2.6999],
        [  2.3377,  -1.9663],
        [ 10.9027,  12.9843],
        [ 20.5850,  -0.1307],
        [  8.3036,  -4.9991],
        [  2.5462, -12.4031],
        [ 11.7573,   9.5996],
        [  5.0440,  -1.8277],
        [ -2.6479,   6.5422],
        [ 17.3304,   6.0126],
        [ 12.7672,  15.6719],
        [  6.5453,  14.1696],
        [ 18.4832,   7.3933],
        [ 13.5521,  25.2118],
        [ 13.0309,  20.1154],
        [ 10.6127,  21.1634],
        [ 23.1276, -13.2712],
        [  7.9678,   9.0587],
        [ 12.9362,  -2.4183],
        [ 15.1208,   4.2957],
        [ -4.1088,   9.8933],
        [ 10.4443,  -2.9217],
        [ -0.9110,   9.6404],
        [  6.7562,  -9.7545],
        [  5.1130,  -2.8978],
        [  6.6381,  -1.6165],
        [  9.2790,  18.0670],
        [ 10.4976,  18.9966],
        [ 17.2387,  15.9774],
        [ 12.2186, -31.1337],
        [ 14.6267,  25.5557],
        [ 12.8918,  23.6516],
        [ -4.0625,  11.2016],
        [  9.2642,  21.2242],
        [ 14.4419,  22.3678],
        [ 13.8875,  -3.5751],
        [ -0.7884,  12.0561],
        [  6.0401,   0.5531],
        [  1.4328,  13.4119],
        [  8.3474,   9.5667],
        [ -0.6713,   6.5931],
        [  9.8159,  11.3034],
        [  7.6871,   1.6347],
        [ 23.4415,   8.8372],
        [ 28.2203, -16.4298],
        [  7.5174,  23.4915],
        [ -0.7021,   9.5593],
        [ 14.2473,  -4.4635],
        [  4.7543,   3.4490],
        [  6.2426,  25.1736],
        [ 10.8532,  -1.1033],
        [  1.5822,  -1.9146],
        [ 16.6122,  18.3680],
        [  9.2611,   5.7677],
        [ -3.3480,   9.5609],
        [ 20.8393,  22.6472],
        [  2.3201,   3.3031],
        [ 16.9484,   0.7997],
        [ 17.7361,  -6.5161],
        [  7.1132,  25.5155],
        [  7.4370,   7.4297],
        [ 10.2527,  14.3897],
        [ 17.8236,  -1.7817],
        [ -0.7290,  10.2680],
        [  1.9045,   5.0635],
        [  2.3149,   9.4119],
        [  7.6825,  11.6743],
        [  7.5551,  11.7384],
        [ -2.2275,   1.5170],
        [  7.9552,   4.9618],
        [ 11.2187,  -7.7551],
        [  6.7213,   3.9924],
        [  5.2802,   9.0470],
        [ -2.4610,   7.9139],
        [  3.4270,   4.1493],
        [ 10.4900,  29.2364],
        [  0.7597,  12.2707],
        [ 10.5136,  -9.6941],
        [ 20.3856,  -2.9476],
        [ 15.3190, -13.2861],
        [  3.4118,  13.8905],
        [  9.9161,  21.5014],
        [ 15.5145,  26.8595],
        [ -2.7403,  -1.3550],
        [ 15.4106,   9.9094],
        [  7.5543,  24.6648],
        [ 23.2500, -26.4669],
        [ 17.1441,   0.1236],
        [  2.7808,  -0.1705],
        [  6.0917,  -1.8993],
        [  4.7109,  11.3662],
        [ 17.5013,  -7.1730],
        [ 17.6416,  14.9574],
        [ 13.1510,  -1.8556],
        [ 10.1894,  24.3751]], device='cuda:0')
output.data.max(1)[1]:  tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1], device='cuda:0')
tot_sum:  tensor(80.0931, device='cuda:0') 0
The device used will be: 
True
cuda:0
output.detach().clone():  tensor([[ 1.7078e+01, -1.0359e+00],
        [ 8.4121e+00,  8.9723e+00],
        [ 5.1631e+00,  1.6194e+01],
        [-3.4021e+00,  1.4697e+01],
        [ 3.6435e+00,  5.0527e+00],
        [-4.7630e+00,  2.2065e+01],
        [-1.7735e+01,  1.1220e+01],
        [ 1.4790e+01, -7.1449e+00],
        [ 1.6296e+01,  1.5923e+01],
        [ 4.7604e+00,  9.1063e+00],
        [-1.8880e+00,  5.1924e+00],
        [-1.0299e+00, -3.8987e-01],
        [ 8.6155e+00,  1.9823e+01],
        [-8.7212e+00,  1.8577e+01],
        [ 1.0726e+01,  1.2617e+01],
        [ 2.8098e+00, -3.8181e+00],
        [ 2.3878e+00,  8.3258e+00],
        [-3.4573e+00,  1.0422e+01],
        [ 1.1806e+01,  5.8817e-02],
        [ 1.1774e+01, -1.2107e+01],
        [-1.7404e+01,  2.1611e+01],
        [ 1.8216e+01,  2.1696e+01],
        [ 6.1239e+00,  2.8642e+01],
        [ 4.2777e+00,  7.4373e-01],
        [ 6.1833e+00, -3.9933e+00],
        [-6.5423e+00,  1.0735e+01],
        [ 2.5653e+01,  1.1264e+01],
        [-6.5304e-01,  6.4374e+00],
        [ 9.8390e+00,  2.4061e+01],
        [ 5.6041e+00,  2.6674e+01],
        [ 7.7069e+00,  8.0382e+00],
        [ 8.5640e+00,  3.0680e+00],
        [-5.2695e-02,  1.7516e+01],
        [-1.8413e+01,  2.8741e+01],
        [ 7.7032e+00,  5.0000e-01],
        [ 1.1760e+01, -4.6215e+00],
        [ 1.9060e+01,  1.2356e+01],
        [-2.8260e+01,  2.7781e+01],
        [ 6.7550e+00,  2.2068e-01],
        [-2.5586e+00,  6.9019e+00],
        [-6.2082e+00,  8.8184e+00],
        [-5.2777e+00,  1.5590e+01],
        [ 1.3148e+01, -9.0306e+00],
        [ 5.4315e+00, -1.5375e+00],
        [ 1.6859e+01,  1.6357e+01],
        [ 1.3719e+01,  2.2514e+01],
        [ 1.6260e+01, -6.1750e+00],
        [ 1.9812e+01, -4.7830e+00],
        [ 7.0694e+00,  5.5984e+00],
        [ 1.5075e+01,  4.2180e+00],
        [-1.1806e+01,  1.8400e+01],
        [ 2.8555e+00, -6.4035e-01],
        [ 1.7100e+01,  1.4317e+01],
        [ 1.5562e+01,  8.3250e+00],
        [ 1.9404e+01, -3.1007e+00],
        [ 3.4141e+00,  5.1594e+00],
        [ 9.9982e+00,  1.6760e+01],
        [ 1.1678e+01,  9.0367e+00],
        [-3.4317e+00,  5.4475e+00],
        [-3.1853e+00,  2.2402e+01],
        [-2.0920e+01,  2.3681e+01],
        [-1.9901e+01,  2.7423e+01],
        [ 7.4813e+00,  5.6552e+00],
        [-3.3687e+01,  4.9369e+01],
        [ 2.1570e+01,  1.9652e+01],
        [-6.9543e-01,  1.1644e+01],
        [ 7.7876e+00,  1.5591e+01],
        [ 1.3729e+01, -3.2116e+00],
        [ 1.3597e+01, -7.6754e+00],
        [ 1.6110e+01,  1.4863e+01],
        [-1.1566e+01,  1.7019e+01],
        [ 2.7137e+01,  9.0983e+00],
        [ 1.4815e+01,  1.7269e+01],
        [ 2.1451e+01,  8.0588e+00],
        [ 8.8346e+00, -2.7066e-01],
        [-2.9428e+01,  1.3617e+01],
        [ 1.0541e+01,  1.7619e+00],
        [ 1.7583e+01,  1.6982e+01],
        [ 9.9941e+00, -6.1236e+00],
        [-6.5632e+00,  4.0865e+01],
        [ 5.9285e+00,  1.9391e+01],
        [ 1.2485e+01,  3.5920e+00],
        [-2.2153e+01,  2.4186e+01],
        [-6.9292e-01,  6.3161e+00],
        [ 8.6189e+00,  4.7474e+00],
        [ 2.6976e-01,  1.4597e+01],
        [-1.0809e-01,  7.0760e+00],
        [-6.3881e+00,  8.6328e+00],
        [ 7.3929e+00, -4.7641e+00],
        [ 8.7782e+00,  9.7159e-01],
        [ 6.0617e+00,  1.1686e+01],
        [ 1.1406e+01,  1.5939e+01],
        [ 1.8658e+01,  1.3118e+01],
        [ 7.0022e-01, -1.1311e+01],
        [ 3.0266e+00,  1.6920e+01],
        [ 5.1353e+00, -1.1466e+00],
        [ 9.8475e+00, -4.3108e+00],
        [ 6.7371e+00,  1.0531e+01],
        [ 9.6225e+00,  1.3401e+01],
        [ 1.1586e+01,  3.1900e+01],
        [ 5.5360e+00, -3.5538e+00],
        [ 1.1625e+01,  2.3761e+01],
        [ 1.5307e+01, -8.1757e+00],
        [ 9.9352e+00,  9.4215e+00],
        [ 1.1656e+01, -3.5733e+00],
        [ 1.2276e+00,  6.3486e+00],
        [-1.5491e+00,  2.3684e+00],
        [ 1.4792e+01,  6.3267e+00],
        [ 1.0800e+01, -2.0703e+00],
        [ 6.7941e+00,  1.3236e+01],
        [ 1.6997e+01,  1.3491e+01],
        [ 1.6801e+01,  1.0446e+01],
        [ 4.0268e+00, -8.1222e+00],
        [ 2.9016e+00,  1.4777e+01],
        [ 3.1107e+00,  1.7847e+01],
        [ 1.6471e+01,  4.2887e+00],
        [ 9.7944e+00,  6.4595e+00],
        [ 1.5509e+01, -8.1283e+00],
        [ 5.8395e+00,  3.4821e+00],
        [ 2.3828e+01,  1.8546e+01],
        [ 3.0857e+00,  4.5211e-02],
        [ 1.6841e+01,  1.8946e+01],
        [-4.0655e+01,  2.1740e+01],
        [ 8.2849e+00,  4.7453e+00],
        [ 2.8690e+00,  7.1377e-01],
        [ 1.3922e+01,  1.7087e+01],
        [ 1.5176e+01,  6.1239e+00],
        [-4.5676e+00,  1.3665e+01],
        [ 2.4805e+00, -2.1091e+00],
        [ 1.2341e+01,  1.3535e+01],
        [ 1.3351e+01,  2.4961e+01],
        [ 4.7912e-01,  5.4471e+00],
        [-1.9811e+01,  2.4472e+01],
        [-3.7064e+00,  7.1229e+00],
        [-1.0305e+01,  1.5205e+01],
        [ 3.6498e+00,  1.5767e+01],
        [ 1.3178e+01,  1.9451e+01],
        [ 1.9462e+01,  1.5518e+01],
        [ 7.1620e+00,  1.7480e+01],
        [ 1.9016e+01,  1.9748e+01],
        [ 7.5707e+00, -4.3545e+00],
        [ 1.2524e+01, -2.7408e+00],
        [ 1.5785e+01,  1.3709e+01],
        [ 2.6727e+00,  9.0540e+00],
        [ 1.2735e+01,  9.3991e+00],
        [-6.2646e+00,  1.2431e+01],
        [-1.0261e+01,  1.7824e+01],
        [ 4.9888e+00, -5.6993e+00],
        [ 9.5683e-01,  1.0311e+01],
        [ 3.4551e+00,  1.3911e+00],
        [ 2.4516e+01,  1.2294e+01],
        [ 5.3786e+00,  3.0036e+01],
        [ 6.2565e+00,  4.9634e+00],
        [ 6.9345e+00,  1.6887e+01],
        [ 2.0690e+01,  1.1980e+01],
        [ 1.3361e+01, -1.3317e+00],
        [ 8.9931e+00,  7.8202e+00],
        [ 1.3648e+01,  2.2896e+01],
        [ 2.1235e+01, -1.7351e+00],
        [-1.0007e+01,  2.7445e+01],
        [ 1.1803e+01, -4.0172e+00],
        [ 9.4814e+00,  1.1989e+01],
        [ 7.6379e+00,  9.3884e+00],
        [ 1.2381e+01, -4.4234e+00],
        [-1.1631e+01,  1.7524e+01],
        [ 1.0650e+01, -7.1869e+00],
        [ 1.6436e+01,  1.6710e+01],
        [-1.1133e+01,  1.5410e+01],
        [-2.3234e+00,  1.4137e+01],
        [-4.3229e+00,  1.3982e+01],
        [-3.3468e+01,  2.8223e+01],
        [ 6.9261e+00, -3.2642e+00],
        [-9.7787e+00,  2.9982e+00],
        [ 1.7604e+01,  2.5862e+01],
        [ 2.2954e+01,  9.6452e+00],
        [ 3.7618e+00,  2.4784e+01],
        [ 3.0760e+00, -1.2933e+01],
        [-7.5649e-01,  1.2547e+01],
        [ 6.0752e+00,  2.9142e+01],
        [ 2.0736e+01, -1.1879e+01],
        [ 8.7386e+00, -1.1737e+01],
        [ 1.1459e+01,  6.4548e+00],
        [ 1.5424e+00,  1.6339e+01],
        [-1.8641e+01,  2.8180e+01],
        [ 1.3481e+01,  1.5222e+01],
        [ 9.1615e+00,  5.6831e+00],
        [ 3.4051e-02,  1.0287e+01],
        [ 3.9028e+00,  2.3099e+00],
        [ 1.9870e+01,  1.3627e+01],
        [ 5.1654e+00,  1.5289e+01],
        [ 1.7592e+01,  2.2134e+01],
        [-1.8317e+00,  1.4994e+01],
        [ 8.3253e+00,  2.5595e+01],
        [-1.7555e+01,  1.4465e+01],
        [ 1.9836e+01,  1.8275e+01],
        [ 5.3466e+00,  7.5703e+00],
        [-3.3017e+01,  2.9800e+01],
        [ 1.8882e+01,  2.1301e+01],
        [-2.0718e+01,  1.8324e+01],
        [ 1.6952e+01, -2.6250e+00]], device='cuda:0')
output.data.max(1)[1]:  tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0], device='cuda:0')
tot_sum:  tensor(75.6501, device='cuda:0') 1
The device used will be: 
True
cuda:0
output.detach().clone():  tensor([[ 10.6501, -20.5072],
        [ 17.6129, -19.0328],
        [ 21.1709, -26.4161],
        [  1.4200,   4.2031],
        [ 18.4530, -15.3222],
        [-10.0181,  13.3223],
        [  0.7994,   8.0575],
        [-10.9753,  17.5683],
        [ 22.3649, -14.9839],
        [ 25.0345, -15.2509],
        [-21.7938,  55.6939],
        [-21.0868,  35.9313],
        [ 20.0980,  -6.2314],
        [ 35.4440, -27.2410],
        [ 28.7086, -15.5722],
        [-25.2876,  14.6767],
        [-23.2327,  61.3440],
        [ -2.0562,   9.6191],
        [ -9.0512,  30.0088],
        [  9.5853,  -7.3407],
        [ -5.9145,  41.1311],
        [-27.0254,  70.5115],
        [ 11.2183,  -1.3622],
        [  1.7597,  15.6245],
        [ 10.4966,   0.7232],
        [ 20.0181,  -3.7642],
        [ 15.8944, -22.4098],
        [  5.0696,   5.2425],
        [-24.9987,  59.9785],
        [ -6.2550,  31.5746],
        [ 20.8394,   7.8638],
        [-13.2240,  45.4430],
        [  9.9977,  -5.3365],
        [-18.9464,  50.3635],
        [ -1.0113,  12.7379],
        [ 32.2162,  -6.3399],
        [ 15.6013,  16.4822],
        [-17.9215,  47.4153],
        [ 14.8710,  -9.8496],
        [ -5.2796,  13.9757],
        [-13.4712,  37.3580],
        [ 14.2283,  22.0064],
        [  9.5413,  10.4047],
        [ 13.5994,   1.2671],
        [-13.4020,  30.6694],
        [ 17.2709,   4.8628],
        [ 23.6066,  -5.8587],
        [ 23.3227, -11.2935],
        [-24.0079,  41.0341],
        [ 16.4317,   2.8615],
        [  4.1180,  10.2090],
        [ 19.7706,  -7.9803],
        [-15.7356,  51.1217],
        [ 29.2252, -16.0631],
        [ -4.5902,  22.0535],
        [-24.7853,  57.9313],
        [-31.0287,  59.5748],
        [ 25.7942, -17.2208],
        [-24.7517,  52.0973],
        [ 14.9222,   2.5155],
        [ 26.0488, -28.4425],
        [ 12.1878,  -0.8926],
        [-22.5799,  64.9535],
        [ 26.5266, -23.3105],
        [-27.4902,  63.7232],
        [ -3.8501,   9.7717],
        [ 27.4701, -18.9062],
        [-14.5633,  49.3293],
        [ 27.2207, -16.2719],
        [-13.7900,  50.5994],
        [ 20.1316, -20.4642],
        [ 21.7259,  -9.0582],
        [  1.5283,   8.7996],
        [ 16.4689, -12.1923],
        [ 49.9189, -26.0979],
        [-18.3539,  20.5439],
        [ 10.1734,  -3.2056],
        [ 29.6530, -14.6244],
        [ 14.3031,  -9.9598],
        [  2.6560,  13.6471],
        [ 35.7837, -29.8914],
        [-25.4016,  58.8988],
        [ -1.2450,   3.8551],
        [-29.4233,  39.7438],
        [-21.3033,  60.0668],
        [ 17.0969, -32.9083],
        [-22.8740,  57.4382],
        [-21.3913,  61.5735],
        [ -2.6535,  -0.3450],
        [ 29.4278, -11.5469],
        [ 16.0353, -10.1092],
        [ 42.8480, -11.4321],
        [-13.0797,  16.6623],
        [ 23.9024, -14.2432],
        [ -7.5418,  23.2237],
        [ 14.5487, -13.7296],
        [-16.4610,  43.7072],
        [ -5.0965,   4.0881],
        [-16.9280,  50.1438],
        [ 16.0602,  -7.9778],
        [ -6.6943,  17.9618],
        [ 15.3471, -13.3145],
        [  7.8566,  11.5000],
        [  2.0584,   4.9171],
        [ 20.9874,  -3.6855],
        [ -4.5838,  27.9620],
        [ 10.1449, -13.3621],
        [ 22.2955,  -2.2656],
        [ -0.1286,   9.8782],
        [-20.3266,  55.7410],
        [  6.3888,  -2.2711],
        [-11.9400,  31.3562],
        [ 23.6722, -21.6566],
        [  3.8259,   7.9880],
        [ 12.7850,  -8.8910],
        [ 33.7468, -20.7167],
        [  6.4267,  23.0828],
        [ -9.0983,  28.3150],
        [ -8.9166,  30.0355],
        [ 25.6903,  -4.2202],
        [-13.5861,  21.3721],
        [  0.6322,  19.8220],
        [ -3.2079,  10.1730],
        [  6.6819,   6.6843],
        [ 13.8646,  -9.8759],
        [-23.4861,  60.4034],
        [-12.2076,  44.4859],
        [ 19.6976, -11.7401],
        [  5.2016,  -4.8304],
        [-15.9546,  25.4933],
        [  5.0244,   3.7048],
        [ 10.3390, -29.2544],
        [ 35.5797, -10.9375],
        [-29.7333,  69.4594],
        [-25.4027,  42.2159],
        [ 12.6250,   5.2888],
        [ -4.0986,  13.1957],
        [ 23.0773, -21.0951],
        [ 11.5474,  -8.9187],
        [-22.2896,  54.8882],
        [ 29.6111, -10.1110],
        [-26.2318,  52.9896],
        [ -8.7796,  44.5665],
        [ 16.8763,   4.4805],
        [-22.6310,  43.3456],
        [ 12.6523,   8.6474],
        [-20.3477,  56.8908],
        [ 22.0274,   1.3154],
        [-20.5667,  33.4835],
        [ 15.4755, -12.7149],
        [-19.0270,  51.6971],
        [ 21.7352, -11.6505],
        [ 15.9321, -12.0618],
        [-26.0879,  60.0082],
        [ 48.5162, -27.7562],
        [  1.6511,  21.3722],
        [-20.6572,  61.8785],
        [  1.1551,  -1.8658],
        [  7.4141,  -3.2220],
        [-18.7864,  58.6977],
        [  5.9469,   9.7981],
        [  3.3657,   2.8010],
        [ -6.6071,  -0.1735],
        [  2.8764,   7.6411],
        [ 17.7682,  -5.9033],
        [-19.3798,  49.8679],
        [-10.6089,  41.5825],
        [ 16.6643, -11.1277],
        [ 11.7098,  -0.9963],
        [-21.3015,  64.7672],
        [-34.4795,  67.1078],
        [ 19.7418,  -2.0638],
        [-19.9547,  58.2666],
        [ 10.3361,  -6.6744],
        [ 13.7284, -10.2651],
        [ -6.9366,  14.5126],
        [ 26.8885,   7.4136],
        [ 10.2672,  -2.6222],
        [ 21.6336, -22.1131],
        [-25.6281,  56.6367],
        [  5.1407,  -0.2407],
        [ 22.0038, -19.7887],
        [ 44.9137, -28.8369],
        [  9.3629,  -5.9465],
        [  4.8949,  11.1468],
        [ -3.1935,  -0.9021],
        [-32.3416,  29.2508],
        [ 17.2198,  -7.8983],
        [-15.1478,  46.9235],
        [ 20.4873,  -8.1849],
        [ -7.5224,  13.3067],
        [ -0.7545,   6.9671],
        [-12.6482,  18.1009],
        [ -0.6363,   7.7752],
        [ 22.2655, -15.1339],
        [ 18.6138,  -6.9820],
        [-23.4767,  28.3228],
        [  7.1217,  21.3171],
        [ -2.3777,   9.9000],
        [ -4.5634,   8.7705]], device='cuda:0')
output.data.max(1)[1]:  tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1], device='cuda:0')
tot_sum:  tensor(80.2628, device='cuda:0') 2
The device used will be: 
True
cuda:0
output.detach().clone():  tensor([[-1.0340e+01,  2.2067e+01],
        [ 4.4147e+01, -2.2677e+01],
        [ 1.7626e+01,  2.4747e+01],
        [-9.0672e+00,  1.6643e+01],
        [ 2.8776e+01, -1.8992e+01],
        [-1.6777e+01,  3.1806e+01],
        [ 2.6949e+00,  3.3092e+01],
        [ 5.4953e+00,  2.1262e+01],
        [-6.7350e+00,  1.7253e+01],
        [ 8.9839e+00,  1.1046e+01],
        [ 8.2880e-01,  2.4754e+00],
        [ 3.4455e+01, -1.6574e+01],
        [ 5.0897e+00,  3.0296e+01],
        [-4.9051e+00,  1.6719e+01],
        [-3.2368e+00,  1.8982e+01],
        [ 8.6556e+00,  3.1070e+01],
        [ 1.0497e+01, -6.6267e+00],
        [ 4.7396e+01, -2.9959e+01],
        [ 6.3897e+00,  1.1073e+01],
        [-1.3772e+01,  1.9694e+01],
        [ 8.1272e-01, -1.1424e+01],
        [-1.5296e+01,  2.8326e+01],
        [ 1.0051e+01,  2.2577e+01],
        [-1.5429e+01,  2.0275e+01],
        [ 1.5895e+01,  2.0338e+01],
        [ 4.7425e+00,  2.8473e+01],
        [ 2.3477e+00,  1.7009e+01],
        [ 9.9774e+00,  2.1031e+01],
        [ 3.5848e+01, -2.7275e+01],
        [ 2.1820e+01, -3.7631e+01],
        [ 3.3907e+01, -2.9564e+01],
        [ 5.3076e+00,  1.7716e+01],
        [ 1.1443e+01,  2.8739e+01],
        [ 2.4977e+01,  1.1258e+01],
        [ 1.4897e+01,  1.3565e+00],
        [ 1.5479e+01, -1.0817e+01],
        [ 7.0640e+00, -5.0820e+00],
        [ 2.3363e+00,  1.0800e+00],
        [ 2.8756e+01, -5.2640e-02],
        [ 8.5440e+00, -2.9211e+00],
        [ 2.1611e+00,  3.0058e+01],
        [ 2.7472e+01,  9.3371e+00],
        [ 1.6211e+01, -6.8893e+00],
        [ 2.2275e+01,  1.2704e+01],
        [-1.0379e+01,  3.1093e+01],
        [ 2.0354e+01, -1.0695e+01],
        [-1.5732e+01,  2.8649e+01],
        [ 1.0527e+01, -1.7307e+01],
        [-8.6488e-01,  6.6491e+00],
        [ 1.9681e+01, -1.5564e+01],
        [ 2.0962e+01, -4.9207e+00],
        [ 2.9945e+01, -9.4907e+00],
        [ 1.4795e+01,  7.3388e+00],
        [ 1.8764e+01,  1.0730e+01],
        [ 1.5961e+01, -1.1061e+01],
        [ 5.8720e+01, -2.4418e+01],
        [ 2.2555e+01, -1.8348e+01],
        [ 7.4444e+00,  2.3971e+01],
        [ 8.4867e+00,  1.0929e+01],
        [ 4.0891e+00,  9.2769e+00],
        [ 1.7694e+01, -1.1102e+01],
        [-4.9790e+00,  1.4729e+01],
        [ 4.6262e+01, -4.4230e+01],
        [-1.5317e+00, -1.4669e+00],
        [-6.2022e+00,  1.4405e+01],
        [ 4.7297e+01, -4.3308e+01],
        [ 8.4143e-01,  3.3058e+01],
        [ 2.4216e+00,  3.2993e+01],
        [ 7.9979e+00, -2.0719e+00],
        [ 2.6393e+01, -2.4178e+01],
        [ 2.1696e+01, -1.1384e+01],
        [ 2.6370e+01, -8.6221e+00],
        [ 1.9872e+01,  9.6377e+00],
        [ 3.4848e+01, -1.5348e+01],
        [ 6.2930e+00,  2.7204e+01],
        [ 8.5386e+00,  8.4875e+00],
        [ 3.0445e+01, -2.0117e+01],
        [ 7.3779e+00,  7.0437e-01],
        [ 4.7187e+01, -4.4427e+01],
        [ 5.9208e+00,  6.3815e+00],
        [ 2.7271e+01, -1.0458e+01],
        [ 4.7339e-01,  1.0475e+01],
        [ 2.8862e+01, -1.1595e+01],
        [ 1.3209e+01, -5.5643e+00],
        [ 5.2390e+01, -4.6498e+01],
        [ 1.9464e+01,  3.8789e+00],
        [ 7.5596e+00,  8.5171e+00],
        [ 3.5498e+01, -2.5642e+01],
        [ 2.6366e+01,  5.7171e+00],
        [ 2.4866e+01, -6.9522e+00],
        [-7.1871e+00,  1.5751e+01],
        [-1.0133e+01,  1.3795e+01],
        [ 8.8961e+00,  2.3713e+00],
        [ 5.3727e+01, -5.6944e+01],
        [-1.0736e+01,  1.6188e+01],
        [ 3.6619e+01, -4.1865e+01],
        [-2.9336e+01,  3.2918e+01],
        [ 1.6360e+01, -6.2754e+00],
        [ 2.3878e+01,  1.3040e+01],
        [ 1.4642e+01, -3.0106e+00],
        [ 1.3265e+01, -1.0655e+01],
        [ 8.0935e+00,  3.0218e+01],
        [ 8.6856e+00,  2.5878e+01],
        [-1.2988e+01,  2.0127e+01],
        [ 1.7655e+01,  2.1108e+01],
        [ 3.8229e+01, -2.5067e+01],
        [-6.0200e+00,  9.4832e+00],
        [-1.0279e+01,  4.2196e-01],
        [ 3.2998e+01, -1.3052e+01],
        [ 1.4033e+01,  1.9113e+01],
        [ 2.3806e+01,  1.7415e+01],
        [ 2.4702e+01, -5.9558e-02],
        [ 4.5742e+00,  2.9346e+01],
        [ 1.5518e+01,  1.6382e-01],
        [-8.6705e+00,  2.7964e+01],
        [-8.5695e+00,  1.5545e+01],
        [ 2.3273e+01,  9.3257e+00],
        [-1.8278e+01,  3.3123e+01],
        [-6.9154e+00,  3.5354e+01],
        [ 1.5940e+01,  1.1306e+01],
        [ 1.0644e+01,  2.1132e+01],
        [-1.0962e+01,  1.4856e+01],
        [-1.0675e+00,  2.9613e+01],
        [ 2.0546e+01, -1.1851e+01],
        [-1.6926e+01,  2.5782e+01],
        [ 2.9190e-01,  7.6655e+00],
        [ 7.9235e+00,  9.4605e+00],
        [ 1.2667e+01,  1.1991e+00],
        [-2.6725e+00,  4.6138e+01],
        [ 6.5281e+00,  3.1583e+01],
        [ 8.4831e+00, -5.3525e+00],
        [-3.5222e+00,  1.2251e+01],
        [ 3.1471e+01, -3.1951e+01],
        [ 1.2639e+01, -9.4235e+00],
        [ 2.7104e+00,  1.4816e+00],
        [ 2.3042e+01, -1.0374e+01],
        [ 2.1727e+01, -6.8602e+00],
        [-6.5286e+00,  2.0855e+01],
        [-1.3339e+00,  8.7149e+00],
        [ 5.4893e+00,  1.5271e+01],
        [ 2.7457e+01, -1.0155e+01],
        [ 2.0863e+01,  2.5148e+00],
        [ 3.8523e+00,  1.9969e+01],
        [ 7.8831e+00,  2.3090e+01],
        [ 4.2908e+01, -1.0630e+01],
        [ 3.7150e+01, -3.9544e+01],
        [ 1.3507e+00,  3.6705e+00],
        [ 5.8245e+00,  1.8062e+01],
        [-1.8479e+00,  3.8634e+00],
        [-1.8478e+01,  1.5387e+01],
        [ 2.6778e+01, -5.8210e+00],
        [ 1.5076e+01,  2.6019e+00],
        [ 2.8133e+01,  1.5352e+01],
        [ 2.2285e+00,  2.5117e+01],
        [-3.2259e+00,  3.4643e+01],
        [ 1.6810e+01,  4.3082e+00],
        [-1.2885e+01,  2.3205e+01],
        [-1.2052e+01,  1.1341e+01],
        [ 1.4810e+01, -7.8421e+00],
        [ 2.6798e+01,  7.9678e+00],
        [ 1.0531e+01,  1.4977e+00],
        [ 3.7769e+00,  3.7858e+00],
        [ 2.4694e+01, -1.3474e+01],
        [ 7.3829e+00,  1.1834e+01],
        [ 1.2212e+01, -1.8727e+01],
        [ 1.9400e+01,  1.9565e+00],
        [-7.8942e+00,  3.7986e+00],
        [-9.6815e+00,  1.9220e+01],
        [-5.3301e+00,  2.1633e+01],
        [ 9.2634e+00,  2.5953e+01],
        [ 9.5917e+00,  2.0896e+01],
        [ 1.9138e+01,  6.1808e+00],
        [ 9.5322e-01,  3.5590e+01],
        [ 1.1548e+01,  3.1729e-01],
        [ 7.5693e+00, -4.7071e-02],
        [ 3.4240e+01, -8.3635e+00],
        [-1.5747e+01,  2.1284e+01],
        [ 1.5716e+01, -1.7136e+01],
        [ 3.4656e+00, -3.7982e+00],
        [ 3.2372e-01,  6.8078e+00],
        [ 1.3161e+01,  8.1385e+00],
        [-1.5764e+01,  1.7954e+01],
        [ 3.6098e+01, -3.1822e+01],
        [ 3.4473e+01, -2.3178e+01],
        [ 9.9727e+00,  7.4070e-01],
        [ 1.1077e+01,  2.8834e+01],
        [-7.5896e+00,  1.4555e+01],
        [ 3.1354e+01, -2.2257e+01],
        [-1.2673e+00,  3.8958e+00],
        [-8.5856e+00,  1.6371e+01],
        [ 2.9947e+01, -4.8863e+01],
        [-1.8654e+01,  1.9026e+01],
        [ 1.5838e+01,  1.6832e+01],
        [-1.6410e+01,  2.2622e+01],
        [ 2.2632e+01, -1.0842e+01],
        [ 3.8354e+01, -3.2187e+01],
        [-5.9308e+00,  4.9223e+00],
        [ 1.0944e+01, -8.6997e+00],
        [ 2.9612e+01, -1.6476e+01],
        [ 1.6136e+01, -1.5317e+01]], device='cuda:0')
output.data.max(1)[1]:  tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0], device='cuda:0')
tot_sum:  tensor(79.7715, device='cuda:0') 3
The device used will be: 
True
cuda:0
output.detach().clone():  tensor([[-1.0952e+01,  1.0950e+00],
        [-7.5105e+00,  1.3432e+01],
        [ 1.2918e+01, -9.7877e+00],
        [ 8.7726e+00, -6.5282e+00],
        [ 1.3149e+01,  1.1774e+00],
        [ 1.0798e+00,  1.0647e+01],
        [ 4.8263e+00, -6.2464e+00],
        [ 2.3224e+01, -2.0464e+01],
        [ 2.1555e+01, -2.6801e+01],
        [ 2.2461e+01, -1.2134e+01],
        [-1.4877e+00,  3.8030e+01],
        [-3.8798e+00,  3.4368e+01],
        [ 4.9166e+00,  6.3783e+00],
        [ 5.6879e+00,  2.9292e+01],
        [ 7.0864e+00,  1.3673e+01],
        [-3.6294e+00,  3.2175e+01],
        [ 9.2535e+00, -1.0508e+00],
        [ 4.5179e-01, -1.1626e+00],
        [ 2.3190e+01,  9.0885e+00],
        [ 2.4010e+00,  3.9972e+01],
        [ 2.8966e-01,  3.0684e+01],
        [ 9.8743e+00, -3.7079e+00],
        [ 1.1725e+00,  1.4374e+00],
        [ 2.7387e+01, -3.0604e+01],
        [ 9.7932e+00, -1.6309e+01],
        [ 7.8283e+00,  1.3471e+01],
        [ 8.9298e-01,  9.6244e+00],
        [-3.9855e+00,  1.4699e+01],
        [ 8.0506e+00,  9.2511e+00],
        [-1.7747e+01,  2.3672e+01],
        [ 4.3901e+00,  1.3940e+00],
        [ 3.7626e+00,  3.0801e+01],
        [ 1.1958e+00, -3.5898e+00],
        [-7.2744e+00,  2.0411e+01],
        [-2.3110e+00,  2.1727e+01],
        [-1.9934e+00,  1.2942e+01],
        [-6.4791e+00,  3.9107e+01],
        [ 7.4574e+00, -5.4420e+00],
        [-4.6906e+00,  1.8057e+01],
        [-5.1117e+00,  2.6468e+01],
        [ 8.7518e+00,  9.9761e-01],
        [ 4.6682e+00,  6.5514e+00],
        [ 2.3101e+01, -9.9392e+00],
        [-4.2483e+00,  3.7745e+01],
        [ 1.3785e+01,  1.8968e+00],
        [-2.2876e+01,  3.8952e+01],
        [ 2.4097e+01, -3.6429e+00],
        [ 3.2780e+00,  3.3639e+01],
        [-1.0636e+01,  2.5481e+01],
        [ 2.6606e+00,  2.8755e+01],
        [-1.2426e+01,  3.3140e+01],
        [-3.2570e+00,  6.7201e+00],
        [ 9.2303e+00,  1.6089e+01],
        [-1.0357e+01,  6.2608e+00],
        [ 3.2112e+00,  9.8186e+00],
        [ 1.1314e+01, -5.6916e+00],
        [ 6.6076e+00,  1.2685e+01],
        [ 1.6233e+01,  8.4084e+00],
        [-9.6157e+00,  2.1918e+01],
        [ 6.8683e+00,  1.2488e+01],
        [ 2.8749e+00,  3.2511e+01],
        [-9.9139e+00, -5.8978e+00],
        [ 1.7326e+01, -5.5124e+00],
        [-1.0196e+00,  3.2795e+01],
        [ 8.2619e+00,  7.7794e+00],
        [ 5.3825e+00,  3.0850e+01],
        [ 2.5220e-01,  3.0756e+01],
        [-1.0257e+01,  2.6560e+01],
        [ 3.9151e+00, -3.0963e+00],
        [ 5.2190e-01,  3.5265e+01],
        [ 1.5118e+01, -1.1814e+01],
        [ 1.2530e+01,  7.4153e+00],
        [-1.6684e+01,  2.1530e+01],
        [ 1.1829e+01, -7.6223e+00],
        [ 1.4876e+01, -2.2080e+00],
        [-6.7293e-01,  1.8421e+01],
        [-1.7091e+01,  2.5173e+01],
        [ 7.0254e+00,  3.6440e+01],
        [ 9.2785e+00,  1.1679e+01],
        [ 2.7147e+01, -5.6770e+00],
        [ 2.6822e+01, -9.5547e+00],
        [-5.2200e+00,  4.0634e+01],
        [ 1.2083e+01,  2.7827e+01],
        [-1.0749e+01,  1.8130e+01],
        [ 3.5056e-01,  3.5884e+01],
        [-1.3762e+01,  2.6678e+01],
        [-1.7738e+01,  2.7822e+01],
        [ 1.6254e+01,  9.6218e+00],
        [ 7.8648e+00,  8.3102e-01],
        [ 8.0299e+00,  9.3541e+00],
        [ 1.2396e+01, -8.1195e+00],
        [-3.0255e+00,  1.8770e+01],
        [ 2.4357e+00,  1.4344e+00],
        [ 4.6870e+00,  3.2122e+01],
        [ 1.3491e+00,  3.1797e+01],
        [-8.3284e+00,  1.5460e+01],
        [ 1.3474e+01,  3.0012e+01],
        [ 1.7189e+00,  1.9400e+01],
        [ 1.3782e+01, -9.5901e+00],
        [ 1.8409e+00,  5.7217e+00],
        [ 9.1661e+00,  1.9272e+01],
        [-7.0713e-01,  2.0737e+01],
        [ 1.0504e+01, -4.6122e+00],
        [ 1.8386e+01, -1.3725e+01],
        [ 2.7969e+01, -1.0089e+01],
        [ 5.0722e+00, -6.0809e+00],
        [-4.3715e-01,  2.6520e+00],
        [ 5.2907e+00, -3.3088e+00],
        [-1.0976e+01,  1.5875e+01],
        [-9.4842e+00,  1.7006e+01],
        [-8.7797e-02,  3.4008e+01],
        [ 3.0529e+00,  3.6673e+01],
        [ 8.7276e+00,  1.5843e+00],
        [-1.5567e+01,  4.9562e+00],
        [ 7.3845e+00,  2.2109e+01],
        [-1.1231e+01,  2.0552e+01],
        [-4.3130e+00,  3.1071e+01],
        [ 9.3461e+00, -2.3813e+00],
        [ 1.0352e+01,  1.2671e+01],
        [ 2.5585e+01, -8.1479e+00],
        [ 6.4008e+00,  2.3108e+01],
        [-1.5686e+01,  1.2687e+01],
        [ 4.3586e+00,  2.9541e+01],
        [ 1.4731e+01,  8.6468e+00],
        [ 1.2966e+01,  3.8473e+00],
        [ 8.0476e+00, -3.7045e+00],
        [-9.1510e+00,  2.5405e+01],
        [-1.0899e+01,  1.4793e+01],
        [-6.8027e+00,  3.9021e+01],
        [-1.5345e+01,  1.2254e+01],
        [ 5.8168e+00,  3.4365e+01],
        [ 1.9962e+00,  9.8685e+00],
        [-6.0836e+00,  9.6660e+00],
        [-3.6593e-02,  7.1752e+00],
        [ 8.1988e+00, -8.6785e+00],
        [-5.6229e-01,  2.4383e+01],
        [ 2.1561e+01, -1.0294e+01],
        [ 7.1009e+00,  6.7653e+00],
        [ 3.5244e+00,  8.2657e+00],
        [ 1.0772e+01, -5.7511e+00],
        [ 2.5747e-01,  8.3157e+00],
        [ 1.7696e+00,  3.6542e+01],
        [ 1.0873e+01,  3.9933e+00],
        [ 2.3176e+01,  1.2040e+01],
        [ 5.4638e+00,  2.9302e+01],
        [ 5.9070e+00,  2.1339e+01],
        [-1.5642e+01,  2.5962e+01],
        [ 2.2976e+01, -1.6384e+01],
        [-1.4520e+01,  1.7149e+01],
        [ 9.7414e+00,  2.8370e+01],
        [ 5.5327e+00,  2.7683e+01],
        [ 2.6899e+00,  2.1197e+01],
        [ 2.6885e+00,  2.8728e+01],
        [-9.2084e+00,  1.6853e+01],
        [ 1.2327e+01,  5.3510e+00],
        [-1.6727e+01,  2.8756e+01],
        [-2.1167e+00,  5.5329e+00],
        [ 2.7315e+01,  6.9865e+00],
        [-2.0558e+01,  2.9287e+01],
        [-7.3536e+00,  1.3566e+01],
        [ 5.9692e+00, -1.8517e+00],
        [ 2.9659e-01, -6.2931e-01],
        [ 3.7393e+00,  5.7992e+00],
        [ 2.9131e+00,  4.6627e+00],
        [ 4.3076e-01,  7.5265e+00],
        [ 9.8114e+00, -7.1475e-01],
        [-1.8097e+00,  8.7773e+00],
        [ 5.6495e+00,  9.3789e+00],
        [ 6.4106e+00,  2.6188e+01],
        [ 1.7746e+01, -1.5714e+01],
        [ 3.8800e-02,  3.6227e+00],
        [ 1.5023e-01,  2.2082e-01],
        [ 1.3424e+01, -1.0209e+01],
        [-5.2851e+00,  1.0821e+01],
        [ 1.6472e+01,  1.5611e+01],
        [-9.5618e+00,  1.6537e+01],
        [-2.5891e+00,  4.1352e+01],
        [-3.8650e+00,  3.1210e+01],
        [ 1.4386e+01,  1.4317e+01],
        [-1.2348e+01,  2.3980e+01],
        [-1.7417e+01,  3.9550e+01],
        [ 5.1031e+00,  3.4606e+00],
        [ 1.9467e+01, -2.6247e+01],
        [-5.3083e+00,  2.8651e+01],
        [-1.2771e+01,  2.2430e+01],
        [-1.4198e+01,  1.6388e+01],
        [ 3.0675e+00,  1.6149e+01],
        [-1.3596e+01,  2.2453e+01],
        [ 9.1153e+00, -3.6627e+00],
        [ 1.5297e+01,  2.5924e+01],
        [ 1.7444e+01, -7.6602e+00],
        [ 2.3665e+00,  1.4659e+01],
        [ 9.7770e+00, -5.7885e+00],
        [-7.3607e+00,  2.5274e+01],
        [ 2.6005e+01, -6.5055e+00],
        [ 6.1379e+00, -2.5054e+01],
        [ 3.1274e+01, -2.1418e+01],
        [-1.4121e+00,  3.4082e+01],
        [-1.3026e+01,  2.0812e+01],
        [-1.3491e+01,  3.0954e+01]], device='cuda:0')
output.data.max(1)[1]:  tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1], device='cuda:0')
tot_sum:  tensor(93.1160, device='cuda:0') 4
max_key : 4
Accuracy of the network on the 1st dataset: 62.500 %
Test loss on the 1st dataset: 0.039
################################## EVALUATION OF TASK 3 ############################################
SEED:  0
block 0, size : 96 16 16
96
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 2.886751345948129
block 1, size : 384 8 8
384
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.8505172717997146
block 2, size : 1536 4 4
1536
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.4252586358998573
block 3, size : 2560 2 2
2560
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.21262931794992865
block 4, size : 2560 1 1
2560
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.1647019614671031
NOWWWW
1 1
2
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
blocks['b%s' % 5] 2560 2560
range = 0.3423265984407288
train_layer_order: consecutive {'t0': {'blocks': [0, 1, 2, 3, 4], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 50, 'lr': 0.005}, 't1': {'blocks': [5], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}
The device used will be: 
True
cuda:0
PARAMSSSS IN LOAD:  {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
avg_deltas:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
topk_layer:  None
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.0.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
topk_layer inside model after retrieval:  [25, 59, 30, 5, 69, 42, 55, 68, 16, 82, 77, 53, 14, 35, 89]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.1.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
topk_layer inside model after retrieval:  [304, 137, 75, 345, 349, 197, 121, 22, 365, 336, 146, 202, 56, 100, 4, 67, 266, 163, 296, 354, 50, 159, 41, 347, 343, 180, 118, 190, 92, 94, 104, 76, 324, 62, 172, 191, 274, 200, 186, 217, 36, 105, 330, 128, 73, 21, 254, 357, 133, 169, 6, 326, 206, 193, 99, 380, 382, 88, 262]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.2.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
topk_layer inside model after retrieval:  [958, 1278, 761, 1436, 856, 95, 14, 1385, 733, 405, 622, 12, 307, 499, 1227, 1274, 1345, 869, 348, 612, 1452, 176, 1532, 250, 632, 1113, 312, 915, 423, 707, 406, 1508, 1291, 542, 942, 1317, 70, 743, 1442, 75, 544, 311, 574, 904, 961, 616, 959, 35, 801, 937, 490, 670, 1297, 994, 790, 434, 1450, 523, 450, 698, 1232, 936, 850, 636, 93, 164, 243, 22, 114, 911, 1238, 626, 1388, 350, 74, 1282, 1281, 1203, 913, 580, 238, 597, 1159, 774, 77, 889, 1025, 1086, 1486, 347, 868, 461, 783, 373, 905, 675, 639, 1245, 67, 359, 100, 673, 281, 1430, 1206, 705, 1101, 1089, 174, 1429, 662, 458, 810, 835, 1006, 1172, 923, 61, 852, 1185, 462, 667, 599, 41, 754, 395, 360, 855, 290, 728, 309, 25, 439, 1171, 1292, 457, 56, 875, 135, 13, 629, 148, 1424, 1002, 138, 1497, 1125, 1401, 614, 538, 964, 824, 1302, 441, 1294, 809, 1437, 1325, 246, 1100, 1186, 745, 324, 313, 356, 731, 276, 167, 335, 105, 212, 456, 851, 649, 596, 1192, 294, 118, 566, 133, 1248, 326, 584, 436, 1116, 447, 15, 933, 102, 818, 1046, 732, 195, 132, 233, 519, 1001, 1522, 674, 1022, 425, 1448, 688, 891, 1494, 1033, 886, 650, 508, 1307, 66, 808, 563, 1324, 872, 134, 110, 413, 209, 119, 1516, 779, 1266, 846, 1178, 717, 955, 215, 799, 973, 1032]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.3.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
topk_layer inside model after retrieval:  [505, 1755, 1843, 1403, 1783, 1554, 1938, 646, 421, 1447, 792, 564, 2241, 1226, 2460, 472, 1424, 507, 2050, 1958, 1198, 419, 2414, 653, 2305, 195, 1720, 497, 2098, 129, 1789, 2428, 2, 534, 454, 1608, 1451, 96, 299, 398, 953, 2233, 1894, 1266, 1926, 878, 1698, 1448, 689, 304, 1303, 2453, 499, 1887, 305, 1385, 2093, 1548, 1406, 456, 1567, 404, 233, 943, 1753, 533, 935, 1162, 2161, 1271, 2154, 586, 2309, 922, 862, 2300, 658, 1873, 1506, 1394, 390, 1364, 861, 627, 2000, 1697, 772, 2185, 2051, 1992, 2162, 2511, 1831, 1677, 425, 1409, 370, 509, 2112, 2227, 561, 110, 1799, 2223, 2022, 199, 492, 1638, 1236, 1021, 2500, 2362, 1436, 1080, 774, 1304, 347, 555, 2254, 436, 1458, 2164, 2278, 2302, 2062, 1076, 2523, 1599, 1088, 657, 923, 1175, 2056, 2018, 2001, 1524, 708, 1647, 2265, 1612, 1051, 1109, 1839, 1227, 122, 2094, 2469, 674, 1796, 1446, 284, 331, 453, 1845, 2042, 1157, 720, 2057, 1375, 1457, 632, 2007, 2151, 249, 696, 2085, 964, 2245, 1405, 2070, 2114, 1074, 757, 51, 164, 1427, 431, 1811, 1510, 1325, 1138, 867, 402, 2420, 1248, 2181, 1564, 2456, 450, 68, 2270, 2315, 761, 1993, 340, 2106, 1809, 291, 80, 2252, 1578, 1781, 748, 2323, 102, 846, 262, 1337, 146, 1699, 2354, 912, 527, 895, 788, 2328, 746, 178, 638, 1221, 724, 2167, 1128, 1536, 735, 1003, 333, 240, 1985, 1253, 737, 715, 2021, 2240, 2179, 2317, 2342, 2044, 2011, 639, 1890, 1380, 780, 1045, 2004, 448, 1733, 231, 980, 1439, 283, 365, 944, 1646, 60, 2427, 1363, 1642, 1348, 1982, 1696, 605, 2341, 2152, 1998, 1341, 252, 726, 1920, 2160, 2250, 2340, 1968, 1867, 2095, 2352, 2248, 2249, 810, 2081, 1830, 723, 104, 987, 54, 1283, 1824, 1433, 1104, 2479, 1219, 2084, 378, 1340, 225, 1402, 2297, 2502, 1484, 1376, 339, 1220, 928, 626, 687, 2322, 616, 1545, 297, 1435, 758, 1676, 1555, 2514, 677, 742, 583, 694, 1689, 278, 903, 1690, 1013, 35, 927, 1362, 2236, 177, 1742, 1851, 156, 277, 346, 2173, 2221, 312, 52, 937, 69, 1874, 752, 1070, 2087, 1619, 2559, 1346, 1891, 1492, 969, 1002, 77, 1351, 2542, 753, 1267, 1056, 1210, 1187, 2174, 242, 1801, 2259, 46, 1670, 2079, 200, 1865, 13, 2242, 1353, 1928, 1044, 1411, 1614, 699, 1844, 1053, 72, 1645, 1356, 89, 884, 2551, 1287, 1961]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.4.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
topk_layer inside model after retrieval:  [1118, 1323, 1657, 557, 888, 1143, 530, 966, 605, 91, 1504, 426, 1167, 2265, 2388, 1233, 1957, 99, 1366, 1970, 2385, 2531, 93, 382, 2393, 1706, 2457, 2559, 674, 2121, 1869, 402, 1250, 770, 6, 1883, 1406, 778, 30, 2335, 172, 315, 1063, 883, 386, 1713, 2218, 70, 2211, 1375, 455, 499, 2060, 2440, 1428, 161, 2415, 757, 703, 654, 139, 2171, 2053, 1066, 1064, 326, 1099, 1471, 15, 1387, 140, 621, 2034, 1373, 607, 534, 1602, 1750, 1090, 1289, 774, 2510, 1999, 2195, 270, 1731, 1273, 1928, 2143, 2250, 1787, 400, 2532, 1556, 1912, 2475, 1513, 1704, 1284, 448, 625, 1676, 1730, 1434, 1607, 3, 612, 1034, 193, 1442, 208, 843, 1241, 1489, 2268, 825, 1384, 1981, 296, 1877, 177, 1298, 1880, 2141, 969, 7, 1477, 1313, 806, 798, 2015, 2337, 1622, 2395, 905, 2286, 1402, 412, 1908, 1084, 2138, 328, 1684, 556, 34, 2410, 2285, 2025, 663, 150, 1252, 0, 549, 2066, 792, 8, 366, 112, 1201, 2070, 704, 2099, 2302, 548, 482, 1982, 1397, 1640, 889, 2191, 2189, 1306, 1326, 2558, 503, 263, 1071, 2436, 1246, 2226, 789, 666, 2081, 593, 718, 544, 568, 1080, 364, 2485, 615, 1618, 1597, 1035, 361, 1700, 233, 24, 2327, 765, 1987, 861, 2021, 628, 1571, 1444, 1580, 2487, 2346, 2452, 2453, 54, 2158, 1452, 2343, 309, 1819, 1135, 1810, 1285, 1892, 2188, 1969, 2326, 1169, 494, 768, 1338, 5, 2228, 182, 1796, 1644, 924, 2432, 2112, 120, 815, 1583, 2207, 2480, 2205, 85, 1012, 1719, 2276, 904, 105, 1723, 1995, 2277, 2172, 2239, 1357, 637, 2085, 1040, 1922, 2512, 1230, 225, 658, 738, 1065, 242, 1468, 1639, 1734, 1210, 689, 788, 108, 1134, 2546, 207, 2413, 1523, 257, 1424, 2516, 671, 1124, 2134, 558, 335, 2224, 332, 1619, 1239, 926, 951, 891, 1445, 239, 493, 1609, 424, 449, 1732, 2311, 1852, 1759, 2254, 340, 1535, 2509, 1634, 447, 1572, 802, 1770, 395, 1030, 1411, 1645, 2212, 562, 311, 1420, 531, 1575, 1996, 502, 259, 202, 670, 2045, 682, 1390, 132, 2518, 565, 2356, 560, 190, 2220, 2149, 1418, 2079, 1355, 1109, 1113, 553, 2540, 2249, 1015, 2538, 564, 705, 214, 2525, 1993, 700, 341, 252, 430, 1937, 1140, 1878, 2011, 783, 1198, 86, 2507, 1275, 603, 1372, 1849, 1221, 2163, 604, 1522, 1339, 1558, 2517, 2535, 1496, 1204, 874, 1641, 1432, 895, 2492, 2271, 1467]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
The device used will be: 
True
cuda:0
The device used will be: 
True
cuda:0
chosen_head:  {'blocks.5.layer.bias': tensor([ 0.0052, -0.0217], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0350,  0.0053, -0.0105,  ...,  0.0090, -0.0074,  0.0153],
        [-0.0067,  0.0172,  0.0149,  ..., -0.0063,  0.0286, -0.0075]],
       device='cuda:0')}

 Model C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c loaded successfuly with best perf



 ----- Architecture Block BatchNorm2dSK3962(5, 5)1.0reflect, number 0 -----
- BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK963842(3, 3)0.6499999761581421reflect, number 1 -----
- BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.4)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK38415362(3, 3)0.25reflect, number 2 -----
- BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block BatchNorm2dSK153625602(3, 3)0.25reflect, number 3 -----
- BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block BatchNorm2dSK256025602(3, 3)0.25reflect, number 4 -----
- BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 5 -----
- Flatten(start_dim=1, end_dim=-1)
- Dropout(p=0.5, inplace=False)
- Linear(in_features=2560, out_features=2, bias=True)
model.heads:  [{'blocks.5.layer.bias': tensor([ 1.5791e-05, -1.6506e-02], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0120,  0.0150,  0.0043,  ...,  0.0072,  0.0111, -0.0045],
        [ 0.0163,  0.0075,  0.0001,  ..., -0.0045,  0.0101,  0.0122]],
       device='cuda:0')}, {'blocks.5.layer.bias': tensor([ 0.0040, -0.0205], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 1.5765e-02,  1.0660e-02,  4.3432e-03,  ...,  1.0930e-02,
          1.1431e-02, -3.5607e-03],
        [ 1.2527e-02,  1.1907e-02,  8.8815e-05,  ..., -8.2031e-03,
          9.7858e-03,  1.1328e-02]], device='cuda:0')}, {'blocks.5.layer.bias': tensor([ 0.0005, -0.0170], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0349,  0.0030,  0.0162,  ..., -0.0045,  0.0023,  0.0014],
        [-0.0066,  0.0195, -0.0117,  ...,  0.0072,  0.0189,  0.0064]],
       device='cuda:0')}, {'blocks.5.layer.bias': tensor([-0.0005, -0.0160], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0531,  0.0097,  0.0127,  ...,  0.0032,  0.0010,  0.0103],
        [-0.0248,  0.0129, -0.0082,  ..., -0.0005,  0.0202, -0.0025]],
       device='cuda:0')}, {'blocks.5.layer.bias': tensor([ 0.0052, -0.0217], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0350,  0.0053, -0.0105,  ...,  0.0090, -0.0074,  0.0153],
        [-0.0067,  0.0172,  0.0149,  ..., -0.0063,  0.0286, -0.0075]],
       device='cuda:0')}]
[Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (5): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=2560, out_features=2, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)]
LAYER NAME:  Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (5): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=2560, out_features=2, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)
LAYER CHILDREN:  [BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=1.4)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
  )
  (layer): Linear(in_features=2560, out_features=2, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)]
subsubl NAME:  Sequential(
  (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=1.4)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): Flatten(start_dim=1, end_dim=-1)
)
subsubl NAME:  Linear(in_features=2560, out_features=2, bias=True)
subsubl NAME:  Dropout(p=0.5, inplace=False)
CONFIG MODE:  unsupervised
CONFIG MODE:  supervised
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([49, 33, 72, 51, 71, 92, 15, 14, 23,  0, 71, 75, 81, 69, 40, 43, 92, 97,
        70, 53], device='cuda:0')
TARGETS AFTER SUB:  tensor([56, 56,  2, 56,  2,  2,  2,  2,  2,  2,  2, 56,  2, 56, 56, 56,  2, 56,
        56,  2])
[2, 56]
TARGETS AFTER CLEANER:  tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0])
200
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 200
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               None
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([19, 29,  0, 11,  1, 86, 90, 28, 23, 31, 39, 96, 82, 17, 71, 39,  8, 97,
        80, 71], device='cuda:0')
TARGETS AFTER SUB:  tensor([56, 56,  2,  2, 56, 56,  2,  2, 56, 56, 56, 56, 56,  2,  2, 56, 56, 56,
        56, 56])
[2, 56]
TARGETS AFTER CLEANER:  tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1])
1000
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 1000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               None
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([19, 29,  0, 11,  1, 86, 90, 28, 23, 31, 39, 96, 82, 17, 71, 39,  8, 97,
        80, 71], device='cuda:0')
TARGETS AFTER SUB:  tensor([56, 56,  2,  2, 56, 56,  2,  2, 56, 56, 56, 56, 56,  2,  2, 56, 56, 56,
        56, 56])
[2, 56]
TARGETS AFTER CLEANER:  tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1])
1000
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 1000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
------------------------
INDICES:  1000
IMAGE SIZE: torch.Size([64, 3, 32, 32])
The device used will be: 
True
cuda:0
output.detach().clone():  tensor([[ 9.3816e+00, -5.7831e+00],
        [ 1.3669e+01,  9.3367e-01],
        [ 4.2521e+00,  1.4350e+01],
        [ 2.5916e+00, -6.0032e+00],
        [ 3.0133e-01,  1.6539e+01],
        [ 3.3391e+00,  3.8397e+00],
        [ 1.7541e+01, -2.7093e+00],
        [ 1.5956e+01, -8.9646e+00],
        [ 3.0028e+00, -3.0150e+00],
        [ 2.3630e+00,  1.2448e+01],
        [ 9.8032e+00,  1.4310e+01],
        [ 9.7014e+00, -1.1323e+01],
        [ 9.9602e+00, -1.1806e+01],
        [ 1.3404e+01, -6.0515e+00],
        [ 7.3819e+00,  1.1940e+01],
        [ 1.2160e+01,  1.2651e+01],
        [ 1.0603e+01,  3.7709e+00],
        [ 5.3318e-01,  5.4250e+00],
        [ 2.6530e+01, -1.8392e+01],
        [ 2.6000e+01,  1.8657e+01],
        [ 4.2752e+00,  8.6627e+00],
        [ 2.2769e+00,  1.8095e+01],
        [ 1.8727e+01, -1.3400e+00],
        [ 1.3852e+01,  2.7520e-01],
        [-5.9814e+00, -1.2446e+00],
        [ 2.9643e+00,  3.7097e+00],
        [ 1.6110e+01, -6.4868e+00],
        [ 2.9289e+00, -8.9108e+00],
        [-2.6495e+00,  4.3128e+00],
        [ 7.9044e+00,  3.3362e+00],
        [-1.4286e+01,  3.2596e+01],
        [ 1.0378e+01, -8.0536e+00],
        [ 2.7952e+00,  1.2260e+01],
        [ 6.3004e+00,  6.3934e+00],
        [ 2.6479e+00,  2.2276e+00],
        [ 1.0648e+01, -3.2061e+00],
        [-5.9823e+00,  1.3688e+01],
        [ 5.3046e+00,  5.6960e+00],
        [ 1.4130e+01, -1.0442e+01],
        [ 2.1515e+01, -1.7129e+01],
        [ 4.6424e+00, -4.9066e+00],
        [ 4.7195e+00, -1.2590e+01],
        [ 1.3854e+01, -5.8419e+00],
        [ 3.6001e-01,  5.5513e+00],
        [ 7.1023e+00,  4.5779e+00],
        [ 8.8982e+00,  4.7950e+00],
        [ 5.5091e+00,  4.5780e+00],
        [ 2.7452e+01, -7.8119e+00],
        [-3.1469e+00,  1.0215e+01],
        [ 3.3501e+01, -8.8619e+00],
        [ 3.5517e+00,  5.4374e+00],
        [ 1.0514e+01, -7.5423e+00],
        [ 9.1467e-01,  2.0387e+01],
        [ 1.0525e+01,  2.9477e+00],
        [ 1.4153e+01, -2.7271e+01],
        [ 5.5768e+00,  3.1262e+00],
        [ 1.9957e+01, -4.8036e+00],
        [ 1.0744e+01, -1.4311e+01],
        [ 1.1453e+01, -1.2109e+01],
        [ 2.6208e+01, -2.2842e+01],
        [ 2.8876e+01, -1.3415e+01],
        [-1.1247e+01,  9.0234e+00],
        [ 1.1489e+01, -6.3507e+00],
        [ 8.7212e+00,  2.3631e-01],
        [ 1.8541e+01, -1.2074e+00],
        [ 2.5990e+01,  5.7136e+00],
        [ 1.6936e+01, -1.9356e+01],
        [ 2.0095e+01, -1.2827e+01],
        [ 1.9248e+00, -1.1979e+00],
        [-2.1521e+00,  9.3736e+00],
        [ 4.0975e+00,  1.6227e+00],
        [ 1.4483e+01, -1.0771e+01],
        [ 1.8717e+01,  1.7109e-02],
        [ 1.2451e+01, -1.4403e+00],
        [ 6.1165e+00,  4.6717e+00],
        [-5.0722e+00,  6.4950e+00],
        [ 2.9759e+01, -7.2585e+00],
        [ 1.5509e+01, -1.2360e+01],
        [ 5.3608e+00,  1.2358e+01],
        [ 2.2329e+01, -7.6164e+00],
        [ 7.8980e+00, -1.4837e+01],
        [ 1.5257e+01,  1.6233e-01],
        [-2.5738e-01,  5.0939e+00],
        [ 1.6449e+01, -1.5685e+01],
        [ 5.1749e+00, -1.6384e-01],
        [ 2.2096e+00, -4.3019e+00],
        [ 2.4470e+01, -1.6135e+01],
        [ 1.2213e+01, -2.6569e+01],
        [ 1.6143e+01, -1.0790e+01],
        [ 4.4563e+00,  4.5812e+00],
        [-5.8615e+00,  2.0155e+01],
        [ 1.8376e+01, -1.8106e+01],
        [-5.6836e+00,  3.4098e+00],
        [ 1.3599e+00,  2.0943e+01],
        [-5.5879e+00,  7.0359e+00],
        [ 1.2181e+01, -1.7440e+00],
        [-2.4059e+00, -1.0613e+01],
        [ 3.0741e+00,  1.4588e+01],
        [ 2.0617e+01, -1.5740e+01],
        [ 2.4511e+00,  3.8604e+00],
        [ 3.2834e-01,  1.8215e+01],
        [ 3.9981e+00,  2.9452e+00],
        [-7.3626e+00,  7.4179e+00],
        [-2.0625e+00,  1.3336e+01],
        [ 1.8716e+01, -1.8325e+01],
        [ 1.0361e+01,  1.8401e+01],
        [ 3.6427e-01,  4.7388e+00],
        [ 3.0428e-01,  8.4303e+00],
        [ 2.4031e+01,  1.6031e+01],
        [ 1.0276e+01,  1.3283e+01],
        [ 5.4663e+00,  2.0696e+01],
        [ 1.4019e+00,  9.4161e+00],
        [ 1.0141e+01, -2.0970e+01],
        [-5.4552e+00,  1.6756e+01],
        [ 4.4816e+00,  6.4540e+00],
        [ 3.7240e+01, -5.7642e+00],
        [ 4.5898e+00,  2.0086e+01],
        [ 9.0713e+00, -1.5473e+01],
        [-3.4553e+00,  1.0867e+01],
        [-5.4934e-01,  9.5195e-02],
        [-3.0678e+00,  7.5862e+00],
        [ 2.4873e+01, -9.8640e+00],
        [ 6.0767e+00,  1.1749e+01],
        [-4.0280e+00,  1.1587e+01],
        [-2.4317e+01,  3.0609e+01],
        [ 6.5228e+00,  3.0370e+00],
        [-4.5286e-01,  7.1599e+00],
        [ 1.5522e+01, -9.8817e+00],
        [ 1.0796e+00, -4.0395e+00],
        [ 2.4489e+01, -2.4823e+01],
        [ 1.2906e+01,  5.8680e+00],
        [ 1.3732e+01, -8.7094e+00],
        [ 2.3552e+01, -2.2928e+01],
        [ 1.5430e+01, -5.5163e+00],
        [ 4.4676e-02,  8.9616e+00],
        [-4.3683e+00,  2.2478e+00],
        [ 1.0064e+01, -5.4876e+00],
        [-3.9587e+00,  5.5577e+00],
        [ 2.6526e+00,  6.8980e+00],
        [-3.6825e-01, -6.8347e+00],
        [ 2.3678e+01,  7.2608e-01],
        [ 1.8862e+01, -3.3207e+00],
        [-2.0678e+01,  1.9076e+01],
        [ 7.6650e+00, -2.2840e+00],
        [ 1.6698e+01, -6.1143e+00],
        [-6.7967e+00,  3.4262e+01],
        [ 1.3379e+01,  1.2289e+01],
        [ 3.8796e+00,  4.2650e+00],
        [ 6.7319e+00,  9.2747e+00],
        [ 2.3186e+00,  8.6222e+00],
        [ 1.0753e+00,  6.2512e+00],
        [ 4.4626e-01, -8.4059e-01],
        [ 2.4541e+01, -6.7065e+00],
        [ 5.5472e-01,  4.5909e+00],
        [-8.4784e-01,  9.9787e-01],
        [ 6.8490e+00,  1.3111e+01],
        [-2.2359e+00, -1.4368e+00],
        [ 1.2529e+01, -5.7067e+00],
        [-4.9143e+00,  3.4713e+00],
        [ 1.7017e+01, -5.5643e+00],
        [-8.8370e+00,  1.4663e+01],
        [ 6.7719e+00, -3.8533e+00],
        [ 1.3357e+01, -3.2830e+00],
        [-6.3902e-01, -3.6154e+00],
        [ 1.3807e+01, -3.1154e+00],
        [ 1.7787e+01, -5.6722e+00],
        [ 6.4865e+00,  2.7658e+00],
        [ 1.2203e+01, -7.9737e+00],
        [ 2.7217e+01, -2.5960e+01],
        [ 4.8062e+00,  9.9889e-02],
        [ 1.2380e+01, -3.4143e+00],
        [ 1.3917e+01,  3.4284e-01],
        [ 1.2377e+01, -5.8189e-01],
        [-1.8503e+00,  1.2291e+01],
        [ 9.5952e+00, -8.8079e+00],
        [ 5.1442e+00, -8.8146e+00],
        [ 8.2489e+00,  1.0927e+01],
        [ 1.2768e+01,  9.3024e+00],
        [ 1.1465e+01,  1.0601e+01],
        [ 1.2040e+01, -9.6008e+00],
        [ 2.0392e+01,  2.9041e+00],
        [-8.6425e+00, -5.9073e+00],
        [ 1.7004e+01, -1.1435e+01],
        [ 1.5035e+00, -1.4578e-01],
        [ 9.9471e+00,  3.0663e+00],
        [ 4.6388e+00, -6.2315e+00],
        [ 5.7325e+00,  5.0938e-01],
        [ 7.1363e+00, -7.0088e+00],
        [ 5.7639e+00,  1.3440e+00],
        [ 4.3597e+00,  1.0360e+01],
        [ 2.8575e+00,  6.6989e+00],
        [-3.6815e+00,  2.3310e+00],
        [ 1.8925e+01, -7.1016e+00],
        [ 2.4261e+01,  1.1198e+01],
        [-1.1668e+01,  2.2666e+01],
        [-4.3018e+00,  1.9632e+01],
        [ 1.6824e+01,  1.0413e+01],
        [-6.5912e+00,  2.3905e+01],
        [ 1.5353e+01, -5.4301e+00],
        [ 1.1561e+01,  1.9286e+01]], device='cuda:0')
output.data.max(1)[1]:  tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1], device='cuda:0')
tot_sum:  tensor(82.2000, device='cuda:0') 0
The device used will be: 
True
cuda:0
output.detach().clone():  tensor([[ 3.7133e+00,  5.1590e-01],
        [-1.4759e+01,  8.7768e+00],
        [ 1.4709e+01, -3.8912e+00],
        [ 9.2565e+00,  6.6619e-01],
        [-1.4328e+00,  1.5560e+01],
        [ 2.7610e-01, -1.8690e+00],
        [ 7.6485e+00,  7.7704e+00],
        [-9.2102e-01,  8.0997e+00],
        [ 8.7616e+00,  7.9469e-01],
        [ 9.8207e+00,  4.9899e+00],
        [ 9.0742e+00,  1.4485e+01],
        [ 9.2153e+00, -1.1489e+01],
        [ 2.8471e+00,  2.8628e+01],
        [ 1.2254e+01,  2.7544e+00],
        [ 1.2752e+01, -1.8168e+00],
        [ 7.9083e+00, -6.8692e-01],
        [ 2.5059e+01, -5.8831e+00],
        [ 1.4850e+01,  1.3911e+01],
        [ 1.0980e+01, -1.3204e+01],
        [-7.9609e+00,  4.2881e+00],
        [ 1.2758e+01, -1.0774e+00],
        [ 1.9391e+00, -3.5607e+00],
        [ 7.5041e+00, -3.9058e+00],
        [ 2.3480e+01, -4.7059e+00],
        [-6.6479e+00,  1.1526e+01],
        [ 3.4305e+00,  7.5667e+00],
        [ 6.8828e+00, -1.5300e+00],
        [ 1.0105e+01, -5.4570e-01],
        [ 7.3205e+00, -6.4657e-01],
        [-2.0429e+00,  1.8049e+01],
        [-4.1300e-01, -7.4573e+00],
        [ 1.3797e-01,  7.5672e+00],
        [ 4.8762e+00,  4.1128e+00],
        [-3.5042e-01,  2.7587e+01],
        [ 6.3959e+00, -1.2504e+00],
        [-1.6274e+01,  1.7873e+01],
        [-3.9207e+00,  4.0706e+00],
        [-1.6558e+01,  2.4696e+01],
        [-3.5901e+00,  5.0127e+00],
        [-6.1926e+00,  4.3466e+00],
        [ 4.9807e+00,  3.0479e+01],
        [ 1.0658e+01,  1.3746e+01],
        [ 1.1927e+01,  5.7917e+00],
        [ 2.1328e+00,  9.1678e+00],
        [-8.4877e+00,  1.8401e+01],
        [ 5.4301e+00,  4.8112e-01],
        [-9.5779e+00, -3.5395e+00],
        [-1.2049e+00, -2.3814e-01],
        [ 5.7195e+00,  1.6324e+00],
        [ 1.5943e+01, -4.1480e+00],
        [-1.2583e+01,  1.0980e+01],
        [-4.5364e+01,  4.5029e+01],
        [ 9.7733e+00, -3.5316e+00],
        [ 3.5891e+00,  5.1453e+00],
        [ 1.7987e+00,  5.2526e-01],
        [ 1.0099e+01,  7.2882e+00],
        [ 1.9030e+00, -1.9153e+00],
        [ 1.4256e+01,  1.0420e+01],
        [-2.7440e+00,  1.3754e+01],
        [-2.8745e+00,  1.6347e+01],
        [ 5.9765e+00, -1.8501e-02],
        [ 8.6334e+00, -1.9265e+00],
        [ 4.3152e+00,  1.4228e+01],
        [ 9.2336e+00,  2.0069e+00],
        [ 9.6920e-01,  2.3960e+00],
        [ 1.7605e+01, -7.6436e-01],
        [ 1.0112e+01, -5.1015e+00],
        [-2.5523e+00,  1.0112e+01],
        [ 1.0774e+01,  1.6692e+01],
        [ 6.3909e+00, -6.6552e+00],
        [ 1.1260e+01,  1.0806e+01],
        [ 5.7942e+00,  2.9087e+00],
        [ 3.1539e+00, -5.2463e+00],
        [-8.2066e+00,  8.4765e+00],
        [-1.0910e+01, -2.1086e+00],
        [ 1.2336e+01,  1.9368e+01],
        [ 7.3021e+00,  1.7352e+00],
        [ 1.6902e+01,  1.3945e+01],
        [ 5.5159e+00,  1.6984e+01],
        [ 1.0478e+01, -9.0298e+00],
        [ 1.7625e+01, -2.9050e+00],
        [ 6.0312e+00,  9.2984e+00],
        [ 1.0026e+01,  2.9874e+00],
        [ 4.2587e+00, -1.3402e+00],
        [ 2.1867e+01, -8.1737e+00],
        [-5.2167e+00,  1.7331e+01],
        [ 1.0072e+01, -5.1664e+00],
        [ 1.4264e+00,  9.3617e+00],
        [ 1.7339e+01,  1.9828e+00],
        [ 1.6186e+01,  3.7740e+00],
        [ 1.7348e+01, -7.2737e+00],
        [ 2.9924e+00,  2.0297e+00],
        [ 6.2817e+00, -1.7635e+00],
        [ 7.7285e+00,  6.6449e+00],
        [ 1.7929e+01, -4.9913e+00],
        [ 8.5237e+00,  9.1379e+00],
        [ 8.6979e+00, -1.2559e+00],
        [-3.3361e+00,  1.5159e+01],
        [-1.1056e-01,  3.0825e+00],
        [ 3.8392e+00,  1.9866e+00],
        [-4.6804e+00,  2.9319e+01],
        [ 6.3220e+00,  1.3318e+01],
        [ 1.6181e+01, -5.1802e+00],
        [ 1.0042e+01,  4.2174e+00],
        [ 4.6045e+00,  6.6687e+00],
        [ 1.7715e+01, -1.5276e+01],
        [ 4.5224e+00,  1.0464e+00],
        [ 2.3965e+01,  1.7027e+00],
        [-3.4996e+01,  4.0636e+01],
        [-1.3135e+01,  2.8190e+01],
        [ 8.7299e+00,  8.2067e-01],
        [ 3.7441e+00, -3.0173e+00],
        [ 7.5865e+00, -4.7875e-01],
        [-1.3117e+00,  5.0002e+00],
        [-3.7162e+00,  1.9177e+01],
        [ 4.4599e+00, -7.8717e+00],
        [ 1.6504e+01,  5.7989e+00],
        [ 2.3902e+01,  2.1116e-01],
        [ 1.8874e+01, -1.0729e+01],
        [ 8.2400e+00, -8.1126e+00],
        [ 9.8151e+00, -3.5036e+00],
        [-6.9248e+00,  2.1637e+01],
        [ 1.3011e+01, -4.0050e+00],
        [-9.0949e+00, -5.4550e+00],
        [ 7.3322e+00,  6.8016e-01],
        [ 1.2717e+01, -5.3903e+00],
        [ 1.2720e-01,  1.0309e+01],
        [ 6.7648e+00, -1.3991e+01],
        [ 7.6742e+00, -2.5713e+00],
        [-8.9778e+00,  2.5755e+00],
        [ 7.7559e+00,  1.0978e+01],
        [ 1.8769e+00, -2.1366e-01],
        [-1.3957e+01,  1.0391e+01],
        [-2.7959e+00,  3.1871e+00],
        [ 4.0627e+00,  1.4247e+01],
        [-4.8776e+00,  1.1170e+01],
        [ 4.9981e+00,  5.6936e+00],
        [-1.5783e-02,  6.8375e+00],
        [ 8.1556e+00,  1.0966e+00],
        [ 1.4107e+01, -6.6953e+00],
        [-1.8385e+01,  7.5549e+00],
        [ 4.3314e+00,  1.3503e+01],
        [-3.3104e+01,  3.3728e+01],
        [ 2.9751e+00,  5.9909e+00],
        [ 1.5766e+01,  9.0451e+00],
        [-2.5496e+01,  1.8557e+01],
        [-5.1484e+00,  9.5344e+00],
        [ 1.0874e+01, -4.3329e-01],
        [ 5.2948e+00,  1.0246e+01],
        [-8.7908e+00, -5.5648e+00],
        [-2.6927e-01, -6.9338e+00],
        [ 2.1274e+01, -2.6725e+00],
        [-5.0941e-01, -3.7451e+00],
        [ 1.3465e+01,  1.1383e+00],
        [ 5.9957e+00,  9.9535e-01],
        [ 1.1500e+01, -6.6636e+00],
        [ 1.0213e+01, -3.1447e+00],
        [ 3.4140e+00,  1.1623e+00],
        [ 6.4582e+00,  8.6954e+00],
        [ 1.1573e+01, -1.0215e+01],
        [ 1.7896e+01, -3.6029e+00],
        [ 6.6246e+00, -9.0453e-01],
        [ 1.0508e+01,  7.3171e+00],
        [-1.6425e+01,  1.7189e+01],
        [ 2.7852e+01,  1.6804e+01],
        [-1.5898e+00, -2.0807e+00],
        [ 1.8019e+00, -2.2562e+00],
        [-9.4953e+00,  1.3208e+01],
        [ 1.5353e+01, -2.6596e+00],
        [ 1.0179e+01, -1.8440e+00],
        [ 6.8183e+00,  1.2477e-01],
        [ 2.8561e-01,  6.9821e+00],
        [ 8.1656e+00, -3.0274e+00],
        [-3.1259e+00,  2.0460e+01],
        [ 9.9919e+00,  7.3221e+00],
        [-4.0866e+00,  1.8919e+01],
        [ 1.7153e+01,  9.0090e+00],
        [ 9.2752e+00,  1.6655e+00],
        [-7.8812e+00,  1.7504e+01],
        [ 4.5637e+00, -4.9581e+00],
        [ 6.7596e+00, -9.7196e+00],
        [-5.2956e+00,  1.5879e+01],
        [ 1.9430e+01, -1.0473e+01],
        [-2.6361e+01,  2.7618e+01],
        [ 1.9426e+01,  1.8754e+00],
        [-3.5875e+00,  8.9684e+00],
        [ 8.0213e+00,  2.0656e+00],
        [-6.3878e+00,  1.7840e+01],
        [ 2.0925e+01, -5.5298e-01],
        [ 3.1346e+01,  8.7156e+00],
        [ 1.3815e+01,  9.4809e+00],
        [-5.2320e+00,  2.8123e+00],
        [ 2.2760e+00, -2.9314e+00],
        [-2.1223e+00,  2.1773e+00],
        [-2.0116e+01,  2.3265e+01],
        [ 2.1676e+00, -1.3805e+00],
        [ 3.8325e+00, -5.1831e+00],
        [ 7.7352e+00, -9.8558e+00],
        [ 7.4409e+00, -2.5656e+00],
        [ 8.0605e+00,  1.4010e+01]], device='cuda:0')
output.data.max(1)[1]:  tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1], device='cuda:0')
tot_sum:  tensor(90.5844, device='cuda:0') 1
The device used will be: 
True
cuda:0
output.detach().clone():  tensor([[ 17.8912,   8.2709],
        [ -3.4103,   1.1362],
        [ -8.1864,  15.1293],
        [ -0.4743,   3.3928],
        [  4.2598,   2.0515],
        [  2.5743,  20.9845],
        [  3.1842,  -3.5788],
        [  2.2268,  -3.8301],
        [ 15.4467,   8.6666],
        [ 38.3396, -20.5057],
        [  4.3670,   2.3068],
        [ -0.1391,   5.5198],
        [  0.9546,  13.8560],
        [-11.6115,  16.7568],
        [ -4.0214,  22.5641],
        [ 21.2183,  -6.8450],
        [ 34.7445, -31.5959],
        [ 10.9707,  -6.5849],
        [ 19.8892,  -4.4289],
        [ 17.7552,  -6.9672],
        [ 11.8968,  -2.2739],
        [ 11.9205,  -6.0948],
        [ 28.8059, -13.7972],
        [  1.5033,  -7.4853],
        [ -5.7477,   1.4931],
        [ 14.3043,   7.9986],
        [ 17.4408,  -2.0220],
        [ 32.0094,  -7.6060],
        [ -0.3659,   7.1875],
        [ 22.0022,  -7.2827],
        [  8.3421,  12.9595],
        [ 23.0332, -13.4770],
        [ 14.8565, -18.4230],
        [ -2.5026,   8.7443],
        [  5.6232,   3.3427],
        [  7.0043, -13.4067],
        [ -1.1148,  -0.9776],
        [ 12.9866, -12.3630],
        [ 38.7307, -19.9565],
        [  4.1782,   9.9488],
        [ -1.3168,  10.0510],
        [ 16.8547,  -8.1519],
        [ 10.5969,   2.3407],
        [ 17.6599,  -5.5457],
        [ 12.2425,  -0.4198],
        [ 10.4035,  14.4074],
        [ 22.6539, -12.5798],
        [ -2.7171,   7.5533],
        [  3.6444,  14.1809],
        [ 25.1334, -13.3382],
        [ 24.0979,  -2.0276],
        [  4.1657,   6.7749],
        [ 41.2695, -21.6295],
        [ 11.7032, -14.1231],
        [ 16.5409,  -7.5520],
        [  7.0229,  -7.4773],
        [ 17.7370,  -2.9051],
        [ 25.0955,  -7.7085],
        [ 20.8324, -15.8104],
        [ 16.8454, -15.4879],
        [-16.9835,  21.8587],
        [ 30.9203, -20.9978],
        [ 15.4387, -15.3114],
        [  4.5632, -15.3931],
        [ -2.5423,   8.4535],
        [ 36.0635,  -8.5983],
        [ -9.8608,  -3.1583],
        [ 28.9368, -25.3386],
        [  7.6053,   9.2350],
        [  0.0688,  -7.2720],
        [ -4.0352,   0.3622],
        [ 19.5288, -12.1770],
        [ 26.3955,   0.8409],
        [ 17.1878, -13.4752],
        [ 25.4897, -10.8869],
        [ 21.9692, -17.0917],
        [ 11.7866,  -6.8808],
        [-12.8743,  11.4311],
        [  5.1738,  -7.0200],
        [ 13.0276, -10.7037],
        [ 18.4662,  26.1904],
        [  4.4415,  15.9305],
        [ 17.7109,  17.7488],
        [ 18.7375,  -8.6507],
        [ 23.6815, -16.2702],
        [ 29.3453,   2.3586],
        [  8.7105,  22.1364],
        [ 23.6877, -25.8084],
        [ 10.7638, -13.7240],
        [ 12.8416,  -3.8844],
        [ -5.5255,   7.1886],
        [ -1.0414,   1.1912],
        [ 27.2797, -19.2675],
        [ 34.7230, -26.3883],
        [ -2.0603,  13.3007],
        [ 18.9140, -13.3453],
        [ -7.0719,   7.7986],
        [  8.2076,  -0.9863],
        [ 15.5668,  -5.6538],
        [ 10.5095,   0.7636],
        [ 10.6468,   4.8940],
        [ 10.4863,   2.5269],
        [ 23.8234,  -9.5640],
        [ -4.1690,   5.5914],
        [ 19.2450, -20.8380],
        [ 18.7282, -13.7174],
        [ -8.2571,   6.9064],
        [-14.2397,  29.2949],
        [ -6.9397,  26.8993],
        [ 19.0081,  -0.4062],
        [ -0.2665,  17.9847],
        [ 22.7679, -16.4761],
        [ -0.6758,  11.6860],
        [ -8.0961,  20.7897],
        [ 26.2292, -14.9288],
        [ -7.8105,  -6.7395],
        [ 18.8596, -18.5898],
        [  0.4080,  13.0649],
        [ 21.6175,   4.0497],
        [  8.1509,  -2.4310],
        [  8.2964,   5.9968],
        [  4.9896,  35.0722],
        [ 18.9657,   9.7955],
        [ 14.1814,  -0.4884],
        [ -1.0867,  18.7482],
        [ 18.6494, -13.2967],
        [  9.5182,   6.4881],
        [  9.2858,  -8.5219],
        [-12.3556,  17.4584],
        [ 17.3959, -16.1396],
        [  9.9414,  -4.3014],
        [  0.7187,   8.2873],
        [ -6.8078,  16.0599],
        [ 28.3171,  -5.0216],
        [ -6.3121,  13.3804],
        [ 31.9444, -21.3611],
        [ -7.2977,   0.0716],
        [  8.9091,  15.7296],
        [ 34.4409, -15.1197],
        [ 24.4980,  -7.1842],
        [ 27.9811, -12.8276],
        [  4.0506,  -4.0630],
        [ 20.1022,  -5.3901],
        [ 35.6439, -24.8262],
        [ 10.0096,  14.6663],
        [ 18.4564,  -8.0201],
        [ 19.5588, -14.9826],
        [ -4.3272,  15.2624],
        [ -0.7830,   1.5701],
        [  3.4460,   2.5119],
        [ 29.4565, -22.4656],
        [  8.9738, -22.0913],
        [ 20.7386,   1.7616],
        [ 19.8119, -20.4673],
        [  3.1495,  -6.5613],
        [  8.6308,  -5.6590],
        [ 33.3075, -29.9424],
        [  6.2530,   4.1871],
        [ 17.4019,  -6.4048],
        [  1.3328,  -2.9545],
        [  0.8848,  -8.7552],
        [ -0.2966,  -6.6430],
        [ 16.6095, -11.4715],
        [ -5.2030,  12.7622],
        [ 14.5811, -16.8055],
        [ -1.7307,  11.2811],
        [ 12.5743, -12.8387],
        [ 17.1642,  -9.7223],
        [ 28.9657,  -9.7903],
        [  3.8996,  13.4341],
        [ 11.8477,  -1.1561],
        [  7.3557,  -3.6673],
        [ 28.2026,  -9.4692],
        [  4.4009,   2.8667],
        [-20.2663,  16.5957],
        [ 16.5866, -14.1473],
        [ 10.2883,  -8.8406],
        [ -1.5535,   8.7321],
        [ 29.9605, -21.8227],
        [ 29.4241, -22.0978],
        [ 30.1277, -11.8186],
        [ 12.9077, -13.2423],
        [ 21.9367,  -6.6074],
        [ 18.1115, -17.7205],
        [ 18.0491, -13.8201],
        [ 23.8046, -16.6970],
        [  8.3764,  -8.3214],
        [ -2.4010,  10.5455],
        [ -2.1124,   6.6305],
        [  2.8907,   6.1464],
        [  4.6471,   4.9124],
        [  8.8565,   2.5957],
        [ 13.2009,   8.8653],
        [-10.8303,  12.4290],
        [  6.7684,   4.2320],
        [ 13.3028,  -5.5978],
        [ 19.9567, -34.3123],
        [ 15.9106,  15.5647],
        [ -5.0868,  11.7936],
        [  5.1887,   6.4913]], device='cuda:0')
output.data.max(1)[1]:  tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1], device='cuda:0')
tot_sum:  tensor(75.2761, device='cuda:0') 2
The device used will be: 
True
cuda:0
output.detach().clone():  tensor([[ -5.2517,  14.2175],
        [  4.1705,   1.5492],
        [ 19.2344, -19.1073],
        [ 20.3564, -17.9173],
        [ 37.1189, -19.2851],
        [-21.1313,  19.6880],
        [ 42.5174, -27.8054],
        [  4.0281,  -4.0407],
        [-13.4390,  20.9981],
        [-28.1898,  43.2449],
        [  2.8474,  20.7112],
        [-12.1935,  21.1506],
        [ 49.0327, -29.3928],
        [-18.9118,  31.6053],
        [ -3.4078,  -4.4627],
        [ 36.3141, -13.0187],
        [ 60.3942, -37.8942],
        [-29.5109,  30.8683],
        [ 16.0081,  -7.0192],
        [-31.8500,  59.3150],
        [ -0.6885,  15.0617],
        [-10.1894,  -4.3608],
        [ 13.6069,  -2.9154],
        [ 22.1849, -36.5407],
        [-28.0883,  33.9993],
        [ 25.8994, -15.9866],
        [ 51.9244, -37.6651],
        [-17.0204,  24.3465],
        [ 37.9668, -20.5800],
        [ 39.6326,  -7.9289],
        [ 20.8273, -22.6735],
        [ -9.8057,  12.7774],
        [-42.7009,  40.5800],
        [-16.4146,  25.1173],
        [-35.8407,  58.1435],
        [ 43.2058, -46.7725],
        [ 41.6926, -17.0539],
        [ 50.1913, -49.8004],
        [-33.1754,  54.4768],
        [  1.9854,   0.9329],
        [ 13.5424,  -3.9863],
        [-21.8295,  14.6262],
        [-31.8245,  39.9689],
        [ 20.8907, -14.0693],
        [-22.7031,  47.3788],
        [ 31.6888, -23.3542],
        [ 19.6636, -18.8766],
        [ 28.1468, -23.0088],
        [ 11.4088,  -1.8493],
        [-25.1670,  42.8851],
        [ 35.2989, -31.0700],
        [ 14.4580,  25.6035],
        [ 12.1458,  12.6649],
        [ 27.4328, -26.8093],
        [-54.7634,  73.0723],
        [-50.2108,  51.6584],
        [-15.1937,  26.8736],
        [ -7.0989,   7.2487],
        [-24.1443,  30.4359],
        [ -6.1447,  19.8375],
        [-35.1729,  33.5694],
        [-10.6008,  30.5602],
        [  2.1870,  10.8261],
        [-24.3732,  21.4129],
        [-29.1729,  34.3182],
        [ -1.2535,  18.9149],
        [ -8.4989,  15.1726],
        [  8.8178,   9.7840],
        [-12.2918,  13.9548],
        [ 38.9036, -39.2384],
        [ 32.2584,  -7.8551],
        [  4.9098,   5.1642],
        [ 40.3305, -40.0609],
        [ 41.4336, -26.2802],
        [-30.5828,  50.9547],
        [ -0.6456,  18.4708],
        [ 10.3530,  16.8833],
        [ -5.3428,  16.6158],
        [-39.3614,  45.3192],
        [-21.1212,  30.1582],
        [-27.1312,  34.1994],
        [-29.6720,  46.5121],
        [ 31.6052, -19.7826],
        [-12.1113,  19.2898],
        [ 40.3333,   4.3231],
        [-25.8150,  36.7501],
        [-44.9687,  60.2979],
        [ 38.0404, -23.0318],
        [-35.4184,  33.3259],
        [ 38.6778, -28.0946],
        [-33.5448,  20.5257],
        [ 24.8677, -17.6002],
        [ 17.6146,  -2.0739],
        [-30.5136,  29.1628],
        [ 14.1703,   7.8958],
        [-33.5462,  40.4889],
        [ 28.7514, -20.7393],
        [ 23.8027, -36.9204],
        [ 24.6218, -19.2693],
        [-35.7457,  43.4506],
        [ 46.5586, -11.0991],
        [  3.9279,  14.8053],
        [ 22.5119,   8.3349],
        [-14.0504,  15.6490],
        [-31.8043,  39.2155],
        [ 27.3916, -29.8116],
        [-29.3085,  25.6378],
        [  1.5843,  24.5777],
        [-13.2400,  26.1775],
        [ -7.1643,   6.7098],
        [ 30.7000, -27.3350],
        [-20.9717,  30.2237],
        [ 31.9324, -32.5879],
        [-19.5542,  30.7944],
        [ 26.9683, -15.1733],
        [-45.5411,  46.9634],
        [-36.8378,  54.1515],
        [ 14.3919,  -9.0112],
        [ 42.2260, -10.7509],
        [-16.2941,  17.0207],
        [-28.4352,  32.9533],
        [-31.7262,  43.0265],
        [ 38.5762, -23.7444],
        [ 11.6391,   2.4877],
        [-36.5386,  36.1439],
        [ 15.3974, -14.6336],
        [ 22.4928, -28.4750],
        [-17.2588,  32.0693],
        [ 40.1795, -22.8459],
        [ 34.1673, -31.0189],
        [-17.6151,  23.9264],
        [ 39.1647, -33.5249],
        [ 47.9450, -39.8074],
        [ 23.7048, -34.5348],
        [-53.6139,  64.6108],
        [ 26.7694, -19.4177],
        [ 26.0357, -13.9216],
        [-17.6357,  39.7058],
        [ 12.6546,  11.4586],
        [  0.1147,  -3.5266],
        [  2.0187,  -3.6118],
        [-21.7115,  26.7222],
        [ 44.2071, -29.6043],
        [-12.5164,  12.2519],
        [-39.2037,  55.2099],
        [-29.8167,  34.6918],
        [ 18.2777,   0.4964],
        [ -9.7706,  18.5047],
        [ 20.9068,  -9.8967],
        [-25.4738,  25.5286],
        [ 41.4475, -25.9873],
        [ 14.5512, -10.8629],
        [-28.5919,  34.4174],
        [ 27.9250, -22.9031],
        [-25.8738,  32.1153],
        [ 23.7412,   1.9259],
        [ -3.2005,  13.2872],
        [-10.4107,  20.8507],
        [ -7.0003,  11.9060],
        [ 41.2115, -37.6135],
        [-34.5929,  39.4290],
        [ 34.0037, -29.6180],
        [-36.1458,  31.8911],
        [ -9.5850,  23.0577],
        [ 34.2151, -18.7964],
        [ 12.1426,  -1.3250],
        [ -4.5698,  11.6773],
        [ 19.2941, -18.0379],
        [ 35.7336, -33.4098],
        [ 37.2555, -27.6327],
        [-22.3313,  31.8816],
        [-29.3006,  44.0200],
        [ 25.0391, -20.4630],
        [-36.5676,  34.3432],
        [-32.9012,  39.6079],
        [ 17.0477, -23.4502],
        [ -2.7609,  10.2027],
        [-18.4752,  16.2010],
        [-28.1609,  24.4878],
        [-16.3097,  27.3100],
        [ 24.0163, -13.5801],
        [ 30.3325, -25.4551],
        [ 36.7800, -33.0675],
        [ 33.9097, -23.9872],
        [ 21.3298,  -2.1545],
        [ -6.0411,   4.4193],
        [ 14.3836,  14.3774],
        [-39.9912,  32.7649],
        [ 22.9077,  -3.5866],
        [-30.9007,  45.1938],
        [-27.2336,  36.2395],
        [-41.4107,  46.5134],
        [ 36.6998, -29.7090],
        [ -4.6935,  16.1456],
        [-46.8681,  57.8086],
        [ 28.9837, -35.9234],
        [-23.1368,  30.3580],
        [ 33.7584, -22.9705],
        [-15.6918,  34.2344],
        [ 43.7352, -38.1666]], device='cuda:0')
output.data.max(1)[1]:  tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0], device='cuda:0')
tot_sum:  tensor(47.9163, device='cuda:0') 3
The device used will be: 
True
cuda:0
output.detach().clone():  tensor([[ 2.0397e+01,  5.7646e+00],
        [-3.5259e+01,  6.2724e+01],
        [-6.7892e-01,  6.9901e+00],
        [ 1.8383e+01, -7.5948e+00],
        [-4.9232e+00,  2.6993e+01],
        [ 1.6292e+01, -7.3261e+00],
        [ 2.2873e+00, -4.5616e+00],
        [-4.8492e+00,  1.0807e+01],
        [-2.1796e+00,  9.1221e+00],
        [ 1.0025e+01,  1.7211e+01],
        [-1.5022e+00,  6.0782e+00],
        [-4.0914e-01,  9.9593e+00],
        [ 2.9073e+00,  7.0151e+00],
        [ 2.1360e+01, -1.4539e+01],
        [-1.2414e+00, -2.1706e+00],
        [ 2.6563e+01, -1.1731e+01],
        [-1.3318e+01,  1.8463e+01],
        [-4.4996e+00,  1.4939e+01],
        [-4.4549e+00,  2.0461e+01],
        [ 7.5346e+00, -7.4076e+00],
        [-2.1570e+01,  2.1305e+01],
        [ 1.2588e+01, -1.9528e+01],
        [ 3.8732e+01, -3.2723e+00],
        [-5.1964e+00,  3.8455e+00],
        [ 1.6377e+01, -9.5798e-01],
        [ 2.5114e+01, -1.4677e+01],
        [ 1.3735e+01, -6.1762e+00],
        [ 7.2485e+00,  1.1927e+01],
        [ 6.4207e+00, -8.0139e+00],
        [ 6.9369e+00, -5.2740e+00],
        [ 2.0717e+00,  7.4876e+00],
        [ 7.7527e+00,  2.3094e+01],
        [ 3.2651e+00,  1.6098e+00],
        [ 1.6002e+01,  2.5406e+00],
        [-2.5515e+01,  3.1807e+01],
        [-2.1612e+00,  1.9001e+01],
        [ 2.0969e+01, -1.0278e+01],
        [ 2.3178e+01, -1.2168e+01],
        [ 6.4014e+00,  7.2914e+00],
        [ 4.3754e+00,  6.4421e+00],
        [ 1.0197e+01,  9.7627e+00],
        [-2.7385e+00, -1.5163e+00],
        [ 1.5902e+01, -1.8322e+01],
        [-7.9951e+00,  2.8367e+01],
        [-8.9323e+00,  2.0172e+01],
        [-1.7514e+01,  3.2843e+01],
        [-8.6979e+00,  4.8759e+01],
        [ 9.0220e+00, -3.6695e+00],
        [ 1.8100e+01, -1.2719e+01],
        [ 7.4288e+00, -7.2792e+00],
        [ 7.1068e+00,  1.7569e+01],
        [ 3.1445e+00, -6.8153e+00],
        [-1.5874e+01,  3.0593e+01],
        [-7.8743e+00,  1.4982e+01],
        [-1.5940e+01,  1.3899e+00],
        [ 1.2595e+01, -9.6764e+00],
        [ 9.8408e+00,  1.9542e+00],
        [ 1.1857e+01, -2.6053e+00],
        [-1.8634e+01,  2.7671e+01],
        [ 1.5493e+01, -1.2336e+00],
        [ 1.3909e+00,  1.8249e+01],
        [ 1.4381e+01,  1.7323e+01],
        [ 5.5675e+00,  7.1259e+00],
        [-5.4382e+00,  9.9561e+00],
        [ 1.6619e+01, -1.2390e+01],
        [ 3.2183e-01,  5.5891e+00],
        [-2.2014e+01,  2.9340e+01],
        [ 1.3113e+01, -1.0789e+01],
        [ 1.0330e+01, -4.7620e+00],
        [ 1.3910e+01,  8.0143e-01],
        [ 9.1909e+00,  1.4368e+01],
        [ 3.3704e+00,  9.5670e+00],
        [ 9.6683e-01, -1.5323e+01],
        [ 5.4545e+00,  6.2254e+00],
        [ 2.1173e+01, -1.3822e+01],
        [-4.2508e+00,  1.4325e+01],
        [-4.2688e+00,  1.5542e+01],
        [ 8.8595e+00,  8.4740e+00],
        [ 1.2139e+01,  3.3208e+00],
        [ 1.7459e+01, -1.1739e+01],
        [ 2.0092e+01, -8.2699e+00],
        [-5.0422e-01,  1.9106e+01],
        [-3.7356e+00,  2.1137e+00],
        [ 8.9467e-02,  2.4024e+01],
        [-1.4588e+01,  2.3290e+01],
        [ 3.1617e+01, -3.0830e+01],
        [ 7.1967e-01, -1.3837e+01],
        [-1.8958e+00, -4.5068e+00],
        [ 1.4801e+01, -6.7420e-01],
        [ 7.0314e+00, -1.3014e+01],
        [ 1.0690e+01, -6.9771e+00],
        [-6.9591e-01, -7.1748e+00],
        [ 4.9963e+01, -2.7463e+01],
        [-2.7443e+00, -4.4590e+00],
        [ 3.9417e+00, -4.3963e+00],
        [-6.4057e+00,  1.7406e+01],
        [-1.9382e+00,  7.7636e+00],
        [-2.2694e+00,  2.7936e+01],
        [-3.4610e+01,  4.5607e+01],
        [ 1.2053e+01, -7.6669e+00],
        [ 3.0032e+01,  1.4428e+00],
        [-3.4464e+00,  1.8501e+01],
        [ 1.5862e+01,  2.8794e+01],
        [-1.9574e+00, -1.6094e+00],
        [-2.2682e+00,  1.1225e+01],
        [ 5.2344e+00,  1.9867e+00],
        [ 2.0783e+01, -2.1118e+01],
        [ 2.3499e+01,  1.1395e+00],
        [-4.0535e+01,  5.8844e+01],
        [-7.9633e+00,  1.4670e+01],
        [ 9.6644e+00, -1.0320e+01],
        [ 2.1949e+01,  1.3464e+00],
        [-8.9673e+00,  3.3371e+01],
        [-9.6894e+00,  2.4063e+01],
        [ 7.9950e+00, -7.3716e+00],
        [-1.0657e+00,  9.2099e+00],
        [ 6.3191e+00, -6.3318e+00],
        [ 1.7692e+01,  1.1069e+01],
        [-6.4698e+00,  9.4414e+00],
        [-1.6546e+01,  1.7903e+01],
        [ 1.4027e+01, -1.3757e+01],
        [-9.4177e+00,  6.4573e+00],
        [ 1.1126e+01, -4.4527e+00],
        [-9.9354e+00,  1.4946e+01],
        [-2.1209e+00,  9.1890e+00],
        [-3.6800e+00,  1.8336e+00],
        [-3.7276e+00,  2.6030e+01],
        [-2.8703e+01,  4.0003e+01],
        [-4.1438e+00,  1.3700e+01],
        [-1.8955e+01,  1.9009e+01],
        [-4.5763e+01,  4.3642e+01],
        [-1.8874e+01,  1.6649e+01],
        [ 1.6019e+00,  6.4101e+00],
        [ 6.5273e+00, -4.9288e+00],
        [ 1.8741e+01, -1.0603e+01],
        [ 1.8830e+01, -4.2271e+00],
        [ 6.7184e+00,  8.8222e+00],
        [-2.1068e+00,  1.6917e+01],
        [-5.9971e+00,  1.6938e+01],
        [ 2.1774e+00,  1.6597e+01],
        [ 9.2214e+00,  8.1653e+00],
        [ 2.0591e+01, -1.8580e+00],
        [ 2.1717e+01, -9.6028e+00],
        [ 7.5918e+00,  2.0309e+00],
        [ 1.4000e+01, -1.0402e+01],
        [ 1.0141e+01, -9.4146e+00],
        [ 1.2034e+01, -1.9476e+00],
        [-1.6000e+01,  1.4396e+01],
        [ 4.2668e+00,  1.5054e+01],
        [ 1.6729e+01, -1.3581e+01],
        [ 1.4109e+01, -4.1959e+00],
        [-1.7374e+01,  2.6108e+01],
        [ 3.8366e+00,  2.4049e+00],
        [ 7.4605e+00, -2.8212e-01],
        [ 1.0381e+01, -6.6931e+00],
        [-1.8974e+00,  6.7333e+00],
        [ 1.4122e+01, -1.3731e+01],
        [-2.5661e+01,  2.7108e+01],
        [ 2.0275e+01, -1.4636e+01],
        [ 1.7459e+01, -6.0070e+00],
        [-1.0879e+00,  1.8806e+01],
        [-2.2278e+00, -1.4453e+00],
        [ 9.5563e+00, -8.3003e+00],
        [-1.4094e+01,  1.6533e+01],
        [ 1.9232e+01, -1.8468e+01],
        [-1.3768e+01,  6.5414e+00],
        [-5.5084e+00,  1.0414e+01],
        [-3.4154e+00,  1.9720e+00],
        [-6.7843e+00,  2.4098e+01],
        [ 6.7990e+00,  6.2140e+00],
        [ 1.4980e+01, -7.7128e+00],
        [-5.0299e+00,  1.0132e+01],
        [ 2.7831e-01, -1.1108e+01],
        [-2.2685e+01,  3.0126e+01],
        [-1.0739e+01,  8.6467e+00],
        [-1.1526e+01,  1.2948e+01],
        [-2.4441e+01,  3.1852e+01],
        [ 1.1823e+01, -6.8015e+00],
        [ 7.6953e+00,  1.3606e+01],
        [-1.0924e+01,  1.8629e+01],
        [ 2.4496e+01,  3.1456e-01],
        [-2.7059e+00, -1.0313e+01],
        [ 1.2474e+01, -7.3359e+00],
        [-6.5042e+00,  1.7439e+01],
        [ 9.8767e+00, -1.5422e+00],
        [-4.9628e+00,  9.8401e+00],
        [ 1.3851e+01, -3.7846e-01],
        [-1.3390e+01,  1.2995e+01],
        [ 6.4981e+00,  1.5568e+01],
        [-5.8498e-01,  3.9499e+00],
        [-6.1418e+00,  2.3976e+01],
        [ 1.7323e+01,  5.0214e-01],
        [ 1.9849e+00,  1.5676e+01],
        [ 2.0323e+01, -9.7401e+00],
        [ 1.9008e+01, -3.8551e+00],
        [-2.5336e+00,  1.1539e+01],
        [ 1.4951e+01,  5.7900e-02],
        [ 1.1920e+01, -4.9297e+00],
        [-8.4107e+00,  2.2704e+01],
        [ 4.7191e+00,  4.2696e+00]], device='cuda:0')
output.data.max(1)[1]:  tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0], device='cuda:0')
tot_sum:  tensor(79.3652, device='cuda:0') 4
max_key : 1
Accuracy of the network on the 1st dataset: 37.500 %
Test loss on the 1st dataset: 0.048
################################## EVALUATION OF TASK 4 ############################################
SEED:  0
block 0, size : 96 16 16
96
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 2.886751345948129
block 1, size : 384 8 8
384
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.8505172717997146
block 2, size : 1536 4 4
1536
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.4252586358998573
block 3, size : 2560 2 2
2560
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.21262931794992865
block 4, size : 2560 1 1
2560
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
range = 0.1647019614671031
NOWWWW
1 1
2
{'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
blocks['b%s' % 5] 2560 2560
range = 0.3423265984407288
train_layer_order: consecutive {'t0': {'blocks': [0, 1, 2, 3, 4], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 50, 'lr': 0.005}, 't1': {'blocks': [5], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}
The device used will be: 
True
cuda:0
PARAMSSSS IN LOAD:  {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
avg_deltas:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
topk_layer:  None
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.0.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
topk_layer inside model after retrieval:  [25, 59, 30, 5, 69, 42, 55, 68, 16, 82, 77, 53, 14, 35, 89]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.1.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
topk_layer inside model after retrieval:  [304, 137, 75, 345, 349, 197, 121, 22, 365, 336, 146, 202, 56, 100, 4, 67, 266, 163, 296, 354, 50, 159, 41, 347, 343, 180, 118, 190, 92, 94, 104, 76, 324, 62, 172, 191, 274, 200, 186, 217, 36, 105, 330, 128, 73, 21, 254, 357, 133, 169, 6, 326, 206, 193, 99, 380, 382, 88, 262]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.2.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
topk_layer inside model after retrieval:  [958, 1278, 761, 1436, 856, 95, 14, 1385, 733, 405, 622, 12, 307, 499, 1227, 1274, 1345, 869, 348, 612, 1452, 176, 1532, 250, 632, 1113, 312, 915, 423, 707, 406, 1508, 1291, 542, 942, 1317, 70, 743, 1442, 75, 544, 311, 574, 904, 961, 616, 959, 35, 801, 937, 490, 670, 1297, 994, 790, 434, 1450, 523, 450, 698, 1232, 936, 850, 636, 93, 164, 243, 22, 114, 911, 1238, 626, 1388, 350, 74, 1282, 1281, 1203, 913, 580, 238, 597, 1159, 774, 77, 889, 1025, 1086, 1486, 347, 868, 461, 783, 373, 905, 675, 639, 1245, 67, 359, 100, 673, 281, 1430, 1206, 705, 1101, 1089, 174, 1429, 662, 458, 810, 835, 1006, 1172, 923, 61, 852, 1185, 462, 667, 599, 41, 754, 395, 360, 855, 290, 728, 309, 25, 439, 1171, 1292, 457, 56, 875, 135, 13, 629, 148, 1424, 1002, 138, 1497, 1125, 1401, 614, 538, 964, 824, 1302, 441, 1294, 809, 1437, 1325, 246, 1100, 1186, 745, 324, 313, 356, 731, 276, 167, 335, 105, 212, 456, 851, 649, 596, 1192, 294, 118, 566, 133, 1248, 326, 584, 436, 1116, 447, 15, 933, 102, 818, 1046, 732, 195, 132, 233, 519, 1001, 1522, 674, 1022, 425, 1448, 688, 891, 1494, 1033, 886, 650, 508, 1307, 66, 808, 563, 1324, 872, 134, 110, 413, 209, 119, 1516, 779, 1266, 846, 1178, 717, 955, 215, 799, 973, 1032]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.3.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
topk_layer inside model after retrieval:  [505, 1755, 1843, 1403, 1783, 1554, 1938, 646, 421, 1447, 792, 564, 2241, 1226, 2460, 472, 1424, 507, 2050, 1958, 1198, 419, 2414, 653, 2305, 195, 1720, 497, 2098, 129, 1789, 2428, 2, 534, 454, 1608, 1451, 96, 299, 398, 953, 2233, 1894, 1266, 1926, 878, 1698, 1448, 689, 304, 1303, 2453, 499, 1887, 305, 1385, 2093, 1548, 1406, 456, 1567, 404, 233, 943, 1753, 533, 935, 1162, 2161, 1271, 2154, 586, 2309, 922, 862, 2300, 658, 1873, 1506, 1394, 390, 1364, 861, 627, 2000, 1697, 772, 2185, 2051, 1992, 2162, 2511, 1831, 1677, 425, 1409, 370, 509, 2112, 2227, 561, 110, 1799, 2223, 2022, 199, 492, 1638, 1236, 1021, 2500, 2362, 1436, 1080, 774, 1304, 347, 555, 2254, 436, 1458, 2164, 2278, 2302, 2062, 1076, 2523, 1599, 1088, 657, 923, 1175, 2056, 2018, 2001, 1524, 708, 1647, 2265, 1612, 1051, 1109, 1839, 1227, 122, 2094, 2469, 674, 1796, 1446, 284, 331, 453, 1845, 2042, 1157, 720, 2057, 1375, 1457, 632, 2007, 2151, 249, 696, 2085, 964, 2245, 1405, 2070, 2114, 1074, 757, 51, 164, 1427, 431, 1811, 1510, 1325, 1138, 867, 402, 2420, 1248, 2181, 1564, 2456, 450, 68, 2270, 2315, 761, 1993, 340, 2106, 1809, 291, 80, 2252, 1578, 1781, 748, 2323, 102, 846, 262, 1337, 146, 1699, 2354, 912, 527, 895, 788, 2328, 746, 178, 638, 1221, 724, 2167, 1128, 1536, 735, 1003, 333, 240, 1985, 1253, 737, 715, 2021, 2240, 2179, 2317, 2342, 2044, 2011, 639, 1890, 1380, 780, 1045, 2004, 448, 1733, 231, 980, 1439, 283, 365, 944, 1646, 60, 2427, 1363, 1642, 1348, 1982, 1696, 605, 2341, 2152, 1998, 1341, 252, 726, 1920, 2160, 2250, 2340, 1968, 1867, 2095, 2352, 2248, 2249, 810, 2081, 1830, 723, 104, 987, 54, 1283, 1824, 1433, 1104, 2479, 1219, 2084, 378, 1340, 225, 1402, 2297, 2502, 1484, 1376, 339, 1220, 928, 626, 687, 2322, 616, 1545, 297, 1435, 758, 1676, 1555, 2514, 677, 742, 583, 694, 1689, 278, 903, 1690, 1013, 35, 927, 1362, 2236, 177, 1742, 1851, 156, 277, 346, 2173, 2221, 312, 52, 937, 69, 1874, 752, 1070, 2087, 1619, 2559, 1346, 1891, 1492, 969, 1002, 77, 1351, 2542, 753, 1267, 1056, 1210, 1187, 2174, 242, 1801, 2259, 46, 1670, 2079, 200, 1865, 13, 2242, 1353, 1928, 1044, 1411, 1614, 699, 1844, 1053, 72, 1645, 1356, 89, 884, 2551, 1287, 1961]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
self.avg_deltas inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
"blocks." +  str(layer_num) + ".layer.weight":  blocks.4.layer.weight
self.avg_deltas_layer inside model after retrieval:  tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')
self.topk_kernels inside model:  {'blocks.0.layer.weight': tensor([-3.0732e-01,  6.2004e-02,  2.4079e-02,  4.2318e-02, -1.0308e+00,
         0.0000e+00,  5.1184e-03, -1.0914e-01, -6.3092e-01,  1.2126e-01,
         6.0158e-02,  1.8052e-01,  1.6743e-02,  2.6058e-03,  3.1853e-01,
        -1.8919e-01,  0.0000e+00, -1.9748e-03, -6.7812e-01, -6.4554e-01,
         2.1121e-01,  3.0998e-01, -4.1433e-02,  1.1619e-02,  2.0795e-01,
         0.0000e+00,  6.1575e-03, -1.1563e-01,  6.2908e-02, -6.0302e-02,
         1.0949e-01,  1.5439e-01, -2.2004e-01,  0.0000e+00,  4.6455e-01,
         0.0000e+00, -5.2896e-01, -4.0274e-02,  3.8203e-01,  4.7018e-02,
         3.7958e-01,  8.1800e-02,  6.7881e-01, -1.1622e-01,  2.0222e-01,
        -8.0046e-02, -1.1306e-01,  1.1826e-01,  1.4270e-03,  1.0000e+00,
        -4.1440e-01,  7.8920e-01,  1.7088e-01,  2.4141e-01, -6.3199e-02,
        -2.3649e-01, -3.1682e-01,  2.4618e-01, -1.2444e-01,  0.0000e+00,
        -1.3195e-01, -4.7311e-02, -3.7195e-01,  4.1627e-02, -1.0868e+00,
        -1.8682e-01, -7.2510e-01, -3.3517e-02,  2.5698e-01,  1.6146e-01,
         1.3267e-01,  2.7747e-01,  1.8132e-02, -7.2455e-01, -1.8216e-02,
        -4.0120e-01,  7.9689e-02, -9.7263e-02, -1.4727e+00,  2.6701e-01,
        -4.3347e-01,  2.7797e-01,  0.0000e+00, -1.2034e-01, -1.0664e-02,
        -2.4671e-01, -1.0875e+00, -1.5006e-01, -1.7191e-02,  0.0000e+00,
         7.6879e-03, -1.0262e-01, -3.0973e-01,  2.3153e-01,  1.0573e-02,
         2.5814e-01], device='cuda:0'), 'blocks.1.layer.weight': tensor([ 2.4910e-02,  4.9589e-02,  5.0859e-04,  1.2097e-02,  3.1600e-01,
        -9.4292e-02,  5.1328e-02, -1.9302e-01, -2.0881e-02,  1.5388e-01,
        -1.2496e-02,  1.2931e-01,  1.1071e-01,  1.1660e-01,  1.3426e-02,
         1.1237e-01, -1.4724e-02,  1.6898e-01, -3.1055e-01, -2.4391e-01,
         5.0592e-02,  1.9117e-02,  2.8652e-01,  1.4917e-01,  3.9151e-02,
         2.1917e-01,  0.0000e+00, -1.9376e-01, -8.2323e-02,  7.5070e-02,
         5.2716e-02, -5.4369e-01, -2.1932e-01, -6.9983e-03, -1.0190e-02,
         0.0000e+00,  1.5315e-01,  9.1252e-02,  3.0170e-03,  9.8799e-04,
         1.2541e-01, -7.7710e-03,  1.6056e-02,  9.1821e-03,  1.3538e-01,
         7.0757e-04, -3.0565e-01,  3.7206e-02, -2.5596e-01, -1.2203e-01,
        -5.1285e-03,  3.0877e-02, -3.4899e-02, -2.6971e-02, -1.4597e-01,
         8.5079e-02,  0.0000e+00, -1.5425e-01,  7.2700e-02, -3.4263e-02,
         1.4924e-02, -6.9339e-01,  3.1548e-02, -3.7928e-01, -2.9458e-01,
        -8.6199e-04, -1.4329e-01, -9.8448e-03,  3.2467e-02, -5.6096e-02,
        -2.7322e-01, -3.0385e-02, -7.9998e-02,  4.5046e-01, -6.6060e-02,
         0.0000e+00,  3.1759e-01, -6.0194e-01, -6.6321e-01,  2.0593e-01,
        -3.0755e-02,  5.0549e-02,  5.1411e-02,  4.8462e-03,  2.5940e-01,
        -5.1629e-04,  1.0782e-02, -1.0359e-01, -7.6829e-03,  7.5223e-03,
        -1.1109e-02,  1.6974e-02, -8.7952e-03,  4.9765e-03, -8.3955e-03,
        -1.1987e-01,  3.1086e-01,  2.1355e-02, -2.7694e-01, -9.5471e-04,
         2.3832e-01,  6.8730e-03,  1.1003e-02, -1.0086e-01,  0.0000e+00,
         1.9720e-01, -2.2484e-02, -9.7629e-02, -6.3950e-03,  1.3677e-01,
        -3.8413e-02,  8.4238e-04, -7.1859e-03, -3.8891e-01,  6.8778e-01,
        -9.7052e-02, -1.6610e-01,  4.1769e-02, -2.7347e-02, -4.8133e-02,
        -3.0100e-01, -3.8762e-01,  6.0738e-02,  1.5124e-01,  6.0637e-02,
        -1.8907e-01, -5.5924e-02, -3.4363e-01,  2.0331e-01,  3.1410e-02,
         4.7463e-03,  1.0640e-01,  5.3147e-02,  1.5037e-02,  1.3077e-01,
         2.1524e-02,  5.2198e-03,  1.3164e-02,  2.0099e-01,  7.9026e-02,
         9.6595e-02,  9.3995e-03,  3.7055e-01, -3.5874e-02,  9.8526e-02,
         2.8852e-02,  1.0000e+00, -1.1081e-01,  7.8389e-02, -5.8612e-03,
        -9.1887e-03, -1.1749e-01,  6.4256e-02,  4.6513e-02,  3.1528e-03,
        -1.6838e-01, -3.3886e-01, -1.0642e+00,  3.3038e-02, -1.0310e+00,
        -3.6412e-02,  2.3831e-02,  1.4697e-02,  0.0000e+00, -6.8486e-02,
         8.1807e-02, -1.7237e-01,  3.5112e-03,  5.3783e-02, -2.7551e-01,
        -2.5572e-02,  2.5913e-02,  1.9343e-01,  1.5006e-01,  8.4163e-02,
        -1.2950e-01,  8.7142e-02, -8.5452e-04,  5.8952e-02,  1.5157e-01,
        -6.7447e-02,  2.1385e-01,  2.6391e-02,  7.0682e-03, -8.7274e-02,
         4.5766e-03,  1.0715e-01,  6.5750e-03,  8.5027e-02, -1.8982e-01,
         2.8085e-01,  0.0000e+00,  1.8163e-02,  5.1818e-02,  2.5496e-02,
         5.7525e-01,  4.8554e-03,  2.7494e-01,  7.4716e-03, -3.3659e-02,
         4.6021e-01, -5.7955e-02,  0.0000e+00,  1.4040e-01, -5.9905e-02,
         3.9373e-02,  2.9868e-01, -2.0237e-03, -6.8345e-03,  2.3085e-01,
        -6.5772e-02,  4.0312e-02,  3.8910e-03, -1.4072e-01,  1.1432e-03,
         2.6089e-02,  5.9313e-02,  2.3267e-01, -2.5802e-01,  9.0022e-02,
         1.0238e-02,  3.2050e-01, -1.3102e-01, -1.0402e-01,  2.0865e-01,
        -1.2459e-01,  1.4418e-01, -2.2894e-01,  3.4705e-02,  1.6073e-01,
        -1.9164e-02, -9.7532e-05,  2.1594e-02,  2.2407e-02,  1.3892e-01,
        -1.3447e-01,  6.8349e-02,  5.5286e-03,  1.1596e-01,  5.0664e-02,
        -6.7710e-02, -4.3202e-02, -7.7252e-04, -3.4488e-01, -8.3400e-03,
        -1.2486e-01,  1.1825e-02, -5.1851e-02,  5.7268e-01, -1.2427e-02,
         2.3513e-01,  5.9099e-03, -5.8898e-02,  1.9073e-02, -3.6878e-01,
         8.1979e-02,  1.2261e-03,  6.0339e-03, -1.9358e-02,  9.5573e-03,
         4.9989e-03, -7.3883e-03,  7.7955e-01,  6.6041e-02, -4.5144e-02,
         5.2484e-02, -1.1475e+00,  6.9778e-02,  2.2525e-03,  9.4430e-04,
        -6.8500e-02,  2.4282e-01,  2.1243e-02,  3.3879e-03,  4.3923e-01,
        -3.7302e-02,  6.3143e-01,  1.4507e-01,  5.6385e-02, -2.1060e-02,
        -2.7273e-02,  2.4752e-01,  6.2206e-02,  2.3957e-03,  3.5520e-02,
         1.3748e-01, -2.5832e-02, -1.6410e-01,  9.5391e-02, -1.8505e-01,
        -4.5665e-02, -3.3623e-01,  2.6092e-02,  1.6869e-02, -2.2671e-02,
        -1.7413e-01,  0.0000e+00, -1.4760e-02,  3.3606e-01,  6.0936e-02,
        -3.2027e-01, -1.6148e-01,  2.7326e-02, -9.6522e-02,  0.0000e+00,
         7.2724e-02,  1.9074e-01,  1.7604e-02,  3.1006e-02, -3.5770e-02,
        -2.5855e-02, -8.4965e-03,  1.1216e-01, -2.9804e-02,  1.3342e-01,
        -2.3612e-03,  2.4429e-02,  0.0000e+00, -8.3404e-03,  8.5032e-04,
        -1.0081e-03,  4.8294e-02,  3.9785e-01, -1.0484e-01,  0.0000e+00,
         2.2480e-01,  1.1226e-01,  1.6279e-01, -1.9647e-01,  2.8488e-01,
        -9.4964e-01,  2.8145e-01, -5.3321e-01,  3.4287e-02,  3.0427e-03,
         5.2719e-02,  0.0000e+00, -1.9075e-02,  1.3901e-01,  5.1143e-02,
         4.0858e-02,  5.3759e-03, -1.4991e-02,  4.3810e-01, -2.4134e-01,
        -4.9540e-02,  3.4447e-02,  7.1591e-01,  7.2405e-02,  0.0000e+00,
         1.9993e-02, -3.9644e-03, -9.0579e-02,  8.9945e-02,  1.1490e-02,
        -5.7800e-01, -1.1726e-02,  2.4544e-01,  5.1768e-01, -5.4292e-02,
         1.2235e-02,  3.2493e-03, -6.1408e-03,  4.4009e-02,  1.2563e-01,
        -4.1637e-01, -2.0629e-02,  1.4630e-01,  2.1412e-02, -2.1262e-02,
         3.7987e-02,  2.3879e-01,  1.6853e-01, -2.4767e-01, -5.8038e-02,
         4.5357e-02, -5.5928e-02,  3.4552e-02,  1.6843e-02,  1.0275e-01,
         1.5417e-01,  9.9985e-02,  0.0000e+00,  1.4695e-02], device='cuda:0'), 'blocks.2.layer.weight': tensor([ 0.0072,  0.0194,  0.1600,  ...,  0.0637, -0.1428,  0.0472],
       device='cuda:0'), 'blocks.3.layer.weight': tensor([-0.1018, -0.0757, -0.0734,  ...,  0.0449, -0.0274,  0.0000],
       device='cuda:0'), 'blocks.4.layer.weight': tensor([ 0.0447, -0.0582,  0.0515,  ..., -0.1348,  0.0055, -0.0296],
       device='cuda:0')}
topk_layer inside model after retrieval:  [1118, 1323, 1657, 557, 888, 1143, 530, 966, 605, 91, 1504, 426, 1167, 2265, 2388, 1233, 1957, 99, 1366, 1970, 2385, 2531, 93, 382, 2393, 1706, 2457, 2559, 674, 2121, 1869, 402, 1250, 770, 6, 1883, 1406, 778, 30, 2335, 172, 315, 1063, 883, 386, 1713, 2218, 70, 2211, 1375, 455, 499, 2060, 2440, 1428, 161, 2415, 757, 703, 654, 139, 2171, 2053, 1066, 1064, 326, 1099, 1471, 15, 1387, 140, 621, 2034, 1373, 607, 534, 1602, 1750, 1090, 1289, 774, 2510, 1999, 2195, 270, 1731, 1273, 1928, 2143, 2250, 1787, 400, 2532, 1556, 1912, 2475, 1513, 1704, 1284, 448, 625, 1676, 1730, 1434, 1607, 3, 612, 1034, 193, 1442, 208, 843, 1241, 1489, 2268, 825, 1384, 1981, 296, 1877, 177, 1298, 1880, 2141, 969, 7, 1477, 1313, 806, 798, 2015, 2337, 1622, 2395, 905, 2286, 1402, 412, 1908, 1084, 2138, 328, 1684, 556, 34, 2410, 2285, 2025, 663, 150, 1252, 0, 549, 2066, 792, 8, 366, 112, 1201, 2070, 704, 2099, 2302, 548, 482, 1982, 1397, 1640, 889, 2191, 2189, 1306, 1326, 2558, 503, 263, 1071, 2436, 1246, 2226, 789, 666, 2081, 593, 718, 544, 568, 1080, 364, 2485, 615, 1618, 1597, 1035, 361, 1700, 233, 24, 2327, 765, 1987, 861, 2021, 628, 1571, 1444, 1580, 2487, 2346, 2452, 2453, 54, 2158, 1452, 2343, 309, 1819, 1135, 1810, 1285, 1892, 2188, 1969, 2326, 1169, 494, 768, 1338, 5, 2228, 182, 1796, 1644, 924, 2432, 2112, 120, 815, 1583, 2207, 2480, 2205, 85, 1012, 1719, 2276, 904, 105, 1723, 1995, 2277, 2172, 2239, 1357, 637, 2085, 1040, 1922, 2512, 1230, 225, 658, 738, 1065, 242, 1468, 1639, 1734, 1210, 689, 788, 108, 1134, 2546, 207, 2413, 1523, 257, 1424, 2516, 671, 1124, 2134, 558, 335, 2224, 332, 1619, 1239, 926, 951, 891, 1445, 239, 493, 1609, 424, 449, 1732, 2311, 1852, 1759, 2254, 340, 1535, 2509, 1634, 447, 1572, 802, 1770, 395, 1030, 1411, 1645, 2212, 562, 311, 1420, 531, 1575, 1996, 502, 259, 202, 670, 2045, 682, 1390, 132, 2518, 565, 2356, 560, 190, 2220, 2149, 1418, 2079, 1355, 1109, 1113, 553, 2540, 2249, 1015, 2538, 564, 705, 214, 2525, 1993, 700, 341, 252, 430, 1937, 1140, 1878, 2011, 783, 1198, 86, 2507, 1275, 603, 1372, 1849, 1221, 2163, 604, 1522, 1339, 1558, 2517, 2535, 1496, 1204, 874, 1641, 1432, 895, 2492, 2271, 1467]
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
The device used will be: 
True
cuda:0
The device used will be: 
True
cuda:0
chosen_head:  {'blocks.5.layer.bias': tensor([ 0.0052, -0.0217], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0350,  0.0053, -0.0105,  ...,  0.0090, -0.0074,  0.0153],
        [-0.0067,  0.0172,  0.0149,  ..., -0.0063,  0.0286, -0.0075]],
       device='cuda:0')}

 Model C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c loaded successfuly with best perf



 ----- Architecture Block BatchNorm2dSK3962(5, 5)1.0reflect, number 0 -----
- BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK963842(3, 3)0.6499999761581421reflect, number 1 -----
- BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.4)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK38415362(3, 3)0.25reflect, number 2 -----
- BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block BatchNorm2dSK153625602(3, 3)0.25reflect, number 3 -----
- BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block BatchNorm2dSK256025602(3, 3)0.25reflect, number 4 -----
- BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 5 -----
- Flatten(start_dim=1, end_dim=-1)
- Dropout(p=0.5, inplace=False)
- Linear(in_features=2560, out_features=2, bias=True)
model.heads:  [{'blocks.5.layer.bias': tensor([ 1.5791e-05, -1.6506e-02], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0120,  0.0150,  0.0043,  ...,  0.0072,  0.0111, -0.0045],
        [ 0.0163,  0.0075,  0.0001,  ..., -0.0045,  0.0101,  0.0122]],
       device='cuda:0')}, {'blocks.5.layer.bias': tensor([ 0.0040, -0.0205], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 1.5765e-02,  1.0660e-02,  4.3432e-03,  ...,  1.0930e-02,
          1.1431e-02, -3.5607e-03],
        [ 1.2527e-02,  1.1907e-02,  8.8815e-05,  ..., -8.2031e-03,
          9.7858e-03,  1.1328e-02]], device='cuda:0')}, {'blocks.5.layer.bias': tensor([ 0.0005, -0.0170], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0349,  0.0030,  0.0162,  ..., -0.0045,  0.0023,  0.0014],
        [-0.0066,  0.0195, -0.0117,  ...,  0.0072,  0.0189,  0.0064]],
       device='cuda:0')}, {'blocks.5.layer.bias': tensor([-0.0005, -0.0160], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0531,  0.0097,  0.0127,  ...,  0.0032,  0.0010,  0.0103],
        [-0.0248,  0.0129, -0.0082,  ..., -0.0005,  0.0202, -0.0025]],
       device='cuda:0')}, {'blocks.5.layer.bias': tensor([ 0.0052, -0.0217], device='cuda:0'), 'blocks.5.layer.weight': tensor([[ 0.0350,  0.0053, -0.0105,  ...,  0.0090, -0.0074,  0.0153],
        [-0.0067,  0.0172,  0.0149,  ..., -0.0063,  0.0286, -0.0075]],
       device='cuda:0')}]
[Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (5): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=2560, out_features=2, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)]
LAYER NAME:  Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (4): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (5): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=2560, out_features=2, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)
LAYER CHILDREN:  [BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=1.4)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
  )
  (layer): Linear(in_features=2560, out_features=2, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)]
subsubl NAME:  Sequential(
  (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=1.4)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(1536, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(2560, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(2560, 2560, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): Flatten(start_dim=1, end_dim=-1)
)
subsubl NAME:  Linear(in_features=2560, out_features=2, bias=True)
subsubl NAME:  Dropout(p=0.5, inplace=False)
CONFIG MODE:  unsupervised
CONFIG MODE:  supervised
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([49, 33, 72, 51, 71, 92, 15, 14, 23,  0, 71, 75, 81, 69, 40, 43, 92, 97,
        70, 53], device='cuda:0')
TARGETS AFTER SUB:  tensor([51, 51, 51, 79, 51, 79, 51, 79, 79, 79, 79, 51, 51, 51, 51, 51, 51, 51,
        79, 51])
[51, 79]
TARGETS AFTER CLEANER:  tensor([0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0])
200
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 200
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               None
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([19, 29,  0, 11,  1, 86, 90, 28, 23, 31, 39, 96, 82, 17, 71, 39,  8, 97,
        80, 71], device='cuda:0')
TARGETS AFTER SUB:  tensor([51, 51, 79, 79, 79, 79, 79, 51, 79, 51, 79, 51, 79, 51, 51, 79, 79, 51,
        79, 79])
[51, 79]
TARGETS AFTER CLEANER:  tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1])
1000
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 1000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               None
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([19, 29,  0, 11,  1, 86, 90, 28, 23, 31, 39, 96, 82, 17, 71, 39,  8, 97,
        80, 71], device='cuda:0')
TARGETS AFTER SUB:  tensor([51, 51, 79, 79, 79, 79, 79, 51, 79, 51, 79, 51, 79, 51, 51, 79, 79, 51,
        79, 79])
[51, 79]
TARGETS AFTER CLEANER:  tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1])
1000
<class 'dataset.FastCIFAR100'>
Dataset FastCIFAR100
    Number of datapoints: 1000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
------------------------
INDICES:  1000
IMAGE SIZE: torch.Size([64, 3, 32, 32])
The device used will be: 
True
cuda:0
output.detach().clone():  tensor([[ 14.2332,  -1.6530],
        [ 14.8147,   1.8774],
        [ -1.7268,   8.2998],
        [ 17.0243,  -3.5551],
        [ 23.6193, -14.8524],
        [  3.1346, -16.2875],
        [ -4.1331,  -4.4281],
        [ 19.6017,   4.8537],
        [  8.3686,   1.6059],
        [  9.7023,  -4.3170],
        [ -3.8838,  -5.0369],
        [  5.9809,   5.6426],
        [ -9.6747,  -1.9794],
        [  8.5065,   4.6153],
        [ 10.0354,  -2.7366],
        [  7.3236,  -6.9320],
        [  0.5879,   5.3109],
        [  1.2738,  -5.9981],
        [ -2.9540,   3.6694],
        [ 10.2984,  -7.1511],
        [ 19.0188,  10.2720],
        [ 11.5241,  17.8762],
        [  2.4501, -13.4524],
        [ 22.9147,   5.3897],
        [  5.0359,  -4.8604],
        [  8.6183, -16.2902],
        [ 23.2355,  -9.8459],
        [ 22.5777,   6.0399],
        [  4.3092,   3.6249],
        [ -4.5064,   1.9077],
        [ -2.3322,   1.0786],
        [  2.1441,   3.6162],
        [ -5.8092,  -6.3422],
        [ 18.8642,   5.0533],
        [ 13.4846,   7.6799],
        [  6.2914,  -4.0736],
        [ -5.1687,   7.8434],
        [ -0.2916,  -4.5095],
        [ -8.8860, -11.2324],
        [ 19.4191,  12.2559],
        [  4.9783,   2.0246],
        [  7.2718,  -2.6212],
        [ -1.7462,   5.0683],
        [ -5.2568,  -7.5647],
        [  0.9516,  17.5125],
        [  0.5979,  13.3975],
        [  9.8035,  -2.1386],
        [  3.0225,   3.0095],
        [ 19.2142,  10.1439],
        [  8.2199,   8.7328],
        [  1.5092,  -3.5525],
        [  5.5907,  -9.5326],
        [  6.3521,   1.1034],
        [  8.0759,  -2.6996],
        [ -2.5744,   0.5357],
        [  0.9134,  -3.8165],
        [  2.1134,   3.6247],
        [ -1.7299,  -4.5795],
        [  4.3287,  -5.5437],
        [ -1.9772,   3.8258],
        [  2.3362,  -0.2085],
        [ -4.7957,  -1.7845],
        [ -1.0365,  -4.4012],
        [ -2.2093,   0.6605],
        [ -0.3739,   6.6614],
        [ 17.9976, -11.9495],
        [ -2.3636,   2.0960],
        [ -3.3120, -14.5686],
        [ 18.4666,   2.5021],
        [ -2.8463,  11.7831],
        [ -2.5142,  -0.4482],
        [ -0.9376,  -9.6583],
        [  8.4568,  -7.6274],
        [  2.6864,   4.8823],
        [  6.0788,  10.1939],
        [ -0.5015,   9.0255],
        [  9.6699,   9.3536],
        [  1.2127,  -7.9238],
        [ -1.6784,   7.3980],
        [ 21.9712,  -5.5871],
        [  4.1801,  -3.1832],
        [ 19.9690,  -6.3670],
        [  1.8363,   3.7158],
        [  1.8326,  -0.7813],
        [ -2.9646,  -2.7724],
        [  1.6792,  25.1820],
        [  5.9879,  -1.7176],
        [  4.0045,  10.4003],
        [  1.9525,  13.0141],
        [  9.8086,   6.1993],
        [ -3.2191,  -8.5113],
        [ 10.6684,  17.9538],
        [  1.5277,  -0.8255],
        [  3.8779,  -1.6699],
        [  2.0056,  13.7969],
        [  0.0717,  -2.7061],
        [ 24.9658,  -1.5403],
        [  0.6484,   0.2000],
        [  0.9082,   2.7484],
        [  5.4406,   6.9568],
        [  3.0390,   6.0911],
        [  4.0677,   1.0435],
        [ 10.8920,  13.1349],
        [  9.0403, -13.7774],
        [ -3.9399,   6.5954],
        [ -1.3276,   1.4798],
        [  4.1514,  11.9274],
        [  4.0466,  -7.7210],
        [  2.9858,   0.7054],
        [ -7.5225,  -6.2548],
        [  3.3738,  -4.4961],
        [  5.7417,  -9.1561],
        [ -2.5696,   2.1274],
        [ -2.8844,   4.7054],
        [ -5.1375,   3.1825],
        [ -1.1135,   2.6963],
        [ -4.4262, -11.8157],
        [ -0.4886,   8.1157],
        [ 10.4026,   9.9036],
        [  6.7794,   3.0554],
        [ -2.5316,   0.5014],
        [  9.6703,  -5.2563],
        [  5.3951,   3.0958],
        [ 12.8050,  -8.2626],
        [  5.0789,   5.2585],
        [  3.0023,   7.6014],
        [ 11.0412,  13.0805],
        [ 13.2398,  -1.3115],
        [ -1.0100,   2.0406],
        [  2.9634,  17.6770],
        [ 20.2481,  -5.4176],
        [  2.5424,   3.4707],
        [  1.5669,   2.3780],
        [ 12.1431,  -0.0962],
        [ 10.6219,   4.0986],
        [  8.2340,   5.9880],
        [  0.4820,   7.6498],
        [  5.5417,  16.6549],
        [ 15.4428,  -3.6973],
        [  8.1773,   2.5262],
        [  7.5120,  -6.5164],
        [-10.8886,  28.8773],
        [ 28.1487,  -4.7863],
        [  6.0118,   8.4394],
        [  3.8014,   7.6944],
        [ 23.7764, -13.1588],
        [  5.5091,  -0.5432],
        [  4.3530,  14.6470],
        [  2.1626,  16.4077],
        [  2.7336,   5.7545],
        [  5.5625,  10.2509],
        [  7.2096,   3.6914],
        [  2.6534,   4.6730],
        [  7.5220,  -1.6810],
        [ 15.0808,  11.9491],
        [  2.6603, -11.7882],
        [ -2.2069,  -4.9335],
        [ -3.8657,   4.1873],
        [ 19.3791,   6.0610],
        [  0.7585,   1.1806],
        [ -5.4208, -11.5923],
        [  3.3636,   4.9354],
        [ 10.9671, -13.6697],
        [  2.2084,  -1.0933],
        [ -5.0867,   7.1857],
        [  4.1877,  10.9886],
        [ 20.5964,  -4.8758],
        [ -2.4100,  16.0536],
        [ 13.5140,   3.4000],
        [ -1.1126,   2.3518],
        [  6.6036,   3.3755],
        [  3.2433,  -3.5805],
        [  1.2236,   5.6170],
        [  9.3972,  25.2819],
        [ 13.7763,   5.0658],
        [ 15.1733,   3.8620],
        [  1.7821,  -5.5518],
        [  5.8375,   3.6499],
        [  9.1899,  -2.2236],
        [  4.9093,  -8.7398],
        [ -3.3612,  10.6393],
        [ -4.5174,   4.4583],
        [  2.8138,   3.3469],
        [  1.8578,   4.0282],
        [-13.2279,  33.6646],
        [ 23.5665,   0.4744],
        [ -2.8306,   8.9329],
        [  3.6761,  -2.4996],
        [ -0.7359,   0.3419],
        [  6.5906,   5.4434],
        [  4.6963, -14.7189],
        [  2.9581,   7.3043],
        [  8.1256,   2.5835],
        [  3.1875,   4.8856],
        [ -2.2976,  -7.1406],
        [  0.0551,   5.2571],
        [ -5.4843,   9.3680],
        [ 15.0863,  -7.1233],
        [  6.3082,   8.5572],
        [ -2.2036,   5.5219]], device='cuda:0')
output.data.max(1)[1]:  tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1], device='cuda:0')
tot_sum:  tensor(83.3619, device='cuda:0') 0
The device used will be: 
True
cuda:0
output.detach().clone():  tensor([[-2.6675e+00,  1.6311e+01],
        [ 8.1852e+00,  2.0432e+01],
        [-7.6898e+00,  4.2752e+00],
        [-1.0735e+01,  2.5600e+01],
        [ 1.0726e+01,  1.7578e+01],
        [-4.4928e+00,  2.8534e+01],
        [-1.3473e+01,  2.7075e+01],
        [-2.9904e+01,  1.6751e+01],
        [-7.3002e+00, -2.7225e+00],
        [-9.0885e+00,  1.4474e+01],
        [-1.5966e+01,  2.1885e+00],
        [-1.5564e+00,  2.2585e+00],
        [-3.8724e+00,  5.4552e+00],
        [ 1.6978e+00,  2.7161e+00],
        [-1.4409e+01,  1.2454e+01],
        [ 1.3646e+01, -4.7097e+00],
        [-4.5107e+00,  7.4083e-01],
        [ 6.3202e+00, -6.1682e+00],
        [-2.1455e+01,  1.4875e+01],
        [-1.3942e+01,  6.8019e+00],
        [ 1.0200e+01,  8.2640e+00],
        [-1.0792e+01,  2.2416e+01],
        [-9.1011e+00,  2.1681e+01],
        [-1.7130e+01,  3.1960e+01],
        [-8.1761e+00,  1.1833e+01],
        [-8.7849e+00, -6.5341e-01],
        [-4.1090e+00,  1.9076e+01],
        [-2.6332e+01,  3.6935e+01],
        [-1.9329e+01,  1.1657e+01],
        [ 4.4800e+00,  1.4090e+01],
        [-5.6382e+00,  1.0289e+01],
        [-5.0921e+00,  7.7667e+00],
        [-1.0875e+00, -9.5594e-01],
        [ 8.2681e-01,  3.0567e+00],
        [-2.7440e+00,  8.5849e+00],
        [ 8.1483e+00,  1.4048e+01],
        [ 1.4920e+01,  1.2110e+01],
        [ 4.3904e+00, -3.6751e+00],
        [-9.6494e+00,  1.8173e+01],
        [-2.6431e+00, -9.0111e+00],
        [ 7.2704e+00,  8.0267e-01],
        [ 1.3296e+00, -3.3412e-01],
        [-1.8017e+00,  7.5211e+00],
        [-2.5816e+00,  4.4025e+00],
        [-3.2131e+00,  1.5260e+01],
        [-1.9701e+01,  6.8794e+00],
        [-3.3274e+00,  1.2886e+00],
        [ 1.4864e+01,  1.0576e+01],
        [ 9.8045e+00,  3.3171e+00],
        [ 9.9853e+00, -3.1449e+00],
        [-2.2654e+00,  2.5628e+01],
        [ 9.1916e+00,  1.1449e+01],
        [ 1.5434e+01, -9.3324e+00],
        [-5.9987e+00,  2.3243e+00],
        [-1.0336e+01,  1.6096e+01],
        [-2.2690e+01,  1.1688e+01],
        [-1.0061e+01,  2.0036e+01],
        [-3.2889e+01,  1.6647e+01],
        [ 1.5837e+01, -8.2684e+00],
        [-1.6315e+00,  7.0076e+00],
        [-9.1957e+00,  4.4713e+00],
        [ 4.3397e+00,  1.9782e+01],
        [-8.3482e+00,  1.7478e+01],
        [-1.6645e+01,  1.4046e+01],
        [ 3.2198e+00, -1.3713e+00],
        [-3.7325e+00,  1.2499e+01],
        [ 9.1332e+00, -1.4870e+01],
        [-1.1026e+01,  1.5569e+01],
        [-1.7792e+01,  2.8409e+01],
        [ 3.5724e+00,  3.4304e+00],
        [ 5.0171e-01,  6.4644e+00],
        [ 1.0848e+01, -1.3752e+01],
        [ 1.3747e+01,  1.3114e+01],
        [ 1.3448e+01,  1.5174e+01],
        [ 2.1601e+00,  2.1265e+01],
        [-6.9540e-01,  6.7434e+00],
        [ 1.5394e+01,  6.1364e-01],
        [ 3.4123e+00, -9.4135e-02],
        [ 1.9578e+01, -5.5500e-01],
        [-1.4416e-01,  1.5947e+01],
        [ 1.0049e+01,  1.9844e+00],
        [-6.8346e+00,  1.2147e+01],
        [ 8.4013e+00,  1.2035e+01],
        [ 4.9674e-01,  7.8022e+00],
        [ 9.4778e+00,  4.5175e+00],
        [ 1.4415e+01,  7.6155e-01],
        [-1.8672e+00,  3.1063e+00],
        [ 8.9243e+00,  7.3483e+00],
        [ 6.3081e+00,  9.9060e-01],
        [-8.8518e+00, -2.7613e-01],
        [-1.0698e+01,  7.9948e+00],
        [ 6.9316e+00, -9.1859e-01],
        [-3.7069e+00,  1.6766e+00],
        [-3.4354e+01,  4.0253e+01],
        [ 2.7612e+01,  1.6789e+00],
        [ 2.2456e+00, -1.3973e+00],
        [ 6.7587e+00, -4.5410e+00],
        [ 1.1622e+01, -9.1253e-01],
        [ 1.7795e+00, -5.7215e+00],
        [-1.0893e+01, -6.1198e+00],
        [ 1.5257e+01,  1.6954e+00],
        [-2.8190e+01,  3.4222e+01],
        [-1.4080e+01,  9.2786e+00],
        [-2.3714e+01,  2.3376e+01],
        [-1.0602e+01,  5.8651e+00],
        [ 9.9280e-01, -4.8234e+00],
        [-1.1198e+01,  1.7359e+01],
        [-6.6917e+00,  1.1657e+01],
        [-9.1045e+00,  2.1033e+01],
        [-7.4267e+00, -4.7249e+00],
        [-1.3870e+01,  2.5616e+01],
        [ 8.2038e+00,  1.2765e+01],
        [-2.5768e+01,  2.5374e+01],
        [-2.3300e+01,  3.1814e+00],
        [-6.0488e+00,  8.1764e+00],
        [-1.6320e+01,  2.9789e+01],
        [-2.7794e+01,  3.0002e+01],
        [-1.8794e+01,  9.8737e+00],
        [ 3.1290e+00,  1.5871e+01],
        [ 9.5846e+00, -8.4082e+00],
        [ 1.2377e+01,  2.0465e-02],
        [-5.2589e+00,  2.1643e+01],
        [ 9.3079e+00,  5.4125e+00],
        [ 9.7216e+00, -9.7808e+00],
        [-8.9150e+00,  1.2237e+01],
        [ 4.7393e+00,  1.1074e+01],
        [ 1.0351e+01, -1.2986e+01],
        [ 4.9948e+00, -4.8194e+00],
        [ 9.0526e+00,  7.0260e+00],
        [ 1.1055e+01, -3.4279e+00],
        [-1.2699e+01,  1.3529e+01],
        [ 1.0707e+01, -2.7728e+00],
        [-1.0078e+01,  3.7685e+00],
        [ 1.2286e+01, -9.6307e+00],
        [ 4.3295e-01, -1.6866e+00],
        [ 1.2385e+01, -2.0474e+00],
        [-1.0607e+01,  1.2706e+01],
        [-5.4034e+00,  5.7249e+00],
        [-4.0235e+00,  1.9744e+01],
        [ 1.3370e+01,  6.9363e+00],
        [-2.1451e+01,  2.1009e+01],
        [ 1.3698e+01, -6.3722e+00],
        [ 8.0987e+00, -4.1539e+00],
        [-2.3015e+00,  1.2281e+01],
        [-1.7883e+01,  1.8275e+01],
        [-9.9312e+00,  1.0928e+01],
        [ 2.8185e+01,  3.4900e+00],
        [ 2.0418e+01,  1.4261e+01],
        [ 4.4637e+00,  1.3525e+01],
        [ 6.3252e-01,  1.3772e+01],
        [ 9.8812e+00,  8.2208e-01],
        [ 1.5987e+01, -9.4146e+00],
        [-2.1843e+00,  3.2993e+00],
        [ 1.5482e+01,  1.2102e+00],
        [ 1.2991e+01,  1.1465e+01],
        [-1.1251e+01,  1.6803e+01],
        [ 1.4998e+01,  1.4360e+01],
        [-5.0309e+00,  1.3163e+01],
        [-6.7259e+00,  1.2464e+01],
        [-8.0246e+00,  1.3910e+01],
        [-5.9526e+00, -5.7779e+00],
        [-2.5178e+01,  2.2215e+01],
        [ 1.5438e+00,  6.9469e+00],
        [ 5.4912e+00, -4.4400e+00],
        [ 4.5062e+00, -1.3590e+00],
        [ 9.1238e+00,  9.9114e+00],
        [ 1.2037e+01, -2.2023e+00],
        [-4.0316e+00, -6.5644e+00],
        [ 5.4330e+00,  1.8484e+01],
        [-6.9736e+00,  1.1244e+01],
        [ 9.4464e-01,  4.1665e+00],
        [-7.7877e+00, -1.0093e+01],
        [ 5.9784e+00, -1.1416e+01],
        [ 1.2006e+01, -5.7191e+00],
        [ 7.6213e+00,  3.4157e-01],
        [-8.4146e+00, -1.4678e-01],
        [-2.4766e+00,  1.3542e+00],
        [ 1.9732e+01, -1.1244e+01],
        [-8.1375e+00,  2.1527e+01],
        [-4.4595e+00,  1.5360e+01],
        [ 6.0870e+00, -5.0566e+00],
        [-8.6224e+00,  2.0118e+01],
        [-4.4873e+00,  2.9384e+00],
        [ 1.2175e+01,  4.7392e+00],
        [-1.1307e+01,  1.8762e+01],
        [ 1.0159e+01,  4.2918e+00],
        [-1.3778e+00,  9.0425e+00],
        [-3.6101e+01,  3.8040e+01],
        [ 1.9744e+01,  9.6557e+00],
        [ 1.4278e+01, -1.0587e+01],
        [ 9.7803e+00, -2.5024e+00],
        [-1.0519e+01,  1.0251e+01],
        [-1.0639e-01, -1.1087e+00],
        [ 5.1818e+00,  1.8845e+01],
        [-7.9268e+00,  1.2155e+00],
        [ 4.7204e+00,  1.4122e+01],
        [-8.2366e+00,  1.8499e+01],
        [ 1.9388e+01, -5.1662e+00],
        [ 2.1045e+01,  1.1947e-01],
        [-9.3308e+00,  1.8818e+01]], device='cuda:0')
output.data.max(1)[1]:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1], device='cuda:0')
tot_sum:  tensor(78.5661, device='cuda:0') 1
The device used will be: 
True
cuda:0
output.detach().clone():  tensor([[ 1.8093e+01, -1.9216e+01],
        [ 1.0865e+01, -1.0163e+01],
        [ 2.5310e+01, -2.0199e+01],
        [-5.7990e+00,  1.4098e+01],
        [ 1.1883e+01,  5.1445e-01],
        [-4.8336e+00, -8.9438e+00],
        [ 8.2117e+00,  2.6890e+00],
        [ 1.1194e+01,  1.2168e+01],
        [ 1.0003e-01, -9.0209e+00],
        [ 9.0135e+00, -1.6873e+00],
        [ 4.6490e+00, -1.2321e+01],
        [-1.4734e+01,  2.9184e+01],
        [ 8.2147e+00, -2.8297e+00],
        [-3.1344e-01,  6.1542e+00],
        [ 1.4666e-01,  8.9832e+00],
        [-8.8553e-02,  5.9871e+00],
        [-9.6197e+00,  2.9083e+00],
        [ 4.2851e+01, -1.1177e+01],
        [ 6.3978e+00, -4.2702e+00],
        [ 1.5538e+01,  7.3435e-01],
        [ 1.4905e+01, -1.5155e+00],
        [-1.3617e+01, -3.3963e+00],
        [ 1.8705e+01, -1.4292e+01],
        [ 3.2619e+01, -4.0022e+00],
        [-8.9839e+00,  1.0099e+01],
        [ 5.4851e+00, -3.2773e+00],
        [-1.0577e+01,  2.4179e+01],
        [ 6.6463e+00, -5.6509e+00],
        [-3.0697e+00,  3.3911e+00],
        [-1.7589e+00,  1.3793e+01],
        [ 2.6461e+00,  4.8093e+00],
        [-9.5873e+00, -4.3553e-01],
        [ 7.7434e+00, -1.4053e+01],
        [-2.7663e+00,  8.8683e+00],
        [ 1.2448e+01, -2.4686e+00],
        [ 5.1004e+00,  2.5265e+00],
        [ 1.6340e+01,  1.0521e+01],
        [ 2.6026e-01,  3.6844e+00],
        [ 6.3909e+00,  8.4394e+00],
        [ 1.8092e+01,  1.0213e+01],
        [-4.4513e+00,  7.7668e-01],
        [ 1.4841e+01, -1.7744e+01],
        [ 6.0137e+00,  2.3277e+01],
        [ 1.2413e+01,  2.7629e+00],
        [ 1.8774e+01, -8.0653e+00],
        [-2.9788e+00,  3.3702e+00],
        [ 6.8431e+00, -9.5460e+00],
        [-7.6609e+00,  1.3213e+01],
        [ 1.0064e+00,  7.4842e+00],
        [-4.3227e-01,  7.2726e+00],
        [ 6.5976e+00,  2.8895e+00],
        [ 1.3688e+01, -5.2006e+00],
        [-7.9673e+00,  1.3999e+01],
        [-1.4299e+01,  1.4032e+01],
        [ 2.4341e+00, -4.4777e+00],
        [ 1.2862e+01, -3.0275e+00],
        [-3.5970e+00, -4.9645e+00],
        [-3.5995e+00, -7.4030e+00],
        [ 2.4340e+01, -1.8327e+01],
        [ 5.5844e+00,  3.0139e-01],
        [ 2.0639e+01,  5.2490e-01],
        [ 1.3524e+00,  1.3513e+01],
        [ 1.1011e+00,  7.5204e-02],
        [-5.3675e-01,  9.4363e-02],
        [-3.4465e+00, -9.3752e+00],
        [ 2.1861e+01, -1.1243e+01],
        [ 2.6520e+01, -3.0945e+00],
        [ 1.8190e+01, -5.6097e+00],
        [ 8.8128e+00, -4.6067e-02],
        [ 5.6112e+00,  5.0921e+00],
        [ 2.2577e+00, -9.3982e+00],
        [ 4.4708e+00,  8.6507e+00],
        [ 6.0175e+00, -2.1341e+00],
        [ 3.4056e+00, -1.4002e+01],
        [ 2.1824e+01, -5.1324e+00],
        [-4.7064e+00,  8.3628e+00],
        [ 1.6301e+00, -3.9111e-01],
        [ 2.0392e+01,  5.0475e+00],
        [-1.1469e+01, -1.8511e-01],
        [ 1.0156e+00,  4.7445e+00],
        [ 9.3930e+00, -6.2459e+00],
        [ 1.5981e+01,  1.3419e+01],
        [-4.9094e+00,  1.2982e+01],
        [ 1.5116e+01, -7.4516e+00],
        [ 1.1316e+01,  8.9903e+00],
        [-1.2457e+01,  1.8504e+01],
        [-4.7811e+00,  8.0992e+00],
        [ 1.0285e+01,  3.9364e+00],
        [ 5.0760e+00, -4.0457e+00],
        [-1.0459e+00, -5.0313e-01],
        [ 4.0838e+00,  3.8500e+00],
        [ 2.3224e+01,  3.8056e+00],
        [-7.4425e+00,  1.0764e+01],
        [ 3.4743e+00,  7.9575e-01],
        [-6.5528e+00,  3.9538e+00],
        [ 5.0662e+00,  1.0943e+00],
        [ 1.7406e+01,  3.5623e+00],
        [ 1.0160e+01, -1.2239e+00],
        [ 7.3422e+00,  4.7044e+00],
        [ 1.7260e+01, -2.1997e+01],
        [ 1.3484e+01,  4.5038e+00],
        [ 3.1398e+01, -1.0962e+01],
        [ 1.8468e+01, -1.8805e+01],
        [-1.5796e+01,  2.4320e+01],
        [ 1.5911e+01,  8.1294e+00],
        [ 1.1246e+01, -1.6983e+01],
        [-6.8241e+00,  1.7086e+01],
        [ 3.5734e+00, -1.5725e+01],
        [ 1.5159e+01, -2.4287e+01],
        [ 6.1257e+00,  4.4706e-01],
        [-1.2274e+01,  1.7993e+01],
        [ 7.6953e+00,  2.6419e+00],
        [ 1.5070e+01, -7.5017e+00],
        [-1.0084e+00,  2.4926e+01],
        [-5.9766e+00,  7.7974e+00],
        [ 1.4238e+01, -1.4297e+01],
        [ 2.1581e+01,  7.0414e+00],
        [ 3.5631e+01, -3.0255e+01],
        [-1.4186e+00,  6.3842e+00],
        [-7.2644e+00,  4.3016e+00],
        [-1.2772e+00, -1.0453e+01],
        [ 1.6339e+01, -1.2648e+01],
        [-8.1089e+00,  8.2608e+00],
        [ 4.0885e+00,  9.9066e+00],
        [-1.4465e+00,  2.2947e+00],
        [ 1.9023e+00,  1.3064e+01],
        [ 3.6081e-01,  1.8209e+01],
        [ 1.6698e+01, -1.5647e+01],
        [ 1.3310e+01,  2.7686e+00],
        [ 1.4616e+01,  2.2978e+00],
        [-4.0541e+00,  1.5982e+01],
        [ 2.9634e+01, -1.4914e+01],
        [ 1.1688e+01,  4.0322e+00],
        [ 1.2301e+01, -5.0228e+00],
        [ 1.6315e+01,  1.3042e+01],
        [-4.7654e+00,  1.6261e+01],
        [ 1.4750e+01, -1.4575e+01],
        [ 1.1928e+01,  5.0245e+00],
        [-1.3705e+01,  2.1004e+01],
        [ 2.5441e+01, -1.7478e+01],
        [-6.1579e+00,  1.1896e+01],
        [ 4.3843e+00,  5.5899e+00],
        [ 4.0774e+00,  1.2306e+01],
        [ 5.6648e+00, -9.0795e+00],
        [-9.3193e+00, -6.9228e+00],
        [ 1.7605e+01, -2.0239e+01],
        [-9.4036e-03,  8.1411e+00],
        [-4.7070e+00,  2.7518e+00],
        [ 2.8898e+01, -1.0056e+01],
        [ 1.8426e+01, -2.3150e+01],
        [ 1.2634e+01,  1.1821e+01],
        [-4.4346e+00,  2.3957e+00],
        [ 1.9106e+01, -8.2904e-02],
        [ 1.0273e+01, -1.1488e+01],
        [ 1.1414e+01,  2.0544e+00],
        [-9.3172e+00,  5.3751e+00],
        [-6.1358e+00,  7.9842e+00],
        [ 5.8978e+00,  2.8781e+01],
        [ 1.0306e+00, -1.0469e+01],
        [ 4.2118e+00,  7.4114e+00],
        [ 1.1311e+01, -6.6602e+00],
        [-1.0116e+01,  1.1113e+01],
        [ 1.7787e+00, -7.2167e+00],
        [ 3.0351e+00,  1.5070e+00],
        [-2.7340e+00, -3.8464e+00],
        [ 7.4158e+00,  4.3295e+00],
        [-1.1399e+01,  1.8402e+01],
        [ 1.8098e+01, -1.5881e+01],
        [-5.5629e+00,  1.7929e+00],
        [ 1.3790e+01, -7.5030e+00],
        [-1.1660e+01,  1.3758e+01],
        [-2.4910e+01,  4.7909e+00],
        [ 1.1661e+01, -1.0831e+01],
        [-1.4153e+01, -3.7284e+00],
        [-5.4562e+00,  2.4491e+01],
        [ 1.3248e+01, -1.7079e+01],
        [ 8.6972e+00,  7.1050e+00],
        [-1.1425e+01, -1.7280e+00],
        [-9.0531e+00,  9.7682e+00],
        [ 5.0662e+00,  1.0747e+01],
        [ 5.7118e+00, -6.9656e+00],
        [ 2.3780e+01, -3.1395e+00],
        [-2.2394e+00,  7.5514e+00],
        [-5.4939e+00,  8.1492e+00],
        [ 1.0223e+01,  3.4203e+00],
        [-3.3800e+00,  2.9858e+00],
        [ 2.1215e+01,  2.8119e+00],
        [ 3.2688e+00,  1.5731e+01],
        [-2.0628e+00,  3.2333e-02],
        [ 1.1189e+01,  4.8186e+00],
        [-2.0016e+00,  1.2605e+01],
        [-1.9817e+00,  3.9205e+00],
        [-1.8293e+00,  8.7952e+00],
        [ 4.2711e+00,  1.7925e+01],
        [-6.2897e+00,  1.4884e+00],
        [-1.1644e+01,  1.4319e+01],
        [-9.9286e+00,  2.4333e+01],
        [ 1.5197e+00,  6.2929e-02],
        [ 2.0367e+01,  3.7544e+00],
        [ 1.4159e+01,  4.3046e+00]], device='cuda:0')
output.data.max(1)[1]:  tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0], device='cuda:0')
tot_sum:  tensor(85.6074, device='cuda:0') 2
The device used will be: 
True
cuda:0
output.detach().clone():  tensor([[ 26.5972, -16.6183],
        [-14.2707,  11.3078],
        [  1.4754,  -0.2992],
        [ 26.8065, -30.6373],
        [ -2.1085,   9.4346],
        [ 21.5480,   3.8917],
        [ 23.2903,  -6.9066],
        [  8.0437,   1.9304],
        [  9.1386,  -4.5966],
        [  1.4636, -13.6154],
        [  8.9976, -22.7752],
        [-10.6954,   9.1463],
        [ -4.8221,  10.8537],
        [  7.3639,  16.5532],
        [ 16.1687, -11.5184],
        [-30.3724,  37.6500],
        [ 21.4204,  -2.3973],
        [  0.7200,   5.1657],
        [  6.2428,  -8.2865],
        [ 14.6030,  -3.9860],
        [  0.0922,   5.8062],
        [-17.6765,  17.6172],
        [  0.0715,   2.0272],
        [-23.8899,  38.2945],
        [-15.0759,   2.2541],
        [ 29.8780, -17.8315],
        [  8.4003,  -8.7378],
        [ 17.7395,  -1.9264],
        [  7.0290, -18.0316],
        [  7.3645,  -1.3167],
        [-18.8455,   7.1911],
        [-11.6167,   7.6745],
        [  9.5806,   4.6410],
        [ -5.9742,   3.0708],
        [ 10.8025,  -8.5949],
        [  4.3671, -10.6767],
        [ 17.6371,  -0.6849],
        [-30.2725,  32.9276],
        [ 10.6402,  -3.1850],
        [-24.2269,  32.5257],
        [-23.5098,  13.4869],
        [  8.4590,   2.2442],
        [ 19.6760,  -0.6763],
        [-10.3149,  10.6362],
        [-13.3344,  -2.9078],
        [  6.5145,  -2.2446],
        [ 26.4570,  -7.6152],
        [  6.5643, -12.0023],
        [ -0.5904,  11.1938],
        [-24.3741,  30.0933],
        [  9.2394,   4.4039],
        [ 10.0133,  -6.3224],
        [-21.7492,  22.1405],
        [-11.5667,  15.5113],
        [  5.6142,  -6.8680],
        [ -5.9646,  21.7668],
        [-30.8949,  49.3586],
        [ 10.6280,  16.4015],
        [ -5.4764,  15.8135],
        [-11.3971,  25.3921],
        [ 14.1144,   9.2477],
        [ 15.8265, -24.9547],
        [ -9.4296,  29.8658],
        [ -9.6406,  10.6709],
        [  0.7928,   5.0479],
        [ -6.3543,   7.2025],
        [ 20.7174, -28.3897],
        [ -6.9792,  13.5519],
        [  6.7490,   1.1848],
        [ 10.4595,  10.1805],
        [  8.4020,  -7.4067],
        [  6.3426,  -9.0456],
        [ 18.7800,   9.8419],
        [-21.2536,  26.9914],
        [  9.8430,  19.5569],
        [ 22.7083, -14.2178],
        [-24.5895,  28.4727],
        [  2.4522,  -7.1896],
        [ 18.5237, -23.2482],
        [-16.1206,  32.1282],
        [ 15.9021,  13.4556],
        [ 17.0502,  -8.2835],
        [  2.9850,   7.7237],
        [-16.3029,   3.1496],
        [ 11.8000,  -8.6530],
        [-22.7637,  20.8084],
        [  0.6432,   4.6687],
        [  8.7855,  -4.3718],
        [  9.3509,  17.5098],
        [ -4.5477,  16.0431],
        [  8.0820,  15.3431],
        [-24.9589,   7.0779],
        [-38.5127,  56.5008],
        [-16.5162,  26.7783],
        [ -1.1818,   8.1476],
        [  2.8082,   2.3028],
        [-33.0471,  36.3689],
        [-21.9610,  19.9219],
        [ 12.9187,   2.0477],
        [-27.4765,  30.1509],
        [  7.2773,   0.7956],
        [ 15.2046, -14.3755],
        [ -3.8994,   7.2173],
        [ -0.2161,  22.4123],
        [  8.8182,  11.4878],
        [-15.2908,  15.0230],
        [  2.0067,  17.0282],
        [ 20.4107, -21.5334],
        [  9.6536,  19.6369],
        [ 18.4716,  -8.6370],
        [-31.5642,  33.4125],
        [ 31.1830, -15.4628],
        [ -2.5849,   1.3696],
        [-15.0127,  12.4137],
        [-10.4378,  17.2780],
        [-27.9666,  36.4903],
        [ 11.7064,  -3.7437],
        [-23.2646,  37.7153],
        [-14.0082,  15.5907],
        [  5.8722,  15.0962],
        [-11.5223,   9.4918],
        [-21.3644,  23.1850],
        [  0.3304,  -8.8920],
        [ 11.6081,  16.6960],
        [ 10.8005,   0.1001],
        [ 15.9707, -14.9197],
        [ 45.1098, -16.4926],
        [  6.5016,  -1.1167],
        [-11.7370,  18.0242],
        [-17.4564,  10.7449],
        [  4.9237,   7.4734],
        [  4.1279,   7.7998],
        [-20.5310,  21.2460],
        [  1.0416,  -1.4841],
        [ -5.7378,  -3.7006],
        [ 13.1970,  -0.0756],
        [  3.3784,  11.7975],
        [-17.3314,  26.8185],
        [  3.9398, -12.8607],
        [  3.6641,   9.8046],
        [  5.7776,  18.2628],
        [ -5.1584,  -0.5789],
        [ 27.6012, -10.9095],
        [-21.6011,  28.6037],
        [ 30.7690, -12.1990],
        [ 15.7285,   5.4357],
        [ 15.4240,  -0.5939],
        [ -7.9023,   4.2276],
        [ -4.3450,  11.9132],
        [ 22.1220,   1.9046],
        [ 10.6777,  -8.5502],
        [-26.5707,   6.4518],
        [ -6.2905, -10.7229],
        [  5.8684,   0.1444],
        [ -1.6216,   7.7235],
        [  5.5655,  -0.5999],
        [ 15.8767,  -8.2121],
        [ 21.5867,  13.0921],
        [  7.3871,  -6.1482],
        [ 40.5301, -27.9502],
        [-13.1468,  18.6985],
        [ 20.7919,  10.8827],
        [-23.0709,  24.1857],
        [ 11.1931,  -5.8171],
        [ 21.2506,  -7.6491],
        [ 16.5898, -20.0045],
        [  5.9703,  -6.3647],
        [ -1.3794,  18.2930],
        [-10.7893,  16.5492],
        [ 34.9307, -21.5415],
        [ 32.5564, -20.8112],
        [  2.9737,   9.0599],
        [ -2.2374,  10.3690],
        [ -2.8656,  -3.7149],
        [ 16.4328,   7.6885],
        [-16.4894,  24.1162],
        [ -5.6431,   7.8606],
        [  1.3303,  -0.6284],
        [ -0.1744,  -6.9662],
        [ 19.6371, -24.4385],
        [  2.0666,  -4.7013],
        [ -0.5191,   4.1754],
        [ -9.3131,  -1.2830],
        [ -1.4737,  16.3387],
        [-15.0475,  11.2773],
        [ -9.6065,  21.2296],
        [ -8.4640,   8.6158],
        [ -3.0564,   3.2316],
        [  8.8718,  -1.5732],
        [  2.0970, -13.8277],
        [ 14.0398,   0.6804],
        [ 10.6093,   5.4690],
        [  3.5053,  20.9498],
        [  1.2989,   0.6398],
        [-27.5051,  35.9928],
        [-16.4011,  17.3975],
        [ -4.7894,  13.7259],
        [  7.5666,   8.7058],
        [-11.3863,  20.5161],
        [  1.6759,   4.4845]], device='cuda:0')
output.data.max(1)[1]:  tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1], device='cuda:0')
tot_sum:  tensor(74.6816, device='cuda:0') 3
The device used will be: 
True
cuda:0
output.detach().clone():  tensor([[ 2.3475e+01, -2.4598e+01],
        [-1.8799e+01,  2.9416e+01],
        [-1.3400e+01,  2.5797e+01],
        [ 7.7155e+00,  8.5568e+00],
        [ 1.3388e+01, -3.9010e+00],
        [ 3.3266e+01, -2.3287e+01],
        [ 1.3692e+01, -1.2990e+01],
        [ 4.0980e+00, -8.8226e+00],
        [-7.0768e+00,  1.2188e+01],
        [-2.6248e+01,  3.0131e+01],
        [ 1.9086e+01, -1.0954e+01],
        [ 2.6152e+01, -1.4407e+01],
        [ 1.9153e+01, -4.2886e+00],
        [-1.3532e+01,  1.3853e+01],
        [-3.3234e+01,  4.6703e+01],
        [ 1.9149e+01, -2.7711e+01],
        [ 2.4221e+01, -1.8380e+01],
        [ 1.6407e+00,  3.3038e+01],
        [ 1.9961e+01, -1.3914e+00],
        [ 1.8857e+01, -1.7681e+01],
        [-4.2291e+00,  1.1232e+01],
        [-3.2548e+01,  3.3544e+01],
        [ 1.2155e+01, -4.0824e+00],
        [ 1.5218e+01, -4.9557e+00],
        [ 2.4455e+01, -1.0854e+01],
        [-5.2883e+00,  2.6536e+00],
        [ 1.9832e+01, -1.5290e+01],
        [-3.9888e+01,  3.6118e+01],
        [ 2.1387e+01,  2.6532e+00],
        [-1.0533e+01,  1.8467e+01],
        [ 3.2714e+01, -2.2110e+01],
        [-1.5528e+01,  1.8846e+01],
        [ 2.0977e+00,  1.9066e+01],
        [ 1.2029e+01, -2.8995e+00],
        [-2.4278e+01,  3.4615e+01],
        [ 2.7775e+01, -1.9008e+01],
        [ 1.5818e+01, -1.3719e+01],
        [-2.8853e+01,  3.6421e+01],
        [-2.3082e+00,  1.6303e+01],
        [ 2.2308e+01, -2.5723e+01],
        [ 4.7707e+00, -1.3899e+01],
        [ 3.9159e+00, -2.8012e+00],
        [-3.2101e+00,  2.9422e+00],
        [-4.4105e+00,  3.3810e+01],
        [ 1.0980e+01, -1.3579e+01],
        [ 1.0645e+01, -5.3332e+00],
        [-6.1366e+00,  9.8274e+00],
        [ 2.3537e+01, -8.5708e+00],
        [ 5.8412e+00,  8.6093e+00],
        [-2.3141e+01,  5.0170e+01],
        [ 3.1149e+00, -1.9357e+01],
        [ 1.2349e+01,  6.6855e+00],
        [ 2.0086e+01, -8.0521e+00],
        [ 1.5455e+01, -1.5850e+01],
        [-2.8576e+01,  2.8516e+01],
        [ 9.2112e+00, -9.1261e-01],
        [-3.5236e+01,  4.2077e+01],
        [ 1.4699e+01,  1.0208e+00],
        [ 1.1636e+00,  1.4844e+01],
        [ 1.6071e+01, -1.6409e+01],
        [ 1.9708e+01, -1.3676e+01],
        [ 1.5881e+00,  4.4596e+00],
        [ 2.1549e+01,  2.3685e+00],
        [ 1.2735e+01, -1.4765e+01],
        [ 2.8844e+01, -1.5201e+01],
        [-3.9092e+00, -8.9127e+00],
        [ 1.8264e+01, -2.4510e+00],
        [ 6.9555e+00, -1.1956e+00],
        [ 4.4774e-01,  5.7125e+00],
        [ 3.7598e+01, -2.1214e+01],
        [ 1.5203e+01, -2.8981e+01],
        [ 9.5343e+00,  1.9756e+01],
        [-2.1936e+00,  1.6598e+01],
        [-2.0999e+01,  3.9462e+01],
        [ 1.1566e+01,  1.5554e+00],
        [ 1.8955e+01, -2.1658e+01],
        [-1.0184e+01,  5.4463e+00],
        [-1.7711e+01,  2.3998e+01],
        [ 2.4034e+01, -2.1907e+01],
        [-3.3783e+01,  3.0820e+01],
        [ 7.2826e+00,  3.4261e+00],
        [-2.2660e+01,  2.9232e+01],
        [ 1.2852e+01, -1.2287e+00],
        [ 1.3291e+01, -2.2212e+01],
        [-5.3735e+00,  2.4700e+00],
        [ 2.7227e+01, -8.3852e+00],
        [-2.0306e+00, -8.5207e-03],
        [-7.1550e+00,  1.4782e+01],
        [-1.6605e+01,  4.4909e+01],
        [ 4.0480e+00,  2.4569e+01],
        [ 1.2438e+01, -2.3035e+01],
        [-6.6632e+00,  1.1313e+01],
        [ 7.3184e+00,  1.1705e+01],
        [-1.7525e+01,  2.8228e+01],
        [ 1.7756e+01, -7.9211e+00],
        [ 2.4238e+01, -2.9040e+01],
        [-1.1646e+01, -5.3673e+00],
        [ 1.3788e+01, -8.8225e+00],
        [ 1.1170e-02, -9.4497e+00],
        [ 1.0414e+01,  2.1261e+01],
        [ 1.9410e+01, -1.7827e+01],
        [-2.6288e+01,  1.4136e+01],
        [ 4.0232e+00, -8.7632e-01],
        [ 1.0598e+01, -1.6908e+01],
        [-1.3090e+01,  1.1541e+01],
        [-1.3267e+01,  2.2203e+01],
        [ 2.1222e+01, -5.8211e-01],
        [-2.0254e+01,  3.7206e+01],
        [ 1.3944e+01, -1.5198e+01],
        [ 2.4621e+01, -4.9980e-01],
        [ 1.3543e+01, -1.4759e+01],
        [-1.4941e+01,  2.2218e+01],
        [-1.1222e+01,  1.2217e+01],
        [ 8.7548e+00, -2.8565e+00],
        [ 4.3938e+00, -4.4925e-01],
        [ 6.9758e+00,  1.2024e+01],
        [ 1.9132e+01, -9.1580e+00],
        [-2.2685e+01,  2.2837e+01],
        [-5.7314e-01, -3.3691e+00],
        [ 1.3107e+01, -7.2213e+00],
        [ 8.3025e+00,  2.0319e+01],
        [-1.6254e+01,  1.7284e+01],
        [ 1.5993e+01, -2.3666e+01],
        [-2.8668e+01,  2.2930e+01],
        [-1.0886e+01,  4.0244e+01],
        [-1.2262e+01,  6.0712e-01],
        [ 1.8562e+01, -1.6355e+01],
        [-1.0100e+01,  1.1920e+01],
        [-3.2095e+01,  2.2072e+01],
        [ 8.9328e+00,  1.5094e+01],
        [-1.1249e+01,  2.5470e+01],
        [ 8.6613e+00, -1.5242e+01],
        [-4.2634e+00,  7.5851e+00],
        [ 9.3376e-01,  2.1262e+01],
        [-3.3613e+01,  3.4328e+01],
        [ 2.4126e+01, -7.4347e+00],
        [-2.9802e+01,  4.7790e+01],
        [ 1.0195e+01, -4.8107e+00],
        [-2.7152e+01,  3.5639e+01],
        [ 2.1501e+01, -6.6706e+00],
        [ 1.4211e+01, -2.5213e+01],
        [ 1.5537e-01,  6.7363e-01],
        [-2.9499e+01,  4.4674e+01],
        [ 3.5894e+01, -2.2505e+01],
        [ 2.6990e+01, -1.4410e+01],
        [-1.0237e+01,  3.1205e+01],
        [-1.0031e+01,  1.7695e+01],
        [ 2.8707e+01, -5.3454e+00],
        [ 1.7952e+01, -1.0654e+01],
        [-2.3089e+01,  1.6377e+01],
        [ 2.8434e+01, -1.6388e+01],
        [-7.0422e+00,  3.3674e+00],
        [ 3.6046e+00,  2.4973e+00],
        [-2.8040e+01,  1.4887e+01],
        [-2.8045e+01,  7.9262e+00],
        [ 2.7199e-01,  5.2796e+00],
        [-3.0718e+01,  3.3373e+01],
        [ 2.4343e+01, -8.5412e+00],
        [ 2.4266e+00,  9.5010e+00],
        [ 1.4191e+00,  5.1953e-01],
        [-7.1137e+00,  1.2833e+01],
        [ 1.0936e+01, -1.8077e+01],
        [-1.6678e+01,  3.3591e+01],
        [ 1.4501e+01, -1.8332e+01],
        [-1.3128e+01,  1.4179e+01],
        [ 4.6819e+00, -4.5068e+00],
        [ 1.7196e+01, -1.5958e+01],
        [-5.9195e+00,  3.1359e+01],
        [ 1.3624e+01, -5.6616e+00],
        [-5.6231e+00,  2.6059e+01],
        [-3.2555e+01,  1.4674e+01],
        [-7.6624e+00,  2.3741e+01],
        [ 1.1014e+01, -1.1457e+01],
        [-3.2172e+01,  3.4390e+01],
        [-2.1737e+00,  2.9034e+01],
        [-5.3846e+00,  1.3908e+01],
        [-9.1648e+00,  2.9471e+01],
        [-1.5334e+00,  7.5461e+00],
        [ 1.6908e+01, -9.4529e+00],
        [-7.4150e+00,  1.0089e+01],
        [ 3.8506e-01,  6.1814e-03],
        [ 2.7329e+01, -3.9043e+00],
        [-8.5741e+00,  3.1359e+00],
        [ 4.1615e+00, -6.1170e+00],
        [ 1.3103e+01, -8.8333e+00],
        [ 2.1029e+01, -1.7373e+01],
        [ 6.5010e+00, -1.1251e+00],
        [ 3.5701e+01, -2.4801e+01],
        [-1.7242e+00,  8.6900e+00],
        [-3.0420e+00,  4.8902e+00],
        [-1.3438e+01,  2.0764e+01],
        [ 1.8848e+01, -1.0357e+01],
        [-6.8555e-01,  1.5336e+00],
        [ 1.3835e+01, -2.3399e+00],
        [ 1.3058e+01, -7.3200e+00],
        [-1.0254e+00,  1.5745e+01],
        [ 5.0570e+00, -6.4338e-01],
        [-9.0408e+00,  3.3496e+01],
        [ 2.1092e+00, -1.3840e+01],
        [ 5.2713e+00, -7.3151e+00]], device='cuda:0')
output.data.max(1)[1]:  tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0], device='cuda:0')
tot_sum:  tensor(72.9793, device='cuda:0') 4
max_key : 2
Accuracy of the network on the 1st dataset: 44.500 %
Test loss on the 1st dataset: 0.045
results:  {'count': 10, 'R0': {'train_loss': 0.014464263804256916, 'train_acc': 84.64999794960022, 'test_loss': 0.005055953748524189, 'test_acc': 82.0, 'convergence': 23.65986442565918, 'R1': 0, 'dataset_sup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [32, 93]}, 'dataset_unsup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 50, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [32, 93]}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3, 4], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 50, 'lr': 0.005}, 't1': {'blocks': [5], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}}, 'model_config': {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 3, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.21262931794992865, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 1536, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b4': {'arch': 'CNN', 'preset': 'softkrotov-c2560-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 4, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.1647019614671031, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 2560, 'kernel_size': 3, 'in_channels': 2560, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b5': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 5, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.3423265984407288, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 2, 'in_channels': 2560, 'old_channels': 2560, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}, 'R1': {'train_loss': 0.0061482023447752, 'train_acc': 94.37000155448914, 'test_loss': 0.007179258391261101, 'test_acc': 88.0, 'convergence': 23.331533432006836, 'R1': 0, 'dataset_sup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [35, 92]}, 'dataset_unsup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 50, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [35, 92]}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3, 4], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 50, 'lr': 0.005}, 't1': {'blocks': [5], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}}, 'R2': {'train_loss': 0.006392252631485462, 'train_acc': 94.67999935150146, 'test_loss': 0.003225994296371937, 'test_acc': 92.5, 'convergence': 23.05847930908203, 'R1': 0, 'dataset_sup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [16, 41]}, 'dataset_unsup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 50, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [16, 41]}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3, 4], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 50, 'lr': 0.005}, 't1': {'blocks': [5], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}}, 'R3': {'train_loss': 0.005003861151635647, 'train_acc': 96.07999920845032, 'test_loss': 0.0020557385869324207, 'test_acc': 96.0, 'convergence': 22.7949275970459, 'R1': 0, 'dataset_sup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [2, 56]}, 'dataset_unsup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 50, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [2, 56]}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3, 4], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 50, 'lr': 0.005}, 't1': {'blocks': [5], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}}, 'R4': {'train_loss': 0.010478164069354534, 'train_acc': 92.35000014305115, 'test_loss': 0.006333355326205492, 'test_acc': 89.0, 'convergence': 22.592145919799805, 'R1': 0, 'dataset_sup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [51, 79]}, 'dataset_unsup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 50, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [51, 79]}, 'train_config': {'t0': {'blocks': [0, 1, 2, 3, 4], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 50, 'lr': 0.005}, 't1': {'blocks': [5], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}}, 'cl_hyper': {'training_mode': 'consecutive', 'cf_sol': True, 'head_sol': True, 'top_k': 0.15, 'topk_lock': False, 'high_lr': 0.15, 'low_lr': 1.0, 't_criteria': 'activations', 'delta_w_interval': 50.0, 'heads_basis_t': 0.9109999999999999, 'classes_per_task': 2, 'n_tasks': 5, 'selected_classes': [[32, 93], [35, 92], [16, 41], [2, 56], [51, 79]], 'evaluated_tasks': [0, 1, 2, 3, 4]}, 'eval_0': {'test_loss': 0.0367557629942894, 'test_acc': 45.5, 'convergence': 23.998586654663086, 'R1': 0, 'dataset_sup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [32, 93]}, 'dataset_unsup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 50, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [32, 93]}}, 'eval_1': {'test_loss': 0.03650667145848274, 'test_acc': 60.0, 'convergence': 23.998586654663086, 'R1': 0, 'dataset_sup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [35, 92]}, 'dataset_unsup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 50, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [35, 92]}}, 'eval_2': {'test_loss': 0.03895516321063042, 'test_acc': 62.5, 'convergence': 23.998586654663086, 'R1': 0, 'dataset_sup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [16, 41]}, 'dataset_unsup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 50, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [16, 41]}}, 'eval_3': {'test_loss': 0.04780279099941254, 'test_acc': 37.5, 'convergence': 23.998586654663086, 'R1': 0, 'dataset_sup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [2, 56]}, 'dataset_unsup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 50, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [2, 56]}}, 'eval_4': {'test_loss': 0.04517805948853493, 'test_acc': 44.5, 'convergence': 23.998586654663086, 'R1': 0, 'dataset_sup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [51, 79]}, 'dataset_unsup': {'name': 'CIFAR100', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 2, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 50, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 2, 'selected_classes': [51, 79]}}}
experiments/EXP_C100_2C_test/TASKS_CL_CIFAR100_d1_5tasks/C100_CLe3d80eb0-eb72-43c2-8b45-5b3c06fba41c.json


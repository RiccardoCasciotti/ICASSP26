--------------- /leonardo/prod/opt/modulefiles/deeplrn/libraries ---------------
cineca-ai/3.0.0  cineca-ai/4.0.0  cineca-ai/4.1.1(default)  
cineca-ai/3.0.1  cineca-ai/4.1.0  cineca-ai/4.3.0           

Key:
(symbolic-version)  
The device used will be: 
True
cuda:0
BLOCKS:  {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3}}, 'b3': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 3, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.25, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10}, 'pool': None, 'activation': None}}
CL:  True
{'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True}
CL:  True
{'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': True}
SEED:  0
block 0, size : 96 16 16
range = 2.886751345948129
block 1, size : 384 8 8
range = 0.8505172717997146
block 2, size : 1536 4 4
range = 0.4252586358998573
range = 0.11048543456039805
The device used will be: 
True
cuda:0
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}

 ----- Architecture Block BatchNorm2dSK3962(5, 5)1.0reflect, number 0 -----
- BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK963842(3, 3)0.6499999761581421reflect, number 1 -----
- BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.4)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK38415362(3, 3)0.25reflect, number 2 -----
- BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 3 -----
- Flatten(start_dim=1, end_dim=-1)
- Dropout(p=0.5, inplace=False)
- Linear(in_features=24576, out_features=4, bias=True)
[Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=24576, out_features=4, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)]
LAYER NAME:  Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=24576, out_features=4, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)
LAYER CHILDREN:  [BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=1.4)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
  )
  (layer): Linear(in_features=24576, out_features=4, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)]
subsubl NAME:  Sequential(
  (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=1.4)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): Flatten(start_dim=1, end_dim=-1)
)
subsubl NAME:  Linear(in_features=24576, out_features=4, bias=True)
subsubl NAME:  Dropout(p=0.5, inplace=False)

 ********** Hebbian Unsupervised learning of blocks [0] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([6, 6, 6, 9, 7, 9, 7, 6, 7, 4, 9, 4, 9, 6, 6, 4, 9, 4, 9, 4])
[4, 6, 7, 9]
TARGETS AFTER CLEANER:  tensor([1, 1, 1, 3, 2, 3, 2, 1, 2, 0, 3, 0, 3, 1, 1, 0, 3, 0, 3, 0])
4000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 4000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([6, 9, 9, 4, 7, 4, 7, 7, 9, 9, 9, 6, 4, 6, 6, 6, 4, 9, 4, 7])
[4, 6, 7, 9]
TARGETS AFTER CLEANER:  tensor([1, 3, 3, 0, 2, 0, 2, 2, 3, 3, 3, 1, 0, 1, 1, 1, 0, 3, 0, 2])
20000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 20000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([6, 9, 9, 4, 7, 4, 7, 7, 9, 9, 9, 6, 4, 6, 6, 6, 4, 9, 4, 7])
[4, 6, 7, 9]
TARGETS AFTER CLEANER:  tensor([1, 3, 3, 0, 2, 0, 2, 2, 3, 3, 3, 1, 0, 1, 1, 1, 0, 3, 0, 2])
20000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 20000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: ToTensor()
------------------------
INDICES:  20000
IMAGE SIZE: torch.Size([10, 3, 32, 32])
SAVING FOLDER FOR UNSUP:  C10_4C_CL
DEPTH:  3
WTA IN delta_weight:  tensor([[[-3.3278e-32, -6.0448e-41, -8.2326e-39,  ..., -6.2286e-31,
          -2.9810e-29, -6.7055e-16],
         [-1.6301e-41, -3.5032e-44, -4.7265e-39,  ..., -2.5346e-30,
          -1.3192e-29, -2.2429e-16],
         [-6.2336e-40, -5.3221e-42, -7.5670e-44,  ..., -1.4056e-26,
          -7.4675e-23, -1.0069e-17],
         ...,
         [-5.9258e-13, -3.4188e-14, -8.5960e-14,  ..., -1.4778e-07,
          -8.2015e-07, -1.0694e-06],
         [-1.3492e-26, -5.6175e-28, -2.9086e-27,  ..., -2.1296e-07,
          -6.9171e-07, -2.5316e-06],
         [-8.4481e-22, -1.1103e-25, -2.9226e-29,  ..., -1.6477e-07,
          -1.7591e-07, -9.8195e-07]],

        [[-8.7356e-21, -8.1183e-29, -5.8198e-31,  ..., -7.1415e-21,
          -2.5420e-28, -1.2765e-20],
         [-6.1478e-30, -2.7540e-31, -4.7305e-32,  ..., -7.2891e-19,
          -2.7903e-25, -5.6251e-16],
         [-1.0411e-28, -1.6488e-28, -5.2024e-32,  ..., -2.8772e-14,
          -4.2597e-18, -6.0117e-17],
         ...,
         [-6.0273e-19, -9.3130e-19, -3.6450e-19,  ..., -1.9646e-12,
          -1.1995e-11, -3.9320e-12],
         [-5.9697e-17, -1.1501e-18, -2.0563e-15,  ..., -6.5833e-13,
          -6.1490e-12, -2.9062e-12],
         [-2.6741e-14, -6.0721e-15, -2.7358e-16,  ..., -1.9447e-12,
          -8.3229e-12, -8.9669e-12]],

        [[-2.1770e-23, -4.5546e-26, -1.0333e-28,  ..., -1.0591e-17,
          -1.0853e-25, -4.7896e-20],
         [-1.1032e-29, -1.7822e-30, -2.7466e-33,  ..., -2.6284e-18,
          -6.0912e-26, -7.6574e-20],
         [-5.7612e-29, -1.4000e-28, -7.8033e-36,  ..., -3.4892e-15,
          -1.1475e-18, -6.1463e-21],
         ...,
         [-3.3292e-15, -3.2619e-16, -4.7737e-17,  ..., -2.5306e-11,
          -1.1639e-10, -4.5791e-11],
         [-1.9650e-22, -7.0851e-26, -1.4909e-25,  ..., -3.9071e-11,
          -5.7636e-11, -5.5965e-11],
         [-7.1615e-24, -8.2706e-24, -1.0160e-26,  ..., -8.0505e-11,
          -5.7812e-11, -8.6745e-11]],

        ...,

        [[-6.4395e-15, -3.5986e-19, -1.4903e-10,  ..., -2.7444e-17,
          -2.8776e-15, -4.4508e-02],
         [-7.9605e-21, -2.0261e-21, -8.0743e-17,  ..., -5.3681e-18,
          -3.5776e-14, -1.6103e-01],
         [-1.0543e-16, -9.1781e-21, -2.9461e-20,  ..., -3.2901e-15,
          -2.5230e-07,  9.2062e-01],
         ...,
         [-8.0985e-13, -7.1389e-14, -2.7033e-13,  ..., -2.5572e-20,
          -5.7089e-19, -1.7625e-19],
         [-8.7041e-15, -3.4331e-17, -1.8000e-16,  ..., -3.9293e-19,
          -1.0745e-17, -1.9320e-17],
         [-3.6213e-04, -6.3189e-08, -1.7933e-07,  ..., -4.1391e-19,
          -2.9749e-18, -1.1715e-17]],

        [[-2.6589e-27, -8.9252e-33, -1.3727e-38,  ..., -1.3369e-23,
          -1.5740e-34, -6.0699e-31],
         [-7.3430e-33, -1.5270e-37, -1.1715e-42,  ..., -3.5062e-22,
          -9.9097e-32, -3.8547e-27],
         [-2.1015e-32, -2.7540e-35, -2.7746e-43,  ..., -2.5058e-18,
          -2.0181e-24, -2.1815e-26],
         ...,
         [-1.7910e-15, -5.3342e-18, -4.0525e-19,  ..., -5.5237e-10,
          -2.6290e-09, -7.3586e-10],
         [-9.3639e-23, -7.3997e-25, -3.6787e-24,  ..., -4.0691e-09,
          -3.7770e-09, -5.2909e-09],
         [-1.0831e-17, -3.0950e-19, -1.6507e-22,  ..., -8.6127e-10,
          -4.7410e-10, -7.5735e-10]],

        [[-9.6588e-24, -1.5573e-24, -5.8727e-25,  ..., -6.9545e-19,
          -5.3896e-20, -4.6818e-14],
         [-3.4298e-27, -6.9891e-28, -2.4694e-26,  ..., -3.1171e-18,
          -1.6868e-17, -2.8306e-11],
         [-4.1405e-27, -7.7634e-26, -2.1748e-26,  ..., -2.5581e-15,
          -1.7706e-10, -1.4797e-10],
         ...,
         [-9.8915e-13, -1.4466e-11, -1.0392e-13,  ..., -2.3304e-12,
          -1.2398e-11, -1.9693e-12],
         [-1.5858e-14, -4.6212e-15, -5.7176e-12,  ..., -6.7823e-12,
          -4.2343e-11, -2.7847e-11],
         [-3.2085e-08, -1.6821e-07, -1.1820e-10,  ..., -7.5965e-12,
          -6.9968e-12, -1.0316e-11]]], device='cuda:0')
LAYER_NUM:  0
FINAL SUM LENNNN  96
FINAL_SUM:  [43, 29, 77, 26, 91, 67, 35, 84, 87, 46]
acts len:  1
acts keys:  ['conv0']
acts:  {'conv0': [43, 29, 77, 26, 91, 67, 35, 84, 87, 46, 25, 4, 55, 34, 5, 11, 95, 15, 75, 37, 53, 49, 92, 64, 45, 56, 30, 94, 8, 72]}
final_sum len:  96
delta_weights INFO: 
NUM OF TRACKED COV LAYERS:  1
NUM OF TRACKED WEIGHTS CHANGES PER LAYER:  20
avg_deltas INFO:  <class 'dict'>
avg_deltas keys:  ['blocks.0.layer.weight']
avg_deltas size:  1
num of averages for 0 layer:  torch.Size([96])
################################################
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 1.41e-01	time: 00:00:34	Acc_train 0.00	Acc_test 0.00	convergence: 2.40e+01	R1: 0	Info MB:0.000e+00/SB:0.000e+00/MW:1.720e-02/SW:5.378e-01/MR:4.409e+00/SR:1.515e+00/MeD:1.129e+00/MaD:5.785e+00/MW:0.594/MAW:0.406
|       0 |      1 |       2 |       3 |       4 |       5 |       6 |       7 |       8 |       9 |         10 |      11 |      12 |      13 |      14 |      15 |         16 |      17 |      18 |      19 |      20 |      21 |      22 |      23 |     24 |         25 |      26 |     27 |       28 |      29 |
|---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+------------+---------+---------+---------+---------+---------+------------+---------+---------+---------+---------+---------+---------+---------+--------+------------+---------+--------+----------+---------|
|   0.123 |   0.12 |   0.134 |   0.124 |   0.149 |   0.128 |   0.138 |   0.164 |   0.138 |   0.174 |   0.000937 |   0.152 |   0.159 |   0.106 |   0.124 |   0.157 |   0.000371 |   0.157 |   0.162 |   0.169 |   0.178 |   0.167 |   0.134 |   0.202 |   0.18 |   0.000356 |   0.172 |   0.14 |   0.0929 |   0.188 |
|   3.37  |   3.24 |   3.82  |   3.41  |   4.48  |   3.56  |   3.97  |   5.2   |   3.99  |   5.73  |   1        |   4.59  |   4.97  |   2.76  |   3.4   |   4.84  |   1        |   4.85  |   5.09  |   5.45  |   5.96  |   5.34  |   3.83  |   7.36  |   6.05 |   1        |   5.62  |   4.08 |   2.35   |   6.5   |
|   0.59  |   0.49 |   0.57  |   0.62  |   0.61  |   0.69  |   0.52  |   0.53  |   0.64  |   0.53  |   2.79     |   0.55  |   0.55  |   0.5   |   0.57  |   0.45  |  14.83     |   0.61  |   0.57  |   0.64  |   0.64  |   0.65  |   0.54  |   0.6   |   0.56 |  22.38     |   0.46  |   0.94 |   0.57   |   0.62  |
| nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan        | nan     | nan     | nan     | nan     | nan     | nan        | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan        | nan     | nan    | nan      | nan     |
| nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan        | nan     | nan     | nan     | nan     | nan     | nan        | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan        | nan     | nan    | nan      | nan     |
| nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan        | nan     | nan     | nan     | nan     | nan     | nan        | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan        | nan     | nan    | nan      | nan     |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_4C_CL/models
Current stored value:  [43, 29, 77, 26, 91] tensor([-0.2398,  0.0577,  0.0204,  0.0545, -0.4734], device='cuda:0')

 ********** Hebbian Unsupervised learning of blocks [1] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([6, 6, 6, 9, 7, 9, 7, 6, 7, 4, 9, 4, 9, 6, 6, 4, 9, 4, 9, 4])
[4, 6, 7, 9]
TARGETS AFTER CLEANER:  tensor([1, 1, 1, 3, 2, 3, 2, 1, 2, 0, 3, 0, 3, 1, 1, 0, 3, 0, 3, 0])
4000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 4000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([6, 9, 9, 4, 7, 4, 7, 7, 9, 9, 9, 6, 4, 6, 6, 6, 4, 9, 4, 7])
[4, 6, 7, 9]
TARGETS AFTER CLEANER:  tensor([1, 3, 3, 0, 2, 0, 2, 2, 3, 3, 3, 1, 0, 1, 1, 1, 0, 3, 0, 2])
20000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 20000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([6, 9, 9, 4, 7, 4, 7, 7, 9, 9, 9, 6, 4, 6, 6, 6, 4, 9, 4, 7])
[4, 6, 7, 9]
TARGETS AFTER CLEANER:  tensor([1, 3, 3, 0, 2, 0, 2, 2, 3, 3, 3, 1, 0, 1, 1, 1, 0, 3, 0, 2])
20000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 20000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: ToTensor()
------------------------
INDICES:  20000
IMAGE SIZE: torch.Size([10, 3, 32, 32])
SAVING FOLDER FOR UNSUP:  C10_4C_CL
DEPTH:  3
ACTIVATIONS SHAPE:  tensor([[-145.9966, -113.8699, -165.5342, -228.2547, -234.5464, -295.5631,
         -250.8976, -193.0984, -209.2579, -207.2762, -228.2223, -188.8922,
         -219.6779, -256.3231, -261.3726, -210.4164],
        [ -53.5790,  -75.4274,  -61.4417, -141.7955, -193.7119, -252.6827,
         -214.9247, -192.9111, -237.6683, -259.5793, -248.6241, -199.2166,
         -193.3143, -193.0906, -164.1706, -163.2955],
        [ -55.0907,  -23.4675,   -7.7980,  -99.3861, -184.6363, -229.6335,
         -218.9770, -215.0327, -214.1996, -219.3141, -228.4987, -171.4789,
         -214.4598, -231.1911, -256.4923, -212.2795],
        [ -13.7751,   46.3013,   49.2542,  -35.9582, -142.0500, -178.0197,
         -161.3885, -211.7457, -243.7361, -271.4955, -158.3707, -108.5497,
         -191.0553, -203.1387, -280.6052, -238.4308],
        [   3.1653,   33.1714,   83.6556,   38.0244,  -47.2137,  -99.0945,
         -155.8555, -166.1474, -240.9534, -209.9149, -128.7819, -151.5375,
         -122.6774, -153.3870, -180.4822, -203.8335],
        [  -2.0512,   -2.1805,   90.4400,  107.8004,  -29.9219, -123.1009,
         -185.6316, -277.9161, -233.7271, -186.4530, -121.9667,  -83.3204,
          -25.2210, -130.6392, -154.1566, -223.3132],
        [  28.7672,   30.6910,   85.3765,   19.5017,  -17.0193, -112.8707,
         -183.7753, -212.7211, -154.5413, -200.2037, -113.4602,  -34.4912,
          -56.8274, -146.1040, -156.0877, -200.4556],
        [  71.1150,   17.7216,  127.1232,   51.3027,   31.3452,   21.9154,
         -105.9128, -151.5307, -175.5131, -159.0834, -160.9251,  -40.2778,
         -106.6823,  -69.4032, -116.2095, -133.8062],
        [  65.0776,   47.7433,  144.6084,   78.4444,   62.9667,  112.3810,
         -139.0135, -147.5229, -193.9832, -127.7160, -109.1877,  -75.0842,
         -149.5621, -123.6876, -145.3575, -147.7576],
        [   4.7706,   -0.6816,  111.9906,  165.2125,  118.6449,  112.0870,
          -82.9669, -163.0054, -264.5347, -205.7367, -134.4832, -120.4336,
         -179.7702, -235.9221, -204.2485, -166.7773],
        [ -47.1424,  -16.5946,   44.4689,   66.0037,   17.5393,   18.1759,
          -38.3406,  -97.9650, -150.8389, -122.4702,  -75.4484,  -80.1646,
         -113.3589, -132.9715, -134.1121, -153.1500],
        [ -64.5877,  -11.0162,   11.1506,  -77.7862,  -74.8443,  -86.2408,
          -39.9417,  -85.7117,  -59.5852, -120.4717, -107.4163,  -88.1054,
         -134.4874, -123.7359, -126.5355,  -78.4400],
        [ -88.6746,  -12.6270,  -66.8931, -108.9501, -107.9423,  -70.2275,
          -62.4328, -118.3716, -131.8690, -131.5023,  -71.5584,  -55.9037,
         -171.9902, -191.4044, -175.0978, -115.6539],
        [ -72.9655,  -68.5071,  -90.3566, -127.9104, -201.6492, -133.1001,
          -57.1805, -171.2333, -211.0240, -147.9106,  -95.5263, -110.9893,
         -128.5361, -198.0786, -242.7667, -127.7476],
        [ -33.2066,   -6.4930,  -65.1192, -122.4144, -155.7165, -121.4853,
          -61.3396, -125.3162, -206.3380, -135.4962, -137.1728, -119.0094,
         -109.0385, -142.5723, -184.0527,  -94.2781],
        [-162.4653, -136.1021, -179.9093, -192.6743, -227.8340, -175.1751,
         -137.5378, -209.7898, -259.8975, -216.1261, -195.5671, -157.7593,
         -182.1839, -240.0961, -269.3720, -163.9337]], device='cuda:0')
LAYER_NUM:  1
FINAL SUM LENNNN  384
FINAL_SUM:  [204, 78, 174, 243, 49, 77, 116, 259, 300, 63]
acts len:  2
acts keys:  ['conv0', 'conv1']
acts:  {'conv0': [43, 29, 77, 26, 91, 67, 35, 84, 87, 46, 25, 4, 55, 34, 5, 11, 95, 15, 75, 37, 53, 49, 92, 64, 45, 56, 30, 94, 8, 72], 'conv1': [204, 78, 174, 243, 49, 77, 116, 259, 300, 63, 254, 61, 269, 291, 332, 359, 99, 260, 46, 112, 157, 64, 19, 98, 225, 158, 210, 127, 270, 8, 227, 330, 147, 71, 32, 245, 356, 59, 362, 53, 66, 110, 352, 184, 266, 340, 235, 125, 366, 365, 294, 39, 191, 349, 7, 247, 18, 115, 338, 156, 303, 324, 180, 159, 279, 47, 280, 164, 72, 218, 69, 369, 107, 242, 296, 113, 26, 355, 151, 119, 31, 92, 173, 199, 201, 27, 120, 213, 104, 345, 152, 382, 264, 346, 103, 335, 52, 251, 360, 301, 121, 222, 249, 351, 286, 160, 376, 255, 348, 374, 54, 215, 161, 87, 171, 282]}
final_sum len:  384
delta_weights INFO: 
NUM OF TRACKED COV LAYERS:  1
NUM OF TRACKED WEIGHTS CHANGES PER LAYER:  20
avg_deltas INFO:  <class 'dict'>
avg_deltas keys:  ['blocks.0.layer.weight', 'blocks.1.layer.weight']
avg_deltas size:  2
num of averages for 1 layer:  torch.Size([96])
################################################
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 1.41e-01	time: 00:01:06	Acc_train 0.00	Acc_test 0.00	convergence: 2.40e+01	R1: 0	Info MB:0.000e+00/SB:0.000e+00/MW:5.251e-03/SW:1.834e-01/MR:5.230e+00/SR:1.325e+00/MeD:1.048e+00/MaD:4.229e+00/MW:0.559/MAW:0.441
|         0 |        1 |        2 |         3 |         4 |        5 |         6 |         7 |        8 |        9 |       10 |      11 |       12 |       13 |        14 |      15 |       16 |        17 |        18 |     19 |       20 |       21 |       22 |       23 |        24 |       25 |        26 |       27 |       28 |        29 |
|-----------+----------+----------+-----------+-----------+----------+-----------+-----------+----------+----------+----------+---------+----------+----------+-----------+---------+----------+-----------+-----------+--------+----------+----------+----------+----------+-----------+----------+-----------+----------+----------+-----------|
|   0.00974 |   0.0114 |   0.0103 |   0.00818 |   0.00735 |   0.0109 |   0.00937 |   0.00827 |   0.0114 |   0.0109 |   0.0101 |   0.012 |   0.0103 |   0.0127 |   0.00825 |   0.011 |   0.0103 |   0.00994 |   0.00651 |   0.01 |   0.0116 |   0.0107 |   0.0116 |   0.0113 |   0.00702 |   0.0119 |   0.00844 |   0.0104 |   0.0111 |   0.00892 |
|   4.8     |   6.23   |   5.23   |   3.68    |   3.16    |   5.78   |   4.51    |   3.74    |   6.19   |   5.76   |   5.09   |   6.79  |   5.27   |   7.41   |   3.72    |   5.88  |   5.22   |   4.95    |   2.7     |   5    |   6.42   |   5.57   |   6.4    |   6.12   |   2.97    |   6.66   |   3.85    |   5.3    |   5.91   |   4.19    |
|   0.23    |   0.19   |   0.18   |   0.23    |   0.58    |   0.23   |   0.22    |   0.25    |   0.18   |   0.21   |   0.23   |   0.28  |   0.23   |   0.23   |   0.18    |   0.22  |   0.21   |   0.33    |   0.2     |   0.23 |   0.22   |   0.3    |   0.38   |   0.31   |   0.46    |   0.33   |   0.24    |   0.22   |   0.22   |   0.27    |
| nan       | nan      | nan      | nan       | nan       | nan      | nan       | nan       | nan      | nan      | nan      | nan     | nan      | nan      | nan       | nan     | nan      | nan       | nan       | nan    | nan      | nan      | nan      | nan      | nan       | nan      | nan       | nan      | nan      | nan       |
| nan       | nan      | nan      | nan       | nan       | nan      | nan       | nan       | nan      | nan      | nan      | nan     | nan      | nan      | nan       | nan     | nan      | nan       | nan       | nan    | nan      | nan      | nan      | nan      | nan       | nan      | nan       | nan      | nan      | nan       |
| nan       | nan      | nan      | nan       | nan       | nan      | nan       | nan       | nan      | nan      | nan      | nan     | nan      | nan      | nan       | nan     | nan      | nan       | nan       | nan    | nan      | nan      | nan      | nan      | nan       | nan      | nan       | nan      | nan      | nan       |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_4C_CL/models
Current stored value:  [43, 29, 77, 26, 91] tensor([-0.2398,  0.0577,  0.0204,  0.0545, -0.4734], device='cuda:0')

 ********** Hebbian Unsupervised learning of blocks [2] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([6, 6, 6, 9, 7, 9, 7, 6, 7, 4, 9, 4, 9, 6, 6, 4, 9, 4, 9, 4])
[4, 6, 7, 9]
TARGETS AFTER CLEANER:  tensor([1, 1, 1, 3, 2, 3, 2, 1, 2, 0, 3, 0, 3, 1, 1, 0, 3, 0, 3, 0])
4000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 4000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([6, 9, 9, 4, 7, 4, 7, 7, 9, 9, 9, 6, 4, 6, 6, 6, 4, 9, 4, 7])
[4, 6, 7, 9]
TARGETS AFTER CLEANER:  tensor([1, 3, 3, 0, 2, 0, 2, 2, 3, 3, 3, 1, 0, 1, 1, 1, 0, 3, 0, 2])
20000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 20000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([6, 9, 9, 4, 7, 4, 7, 7, 9, 9, 9, 6, 4, 6, 6, 6, 4, 9, 4, 7])
[4, 6, 7, 9]
TARGETS AFTER CLEANER:  tensor([1, 3, 3, 0, 2, 0, 2, 2, 3, 3, 3, 1, 0, 1, 1, 1, 0, 3, 0, 2])
20000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 20000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: ToTensor()
------------------------
INDICES:  20000
IMAGE SIZE: torch.Size([10, 3, 32, 32])
SAVING FOLDER FOR UNSUP:  C10_4C_CL
DEPTH:  3
ACTIVATIONS SHAPE:  tensor([[-16.2339,  -9.4439, -15.6185, -27.1715, -24.7925, -28.7864, -12.6234,
          -6.6498, -19.5774, -25.2573, -28.8091, -22.5078, -33.4009, -46.7816,
         -54.3491, -45.6766],
        [  5.9373,   2.3473,   9.2894,  -8.3062, -15.4040, -16.8538,  -4.5282,
          -5.4938, -21.8343, -32.7358, -32.4223, -22.2484, -25.6408, -33.2557,
         -33.8666, -37.0267],
        [ 17.0273,  22.5564,  26.5272,   9.4618,  -3.9122,  -0.3021,   8.0887,
           2.9404,  -6.4911, -16.0789, -23.0487, -12.4937, -22.0759, -31.9739,
         -45.3291, -41.5105],
        [ 28.6898,  38.9360,  39.6889,  23.1259,   9.6094,  11.0340,  20.9837,
           7.2501,  -6.9085, -22.0182,  -3.4690,   5.2699, -11.9298, -17.5119,
         -40.5201, -36.7426],
        [ 29.7300,  32.0999,  43.1376,  32.9468,  18.5789,  15.3779,  12.5793,
          10.6560,  -6.1841,  -2.3744,   9.1108,   4.1519,   9.3260,   0.6769,
         -11.7073, -19.5226],
        [ 21.9330,  19.9546,  37.1156,  34.8296,   9.4312,  -0.9193,  -4.2556,
         -12.9939,  -2.8669,   3.3382,  13.1160,  23.4017,  36.3508,  10.6428,
           2.6277, -16.0885],
        [ 25.6309,  21.9488,  31.9749,  10.2095,   5.3340,  -1.4753,  -3.5000,
           1.7646,  14.1939,   2.7327,  15.9149,  33.2528,  33.3052,  12.0983,
           9.7490,  -7.6522],
        [ 30.5422,  15.1518,  32.5735,  15.2794,  17.5402,  29.4112,  17.5453,
          14.2528,  13.1336,  15.4673,  13.1712,  36.8180,  24.3706,  30.9050,
          20.1601,   9.5420],
        [ 29.7701,  18.8185,  39.2010,  24.3108,  29.9186,  50.3713,   8.2565,
          10.8156,   5.5237,  17.9603,  22.0772,  30.8898,  16.9308,  23.5766,
          11.3901,   5.1509],
        [ 15.3675,   8.5999,  36.3274,  48.4511,  46.1300,  51.4972,  17.8045,
           3.7622, -13.4770,   0.8381,  18.1363,  26.2331,  13.6717,  -2.7928,
          -7.1468,  -9.3936],
        [  2.4419,   7.3294,  27.3068,  34.8761,  28.8746,  35.7889,  29.7434,
          17.5963,   7.7265,  14.6493,  29.0226,  33.7924,  25.1994,  11.7930,
           2.4749, -10.9220],
        [ -4.8360,   6.9849,  18.7976,   4.5417,   9.4078,  17.7258,  32.1677,
          23.5194,  26.9431,  13.4205,  15.4193,  22.0817,  10.1516,   7.9926,
           0.5428,   5.0110],
        [-10.9165,   5.5718,  -1.0136,  -7.2487,  -0.1647,  19.7467,  27.6594,
          16.2498,   9.0137,   6.2130,  14.7333,  18.5257, -10.2204, -13.3338,
          -8.7833,  -1.9507],
        [ -7.3254,  -8.6967,  -7.6625, -13.8164, -18.9328,   5.4481,  26.4997,
           1.6579, -11.6791,  -5.5433,   5.8556,   1.7759,  -9.1171, -21.6992,
         -26.9801,  -8.1587],
        [ -3.1227,   0.8380,  -6.5041, -17.4910, -18.2307,   2.3822,  20.7239,
           4.0045, -20.2218, -12.6843, -14.8005, -12.4300, -12.8434, -20.7842,
         -26.3311,  -9.8583],
        [-31.9777, -26.1072, -29.3966, -31.9731, -34.2736, -12.1145,   3.0314,
         -16.5833, -33.2155, -31.4433, -30.4243, -24.6788, -29.1430, -38.8461,
         -44.2787, -25.1488]], device='cuda:0')
LAYER_NUM:  2
FINAL SUM LENNNN  1536
FINAL_SUM:  [660, 268, 1082, 1195, 1030, 205, 75, 760, 350, 269]
acts len:  3
acts keys:  ['conv0', 'conv1', 'conv2']
acts:  {'conv0': [43, 29, 77, 26, 91, 67, 35, 84, 87, 46, 25, 4, 55, 34, 5, 11, 95, 15, 75, 37, 53, 49, 92, 64, 45, 56, 30, 94, 8, 72], 'conv1': [204, 78, 174, 243, 49, 77, 116, 259, 300, 63, 254, 61, 269, 291, 332, 359, 99, 260, 46, 112, 157, 64, 19, 98, 225, 158, 210, 127, 270, 8, 227, 330, 147, 71, 32, 245, 356, 59, 362, 53, 66, 110, 352, 184, 266, 340, 235, 125, 366, 365, 294, 39, 191, 349, 7, 247, 18, 115, 338, 156, 303, 324, 180, 159, 279, 47, 280, 164, 72, 218, 69, 369, 107, 242, 296, 113, 26, 355, 151, 119, 31, 92, 173, 199, 201, 27, 120, 213, 104, 345, 152, 382, 264, 346, 103, 335, 52, 251, 360, 301, 121, 222, 249, 351, 286, 160, 376, 255, 348, 374, 54, 215, 161, 87, 171, 282], 'conv2': [660, 268, 1082, 1195, 1030, 205, 75, 760, 350, 269, 1483, 914, 1218, 191, 919, 1490, 1141, 1184, 81, 554, 1425, 1471, 1005, 1458, 941, 415, 1251, 576, 294, 1510, 97, 1261, 1113, 1154, 1008, 1174, 666, 168, 553, 417, 272, 1149, 459, 27, 1094, 964, 1409, 1068, 1284, 1157, 985, 1354, 987, 693, 726, 1500, 1305, 1383, 108, 1400, 617, 503, 1050, 973, 1389, 315, 1039, 1393, 811, 1430, 1532, 737, 303, 1211, 517, 627, 451, 7, 998, 548, 68, 537, 128, 1053, 674, 1464, 301, 1474, 1382, 743, 786, 1404, 870, 501, 765, 607, 193, 1505, 1270, 634, 1349, 171, 196, 1310, 882, 138, 540, 1230, 1192, 112, 325, 377, 252, 862, 545, 151, 286, 814, 133, 1001, 436, 1213, 525, 1076, 730, 822, 296, 689, 1200, 812, 696, 261, 582, 796, 856, 640, 1359, 61, 1135, 453, 1412, 1323, 1016, 1222, 1086, 1205, 868, 817, 1065, 637, 837, 55, 557, 772, 694, 887, 1411, 1343, 408, 1121, 1189, 1239, 903, 1357, 456, 320, 710, 1434, 394, 564, 490, 927, 401, 746, 323, 588, 1078, 648, 99, 1024, 1371, 482, 567, 1160, 1501, 496, 1066, 506, 388, 71, 1085, 847, 638, 427, 600, 335, 431, 293, 866, 22, 924, 1021, 511, 1313, 1307, 1013, 1124, 1148, 167, 289, 1197, 1194, 1378, 650, 844, 959, 1319, 926, 556, 468, 832, 810, 212, 175, 184, 263, 1352, 454, 1370, 1106, 676, 1060, 463, 997, 915, 524, 519, 579, 422, 492, 939, 479, 94, 843, 609, 77, 994, 1506, 69, 996, 1327, 245, 763, 727, 1482, 539, 1273, 929, 188, 931, 896, 392, 1155, 533, 1167, 47, 117, 1387, 1493, 1132, 88, 1530, 1044, 777, 1446, 838, 626, 466, 601, 683, 574, 63, 381, 217, 13, 233, 995, 877, 884, 1299, 444, 226, 179, 1312, 278, 197, 284, 1028, 897, 347, 119, 864, 54, 1019, 910, 1438, 749, 599, 369, 592, 752, 704, 595, 1045, 593, 48, 654, 1461, 430, 1000, 1265, 1456, 888, 127, 736, 1507, 137, 1221, 1416, 79, 804, 257, 1073, 1225, 1153, 1111, 467, 945, 956, 1435, 238, 863, 1351, 1035, 410, 1479, 264, 1137, 779, 873, 705, 154, 1097, 1441, 1186, 715, 163, 484, 1185, 971, 671, 605, 830, 766, 488, 1236, 1534, 1484, 1201, 1052, 267, 34, 283, 583, 1253, 682, 4, 1054, 950, 979, 1462, 337, 699, 89, 701, 1196, 107, 1040, 1276, 505, 1413, 194, 49, 1324, 1041, 1051, 858, 840, 470, 287, 1231, 334, 986, 1376, 1057, 536, 407, 157, 149, 1495, 1451, 1144, 15, 1173, 679, 177, 295, 1146, 1372, 498, 142, 1342, 1136, 759, 8, 426, 775, 603, 497, 111, 629, 332, 1341, 806, 827, 229, 41, 1007, 352, 354, 713, 1303, 1133, 318, 275, 741, 967, 684, 747, 1156, 1491, 662, 348, 242, 1395, 824, 944, 442, 1263, 1470, 1096, 867]}
final_sum len:  1536
delta_weights INFO: 
NUM OF TRACKED COV LAYERS:  1
NUM OF TRACKED WEIGHTS CHANGES PER LAYER:  20
avg_deltas INFO:  <class 'dict'>
avg_deltas keys:  ['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight']
avg_deltas size:  3
num of averages for 2 layer:  torch.Size([96])
################################################
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 4.21e-02	time: 00:01:39	Acc_train 0.00	Acc_test 0.00	convergence: 1.78e+01	R1: 0	Info MB:0.000e+00/SB:0.000e+00/MW:3.654e-03/SW:3.225e-01/MR:1.880e+01/SR:2.459e+00/MeD:1.942e+00/MaD:1.779e+01/MW:0.440/MAW:0.560
|        0 |        1 |        2 |        3 |        4 |       5 |        6 |        7 |       8 |        9 |       10 |       11 |       12 |       13 |       14 |       15 |      16 |       17 |       18 |       19 |       20 |       21 |       22 |       23 |       24 |       25 |       26 |       27 |       28 |       29 |
|----------+----------+----------+----------+----------+---------+----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------|
|   0.0452 |   0.0434 |   0.0441 |   0.0419 |   0.0422 |   0.042 |   0.0468 |   0.0358 |   0.041 |   0.0456 |   0.0403 |   0.0402 |   0.0443 |   0.0352 |   0.0412 |   0.0422 |   0.046 |   0.0434 |   0.0447 |   0.0429 |   0.0427 |   0.0395 |   0.0406 |   0.0433 |   0.0415 |   0.0376 |   0.0423 |   0.0426 |   0.0409 |   0.0444 |
|  21.41   |  19.82   |  20.48   |  18.58   |  18.79   |  18.65  |  22.92   |  13.8    |  17.83  |  21.75   |  17.28   |  17.13   |  20.62   |  13.39   |  18      |  18.78   |  22.14  |  19.81   |  21.02   |  19.43   |  19.23   |  16.62   |  17.48   |  19.74   |  18.26   |  15.16   |  18.92   |  19.13   |  17.75   |  20.69   |
|   0.04   |   0.03   |   0.06   |   0.08   |   0.03   |   0.06  |   0.07   |   0.04   |   0.07  |   0.05   |   0.09   |   0.08   |   0.16   |   0.06   |   0.18   |   0.04   |   0.04  |   0.09   |   0.06   |   0.03   |   0.07   |   0.07   |   0.12   |   0.05   |   0.05   |   0.05   |   0.06   |   0.07   |   0.05   |   0.05   |
| nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_4C_CL/models
Current stored value:  [43, 29, 77, 26, 91] tensor([-0.2398,  0.0577,  0.0204,  0.0545, -0.4734], device='cuda:0')

 ********** Supervised learning of blocks [3] **********
SAVING FOLDER FOR SUP:  C10_4C_CL
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([6, 6, 6, 9, 7, 9, 7, 6, 7, 4, 9, 4, 9, 6, 6, 4, 9, 4, 9, 4])
[4, 6, 7, 9]
TARGETS AFTER CLEANER:  tensor([1, 1, 1, 3, 2, 3, 2, 1, 2, 0, 3, 0, 3, 1, 1, 0, 3, 0, 3, 0])
4000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 4000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([6, 9, 9, 4, 7, 4, 7, 7, 9, 9, 9, 6, 4, 6, 6, 6, 4, 9, 4, 7])
[4, 6, 7, 9]
TARGETS AFTER CLEANER:  tensor([1, 3, 3, 0, 2, 0, 2, 2, 3, 3, 3, 1, 0, 1, 1, 1, 0, 3, 0, 2])
20000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 20000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([6, 9, 9, 4, 7, 4, 7, 7, 9, 9, 9, 6, 4, 6, 6, 6, 4, 9, 4, 7])
[4, 6, 7, 9]
TARGETS AFTER CLEANER:  tensor([1, 3, 3, 0, 2, 0, 2, 2, 3, 3, 3, 1, 0, 1, 1, 1, 0, 3, 0, 2])
20000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 20000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: ToTensor()
------------------------
INDICES:  20000
IMAGE SIZE: torch.Size([64, 3, 32, 32])
Epoch: [1/50]	lr: 1.00e-03	time: 00:02:07	Loss_train 0.15777	Acc_train 78.23	/	Loss_test 0.00389	Acc_test 87.93
BLOCKS IN SUP:  [3]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_4C_CL/models
Current stored value:  [43, 29, 77, 26, 91] tensor([-0.2398,  0.0577,  0.0204,  0.0545, -0.4734], device='cuda:0')
Epoch: [10/50]	lr: 1.00e-03	time: 00:02:16	Loss_train 0.10921	Acc_train 88.34	/	Loss_test 0.00622	Acc_test 91.70
BLOCKS IN SUP:  [3]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_4C_CL/models
Current stored value:  [43, 29, 77, 26, 91] tensor([-0.2398,  0.0577,  0.0204,  0.0545, -0.4734], device='cuda:0')
Epoch: [20/50]	lr: 2.50e-04	time: 00:02:25	Loss_train 0.07273	Acc_train 92.73	/	Loss_test 0.00566	Acc_test 92.43
BLOCKS IN SUP:  [3]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_4C_CL/models
Current stored value:  [43, 29, 77, 26, 91] tensor([-0.2398,  0.0577,  0.0204,  0.0545, -0.4734], device='cuda:0')
Epoch: [30/50]	lr: 1.25e-04	time: 00:02:34	Loss_train 0.04973	Acc_train 94.14	/	Loss_test 0.00535	Acc_test 92.78
BLOCKS IN SUP:  [3]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_4C_CL/models
Current stored value:  [43, 29, 77, 26, 91] tensor([-0.2398,  0.0577,  0.0204,  0.0545, -0.4734], device='cuda:0')
Epoch: [40/50]	lr: 3.13e-05	time: 00:02:44	Loss_train 0.03921	Acc_train 94.87	/	Loss_test 0.00519	Acc_test 92.82
BLOCKS IN SUP:  [3]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_4C_CL/models
Current stored value:  [43, 29, 77, 26, 91] tensor([-0.2398,  0.0577,  0.0204,  0.0545, -0.4734], device='cuda:0')
Epoch: [50/50]	lr: 7.81e-06	time: 00:02:53	Loss_train 0.03653	Acc_train 95.05	/	Loss_test 0.00511	Acc_test 92.90
BLOCKS IN SUP:  [3]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_4C_CL/models
Current stored value:  [43, 29, 77, 26, 91] tensor([-0.2398,  0.0577,  0.0204,  0.0545, -0.4734], device='cuda:0')
RESULT:  {'train_loss': 0.03653339296579361, 'train_acc': 95.05400061607361, 'test_loss': 0.005112210754305124, 'test_acc': 92.9000015258789, 'convergence': 17.80170440673828, 'R1': 0, 'dataset_sup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 4, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 4, 'selected_classes': [4, 6, 7, 9]}, 'dataset_unsup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 4, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 4, 'selected_classes': [4, 6, 7, 9]}, 'train_config': {'t0': {'blocks': [0], 'mode': 'unsupervised', 'lr': 0.08, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't2': {'blocks': [1], 'mode': 'unsupervised', 'lr': 0.005, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't4': {'blocks': [2], 'mode': 'unsupervised', 'lr': 0.01, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't6': {'blocks': [3], 'mode': 'supervised', 'lr': 0.001, 'nb_epoch': 50, 'batch_size': 64, 'print_freq': 10}}}
IN R1:  {'R1': {'train_loss': 0.03653339296579361, 'train_acc': 95.05400061607361, 'test_loss': 0.005112210754305124, 'test_acc': 92.9000015258789, 'convergence': 17.80170440673828, 'R1': 0, 'dataset_sup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 4, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 4, 'selected_classes': [4, 6, 7, 9]}, 'dataset_unsup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 4, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 4, 'selected_classes': [4, 6, 7, 9]}, 'train_config': {'t0': {'blocks': [0], 'mode': 'unsupervised', 'lr': 0.08, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't2': {'blocks': [1], 'mode': 'unsupervised', 'lr': 0.005, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't4': {'blocks': [2], 'mode': 'unsupervised', 'lr': 0.01, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't6': {'blocks': [3], 'mode': 'supervised', 'lr': 0.001, 'nb_epoch': 50, 'batch_size': 64, 'print_freq': 10}}}}
SEED:  0
block 0, size : 96 16 16
range = 2.886751345948129
block 1, size : 384 8 8
range = 0.8505172717997146
block 2, size : 1536 4 4
range = 0.4252586358998573
range = 0.11048543456039805
The device used will be: 
True
cuda:0
PARAMSSSS IN LOAD:  {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 3, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.11048543456039805, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 4, 'in_channels': 24576, 'old_channels': 1536, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
Previously stored value:  [43, 29, 77, 26, 91] tensor([-0.2398,  0.0577,  0.0204,  0.0545, -0.4734], device='cuda:0')

 Model C10_4C_CL loaded successfuly with best perf



 ----- Architecture Block BatchNorm2dSK3962(5, 5)1.0reflect, number 0 -----
- BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK963842(3, 3)0.6499999761581421reflect, number 1 -----
- BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.4)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK38415362(3, 3)0.25reflect, number 2 -----
- BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 3 -----
- Flatten(start_dim=1, end_dim=-1)
- Dropout(p=0.5, inplace=False)
- Linear(in_features=24576, out_features=4, bias=True)
[Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=24576, out_features=4, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)]
LAYER NAME:  Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=24576, out_features=4, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)
LAYER CHILDREN:  [BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=1.4)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
  )
  (layer): Linear(in_features=24576, out_features=4, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)]
subsubl NAME:  Sequential(
  (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=1.4)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): Flatten(start_dim=1, end_dim=-1)
)
subsubl NAME:  Linear(in_features=24576, out_features=4, bias=True)
subsubl NAME:  Dropout(p=0.5, inplace=False)

 ********** Hebbian Unsupervised learning of blocks [0] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([8, 8, 0, 1, 1, 0, 8, 8, 0, 2, 0, 2, 1, 0, 8, 0, 8, 8, 2, 1])
[0, 1, 2, 8]
TARGETS AFTER CLEANER:  tensor([3, 3, 0, 1, 1, 0, 3, 3, 0, 2, 0, 2, 1, 0, 3, 0, 3, 3, 2, 1])
4000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 4000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([1, 1, 2, 8, 2, 2, 2, 0, 0, 1, 0, 2, 2, 1, 1, 1, 2, 2, 0, 2])
[0, 1, 2, 8]
TARGETS AFTER CLEANER:  tensor([1, 1, 2, 3, 2, 2, 2, 0, 0, 1, 0, 2, 2, 1, 1, 1, 2, 2, 0, 2])
20000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 20000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([1, 1, 2, 8, 2, 2, 2, 0, 0, 1, 0, 2, 2, 1, 1, 1, 2, 2, 0, 2])
[0, 1, 2, 8]
TARGETS AFTER CLEANER:  tensor([1, 1, 2, 3, 2, 2, 2, 0, 0, 1, 0, 2, 2, 1, 1, 1, 2, 2, 0, 2])
20000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 20000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: ToTensor()
------------------------
INDICES:  20000
IMAGE SIZE: torch.Size([10, 3, 32, 32])
SAVING FOLDER FOR UNSUP:  C10_4C_CL
DEPTH:  3
ACTIVATIONS SHAPE:  tensor([[-2122.5381, -2112.7266, -1939.7430, -2229.9915, -2166.9150, -2203.0918,
         -2337.4565, -2167.1948, -2057.3481, -2253.6091, -2308.5823, -2326.1626,
         -2479.8179, -2719.5044, -2906.5493, -2515.2786],
        [-1592.1914, -1623.1573, -1365.7332, -1775.6228, -1862.0417, -1913.4617,
         -1970.9767, -1841.9200, -1856.0747, -1958.7683, -2046.4148, -2135.4287,
         -2334.2371, -2595.3794, -2682.2407, -2084.5862],
        [-1066.1268, -1007.2018,  -748.0360, -1108.6760, -1060.1328, -1008.1513,
         -1072.3406,  -933.0483,  -984.0961, -1025.2152, -1140.3572, -1355.9819,
         -1641.1296, -1976.5969, -2203.4170, -1696.6689],
        [ -509.9810,  -550.2380,  -180.6301,  -327.2779,  -233.7046,   -95.9259,
           -13.5852,   -54.9563,  -215.9958,  -129.0559,  -235.5076,  -516.0461,
          -905.1897, -1326.8796, -1714.6677, -1240.6643],
        [   74.6601,   -91.1971,   461.0698,   447.3956,   627.1044,   700.4404,
           945.4410,   946.8441,   818.7646,   798.0981,   742.9163,   382.0464,
           -66.3689,  -616.4064, -1153.2754,  -748.9078],
        [  507.6540,   325.8156,   910.5847,   930.1823,  1296.1989,  1494.3013,
          1654.3882,  1610.8580,  1472.3054,  1404.0344,  1349.8757,  1031.8271,
           638.7659,  -114.3654,  -473.3932,  -264.9559],
        [ 1043.9060,   820.1208,  1294.4067,  1251.4688,  1564.2716,  1836.6442,
          1973.8118,  1939.2332,  1776.0632,  1516.4639,  1233.3049,  1155.7878,
           863.6623,   318.0257,   -57.0339,   181.7011],
        [ 1516.0326,  1109.8800,  1654.9639,  1617.1333,  1815.5898,  2000.4884,
          1924.7661,  1804.7087,  1752.5310,  1558.9775,  1241.5448,  1226.4141,
          1094.7087,   683.4207,   320.6532,   441.6363],
        [ 1671.1971,  1145.4869,  1814.5289,  1775.5947,  1765.7629,  1935.4229,
          1895.8650,  1743.3062,  1636.6213,  1456.0304,  1298.7878,  1320.4668,
          1235.7045,   875.2137,   524.8336,   669.0461],
        [ 1247.9875,   757.1881,  1485.6201,  1502.6550,  1542.7094,  1738.8645,
          1788.7463,  1836.2325,  1681.8467,  1417.9645,  1342.7341,  1338.3250,
          1122.1101,   797.3535,   520.4686,   616.8055],
        [  826.0286,   367.9234,  1124.5377,  1069.1826,  1299.9098,  1592.7402,
          1797.5482,  1877.9387,  1702.7163,  1406.0210,  1214.2158,  1195.2052,
           971.7441,   654.6920,   366.2449,   402.1526],
        [  543.2884,   148.2821,   844.7369,   847.9983,  1009.0647,  1351.0347,
          1595.5493,  1760.3950,  1681.6501,  1450.1536,  1287.3503,  1067.1982,
           777.9042,   492.8951,   188.6569,   204.8415],
        [  179.2102,  -183.8904,   322.3454,   470.9867,   681.7946,   894.3118,
          1040.5010,  1156.5054,  1183.9928,  1192.6042,   945.5669,   709.8507,
           431.4176,   183.3581,   -19.1239,   -80.1328],
        [ -146.5038,  -471.2246,  -124.3643,  -103.5218,    49.8522,   180.7961,
           343.0785,   410.8065,   635.2560,   535.5599,   344.7355,   132.4877,
          -115.8628,  -300.7290,  -539.1206,  -478.7062],
        [ -576.6780,  -834.5021,  -506.7276,  -555.6016,  -523.6331,  -362.0844,
          -145.9709,    13.5559,   186.6505,   -11.0824,  -105.3964,  -150.4659,
          -473.1953,  -670.8705,  -947.7269,  -794.6317],
        [-1559.5784, -1688.2263, -1476.6671, -1570.3875, -1456.8828, -1441.4746,
         -1212.0193, -1141.7939, -1082.7771, -1200.9060, -1332.8948, -1330.0312,
         -1519.1062, -1674.5979, -1760.4341, -1535.3673]], device='cuda:0')
LAYER_NUM:  0
FINAL SUM LENNNN  96
FINAL_SUM:  [77, 25, 10, 58, 16, 43, 49, 29, 41, 19]
acts len:  3
acts keys:  ['conv0', 'conv1', 'conv2']
acts:  {'conv0': [77, 25, 10, 58, 16, 43, 49, 29, 41, 19, 5, 53, 86, 84, 30, 26, 11, 57, 91, 59, 8, 67, 46, 18, 95, 27, 88, 24, 28, 74], 'conv1': [204, 78, 174, 243, 49, 77, 116, 259, 300, 63, 254, 61, 269, 291, 332, 359, 99, 260, 46, 112, 157, 64, 19, 98, 225, 158, 210, 127, 270, 8, 227, 330, 147, 71, 32, 245, 356, 59, 362, 53, 66, 110, 352, 184, 266, 340, 235, 125, 366, 365, 294, 39, 191, 349, 7, 247, 18, 115, 338, 156, 303, 324, 180, 159, 279, 47, 280, 164, 72, 218, 69, 369, 107, 242, 296, 113, 26, 355, 151, 119, 31, 92, 173, 199, 201, 27, 120, 213, 104, 345, 152, 382, 264, 346, 103, 335, 52, 251, 360, 301, 121, 222, 249, 351, 286, 160, 376, 255, 348, 374, 54, 215, 161, 87, 171, 282], 'conv2': [660, 268, 1082, 1195, 1030, 205, 75, 760, 350, 269, 1483, 914, 1218, 191, 919, 1490, 1141, 1184, 81, 554, 1425, 1471, 1005, 1458, 941, 415, 1251, 576, 294, 1510, 97, 1261, 1113, 1154, 1008, 1174, 666, 168, 553, 417, 272, 1149, 459, 27, 1094, 964, 1409, 1068, 1284, 1157, 985, 1354, 987, 693, 726, 1500, 1305, 1383, 108, 1400, 617, 503, 1050, 973, 1389, 315, 1039, 1393, 811, 1430, 1532, 737, 303, 1211, 517, 627, 451, 7, 998, 548, 68, 537, 128, 1053, 674, 1464, 301, 1474, 1382, 743, 786, 1404, 870, 501, 765, 607, 193, 1505, 1270, 634, 1349, 171, 196, 1310, 882, 138, 540, 1230, 1192, 112, 325, 377, 252, 862, 545, 151, 286, 814, 133, 1001, 436, 1213, 525, 1076, 730, 822, 296, 689, 1200, 812, 696, 261, 582, 796, 856, 640, 1359, 61, 1135, 453, 1412, 1323, 1016, 1222, 1086, 1205, 868, 817, 1065, 637, 837, 55, 557, 772, 694, 887, 1411, 1343, 408, 1121, 1189, 1239, 903, 1357, 456, 320, 710, 1434, 394, 564, 490, 927, 401, 746, 323, 588, 1078, 648, 99, 1024, 1371, 482, 567, 1160, 1501, 496, 1066, 506, 388, 71, 1085, 847, 638, 427, 600, 335, 431, 293, 866, 22, 924, 1021, 511, 1313, 1307, 1013, 1124, 1148, 167, 289, 1197, 1194, 1378, 650, 844, 959, 1319, 926, 556, 468, 832, 810, 212, 175, 184, 263, 1352, 454, 1370, 1106, 676, 1060, 463, 997, 915, 524, 519, 579, 422, 492, 939, 479, 94, 843, 609, 77, 994, 1506, 69, 996, 1327, 245, 763, 727, 1482, 539, 1273, 929, 188, 931, 896, 392, 1155, 533, 1167, 47, 117, 1387, 1493, 1132, 88, 1530, 1044, 777, 1446, 838, 626, 466, 601, 683, 574, 63, 381, 217, 13, 233, 995, 877, 884, 1299, 444, 226, 179, 1312, 278, 197, 284, 1028, 897, 347, 119, 864, 54, 1019, 910, 1438, 749, 599, 369, 592, 752, 704, 595, 1045, 593, 48, 654, 1461, 430, 1000, 1265, 1456, 888, 127, 736, 1507, 137, 1221, 1416, 79, 804, 257, 1073, 1225, 1153, 1111, 467, 945, 956, 1435, 238, 863, 1351, 1035, 410, 1479, 264, 1137, 779, 873, 705, 154, 1097, 1441, 1186, 715, 163, 484, 1185, 971, 671, 605, 830, 766, 488, 1236, 1534, 1484, 1201, 1052, 267, 34, 283, 583, 1253, 682, 4, 1054, 950, 979, 1462, 337, 699, 89, 701, 1196, 107, 1040, 1276, 505, 1413, 194, 49, 1324, 1041, 1051, 858, 840, 470, 287, 1231, 334, 986, 1376, 1057, 536, 407, 157, 149, 1495, 1451, 1144, 15, 1173, 679, 177, 295, 1146, 1372, 498, 142, 1342, 1136, 759, 8, 426, 775, 603, 497, 111, 629, 332, 1341, 806, 827, 229, 41, 1007, 352, 354, 713, 1303, 1133, 318, 275, 741, 967, 684, 747, 1156, 1491, 662, 348, 242, 1395, 824, 944, 442, 1263, 1470, 1096, 867]}
final_sum len:  96
delta_weights INFO: 
NUM OF TRACKED COV LAYERS:  1
NUM OF TRACKED WEIGHTS CHANGES PER LAYER:  20
avg_deltas INFO:  <class 'dict'>
avg_deltas keys:  ['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight']
avg_deltas size:  3
num of averages for 0 layer:  torch.Size([96])
################################################
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 1.32e-01	time: 00:00:36	Acc_train 0.00	Acc_test 0.00	convergence: 2.40e+01	R1: 0	Info MB:0.000e+00/SB:0.000e+00/MW:6.425e-03/SW:4.945e-01/MR:4.061e+00/SR:1.367e+00/MeD:1.021e+00/MaD:3.069e+00/MW:0.625/MAW:0.375
|      0 |       1 |      2 |       3 |       4 |        5 |       6 |       7 |       8 |      9 |        10 |      11 |      12 |      13 |      14 |      15 |         16 |     17 |      18 |      19 |      20 |      21 |      22 |      23 |      24 |         25 |      26 |       27 |        28 |      29 |
|--------+---------+--------+---------+---------+----------+---------+---------+---------+--------+-----------+---------+---------+---------+---------+---------+------------+--------+---------+---------+---------+---------+---------+---------+---------+------------+---------+----------+-----------+---------|
|   0.16 |   0.116 |   0.12 |   0.109 |   0.157 |   0.0745 |   0.141 |   0.146 |   0.155 |   0.19 |   0.00125 |   0.152 |   0.128 |   0.118 |   0.139 |   0.165 |   0.000314 |   0.16 |   0.158 |   0.165 |   0.165 |   0.157 |   0.142 |   0.176 |   0.165 |   0.000288 |   0.143 |   0.0521 |   0.00519 |   0.164 |
|   5.02 |   3.12  |   3.25 |   2.86  |   4.84  |   1.87   |   4.11  |   4.33  |   4.75  |   6.63 |   1       |   4.61  |   3.56  |   3.16  |   4.03  |   5.26  |   1        |   4.99 |   4.89  |   5.26  |   5.26  |   4.87  |   4.17  |   5.82  |   5.25  |   1        |   4.21  |   1.42   |   1       |   5.2   |
|   0.48 |   0.51  |   0.55 |   0.46  |   0.46  |   0.6    |   0.43  |   0.44  |   0.47  |   0.54 |   2.49    |   0.49  |   0.31  |   0.49  |   0.43  |   0.42  |  22.87     |   0.48 |   0.46  |   0.46  |   0.49  |   0.46  |   0.43  |   0.54  |   0.46  |  25.62     |   0.31  |   0.46   |   0.6     |   0.4   |
| nan    | nan     | nan    | nan     | nan     | nan      | nan     | nan     | nan     | nan    | nan       | nan     | nan     | nan     | nan     | nan     | nan        | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan        | nan     | nan      | nan       | nan     |
| nan    | nan     | nan    | nan     | nan     | nan      | nan     | nan     | nan     | nan    | nan       | nan     | nan     | nan     | nan     | nan     | nan        | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan        | nan     | nan      | nan       | nan     |
| nan    | nan     | nan    | nan     | nan     | nan      | nan     | nan     | nan     | nan    | nan       | nan     | nan     | nan     | nan     | nan     | nan        | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan        | nan     | nan      | nan       | nan     |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_4C_CL/models
Current stored value:  [77, 25, 10, 58, 16] tensor([-1.1082, -0.4946, -0.4195, -0.6116, -0.7314], device='cuda:0')

 ********** Hebbian Unsupervised learning of blocks [1] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([8, 8, 0, 1, 1, 0, 8, 8, 0, 2, 0, 2, 1, 0, 8, 0, 8, 8, 2, 1])
[0, 1, 2, 8]
TARGETS AFTER CLEANER:  tensor([3, 3, 0, 1, 1, 0, 3, 3, 0, 2, 0, 2, 1, 0, 3, 0, 3, 3, 2, 1])
4000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 4000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([1, 1, 2, 8, 2, 2, 2, 0, 0, 1, 0, 2, 2, 1, 1, 1, 2, 2, 0, 2])
[0, 1, 2, 8]
TARGETS AFTER CLEANER:  tensor([1, 1, 2, 3, 2, 2, 2, 0, 0, 1, 0, 2, 2, 1, 1, 1, 2, 2, 0, 2])
20000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 20000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([1, 1, 2, 8, 2, 2, 2, 0, 0, 1, 0, 2, 2, 1, 1, 1, 2, 2, 0, 2])
[0, 1, 2, 8]
TARGETS AFTER CLEANER:  tensor([1, 1, 2, 3, 2, 2, 2, 0, 0, 1, 0, 2, 2, 1, 1, 1, 2, 2, 0, 2])
20000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 20000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: ToTensor()
------------------------
INDICES:  20000
IMAGE SIZE: torch.Size([10, 3, 32, 32])
SAVING FOLDER FOR UNSUP:  C10_4C_CL
DEPTH:  3
ACTIVATIONS SHAPE:  tensor([[-52.7658, -51.6241, -52.1126, -43.5764, -47.3091, -54.3379, -60.0977,
         -58.1195, -53.4217, -36.4417, -28.3549, -32.6120, -36.6052, -47.4575,
         -40.0528, -35.4777],
        [-50.2732, -49.5381, -50.8949, -38.9899, -40.9593, -49.7911, -53.0661,
         -50.9651, -32.5723, -17.5451, -16.4133, -15.7338, -22.3317, -36.2369,
         -30.8163, -23.1920],
        [-48.5653, -45.7462, -45.6259, -46.1345, -48.2436, -35.0000, -29.8358,
         -19.5482, -10.2433,   2.6325,   0.9272,   1.0773,  -9.4604, -25.1613,
         -28.5843,  -5.6552],
        [-44.9104, -35.1665, -41.0898, -48.7091, -36.0471, -29.5069, -17.7381,
          -3.4301,   4.4800,  10.5316,   6.0263,   1.1885, -16.1862, -24.6932,
         -28.4342, -12.2973],
        [-41.0698, -34.4331, -37.6277, -39.7617, -33.1898, -19.5961,   0.4416,
           3.8851,   6.3237,  16.6980,  19.5025,  12.8072,  -2.4430, -26.7459,
         -28.4837, -11.4409],
        [-22.6604, -14.2215, -30.4287, -28.4999, -22.8493,  -6.7830,   5.7502,
           3.1507,   4.5474,  18.6256,  11.9933,  18.5561,   6.0994, -11.6462,
         -13.2150, -10.1566],
        [  0.7505,  17.5651,  -6.4875, -14.0208,  -9.8764,   4.1089,  15.5428,
          -4.6787, -10.3563,  -1.5188,  -1.5884,  -3.8167,   1.5892,  -4.6029,
         -11.8292,  -6.2886],
        [  5.4381,  17.0214,   9.6511,  -1.4385,   5.5566,  20.0467,  25.9251,
           9.8276,  11.3526,   3.4232, -10.9188, -19.5268, -15.6173,  -7.1638,
         -11.1451,  -0.5671],
        [  9.9820,  10.4725,  14.9103,  20.2478,  16.7300,  17.1287,  24.0648,
          17.1460,  14.3515,   0.1871,  -3.2121,  -6.5420, -17.5957, -24.8196,
         -18.6156,   7.0569],
        [ 13.7918,  11.7137,  29.4810,  25.9036,   8.5472,  15.0553,  14.4569,
           8.6440,   7.8687,  15.9884,  13.7686,  -1.3051, -26.1002, -30.6691,
         -14.1781,  10.7422],
        [ 27.9907,  18.5236,  35.3085,  20.8238,   4.5100,  11.9393,  17.0005,
           9.7910,  12.0148,  23.5525,  19.3643, -17.5915, -49.1952, -42.6406,
         -24.4233,   3.9857],
        [ 50.3955,  41.4509,  38.4894,  11.0159,  -2.8137,  -7.2466,   4.3523,
          15.5412,  18.0698,   8.0922, -11.4796, -32.4672, -53.8894, -50.6558,
         -24.8703,  -5.9134],
        [ 42.0279,  38.0190,  27.7990,   9.2543,   0.7213,  -8.7452,   1.4134,
           8.3272,  10.6170, -11.5266, -18.0712, -38.6174, -52.9347, -51.9200,
         -28.1752, -16.8969],
        [  7.6388,   3.7421,   1.2093, -14.6254, -12.5166, -15.3275, -14.1825,
          -3.7774,  -5.7829, -25.6700, -31.4312, -44.6932, -50.0443, -45.8655,
         -35.4384, -16.1348],
        [-10.2917, -14.5476,  -9.2907,  -8.4766,  -8.4407, -18.7984, -15.1415,
          -5.9223, -17.1130, -27.5450, -27.4971, -35.6074, -42.2019, -31.6121,
         -26.5117, -13.0508],
        [-33.5071, -26.0988, -30.1840, -39.7670, -38.6485, -38.4220, -35.0694,
         -21.0398, -28.3131, -39.1820, -41.7681, -49.8972, -52.6761, -54.6032,
         -45.9710, -26.7296]], device='cuda:0')
LAYER_NUM:  1
FINAL SUM LENNNN  384
FINAL_SUM:  [266, 156, 204, 174, 296, 61, 259, 158, 93, 351]
acts len:  3
acts keys:  ['conv0', 'conv1', 'conv2']
acts:  {'conv0': [77, 25, 10, 58, 16, 43, 49, 29, 41, 19, 5, 53, 86, 84, 30, 26, 11, 57, 91, 59, 8, 67, 46, 18, 95, 27, 88, 24, 28, 74], 'conv1': [266, 156, 204, 174, 296, 61, 259, 158, 93, 351, 147, 359, 225, 19, 282, 243, 13, 270, 175, 8, 247, 66, 327, 210, 73, 179, 205, 322, 63, 223, 116, 374, 59, 180, 294, 209, 372, 300, 338, 325, 317, 260, 277, 125, 99, 25, 4, 201, 340, 119, 40, 324, 257, 184, 279, 77, 171, 71, 215, 74, 18, 370, 58, 346, 360, 50, 32, 207, 149, 64, 46, 113, 173, 186, 49, 383, 364, 353, 335, 254, 110, 86, 339, 246, 80, 115, 26, 245, 251, 47, 129, 236, 194, 272, 355, 369, 78, 352, 94, 197, 330, 27, 264, 41, 376, 321, 242, 269, 238, 139, 126, 361, 151, 68, 213, 92], 'conv2': [660, 268, 1082, 1195, 1030, 205, 75, 760, 350, 269, 1483, 914, 1218, 191, 919, 1490, 1141, 1184, 81, 554, 1425, 1471, 1005, 1458, 941, 415, 1251, 576, 294, 1510, 97, 1261, 1113, 1154, 1008, 1174, 666, 168, 553, 417, 272, 1149, 459, 27, 1094, 964, 1409, 1068, 1284, 1157, 985, 1354, 987, 693, 726, 1500, 1305, 1383, 108, 1400, 617, 503, 1050, 973, 1389, 315, 1039, 1393, 811, 1430, 1532, 737, 303, 1211, 517, 627, 451, 7, 998, 548, 68, 537, 128, 1053, 674, 1464, 301, 1474, 1382, 743, 786, 1404, 870, 501, 765, 607, 193, 1505, 1270, 634, 1349, 171, 196, 1310, 882, 138, 540, 1230, 1192, 112, 325, 377, 252, 862, 545, 151, 286, 814, 133, 1001, 436, 1213, 525, 1076, 730, 822, 296, 689, 1200, 812, 696, 261, 582, 796, 856, 640, 1359, 61, 1135, 453, 1412, 1323, 1016, 1222, 1086, 1205, 868, 817, 1065, 637, 837, 55, 557, 772, 694, 887, 1411, 1343, 408, 1121, 1189, 1239, 903, 1357, 456, 320, 710, 1434, 394, 564, 490, 927, 401, 746, 323, 588, 1078, 648, 99, 1024, 1371, 482, 567, 1160, 1501, 496, 1066, 506, 388, 71, 1085, 847, 638, 427, 600, 335, 431, 293, 866, 22, 924, 1021, 511, 1313, 1307, 1013, 1124, 1148, 167, 289, 1197, 1194, 1378, 650, 844, 959, 1319, 926, 556, 468, 832, 810, 212, 175, 184, 263, 1352, 454, 1370, 1106, 676, 1060, 463, 997, 915, 524, 519, 579, 422, 492, 939, 479, 94, 843, 609, 77, 994, 1506, 69, 996, 1327, 245, 763, 727, 1482, 539, 1273, 929, 188, 931, 896, 392, 1155, 533, 1167, 47, 117, 1387, 1493, 1132, 88, 1530, 1044, 777, 1446, 838, 626, 466, 601, 683, 574, 63, 381, 217, 13, 233, 995, 877, 884, 1299, 444, 226, 179, 1312, 278, 197, 284, 1028, 897, 347, 119, 864, 54, 1019, 910, 1438, 749, 599, 369, 592, 752, 704, 595, 1045, 593, 48, 654, 1461, 430, 1000, 1265, 1456, 888, 127, 736, 1507, 137, 1221, 1416, 79, 804, 257, 1073, 1225, 1153, 1111, 467, 945, 956, 1435, 238, 863, 1351, 1035, 410, 1479, 264, 1137, 779, 873, 705, 154, 1097, 1441, 1186, 715, 163, 484, 1185, 971, 671, 605, 830, 766, 488, 1236, 1534, 1484, 1201, 1052, 267, 34, 283, 583, 1253, 682, 4, 1054, 950, 979, 1462, 337, 699, 89, 701, 1196, 107, 1040, 1276, 505, 1413, 194, 49, 1324, 1041, 1051, 858, 840, 470, 287, 1231, 334, 986, 1376, 1057, 536, 407, 157, 149, 1495, 1451, 1144, 15, 1173, 679, 177, 295, 1146, 1372, 498, 142, 1342, 1136, 759, 8, 426, 775, 603, 497, 111, 629, 332, 1341, 806, 827, 229, 41, 1007, 352, 354, 713, 1303, 1133, 318, 275, 741, 967, 684, 747, 1156, 1491, 662, 348, 242, 1395, 824, 944, 442, 1263, 1470, 1096, 867]}
final_sum len:  384
delta_weights INFO: 
NUM OF TRACKED COV LAYERS:  1
NUM OF TRACKED WEIGHTS CHANGES PER LAYER:  20
avg_deltas INFO:  <class 'dict'>
avg_deltas keys:  ['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight']
avg_deltas size:  3
num of averages for 1 layer:  torch.Size([96])
################################################
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 1.32e-01	time: 00:01:13	Acc_train 0.00	Acc_test 0.00	convergence: 2.40e+01	R1: 0	Info MB:0.000e+00/SB:0.000e+00/MW:4.776e-03/SW:1.923e-01/MR:5.366e+00/SR:1.782e+00/MeD:1.421e+00/MaD:4.366e+00/MW:0.609/MAW:0.391
|        0 |        1 |         2 |          3 |         4 |        5 |        6 |         7 |        8 |        9 |       10 |       11 |        12 |       13 |      14 |       15 |        16 |       17 |        18 |       19 |       20 |       21 |       22 |        23 |       24 |       25 |       26 |      27 |       28 |        29 |
|----------+----------+-----------+------------+-----------+----------+----------+-----------+----------+----------+----------+----------+-----------+----------+---------+----------+-----------+----------+-----------+----------+----------+----------+----------+-----------+----------+----------+----------+---------+----------+-----------|
|   0.0109 |   0.0133 |   0.00897 |   0.000984 |   0.00858 |   0.0114 |   0.0121 |   0.00912 |   0.0101 |   0.0112 |   0.0123 |   0.0118 |   0.00256 |   0.0134 |   0.011 |   0.0116 |   0.00672 |   0.0115 |   0.00361 |   0.0118 |   0.0117 |   0.0117 |   0.0118 |   0.00858 |   0.0103 |   0.0111 |   0.0112 |   0.011 |   0.0116 |   0.00398 |
|   5.78   |   8.09   |   4.22    |   1.04     |   3.95    |   6.17   |   6.85   |   4.33    |   5.08   |   6      |   7.01   |   6.53   |   1.26    |   8.19   |   5.84  |   6.38   |   2.81    |   6.32   |   1.52    |   6.57   |   6.45   |   6.49   |   6.53   |   3.94    |   5.22   |   5.89   |   6.06   |   5.83  |   6.39   |   1.64    |
|   0.12   |   0.15   |   0.15    |   0.52     |   0.14    |   0.1    |   0.2    |   0.1     |   0.12   |   0.16   |   0.16   |   0.17   |   0.28    |   0.2    |   0.09  |   0.17   |   0.18    |   0.18   |   0.08    |   0.11   |   0.18   |   0.1    |   0.16   |   0.15    |   0.11   |   0.18   |   0.11   |   0.14  |   0.14   |   0.17    |
| nan      | nan      | nan       | nan        | nan       | nan      | nan      | nan       | nan      | nan      | nan      | nan      | nan       | nan      | nan     | nan      | nan       | nan      | nan       | nan      | nan      | nan      | nan      | nan       | nan      | nan      | nan      | nan     | nan      | nan       |
| nan      | nan      | nan       | nan        | nan       | nan      | nan      | nan       | nan      | nan      | nan      | nan      | nan       | nan      | nan     | nan      | nan       | nan      | nan       | nan      | nan      | nan      | nan      | nan       | nan      | nan      | nan      | nan     | nan      | nan       |
| nan      | nan      | nan       | nan        | nan       | nan      | nan      | nan       | nan      | nan      | nan      | nan      | nan       | nan      | nan     | nan      | nan       | nan      | nan       | nan      | nan      | nan      | nan      | nan       | nan      | nan      | nan      | nan     | nan      | nan       |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_4C_CL/models
Current stored value:  [77, 25, 10, 58, 16] tensor([-1.1082, -0.4946, -0.4195, -0.6116, -0.7314], device='cuda:0')

 ********** Hebbian Unsupervised learning of blocks [2] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([8, 8, 0, 1, 1, 0, 8, 8, 0, 2, 0, 2, 1, 0, 8, 0, 8, 8, 2, 1])
[0, 1, 2, 8]
TARGETS AFTER CLEANER:  tensor([3, 3, 0, 1, 1, 0, 3, 3, 0, 2, 0, 2, 1, 0, 3, 0, 3, 3, 2, 1])
4000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 4000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([1, 1, 2, 8, 2, 2, 2, 0, 0, 1, 0, 2, 2, 1, 1, 1, 2, 2, 0, 2])
[0, 1, 2, 8]
TARGETS AFTER CLEANER:  tensor([1, 1, 2, 3, 2, 2, 2, 0, 0, 1, 0, 2, 2, 1, 1, 1, 2, 2, 0, 2])
20000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 20000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([1, 1, 2, 8, 2, 2, 2, 0, 0, 1, 0, 2, 2, 1, 1, 1, 2, 2, 0, 2])
[0, 1, 2, 8]
TARGETS AFTER CLEANER:  tensor([1, 1, 2, 3, 2, 2, 2, 0, 0, 1, 0, 2, 2, 1, 1, 1, 2, 2, 0, 2])
20000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 20000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: ToTensor()
------------------------
INDICES:  20000
IMAGE SIZE: torch.Size([10, 3, 32, 32])
SAVING FOLDER FOR UNSUP:  C10_4C_CL
DEPTH:  3
ACTIVATIONS SHAPE:  tensor([[-3.2059e+01, -3.0312e+01, -3.1825e+01, -2.0094e+01, -2.5657e+01,
         -3.9290e+01, -4.8781e+01, -4.8324e+01, -4.3727e+01, -2.4541e+01,
         -1.4314e+01, -1.6930e+01, -2.0576e+01, -3.6951e+01, -3.0204e+01,
         -2.5173e+01],
        [-2.7152e+01, -2.4980e+01, -2.7974e+01, -1.2677e+01, -1.7087e+01,
         -3.0410e+01, -3.7900e+01, -3.7040e+01, -1.4816e+01,  3.6352e+00,
          3.4630e+00,  7.4143e+00, -1.4858e+00, -2.3501e+01, -1.7168e+01,
         -7.2810e+00],
        [-2.5041e+01, -1.8489e+01, -1.9868e+01, -2.0593e+01, -2.5399e+01,
         -1.0759e+01, -7.9778e+00,  2.5046e+00,  1.2222e+01,  2.4943e+01,
          1.9455e+01,  2.4353e+01,  1.0769e+01, -1.0729e+01, -1.5013e+01,
          1.3132e+01],
        [-2.4029e+01, -7.3444e+00, -1.8357e+01, -2.9845e+01, -1.5694e+01,
         -1.2763e+01,  1.2113e+00,  1.5741e+01,  2.5612e+01,  2.9720e+01,
          2.2920e+01,  1.9358e+01, -4.0242e-02, -9.4897e+00, -1.4615e+01,
          4.5422e+00],
        [-2.4306e+01, -1.3039e+01, -1.9504e+01, -2.5841e+01, -1.9648e+01,
         -6.4269e+00,  1.5527e+01,  1.8024e+01,  1.7465e+01,  3.0770e+01,
          3.7285e+01,  3.4672e+01,  1.8516e+01, -1.1477e+01, -1.1570e+01,
          1.1176e+01],
        [-3.6558e+00,  8.0810e+00, -1.6151e+01, -1.5532e+01, -1.1969e+01,
          5.5141e+00,  1.5686e+01,  1.1241e+01,  9.5397e+00,  2.8358e+01,
          2.5190e+01,  3.8990e+01,  2.7530e+01,  2.4137e+00,  5.3295e+00,
          6.2612e+00],
        [ 2.0858e+01,  4.4266e+01,  8.5342e+00,  2.8225e+00,  3.7499e+00,
          1.4217e+01,  2.4661e+01, -7.1042e+00, -1.5258e+01, -4.5587e+00,
          1.6550e+00,  2.2843e+00,  1.7925e+01,  5.5054e+00, -6.5671e+00,
         -1.6650e+00],
        [ 1.8796e+01,  3.2070e+01,  2.2082e+01,  1.1280e+01,  1.5898e+01,
          3.0206e+01,  3.4870e+01,  9.1726e+00,  1.4735e+01,  4.2126e+00,
         -1.0908e+01, -1.7735e+01, -1.2112e+01, -2.7569e+00, -1.2814e+01,
          9.7180e-01],
        [ 1.6359e+01,  1.9100e+01,  2.6185e+01,  3.2118e+01,  2.5409e+01,
          2.3351e+01,  2.9935e+01,  1.9391e+01,  1.6709e+01,  4.0322e-02,
         -6.5932e-01,  4.8599e+00, -1.2097e+01, -2.7526e+01, -2.0626e+01,
          1.4354e+01],
        [ 1.9285e+01,  2.2938e+01,  4.3550e+01,  3.7986e+01,  1.2874e+01,
          1.9384e+01,  1.8689e+01,  1.0176e+01,  8.8158e+00,  2.0311e+01,
          2.2466e+01,  9.7876e+00, -2.0561e+01, -3.6806e+01, -1.2958e+01,
          1.8810e+01],
        [ 3.6913e+01,  3.7608e+01,  5.0770e+01,  3.2013e+01,  6.0345e+00,
          1.4090e+01,  1.7976e+01,  7.0248e+00,  1.0337e+01,  2.7511e+01,
          2.8603e+01, -1.6607e+01, -5.4549e+01, -5.3342e+01, -2.7479e+01,
          1.0908e+01],
        [ 6.4058e+01,  6.7739e+01,  5.4536e+01,  1.7546e+01, -2.9060e-01,
         -1.4509e+01, -1.8502e+00,  1.7073e+01,  2.1446e+01,  9.2732e+00,
         -9.8939e+00, -3.0175e+01, -5.5677e+01, -6.0589e+01, -2.9676e+01,
         -3.5135e+00],
        [ 5.1147e+01,  5.6354e+01,  3.6667e+01,  9.3965e+00, -2.9522e-01,
         -1.3762e+01, -4.9413e+00,  8.9158e+00,  1.6097e+01, -9.8274e+00,
         -8.1319e+00, -3.0896e+01, -4.5803e+01, -5.2020e+01, -2.7286e+01,
         -1.5552e+01],
        [ 5.6588e+00,  6.7493e+00,  4.3129e+00, -1.7312e+01, -1.2515e+01,
         -1.9078e+01, -1.9391e+01, -4.3769e+00, -1.4533e+00, -2.2473e+01,
         -2.1946e+01, -3.2718e+01, -3.8839e+01, -3.6152e+01, -2.7885e+01,
         -9.2998e+00],
        [-7.5248e+00, -9.9933e+00, -9.4612e-01,  1.2813e+00, -1.2310e+00,
         -1.2160e+01, -6.8575e+00,  2.0487e+00, -9.6224e+00, -2.2876e+01,
         -1.3382e+01, -1.7165e+01, -2.6186e+01, -1.6595e+01, -1.3057e+01,
         -4.0411e+00],
        [-3.4863e+01, -1.8568e+01, -2.6036e+01, -3.5748e+01, -3.4474e+01,
         -3.3211e+01, -3.0492e+01, -1.4120e+01, -1.8318e+01, -3.1056e+01,
         -2.8550e+01, -3.2467e+01, -3.4625e+01, -4.1674e+01, -3.5765e+01,
         -1.8490e+01]], device='cuda:0')
LAYER_NUM:  2
FINAL SUM LENNNN  1536
FINAL_SUM:  [1261, 660, 1141, 268, 415, 637, 296, 870, 1195, 205]
acts len:  3
acts keys:  ['conv0', 'conv1', 'conv2']
acts:  {'conv0': [77, 25, 10, 58, 16, 43, 49, 29, 41, 19, 5, 53, 86, 84, 30, 26, 11, 57, 91, 59, 8, 67, 46, 18, 95, 27, 88, 24, 28, 74], 'conv1': [266, 156, 204, 174, 296, 61, 259, 158, 93, 351, 147, 359, 225, 19, 282, 243, 13, 270, 175, 8, 247, 66, 327, 210, 73, 179, 205, 322, 63, 223, 116, 374, 59, 180, 294, 209, 372, 300, 338, 325, 317, 260, 277, 125, 99, 25, 4, 201, 340, 119, 40, 324, 257, 184, 279, 77, 171, 71, 215, 74, 18, 370, 58, 346, 360, 50, 32, 207, 149, 64, 46, 113, 173, 186, 49, 383, 364, 353, 335, 254, 110, 86, 339, 246, 80, 115, 26, 245, 251, 47, 129, 236, 194, 272, 355, 369, 78, 352, 94, 197, 330, 27, 264, 41, 376, 321, 242, 269, 238, 139, 126, 361, 151, 68, 213, 92], 'conv2': [1261, 660, 1141, 268, 415, 637, 296, 870, 1195, 205, 743, 97, 1174, 554, 760, 1284, 108, 1005, 1458, 168, 1393, 269, 1149, 1160, 48, 924, 545, 607, 627, 1157, 272, 1359, 1409, 997, 350, 1434, 1305, 1412, 1323, 1500, 726, 27, 941, 557, 1474, 1016, 1200, 1154, 303, 392, 1008, 676, 837, 811, 674, 315, 39, 1273, 1078, 998, 294, 634, 22, 1505, 1490, 765, 394, 1425, 1001, 75, 191, 919, 436, 1510, 69, 1483, 501, 903, 1501, 336, 959, 737, 1013, 55, 1066, 466, 973, 746, 1218, 180, 286, 713, 1192, 348, 1307, 914, 408, 1532, 567, 556, 705, 1327, 150, 417, 666, 985, 1053, 451, 163, 468, 779, 964, 1382, 119, 149, 79, 1189, 1082, 89, 1534, 453, 1354, 112, 1161, 817, 388, 650, 927, 626, 1086, 456, 525, 682, 1211, 459, 1383, 1068, 401, 81, 939, 863, 1312, 437, 68, 377, 171, 193, 1251, 128, 965, 454, 463, 1378, 293, 261, 553, 548, 1184, 259, 1389, 1270, 605, 221, 931, 926, 1094, 7, 53, 1113, 245, 442, 537, 127, 929, 873, 507, 896, 814, 910, 741, 1370, 882, 983, 725, 1352, 1136, 1061, 887, 77, 1514, 689, 1430, 1045, 842, 539, 611, 1464, 843, 1462, 274, 810, 855, 142, 576, 1021, 704, 897, 648, 856, 980, 284, 301, 987, 640, 502, 1289, 410, 1315, 1097, 822, 174, 71, 1135, 217, 381, 970, 1039, 617, 1050, 431, 649, 540, 859, 1030, 1439, 325, 1471, 1033, 1208, 1321, 287, 1400, 137, 1310, 759, 196, 1263, 802, 1156, 818, 474, 1363, 1524, 1076, 427, 715, 1517, 786, 995, 671, 847, 1254, 61, 1065, 1132, 730, 944, 1437, 956, 603, 314, 1404, 643, 629, 838, 1375, 1413, 11, 337, 599, 696, 252, 780, 577, 1040, 1387, 1497, 263, 911, 946, 1041, 1186, 868, 564, 467, 1533, 226, 151, 121, 35, 1371, 1137, 574, 782, 482, 1225, 1515, 335, 496, 638, 898, 371, 996, 1435, 1529, 94, 1416, 1509, 736, 32, 950, 160, 1484, 1411, 1062, 1299, 869, 165, 824, 751, 614, 407, 928, 133, 806, 1291, 1015, 1194, 867, 1356, 1148, 1342, 212, 877, 1239, 804, 1055, 1231, 183, 1145, 1345, 267, 1424, 497, 579, 1530, 994, 662, 752, 117, 749, 378, 1440, 609, 369, 138, 416, 1269, 1368, 524, 812, 1313, 1357, 486, 129, 154, 1081, 1196, 1036, 805, 1429, 1111, 1213, 1341, 251, 986, 503, 772, 1044, 218, 1167, 1043, 1343, 444, 976, 1351, 334, 989, 95, 1060, 663, 1446, 84, 422, 1221, 600, 592, 490, 1470, 945, 1358, 99, 1453, 1019, 488, 606, 513, 934, 327, 405, 1388, 8, 1236, 153, 888, 1441, 347, 1099, 15, 773, 1052, 588, 562, 1395, 573, 864, 1461, 111, 1255, 1459, 1230, 1124, 1028, 1507, 519, 1334, 316, 1140, 370, 830, 1110, 1207, 1146, 763, 832, 342, 890, 1355, 1495, 816, 852, 1520]}
final_sum len:  1536
delta_weights INFO: 
NUM OF TRACKED COV LAYERS:  1
NUM OF TRACKED WEIGHTS CHANGES PER LAYER:  20
avg_deltas INFO:  <class 'dict'>
avg_deltas keys:  ['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight']
avg_deltas size:  3
num of averages for 2 layer:  torch.Size([96])
################################################
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 4.07e-02	time: 00:01:52	Acc_train 0.00	Acc_test 0.00	convergence: 1.66e+01	R1: 1	Info MB:0.000e+00/SB:0.000e+00/MW:8.257e-03/SW:3.019e-01/MR:1.765e+01/SR:1.960e+00/MeD:1.559e+00/MaD:1.665e+01/MW:0.452/MAW:0.548
|        0 |        1 |        2 |        3 |        4 |        5 |        6 |        7 |        8 |       9 |       10 |       11 |       12 |       13 |       14 |       15 |       16 |       17 |       18 |       19 |       20 |       21 |       22 |       23 |       24 |       25 |       26 |       27 |       28 |       29 |
|----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------|
|   0.0403 |   0.0414 |   0.0408 |   0.0398 |   0.0409 |   0.0428 |   0.0394 |   0.0388 |   0.0381 |   0.044 |   0.0391 |   0.0414 |   0.0402 |   0.0405 |   0.0411 |   0.0436 |   0.0396 |   0.0404 |   0.0391 |   0.0416 |   0.0412 |   0.0389 |   0.0418 |   0.0428 |   0.0424 |   0.0413 |   0.0393 |   0.0431 |   0.0382 |   0.0435 |
|  17.24   |  18.16   |  17.62   |  16.8    |  17.73   |  19.34   |  16.5    |  16.05   |  15.48   |  20.34  |  16.33   |  18.17   |  17.16   |  17.41   |  17.85   |  20      |  16.66   |  17.35   |  16.25   |  18.32   |  17.94   |  16.15   |  18.43   |  19.28   |  19      |  18.05   |  16.45   |  19.62   |  15.59   |  19.92   |
|   0.04   |   0.02   |   0.05   |   0.07   |   0.03   |   0.04   |   0.08   |   0.03   |   0.05   |   0.07  |   0.08   |   0.06   |   0.09   |   0.04   |   0.07   |   0.04   |   0.08   |   0.08   |   0.05   |   0.04   |   0.07   |   0.07   |   0.05   |   0.06   |   0.07   |   0.03   |   0.08   |   0.06   |   0.06   |   0.05   |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_4C_CL/models
Current stored value:  [77, 25, 10, 58, 16] tensor([-1.1082, -0.4946, -0.4195, -0.6116, -0.7314], device='cuda:0')

 ********** Supervised learning of blocks [3] **********
SAVING FOLDER FOR SUP:  C10_4C_CL
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([8, 8, 0, 1, 1, 0, 8, 8, 0, 2, 0, 2, 1, 0, 8, 0, 8, 8, 2, 1])
[0, 1, 2, 8]
TARGETS AFTER CLEANER:  tensor([3, 3, 0, 1, 1, 0, 3, 3, 0, 2, 0, 2, 1, 0, 3, 0, 3, 3, 2, 1])
4000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 4000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([1, 1, 2, 8, 2, 2, 2, 0, 0, 1, 0, 2, 2, 1, 1, 1, 2, 2, 0, 2])
[0, 1, 2, 8]
TARGETS AFTER CLEANER:  tensor([1, 1, 2, 3, 2, 2, 2, 0, 0, 1, 0, 2, 2, 1, 1, 1, 2, 2, 0, 2])
20000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 20000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([1, 1, 2, 8, 2, 2, 2, 0, 0, 1, 0, 2, 2, 1, 1, 1, 2, 2, 0, 2])
[0, 1, 2, 8]
TARGETS AFTER CLEANER:  tensor([1, 1, 2, 3, 2, 2, 2, 0, 0, 1, 0, 2, 2, 1, 1, 1, 2, 2, 0, 2])
20000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 20000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: ToTensor()
------------------------
INDICES:  20000
IMAGE SIZE: torch.Size([64, 3, 32, 32])
Epoch: [1/50]	lr: 1.00e-03	time: 00:02:26	Loss_train 0.51343	Acc_train 73.04	/	Loss_test 0.01543	Acc_test 82.43
BLOCKS IN SUP:  [3]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_4C_CL/models
Current stored value:  [77, 25, 10, 58, 16] tensor([-1.1082, -0.4946, -0.4195, -0.6116, -0.7314], device='cuda:0')
Epoch: [10/50]	lr: 1.00e-03	time: 00:02:34	Loss_train 0.18262	Acc_train 86.81	/	Loss_test 0.01100	Acc_test 89.72
BLOCKS IN SUP:  [3]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_4C_CL/models
Current stored value:  [77, 25, 10, 58, 16] tensor([-1.1082, -0.4946, -0.4195, -0.6116, -0.7314], device='cuda:0')
Epoch: [20/50]	lr: 2.50e-04	time: 00:02:44	Loss_train 0.09340	Acc_train 91.90	/	Loss_test 0.00867	Acc_test 90.22
BLOCKS IN SUP:  [3]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_4C_CL/models
Current stored value:  [77, 25, 10, 58, 16] tensor([-1.1082, -0.4946, -0.4195, -0.6116, -0.7314], device='cuda:0')
Epoch: [30/50]	lr: 1.25e-04	time: 00:02:53	Loss_train 0.06807	Acc_train 93.23	/	Loss_test 0.00745	Acc_test 91.15
BLOCKS IN SUP:  [3]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_4C_CL/models
Current stored value:  [77, 25, 10, 58, 16] tensor([-1.1082, -0.4946, -0.4195, -0.6116, -0.7314], device='cuda:0')
Epoch: [40/50]	lr: 3.13e-05	time: 00:03:02	Loss_train 0.05486	Acc_train 93.98	/	Loss_test 0.00753	Acc_test 91.00
BLOCKS IN SUP:  [3]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_4C_CL/models
Current stored value:  [77, 25, 10, 58, 16] tensor([-1.1082, -0.4946, -0.4195, -0.6116, -0.7314], device='cuda:0')
Epoch: [50/50]	lr: 7.81e-06	time: 00:03:12	Loss_train 0.05209	Acc_train 94.17	/	Loss_test 0.00744	Acc_test 90.97
BLOCKS IN SUP:  [3]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_4C_CL/models
Current stored value:  [77, 25, 10, 58, 16] tensor([-1.1082, -0.4946, -0.4195, -0.6116, -0.7314], device='cuda:0')
RESULT:  {'train_loss': 0.05209041386842728, 'train_acc': 94.17250156402588, 'test_loss': 0.0074350880458951, 'test_acc': 90.9749984741211, 'convergence': 16.648944854736328, 'R1': 1, 'dataset_sup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 4, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 4, 'selected_classes': [0, 1, 2, 8]}, 'dataset_unsup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 4, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 4, 'selected_classes': [0, 1, 2, 8]}, 'train_config': {'t0': {'blocks': [0], 'mode': 'unsupervised', 'lr': 0.08, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't2': {'blocks': [1], 'mode': 'unsupervised', 'lr': 0.005, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't4': {'blocks': [2], 'mode': 'unsupervised', 'lr': 0.01, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't6': {'blocks': [3], 'mode': 'supervised', 'lr': 0.001, 'nb_epoch': 50, 'batch_size': 64, 'print_freq': 10}}}
IN R2:  {'R1': {'train_loss': 0.03653339296579361, 'train_acc': 95.05400061607361, 'test_loss': 0.005112210754305124, 'test_acc': 92.9000015258789, 'convergence': 17.80170440673828, 'R1': 0, 'dataset_sup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 4, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 4, 'selected_classes': [4, 6, 7, 9]}, 'dataset_unsup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 4, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 4, 'selected_classes': [4, 6, 7, 9]}, 'train_config': {'t0': {'blocks': [0], 'mode': 'unsupervised', 'lr': 0.08, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't2': {'blocks': [1], 'mode': 'unsupervised', 'lr': 0.005, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't4': {'blocks': [2], 'mode': 'unsupervised', 'lr': 0.01, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't6': {'blocks': [3], 'mode': 'supervised', 'lr': 0.001, 'nb_epoch': 50, 'batch_size': 64, 'print_freq': 10}}}, 'R2': {'train_loss': 0.05209041386842728, 'train_acc': 94.17250156402588, 'test_loss': 0.0074350880458951, 'test_acc': 90.9749984741211, 'convergence': 16.648944854736328, 'R1': 1, 'dataset_sup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 4, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 4, 'selected_classes': [0, 1, 2, 8]}, 'dataset_unsup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 4, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 4, 'selected_classes': [0, 1, 2, 8]}, 'train_config': {'t0': {'blocks': [0], 'mode': 'unsupervised', 'lr': 0.08, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't2': {'blocks': [1], 'mode': 'unsupervised', 'lr': 0.005, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't4': {'blocks': [2], 'mode': 'unsupervised', 'lr': 0.01, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't6': {'blocks': [3], 'mode': 'supervised', 'lr': 0.001, 'nb_epoch': 50, 'batch_size': 64, 'print_freq': 10}}}}
SEED:  0
block 0, size : 96 16 16
range = 2.886751345948129
block 1, size : 384 8 8
range = 0.8505172717997146
block 2, size : 1536 4 4
range = 0.4252586358998573
range = 0.11048543456039805
The device used will be: 
True
cuda:0
PARAMSSSS IN LOAD:  {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 3, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.11048543456039805, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 4, 'in_channels': 24576, 'old_channels': 1536, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
Previously stored value:  [77, 25, 10, 58, 16] tensor([-1.1082, -0.4946, -0.4195, -0.6116, -0.7314], device='cuda:0')

 Model C10_4C_CL loaded successfuly with best perf



 ----- Architecture Block BatchNorm2dSK3962(5, 5)1.0reflect, number 0 -----
- BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK963842(3, 3)0.6499999761581421reflect, number 1 -----
- BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.4)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK38415362(3, 3)0.25reflect, number 2 -----
- BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 3 -----
- Flatten(start_dim=1, end_dim=-1)
- Dropout(p=0.5, inplace=False)
- Linear(in_features=24576, out_features=4, bias=True)
[Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=24576, out_features=4, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)]
LAYER NAME:  Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=24576, out_features=4, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)
LAYER CHILDREN:  [BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=1.4)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
  )
  (layer): Linear(in_features=24576, out_features=4, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)]
subsubl NAME:  Sequential(
  (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=1.4)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): Flatten(start_dim=1, end_dim=-1)
)
subsubl NAME:  Linear(in_features=24576, out_features=4, bias=True)
subsubl NAME:  Dropout(p=0.5, inplace=False)
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([6, 6, 6, 9, 7, 9, 7, 6, 7, 4, 9, 4, 9, 6, 6, 4, 9, 4, 9, 4])
[4, 6, 7, 9]
TARGETS AFTER CLEANER:  tensor([1, 1, 1, 3, 2, 3, 2, 1, 2, 0, 3, 0, 3, 1, 1, 0, 3, 0, 3, 0])
4000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 4000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([6, 9, 9, 4, 7, 4, 7, 7, 9, 9, 9, 6, 4, 6, 6, 6, 4, 9, 4, 7])
[4, 6, 7, 9]
TARGETS AFTER CLEANER:  tensor([1, 3, 3, 0, 2, 0, 2, 2, 3, 3, 3, 1, 0, 1, 1, 1, 0, 3, 0, 2])
20000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 20000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([6, 9, 9, 4, 7, 4, 7, 7, 9, 9, 9, 6, 4, 6, 6, 6, 4, 9, 4, 7])
[4, 6, 7, 9]
TARGETS AFTER CLEANER:  tensor([1, 3, 3, 0, 2, 0, 2, 2, 3, 3, 3, 1, 0, 1, 1, 1, 0, 3, 0, 2])
20000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 20000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: ToTensor()
------------------------
INDICES:  20000
IMAGE SIZE: torch.Size([64, 3, 32, 32])
Accuracy of the network on the 1st dataset: 29.175 %
Test loss on the 1st dataset: 0.159

The device used will be: 
True
cuda:0
BLOCKS:  {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3}}, 'b3': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 3, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.25, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10}, 'pool': None, 'activation': None}}
CL:  True
{'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True}
CL:  True
{'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': True}
SEED:  0
block 0, size : 96 16 16
range = 2.886751345948129
block 1, size : 384 8 8
range = 0.8505172717997146
block 2, size : 1536 4 4
range = 0.4252586358998573
range = 0.11048543456039805
The device used will be: 
True
cuda:0
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}

 ----- Architecture Block BatchNorm2dSK3962(5, 5)1.0reflect, number 0 -----
- BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK963842(3, 3)0.6499999761581421reflect, number 1 -----
- BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.4)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK38415362(3, 3)0.25reflect, number 2 -----
- BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 3 -----
- Flatten(start_dim=1, end_dim=-1)
- Dropout(p=0.5, inplace=False)
- Linear(in_features=24576, out_features=4, bias=True)
[Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=24576, out_features=4, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)]
LAYER NAME:  Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=24576, out_features=4, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)
LAYER CHILDREN:  [BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=1.4)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
  )
  (layer): Linear(in_features=24576, out_features=4, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)]
subsubl NAME:  Sequential(
  (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=1.4)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): Flatten(start_dim=1, end_dim=-1)
)
subsubl NAME:  Linear(in_features=24576, out_features=4, bias=True)
subsubl NAME:  Dropout(p=0.5, inplace=False)

 ********** Hebbian Unsupervised learning of blocks [0] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([3, 0, 1, 3, 1, 0, 0, 4, 4, 0, 4, 4, 1, 4, 0, 3, 0, 3, 4, 3])
[0, 1, 3, 4]
TARGETS AFTER CLEANER:  tensor([2, 0, 1, 2, 1, 0, 0, 3, 3, 0, 3, 3, 1, 3, 0, 2, 0, 2, 3, 2])
4000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 4000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([4, 1, 1, 3, 4, 3, 4, 3, 3, 4, 0, 0, 1, 3, 4, 0, 3, 3, 3, 1])
[0, 1, 3, 4]
TARGETS AFTER CLEANER:  tensor([3, 1, 1, 2, 3, 2, 3, 2, 2, 3, 0, 0, 1, 2, 3, 0, 2, 2, 2, 1])
20000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 20000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([4, 1, 1, 3, 4, 3, 4, 3, 3, 4, 0, 0, 1, 3, 4, 0, 3, 3, 3, 1])
[0, 1, 3, 4]
TARGETS AFTER CLEANER:  tensor([3, 1, 1, 2, 3, 2, 3, 2, 2, 3, 0, 0, 1, 2, 3, 0, 2, 2, 2, 1])
20000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 20000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: ToTensor()
------------------------
INDICES:  20000
IMAGE SIZE: torch.Size([10, 3, 32, 32])
SAVING FOLDER FOR UNSUP:  C10_4C_CL
DEPTH:  3
WTA IN delta_weight:  tensor([[[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         ...,
         [-2.6625e-44, -0.0000e+00, -0.0000e+00,  ..., -4.0470e-25,
          -4.7702e-27, -1.4465e-31],
         [-1.6816e-44, -0.0000e+00, -0.0000e+00,  ..., -2.8074e-35,
          -2.9610e-37, -1.4048e-40],
         [-8.6881e-44, -1.4013e-45, -0.0000e+00,  ..., -2.8113e-41,
          -1.2472e-43, -7.0065e-45]],

        [[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -2.1019e-44,
          -8.4078e-45, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -7.8473e-44,
          -4.0638e-44, -8.4078e-45],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         ...,
         [-2.6526e-33, -1.3291e-35, -7.4118e-37,  ..., -2.6152e-23,
          -3.2960e-25, -3.2744e-26],
         [-1.4182e-33, -1.8927e-34, -1.0834e-34,  ..., -5.9470e-30,
          -1.4135e-30, -8.6633e-31],
         [-3.2753e-33, -2.5442e-34, -3.0466e-35,  ..., -1.0283e-31,
          -4.8942e-33, -6.7142e-32]],

        [[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -1.4013e-45, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         ...,
         [-1.4227e-34, -1.4756e-35, -1.9554e-36,  ..., -4.2746e-34,
          -1.5980e-33, -7.4477e-34],
         [-5.7487e-35, -3.2935e-35, -4.9058e-35,  ..., -3.2786e-36,
          -1.1519e-36, -1.9945e-36],
         [-1.3423e-34, -1.4384e-35, -2.3659e-36,  ..., -8.6640e-34,
          -1.2921e-35, -1.2382e-34]],

        ...,

        [[-6.3680e-16, -3.6703e-17, -3.5311e-16,  ..., -1.4079e-12,
          -3.7963e-14, -3.0746e-17],
         [-1.9931e-17, -1.6267e-15, -3.5576e-14,  ..., -1.2890e-11,
          -6.0322e-13, -1.6272e-15],
         [-3.0140e-15, -6.1872e-17, -1.1622e-17,  ..., -3.6213e-15,
          -5.4458e-16, -4.4604e-18],
         ...,
         [-2.0682e-11, -4.2609e-13, -2.3060e-13,  ..., -7.1459e-10,
          -3.1440e-12, -4.8587e-14],
         [-7.6619e-12, -1.2868e-12, -7.4889e-13,  ..., -3.5024e-09,
          -2.7416e-12, -1.5711e-14],
         [-5.0355e-11, -1.3188e-12, -4.8119e-13,  ..., -2.0996e-09,
          -2.8011e-12, -1.2051e-13]],

        [[-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,
          -0.0000e+00, -0.0000e+00],
         ...,
         [-3.6308e-41, -1.0902e-42, -1.2612e-44,  ..., -5.7356e-32,
          -4.0022e-31, -3.8900e-31],
         [-2.4276e-41, -1.0250e-41, -3.6434e-43,  ..., -1.3983e-38,
          -1.5683e-38, -2.5258e-39],
         [-8.4383e-41, -1.2900e-41, -5.5632e-43,  ..., -8.5036e-41,
          -1.1197e-40, -2.3068e-40]],

        [[-2.1019e-44, -1.1070e-43, -7.2868e-44,  ..., -4.1762e-39,
          -2.3232e-39, -5.1073e-41],
         [-1.1210e-44, -3.9236e-44, -3.0927e-42,  ..., -3.9116e-38,
          -4.1143e-38, -1.1992e-39],
         [-1.1491e-43, -1.2612e-44, -2.2421e-44,  ..., -1.1281e-40,
          -1.9780e-40, -2.5321e-41],
         ...,
         [-2.0899e-29, -5.7801e-31, -2.3176e-32,  ..., -9.6460e-15,
          -5.3642e-15, -2.1221e-15],
         [-2.1694e-29, -9.2282e-30, -9.1891e-31,  ..., -9.2664e-22,
          -7.2062e-23, -5.6355e-25],
         [-1.0339e-28, -2.3626e-29, -1.6865e-30,  ..., -2.8208e-27,
          -6.1345e-29, -4.1868e-29]]], device='cuda:0')
LAYER_NUM:  0
FINAL SUM LENNNN  96
FINAL_SUM:  [43, 29, 67, 55, 87, 26, 66, 91, 84, 46]
acts len:  1
acts keys:  ['conv0']
acts:  {'conv0': [43, 29, 67, 55, 87, 26, 66, 91, 84, 46, 35, 37, 77, 15, 34, 4, 25, 5, 30, 75, 92, 11, 95, 56, 42, 49, 13, 53, 72, 40]}
final_sum len:  96
delta_weights INFO: 
NUM OF TRACKED COV LAYERS:  1
NUM OF TRACKED WEIGHTS CHANGES PER LAYER:  20
avg_deltas INFO:  <class 'dict'>
avg_deltas keys:  ['blocks.0.layer.weight']
avg_deltas size:  1
num of averages for 0 layer:  torch.Size([96])
################################################
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 1.40e-01	time: 00:00:47	Acc_train 0.00	Acc_test 0.00	convergence: 2.40e+01	R1: 0	Info MB:0.000e+00/SB:0.000e+00/MW:1.011e-02/SW:5.445e-01/MR:4.408e+00/SR:1.686e+00/MeD:1.273e+00/MaD:5.092e+00/MW:0.601/MAW:0.399
|        0 |       1 |       2 |      3 |       4 |       5 |       6 |      7 |       8 |       9 |      10 |      11 |      12 |      13 |      14 |     15 |         16 |      17 |      18 |      19 |      20 |      21 |      22 |     23 |      24 |        25 |     26 |      27 |      28 |      29 |
|----------+---------+---------+--------+---------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+--------+------------+---------+---------+---------+---------+---------+---------+--------+---------+-----------+--------+---------+---------+---------|
|   0.0962 |   0.111 |   0.112 |   0.14 |   0.147 |   0.137 |   0.152 |   0.17 |   0.138 |   0.199 |   0.173 |   0.157 |   0.163 |   0.103 |   0.141 |   0.17 |   0.000294 |   0.154 |   0.153 |   0.169 |   0.177 |   0.125 |   0.144 |   0.19 |   0.174 |   0.00033 |   0.2  |   0.104 |   0.092 |   0.168 |
|   2.45   |   2.92  |   2.97  |   4.05 |   4.39  |   3.93  |   4.6   |   5.5  |   3.97  |   7.17  |   5.67  |   4.87  |   5.14  |   2.65  |   4.11  |   5.49 |   1        |   4.69  |   4.66  |   5.48  |   5.88  |   3.45  |   4.26  |   6.65 |   5.71  |   1       |   7.24 |   2.67  |   2.32  |   5.41  |
|   0.54   |   0.51  |   0.53  |   0.61 |   0.63  |   0.77  |   0.51  |   0.52 |   0.62  |   0.52  |   0.77  |   0.52  |   0.57  |   0.48  |   0.59  |   0.47 |  17.3      |   0.56  |   0.52  |   0.61  |   0.58  |   0.54  |   0.52  |   0.56 |   0.53  |  21.32    |   0.44 |   0.64  |   0.53  |   0.53  |
| nan      | nan     | nan     | nan    | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan        | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan       | nan    | nan     | nan     | nan     |
| nan      | nan     | nan     | nan    | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan        | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan       | nan    | nan     | nan     | nan     |
| nan      | nan     | nan     | nan    | nan     | nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan        | nan     | nan     | nan     | nan     | nan     | nan     | nan    | nan     | nan       | nan    | nan     | nan     | nan     |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_4C_CL/models
Current stored value:  [43, 29, 67, 55, 87] tensor([-0.2519,  0.0524,  0.0105,  0.0549, -0.4818], device='cuda:0')

 ********** Hebbian Unsupervised learning of blocks [1] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([3, 0, 1, 3, 1, 0, 0, 4, 4, 0, 4, 4, 1, 4, 0, 3, 0, 3, 4, 3])
[0, 1, 3, 4]
TARGETS AFTER CLEANER:  tensor([2, 0, 1, 2, 1, 0, 0, 3, 3, 0, 3, 3, 1, 3, 0, 2, 0, 2, 3, 2])
4000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 4000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([4, 1, 1, 3, 4, 3, 4, 3, 3, 4, 0, 0, 1, 3, 4, 0, 3, 3, 3, 1])
[0, 1, 3, 4]
TARGETS AFTER CLEANER:  tensor([3, 1, 1, 2, 3, 2, 3, 2, 2, 3, 0, 0, 1, 2, 3, 0, 2, 2, 2, 1])
20000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 20000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([4, 1, 1, 3, 4, 3, 4, 3, 3, 4, 0, 0, 1, 3, 4, 0, 3, 3, 3, 1])
[0, 1, 3, 4]
TARGETS AFTER CLEANER:  tensor([3, 1, 1, 2, 3, 2, 3, 2, 2, 3, 0, 0, 1, 2, 3, 0, 2, 2, 2, 1])
20000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 20000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: ToTensor()
------------------------
INDICES:  20000
IMAGE SIZE: torch.Size([10, 3, 32, 32])
SAVING FOLDER FOR UNSUP:  C10_4C_CL
DEPTH:  3
ACTIVATIONS SHAPE:  tensor([[ -44.7228,  -50.7644, -104.0205, -110.5809,  -12.7025,   -9.1974,
          -36.7250,  -55.9772,  -78.8610,  -67.0051,  -79.6289,  -91.5337,
         -113.1687, -104.2711,  -83.9708,  -83.4349],
        [ -41.7589,  -43.5901, -128.0284, -121.2819,  -36.1402,  -54.7317,
         -149.6515, -167.8632, -128.2668, -107.3783, -106.9421, -113.0768,
         -133.6375, -133.8813, -104.3051, -119.6689],
        [  17.4226,  -64.3808,  -96.9153,  -81.4746,  -33.4341,  -63.5710,
         -166.6411, -154.6761, -159.9050, -157.3747, -138.1056, -125.4022,
         -139.9973, -126.8451, -119.4189,  -98.9322],
        [ -47.6226,  -86.6093, -100.0272,  -66.5458,  -62.2040, -101.3874,
         -168.2255, -131.5392, -215.1994, -213.9952, -234.9035, -230.8991,
         -210.4640, -178.0614, -139.4385, -137.6830],
        [ -86.1941, -108.5575, -110.0352, -120.5184, -105.4272, -121.7194,
         -191.2189, -130.8136, -249.6547, -296.7542, -272.9318, -221.3981,
         -216.5752, -203.6938, -171.7006, -152.5617],
        [-127.4959, -125.3116,  -92.5039, -150.0869, -164.8066, -190.7410,
         -177.0378, -207.0159, -230.6710, -254.6863, -252.8377, -235.8514,
         -204.7562, -206.7814, -145.3292, -115.5307],
        [-155.0885, -168.1374, -109.3035, -122.3877, -196.2280, -221.0382,
         -185.6640, -229.6793, -264.5695, -296.2532, -217.4973, -200.2592,
         -147.7941, -182.3790, -143.2795,  -81.5345],
        [-156.1208, -148.4421, -150.3118, -214.6133, -286.3245, -266.1085,
         -275.0543, -208.8822, -229.5532, -274.2053, -265.6862, -238.5654,
         -149.4560,  -94.0106, -134.3244,  -83.0396],
        [ -87.7707,  -85.2421, -171.7076, -250.2000, -305.9707, -336.7899,
         -340.0038, -249.2365, -178.5965, -173.7004, -295.8554, -234.4196,
         -221.2315, -197.6035, -151.4563,  -76.0938],
        [ -95.3252, -124.1068, -191.9387, -253.1409, -313.8599, -338.3422,
         -258.4633, -217.6857, -176.3835, -212.5848, -278.2307, -243.5782,
         -284.1328, -244.2321, -162.5527,  -33.7991],
        [ -75.3773, -129.2851, -154.9529, -245.0366, -267.8451, -290.1418,
         -299.3486, -213.9627, -157.9272, -213.6205, -198.1734, -251.2640,
         -292.8728, -241.4619, -140.3504,  -96.1317],
        [-103.9877, -108.6842, -128.4268, -188.5603, -187.9315, -223.4160,
         -240.5743, -231.7622, -202.1370, -219.2733, -144.6557, -186.6140,
         -180.1415, -182.1900, -138.5589,  -60.9714],
        [ -71.3018,  -53.8118, -117.8334, -193.6801, -244.5823, -266.2369,
         -301.7021, -261.8968, -200.4307, -169.1771, -145.4382, -161.5420,
         -145.1322,  -88.2920,  -83.7596,  -11.3414],
        [-109.0553, -102.6367, -125.9248, -156.3447, -184.2949, -238.5663,
         -275.7207, -238.2811, -199.1602, -124.1356, -117.7730, -102.5524,
         -105.1961,  -85.0120,  -23.3601,   -2.7381],
        [-102.0397,  -89.8629, -106.5483, -108.4090, -131.8616, -106.2362,
         -141.5110, -151.4940, -148.6909, -100.3110,  -63.3154,  -69.6723,
          -95.7116,  -63.0871,   -9.5199,   21.3041],
        [-121.4891, -109.6660, -102.8012, -159.6020, -171.1141, -126.4763,
         -171.1196, -219.7745, -221.4374, -149.5577, -115.1417, -121.6056,
         -142.9589, -144.9787,  -71.8131,  -21.7219]], device='cuda:0')
LAYER_NUM:  1
FINAL SUM LENNNN  384
FINAL_SUM:  [78, 77, 61, 359, 260, 63, 254, 291, 259, 64]
acts len:  2
acts keys:  ['conv0', 'conv1']
acts:  {'conv0': [43, 29, 67, 55, 87, 26, 66, 91, 84, 46, 35, 37, 77, 15, 34, 4, 25, 5, 30, 75, 92, 11, 95, 56, 42, 49, 13, 53, 72, 40], 'conv1': [78, 77, 61, 359, 260, 63, 254, 291, 259, 64, 204, 225, 300, 49, 180, 99, 243, 112, 8, 19, 115, 330, 127, 352, 227, 157, 249, 294, 98, 362, 174, 147, 332, 151, 184, 324, 247, 66, 116, 32, 175, 158, 349, 46, 53, 210, 120, 242, 92, 121, 71, 374, 355, 235, 269, 251, 59, 119, 351, 290, 27, 245, 110, 156, 113, 270, 266, 87, 54, 280, 164, 218, 296, 57, 47, 69, 31, 40, 201, 107, 169, 5, 365, 303, 103, 72, 159, 101, 338, 207, 213, 125, 301, 333, 369, 382, 279, 311, 10, 376, 187, 18, 7, 356, 80, 30, 104, 191, 74, 93, 34, 284, 95, 126, 41, 161]}
final_sum len:  384
delta_weights INFO: 
NUM OF TRACKED COV LAYERS:  1
NUM OF TRACKED WEIGHTS CHANGES PER LAYER:  20
avg_deltas INFO:  <class 'dict'>
avg_deltas keys:  ['blocks.0.layer.weight', 'blocks.1.layer.weight']
avg_deltas size:  2
num of averages for 1 layer:  torch.Size([96])
################################################
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 1.40e-01	time: 00:01:26	Acc_train 0.00	Acc_test 0.00	convergence: 2.40e+01	R1: 0	Info MB:0.000e+00/SB:0.000e+00/MW:5.256e-03/SW:1.920e-01/MR:5.479e+00/SR:1.367e+00/MeD:1.074e+00/MaD:4.479e+00/MW:0.560/MAW:0.440
|         0 |        1 |       2 |        3 |         4 |        5 |         6 |         7 |         8 |        9 |        10 |       11 |       12 |       13 |       14 |       15 |       16 |        17 |        18 |        19 |       20 |       21 |       22 |       23 |         24 |       25 |       26 |        27 |       28 |        29 |
|-----------+----------+---------+----------+-----------+----------+-----------+-----------+-----------+----------+-----------+----------+----------+----------+----------+----------+----------+-----------+-----------+-----------+----------+----------+----------+----------+------------+----------+----------+-----------+----------+-----------|
|   0.00957 |   0.0101 |   0.012 |   0.0075 |   0.00515 |   0.0123 |   0.00951 |   0.00948 |   0.00949 |   0.0113 |   0.00903 |   0.0103 |   0.0116 |   0.0129 |   0.0104 |   0.0114 |   0.0101 |   0.00779 |   0.00994 |   0.00811 |   0.0117 |   0.0116 |   0.0118 |   0.0128 |   8.78e-05 |   0.0127 |   0.0127 |   0.00926 |   0.0119 |   0.00805 |
|   4.66    |   5.07   |   6.77  |   3.25   |   2.06    |   7.07   |   4.62    |   4.59    |   4.6     |   6.07   |   4.26    |   5.25   |   6.42   |   7.63   |   5.29   |   6.15   |   5.05   |   3.42    |   4.95    |   3.63    |   6.5    |   6.34   |   6.61   |   7.58   |   1        |   7.45   |   7.44   |   4.43    |   6.66   |   3.59    |
|   0.24    |   0.17   |   0.2   |   0.22   |   0.65    |   0.24   |   0.17    |   0.22    |   0.18    |   0.17   |   0.26    |   0.24   |   0.31   |   0.2    |   0.16   |   0.21   |   0.15   |   0.25    |   0.29    |   0.23    |   0.18   |   0.39   |   0.46   |   0.23   |   4.26     |   0.32   |   0.18   |   0.21    |   0.18   |   0.3     |
| nan       | nan      | nan     | nan      | nan       | nan      | nan       | nan       | nan       | nan      | nan       | nan      | nan      | nan      | nan      | nan      | nan      | nan       | nan       | nan       | nan      | nan      | nan      | nan      | nan        | nan      | nan      | nan       | nan      | nan       |
| nan       | nan      | nan     | nan      | nan       | nan      | nan       | nan       | nan       | nan      | nan       | nan      | nan      | nan      | nan      | nan      | nan      | nan       | nan       | nan       | nan      | nan      | nan      | nan      | nan        | nan      | nan      | nan       | nan      | nan       |
| nan       | nan      | nan     | nan      | nan       | nan      | nan       | nan       | nan       | nan      | nan       | nan      | nan      | nan      | nan      | nan      | nan      | nan       | nan       | nan       | nan      | nan      | nan      | nan      | nan        | nan      | nan      | nan       | nan      | nan       |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_4C_CL/models
Current stored value:  [43, 29, 67, 55, 87] tensor([-0.2519,  0.0524,  0.0105,  0.0549, -0.4818], device='cuda:0')

 ********** Hebbian Unsupervised learning of blocks [2] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([3, 0, 1, 3, 1, 0, 0, 4, 4, 0, 4, 4, 1, 4, 0, 3, 0, 3, 4, 3])
[0, 1, 3, 4]
TARGETS AFTER CLEANER:  tensor([2, 0, 1, 2, 1, 0, 0, 3, 3, 0, 3, 3, 1, 3, 0, 2, 0, 2, 3, 2])
4000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 4000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([4, 1, 1, 3, 4, 3, 4, 3, 3, 4, 0, 0, 1, 3, 4, 0, 3, 3, 3, 1])
[0, 1, 3, 4]
TARGETS AFTER CLEANER:  tensor([3, 1, 1, 2, 3, 2, 3, 2, 2, 3, 0, 0, 1, 2, 3, 0, 2, 2, 2, 1])
20000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 20000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([4, 1, 1, 3, 4, 3, 4, 3, 3, 4, 0, 0, 1, 3, 4, 0, 3, 3, 3, 1])
[0, 1, 3, 4]
TARGETS AFTER CLEANER:  tensor([3, 1, 1, 2, 3, 2, 3, 2, 2, 3, 0, 0, 1, 2, 3, 0, 2, 2, 2, 1])
20000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 20000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: ToTensor()
------------------------
INDICES:  20000
IMAGE SIZE: torch.Size([10, 3, 32, 32])
SAVING FOLDER FOR UNSUP:  C10_4C_CL
DEPTH:  3
ACTIVATIONS SHAPE:  tensor([[ -5.9484,  -6.1808, -11.4045,  -4.7792,  20.1592,  12.3848,   0.7329,
          -1.7641,  -1.5357,   0.7752,   0.3122,  -0.9044,  -4.0504,   0.6009,
           1.0903,   0.2223],
        [  3.2633,   3.5339,  -8.1946,   0.6963,  21.6088,   7.8258, -18.0158,
         -18.5538,  -9.6100,  -5.5662,  -5.6352,  -5.6825,  -6.9480,  -4.3318,
          -2.5418,  -8.5079],
        [ 26.7520,  10.1954,   8.4967,  19.4756,  33.2319,  16.4766,  -9.0490,
          -4.5610,  -6.3425,  -9.0993,  -5.1886,  -2.4249,  -5.0131,  -1.9187,
          -5.0432,  -2.1165],
        [ 24.9555,  17.6129,  18.2846,  30.6038,  35.2334,  22.1375,   5.4517,
          14.4043, -10.6067, -16.6887, -23.2683, -23.1093, -17.8134, -12.6977,
          -8.1897,  -8.8919],
        [ 30.2338,  26.4491,  29.0461,  27.8861,  32.7501,  30.5619,  14.8616,
          23.9082, -11.2553, -30.1020, -28.2427, -18.4450, -19.7922, -20.0534,
         -15.6275, -10.7755],
        [ 34.9097,  31.3878,  44.6728,  30.7574,  23.7539,  18.1717,  19.3304,
           8.6012,  -7.6053, -19.4404, -21.2482, -17.0449,  -8.9531, -11.6522,
          -5.2364,   1.0252],
        [ 35.4251,  25.7906,  44.7747,  37.2155,  15.8860,   6.6430,  14.1803,
           3.1684, -10.8557, -21.1177,  -6.5910,  -2.7476,  11.4560,  -0.0715,
           1.2002,   9.5362],
        [ 31.1197,  21.9644,  28.8450,  15.4625,  -6.4024,  -5.8892,  -5.6507,
          10.3115,   5.3517,  -5.3575,  -7.2200,  -3.4144,  17.1635,  19.4498,
           4.4721,   9.4745],
        [ 36.0470,  26.1065,  14.6798,  -2.2284, -16.2666, -23.5955, -15.3872,
           9.0308,  24.3237,  21.1915,  -5.8102,   3.6365,  -0.5529,  -4.5110,
          -3.4013,   7.4873],
        [ 27.6071,   9.2691,   1.7726,  -9.2276, -20.5265, -18.7299,   5.1029,
          18.3831,  27.1863,  13.7646,  -4.7091,  -2.7298, -18.2706, -18.2971,
          -8.0428,  15.3714],
        [ 28.5801,   6.8298,  10.3602,  -6.4323,  -8.7378,  -4.4473,  -1.1114,
          14.1254,  24.3293,   6.4306,   4.3453, -11.0021, -26.4004, -20.9701,
          -3.6837,   1.8476],
        [ 23.0729,  11.5219,  14.9228,   4.3854,   8.2223,   7.6536,   6.1565,
           1.8816,   7.3315,  -1.0206,   9.0362,  -3.3918,  -6.0660,  -9.0042,
          -2.7077,   7.9294],
        [ 19.0706,  15.5120,   7.4969,  -6.8743, -11.8426, -11.7652, -18.3900,
         -14.8040,  -1.9720,   2.8877,   3.1347,  -4.3463,  -4.4885,   4.0600,
           1.6797,  13.3754],
        [  3.4614,   0.6209,  -2.1409,  -8.1830,  -7.2244, -15.2363, -25.1087,
         -21.2754, -12.7673,   4.1494,   1.8988,  -1.4308,  -6.4405,  -3.6762,
           5.4655,   8.3826],
        [ -4.3175,  -5.4606,  -6.6423,  -4.8761,  -4.3944,   2.1654,  -9.2158,
         -16.3070, -13.2204,  -2.4212,   0.2602,  -7.7324, -19.7512, -12.1054,
           0.8498,   7.8293],
        [-11.0903, -10.8104,  -8.9161, -19.7947, -15.0495,  -6.6725, -19.5877,
         -34.2309, -32.3366, -14.2890, -13.4045, -21.9382, -32.2567, -32.6098,
         -14.6862,  -2.0986]], device='cuda:0')
LAYER_NUM:  2
FINAL SUM LENNNN  1536
FINAL_SUM:  [108, 128, 268, 870, 1174, 1458, 97, 660, 1490, 1261]
acts len:  3
acts keys:  ['conv0', 'conv1', 'conv2']
acts:  {'conv0': [43, 29, 67, 55, 87, 26, 66, 91, 84, 46, 35, 37, 77, 15, 34, 4, 25, 5, 30, 75, 92, 11, 95, 56, 42, 49, 13, 53, 72, 40], 'conv1': [78, 77, 61, 359, 260, 63, 254, 291, 259, 64, 204, 225, 300, 49, 180, 99, 243, 112, 8, 19, 115, 330, 127, 352, 227, 157, 249, 294, 98, 362, 174, 147, 332, 151, 184, 324, 247, 66, 116, 32, 175, 158, 349, 46, 53, 210, 120, 242, 92, 121, 71, 374, 355, 235, 269, 251, 59, 119, 351, 290, 27, 245, 110, 156, 113, 270, 266, 87, 54, 280, 164, 218, 296, 57, 47, 69, 31, 40, 201, 107, 169, 5, 365, 303, 103, 72, 159, 101, 338, 207, 213, 125, 301, 333, 369, 382, 279, 311, 10, 376, 187, 18, 7, 356, 80, 30, 104, 191, 74, 93, 34, 284, 95, 126, 41, 161], 'conv2': [108, 128, 268, 870, 1174, 1458, 97, 660, 1490, 1261, 637, 205, 1371, 1383, 1082, 294, 1412, 927, 1483, 1005, 919, 1149, 1195, 1425, 168, 75, 350, 617, 301, 1409, 650, 931, 539, 1113, 503, 903, 332, 730, 557, 998, 1532, 1050, 1323, 1389, 191, 459, 607, 760, 1053, 737, 1030, 1387, 914, 964, 1110, 1239, 726, 269, 1284, 1066, 1021, 1222, 525, 1135, 1471, 1039, 627, 1154, 1016, 315, 303, 325, 1382, 537, 171, 956, 985, 1073, 193, 1213, 1052, 377, 1086, 1132, 1327, 882, 553, 1251, 451, 997, 887, 995, 1218, 765, 196, 576, 1500, 1411, 468, 843, 844, 556, 814, 417, 1484, 272, 1510, 1357, 1291, 369, 453, 501, 693, 996, 1464, 436, 727, 837, 1285, 507, 81, 554, 634, 662, 278, 1430, 55, 595, 388, 1310, 1141, 1157, 1189, 1370, 296, 1151, 127, 415, 832, 1265, 959, 252, 763, 1270, 705, 674, 320, 924, 626, 138, 710, 862, 786, 545, 442, 1155, 830, 605, 973, 1378, 1474, 1096, 1505, 519, 68, 1319, 427, 941, 1122, 347, 599, 1507, 611, 431, 630, 564, 1211, 71, 540, 866, 456, 888, 408, 1221, 238, 79, 971, 536, 817, 212, 226, 287, 666, 1121, 236, 979, 245, 61, 1124, 401, 267, 48, 1146, 256, 877, 217, 323, 474, 1352, 1094, 496, 1313, 1359, 606, 184, 133, 49, 847, 1354, 804, 987, 1435, 1273, 775, 1001, 533, 293, 1231, 567, 1184, 115, 1434, 1351, 905, 811, 410, 713, 873, 511, 87, 175, 558, 1015, 1176, 1197, 1305, 54, 1008, 99, 84, 1147, 965, 897, 247, 1393, 590, 812, 142, 691, 1044, 335, 1065, 874, 1257, 1205, 1112, 1263, 112, 1298, 1225, 461, 354, 1160, 430, 749, 1356, 840, 603, 1145, 1400, 7, 704, 88, 588, 111, 392, 479, 950, 992, 1106, 800, 355, 683, 1024, 15, 271, 513, 1404, 1416, 802, 1255, 1167, 1253, 179, 648, 22, 1201, 1076, 577, 711, 1331, 601, 810, 506, 994, 454, 823, 1200, 1343, 1085, 1440, 1441, 777, 119, 772, 53, 334, 1495, 1013, 930, 65, 1373, 25, 1111, 939, 4, 381, 422, 466, 952, 429, 685, 1185, 986, 934, 284, 1358, 1315, 946, 582, 613, 1230, 1456, 898, 531, 583, 1061, 1534, 318, 574, 505, 518, 682, 1196, 475, 35, 167, 274, 640, 1236, 689, 444, 747, 1514, 889, 676, 488, 1105, 1004, 1529, 1321, 170, 1349, 694, 426, 1497, 1136, 600, 945, 1057, 497, 1459, 295, 3, 1045, 1437, 746, 884, 1394, 517, 1390, 610, 638, 419, 910, 352, 261, 117, 200, 174, 1523, 773, 486, 484, 137, 593, 1040, 915, 1303, 1068, 759, 1214, 1279, 27, 1059, 482, 1506, 858, 1139, 928, 492, 366, 1530, 423, 856, 524, 393, 151, 47, 470, 314, 182, 433, 963, 165, 180, 94, 1248, 183, 741, 658, 1398, 855, 1028, 1194, 1060, 592, 736, 649, 701, 645, 1334]}
final_sum len:  1536
delta_weights INFO: 
NUM OF TRACKED COV LAYERS:  1
NUM OF TRACKED WEIGHTS CHANGES PER LAYER:  20
avg_deltas INFO:  <class 'dict'>
avg_deltas keys:  ['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight']
avg_deltas size:  3
num of averages for 2 layer:  torch.Size([96])
################################################
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 4.23e-02	time: 00:02:07	Acc_train 0.00	Acc_test 0.00	convergence: 1.80e+01	R1: 1	Info MB:0.000e+00/SB:0.000e+00/MW:3.700e-03/SW:3.261e-01/MR:1.900e+01/SR:2.552e+00/MeD:2.042e+00/MaD:1.800e+01/MW:0.424/MAW:0.576
|        0 |        1 |        2 |        3 |       4 |        5 |        6 |        7 |        8 |        9 |       10 |       11 |       12 |       13 |       14 |       15 |       16 |       17 |      18 |       19 |       20 |       21 |       22 |       23 |       24 |       25 |       26 |       27 |       28 |      29 |
|----------+----------+----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------|
|   0.0471 |   0.0385 |   0.0453 |   0.0444 |   0.039 |   0.0411 |   0.0451 |   0.0387 |   0.0389 |   0.0457 |   0.0417 |   0.0416 |   0.0337 |   0.0381 |   0.0437 |   0.0382 |   0.0455 |   0.0476 |   0.042 |   0.0423 |   0.0427 |   0.0407 |   0.0393 |   0.0443 |   0.0419 |   0.0366 |   0.0389 |   0.0446 |   0.0371 |   0.043 |
|  23.19   |  15.86   |  21.54   |  20.74   |  16.23  |  17.87   |  21.37   |  15.95   |  16.14   |  21.91   |  18.35   |  18.32   |  12.34   |  15.49   |  20.11   |  15.58   |  21.66   |  23.68   |  18.6   |  18.9    |  19.27   |  17.55   |  16.47   |  20.63   |  18.57   |  14.37   |  16.13   |  20.85   |  14.73   |  19.46  |
|   0.04   |   0.04   |   0.07   |   0.06   |   0.04  |   0.04   |   0.08   |   0.04   |   0.06   |   0.05   |   0.09   |   0.09   |   0.19   |   0.05   |   0.22   |   0.04   |   0.04   |   0.1    |   0.07  |   0.04   |   0.08   |   0.05   |   0.1    |   0.07   |   0.06   |   0.05   |   0.07   |   0.05   |   0.05   |   0.03  |
| nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     |
| nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     |
| nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_4C_CL/models
Current stored value:  [43, 29, 67, 55, 87] tensor([-0.2519,  0.0524,  0.0105,  0.0549, -0.4818], device='cuda:0')

 ********** Supervised learning of blocks [3] **********
SAVING FOLDER FOR SUP:  C10_4C_CL
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([3, 0, 1, 3, 1, 0, 0, 4, 4, 0, 4, 4, 1, 4, 0, 3, 0, 3, 4, 3])
[0, 1, 3, 4]
TARGETS AFTER CLEANER:  tensor([2, 0, 1, 2, 1, 0, 0, 3, 3, 0, 3, 3, 1, 3, 0, 2, 0, 2, 3, 2])
4000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 4000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([4, 1, 1, 3, 4, 3, 4, 3, 3, 4, 0, 0, 1, 3, 4, 0, 3, 3, 3, 1])
[0, 1, 3, 4]
TARGETS AFTER CLEANER:  tensor([3, 1, 1, 2, 3, 2, 3, 2, 2, 3, 0, 0, 1, 2, 3, 0, 2, 2, 2, 1])
20000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 20000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([4, 1, 1, 3, 4, 3, 4, 3, 3, 4, 0, 0, 1, 3, 4, 0, 3, 3, 3, 1])
[0, 1, 3, 4]
TARGETS AFTER CLEANER:  tensor([3, 1, 1, 2, 3, 2, 3, 2, 2, 3, 0, 0, 1, 2, 3, 0, 2, 2, 2, 1])
20000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 20000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: ToTensor()
------------------------
INDICES:  20000
IMAGE SIZE: torch.Size([64, 3, 32, 32])
Epoch: [1/50]	lr: 1.00e-03	time: 00:02:42	Loss_train 0.17846	Acc_train 77.64	/	Loss_test 0.00649	Acc_test 84.00
BLOCKS IN SUP:  [3]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_4C_CL/models
Current stored value:  [43, 29, 67, 55, 87] tensor([-0.2519,  0.0524,  0.0105,  0.0549, -0.4818], device='cuda:0')
Epoch: [10/50]	lr: 1.00e-03	time: 00:02:51	Loss_train 0.13282	Acc_train 87.20	/	Loss_test 0.01762	Acc_test 85.55
BLOCKS IN SUP:  [3]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_4C_CL/models
Current stored value:  [43, 29, 67, 55, 87] tensor([-0.2519,  0.0524,  0.0105,  0.0549, -0.4818], device='cuda:0')
Epoch: [20/50]	lr: 2.50e-04	time: 00:03:01	Loss_train 0.08976	Acc_train 91.66	/	Loss_test 0.00833	Acc_test 90.38
BLOCKS IN SUP:  [3]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_4C_CL/models
Current stored value:  [43, 29, 67, 55, 87] tensor([-0.2519,  0.0524,  0.0105,  0.0549, -0.4818], device='cuda:0')
Epoch: [30/50]	lr: 1.25e-04	time: 00:03:11	Loss_train 0.06494	Acc_train 93.06	/	Loss_test 0.00825	Acc_test 90.38
BLOCKS IN SUP:  [3]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_4C_CL/models
Current stored value:  [43, 29, 67, 55, 87] tensor([-0.2519,  0.0524,  0.0105,  0.0549, -0.4818], device='cuda:0')
Epoch: [40/50]	lr: 3.13e-05	time: 00:03:22	Loss_train 0.04881	Acc_train 94.00	/	Loss_test 0.00775	Acc_test 90.57
BLOCKS IN SUP:  [3]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_4C_CL/models
Current stored value:  [43, 29, 67, 55, 87] tensor([-0.2519,  0.0524,  0.0105,  0.0549, -0.4818], device='cuda:0')
Epoch: [50/50]	lr: 7.81e-06	time: 00:03:32	Loss_train 0.04636	Acc_train 94.16	/	Loss_test 0.00762	Acc_test 90.97
BLOCKS IN SUP:  [3]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_4C_CL/models
Current stored value:  [43, 29, 67, 55, 87] tensor([-0.2519,  0.0524,  0.0105,  0.0549, -0.4818], device='cuda:0')
RESULT:  {'train_loss': 0.046364881098270416, 'train_acc': 94.15749907493591, 'test_loss': 0.007617195602506399, 'test_acc': 90.9749984741211, 'convergence': 18.001705169677734, 'R1': 1, 'dataset_sup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 4, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 4, 'selected_classes': [0, 1, 3, 4]}, 'dataset_unsup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 4, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 4, 'selected_classes': [0, 1, 3, 4]}, 'train_config': {'t0': {'blocks': [0], 'mode': 'unsupervised', 'lr': 0.08, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't2': {'blocks': [1], 'mode': 'unsupervised', 'lr': 0.005, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't4': {'blocks': [2], 'mode': 'unsupervised', 'lr': 0.01, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't6': {'blocks': [3], 'mode': 'supervised', 'lr': 0.001, 'nb_epoch': 50, 'batch_size': 64, 'print_freq': 10}}}
IN R1:  {'R1': {'train_loss': 0.046364881098270416, 'train_acc': 94.15749907493591, 'test_loss': 0.007617195602506399, 'test_acc': 90.9749984741211, 'convergence': 18.001705169677734, 'R1': 1, 'dataset_sup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 4, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 4, 'selected_classes': [0, 1, 3, 4]}, 'dataset_unsup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 4, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 4, 'selected_classes': [0, 1, 3, 4]}, 'train_config': {'t0': {'blocks': [0], 'mode': 'unsupervised', 'lr': 0.08, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't2': {'blocks': [1], 'mode': 'unsupervised', 'lr': 0.005, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't4': {'blocks': [2], 'mode': 'unsupervised', 'lr': 0.01, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't6': {'blocks': [3], 'mode': 'supervised', 'lr': 0.001, 'nb_epoch': 50, 'batch_size': 64, 'print_freq': 10}}}}
SEED:  0
block 0, size : 96 16 16
range = 2.886751345948129
block 1, size : 384 8 8
range = 0.8505172717997146
block 2, size : 1536 4 4
range = 0.4252586358998573
range = 0.11048543456039805
The device used will be: 
True
cuda:0
PARAMSSSS IN LOAD:  {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 3, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.11048543456039805, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 4, 'in_channels': 24576, 'old_channels': 1536, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
Previously stored value:  [43, 29, 67, 55, 87] tensor([-0.2519,  0.0524,  0.0105,  0.0549, -0.4818], device='cuda:0')

 Model C10_4C_CL loaded successfuly with best perf



 ----- Architecture Block BatchNorm2dSK3962(5, 5)1.0reflect, number 0 -----
- BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK963842(3, 3)0.6499999761581421reflect, number 1 -----
- BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.4)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK38415362(3, 3)0.25reflect, number 2 -----
- BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 3 -----
- Flatten(start_dim=1, end_dim=-1)
- Dropout(p=0.5, inplace=False)
- Linear(in_features=24576, out_features=4, bias=True)
[Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=24576, out_features=4, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)]
LAYER NAME:  Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=24576, out_features=4, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)
LAYER CHILDREN:  [BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=1.4)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
  )
  (layer): Linear(in_features=24576, out_features=4, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)]
subsubl NAME:  Sequential(
  (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=1.4)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): Flatten(start_dim=1, end_dim=-1)
)
subsubl NAME:  Linear(in_features=24576, out_features=4, bias=True)
subsubl NAME:  Dropout(p=0.5, inplace=False)

 ********** Hebbian Unsupervised learning of blocks [0] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([8, 8, 9, 5, 9, 8, 5, 8, 9, 5, 2, 9, 5, 5, 9, 2, 9, 5, 5, 9])
[2, 5, 8, 9]
TARGETS AFTER CLEANER:  tensor([2, 2, 3, 1, 3, 2, 1, 2, 3, 1, 0, 3, 1, 1, 3, 0, 3, 1, 1, 3])
4000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 4000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([9, 9, 2, 8, 2, 9, 9, 9, 2, 2, 5, 9, 5, 2, 2, 2, 2, 9, 5, 9])
[2, 5, 8, 9]
TARGETS AFTER CLEANER:  tensor([3, 3, 0, 2, 0, 3, 3, 3, 0, 0, 1, 3, 1, 0, 0, 0, 0, 3, 1, 3])
20000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 20000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([9, 9, 2, 8, 2, 9, 9, 9, 2, 2, 5, 9, 5, 2, 2, 2, 2, 9, 5, 9])
[2, 5, 8, 9]
TARGETS AFTER CLEANER:  tensor([3, 3, 0, 2, 0, 3, 3, 3, 0, 0, 1, 3, 1, 0, 0, 0, 0, 3, 1, 3])
20000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 20000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: ToTensor()
------------------------
INDICES:  20000
IMAGE SIZE: torch.Size([10, 3, 32, 32])
SAVING FOLDER FOR UNSUP:  C10_4C_CL
DEPTH:  3
ACTIVATIONS SHAPE:  tensor([[ 4.8671e+02, -4.2169e+02, -5.6183e+02, -1.1110e+03, -1.2719e+03,
         -1.3450e+03, -1.3901e+03, -1.4284e+03, -1.3273e+03, -1.0963e+03,
         -9.0194e+02, -7.0750e+02, -6.8710e+02, -7.2697e+02, -7.7870e+02,
          1.3198e+02],
        [ 1.0136e+03,  1.3221e+02,  5.9090e+00, -5.3555e+02, -7.5060e+02,
         -9.1070e+02, -1.1000e+03, -1.1485e+03, -1.0005e+03, -8.3902e+02,
         -7.0070e+02, -4.6759e+02, -4.0688e+02, -3.2851e+02, -3.5469e+02,
          7.3478e+02],
        [ 1.4557e+03,  4.3517e+02,  3.9841e+02, -2.3332e+02, -3.4408e+02,
         -6.1627e+02, -7.2065e+02, -8.1568e+02, -7.8393e+02, -7.2394e+02,
         -5.7217e+02, -3.9353e+02, -3.1635e+02, -1.4230e+02, -1.0508e+02,
          9.1057e+02],
        [ 1.4956e+03,  4.9739e+02,  6.0234e+02, -1.6792e+00, -2.0741e+02,
         -3.3011e+02, -3.5024e+02, -5.3696e+02, -6.7733e+02, -6.3486e+02,
         -6.1739e+02, -4.0435e+02, -3.3742e+02, -1.2984e+02, -1.2742e+02,
          1.1115e+03],
        [ 1.5857e+03,  5.6215e+02,  6.9056e+02,  1.9106e+01, -7.8401e+01,
         -5.9733e+01,  4.9626e+01, -1.0101e+02, -3.8148e+02, -4.9827e+02,
         -5.2129e+02, -3.9421e+02, -2.9851e+02, -1.9564e+02, -1.0543e+02,
          1.3175e+03],
        [ 1.7077e+03,  6.5599e+02,  6.9050e+02,  1.0041e+01, -1.0884e+01,
          2.1134e+01,  1.7950e+02, -3.0409e+01, -3.4072e+02, -4.4753e+02,
         -4.7362e+02, -3.3010e+02, -2.0896e+02, -5.4385e+01,  8.5831e+01,
          1.4742e+03],
        [ 2.0283e+03,  8.9772e+02,  8.9739e+02,  1.8589e+02,  7.7897e+01,
          1.8942e+02,  2.9040e+02, -8.0103e+00, -1.9963e+02, -2.6634e+02,
         -2.9237e+02, -1.2711e+02, -1.9473e+01,  7.8601e+01,  1.3134e+02,
          1.7057e+03],
        [ 2.3746e+03,  1.1660e+03,  1.2521e+03,  3.8499e+02,  2.8782e+02,
          2.9565e+02,  3.4834e+02,  2.6900e+02,  3.9801e+01, -5.0323e+01,
         -4.1368e+01, -5.3723e+01,  8.4879e+01,  8.0485e+01,  2.2151e+02,
          1.8526e+03],
        [ 2.5735e+03,  1.3411e+03,  1.4421e+03,  5.4178e+02,  3.0457e+02,
          3.6251e+02,  5.0713e+02,  6.1153e+02,  3.1963e+02,  4.8253e+01,
          1.0773e+02,  1.6254e+02,  2.6692e+02,  1.9290e+02,  2.6911e+02,
          1.8325e+03],
        [ 2.5667e+03,  1.3647e+03,  1.4998e+03,  6.5879e+02,  3.1588e+02,
          4.9439e+02,  8.4658e+02,  1.0961e+03,  7.8343e+02,  4.8117e+02,
          3.4428e+02,  4.4263e+02,  4.8229e+02,  4.4340e+02,  1.8042e+02,
          1.7676e+03],
        [ 2.3259e+03,  1.2069e+03,  1.3666e+03,  6.2433e+02,  2.8378e+02,
          4.7503e+02,  8.6487e+02,  1.2290e+03,  1.1808e+03,  8.7863e+02,
          7.7725e+02,  5.7203e+02,  5.2306e+02,  3.6876e+02,  6.0578e+00,
          1.4127e+03],
        [ 1.8977e+03,  8.0219e+02,  1.0763e+03,  4.2102e+02,  2.1646e+02,
          4.3473e+02,  8.3793e+02,  1.1815e+03,  1.2591e+03,  9.8314e+02,
          7.9062e+02,  3.8294e+02,  1.8152e+02,  1.4302e+02, -2.0458e+02,
          9.4571e+02],
        [ 1.2991e+03,  3.8237e+02,  6.9678e+02,  2.4163e+02,  7.3168e+01,
          1.2793e+02,  4.4543e+02,  8.1321e+02,  9.2347e+02,  6.6833e+02,
          4.8728e+02, -3.3613e+00, -1.5697e+02, -1.7482e+02, -4.5000e+02,
          5.9040e+02],
        [ 7.2502e+02, -6.6962e+00,  2.6521e+02, -2.2820e+02, -2.7876e+02,
         -2.8009e+02, -1.1094e+02,  1.1294e+02,  3.4254e+01,  2.8026e+01,
         -9.0153e+01, -4.3620e+02, -6.1408e+02, -6.5019e+02, -7.9925e+02,
          1.6470e+02],
        [ 3.0521e+01, -5.3789e+02, -3.6182e+02, -7.8709e+02, -8.0047e+02,
         -8.3554e+02, -7.7205e+02, -6.7101e+02, -6.3423e+02, -6.8983e+02,
         -8.3312e+02, -1.0355e+03, -1.2586e+03, -1.2715e+03, -1.3050e+03,
         -3.9961e+02],
        [-5.8308e+02, -1.1560e+03, -1.0894e+03, -1.6798e+03, -1.7304e+03,
         -1.8263e+03, -1.9390e+03, -1.9421e+03, -1.8890e+03, -1.8543e+03,
         -1.7658e+03, -1.8644e+03, -1.9836e+03, -2.0000e+03, -1.9497e+03,
         -1.1709e+03]], device='cuda:0')
LAYER_NUM:  0
FINAL SUM LENNNN  96
FINAL_SUM:  [66, 25, 77, 43, 58, 74, 16, 28, 29, 55]
acts len:  3
acts keys:  ['conv0', 'conv1', 'conv2']
acts:  {'conv0': [66, 25, 77, 43, 58, 74, 16, 28, 29, 55, 41, 49, 86, 67, 84, 87, 19, 11, 5, 10, 30, 46, 92, 59, 70, 24, 34, 91, 80, 18], 'conv1': [78, 77, 61, 359, 260, 63, 254, 291, 259, 64, 204, 225, 300, 49, 180, 99, 243, 112, 8, 19, 115, 330, 127, 352, 227, 157, 249, 294, 98, 362, 174, 147, 332, 151, 184, 324, 247, 66, 116, 32, 175, 158, 349, 46, 53, 210, 120, 242, 92, 121, 71, 374, 355, 235, 269, 251, 59, 119, 351, 290, 27, 245, 110, 156, 113, 270, 266, 87, 54, 280, 164, 218, 296, 57, 47, 69, 31, 40, 201, 107, 169, 5, 365, 303, 103, 72, 159, 101, 338, 207, 213, 125, 301, 333, 369, 382, 279, 311, 10, 376, 187, 18, 7, 356, 80, 30, 104, 191, 74, 93, 34, 284, 95, 126, 41, 161], 'conv2': [108, 128, 268, 870, 1174, 1458, 97, 660, 1490, 1261, 637, 205, 1371, 1383, 1082, 294, 1412, 927, 1483, 1005, 919, 1149, 1195, 1425, 168, 75, 350, 617, 301, 1409, 650, 931, 539, 1113, 503, 903, 332, 730, 557, 998, 1532, 1050, 1323, 1389, 191, 459, 607, 760, 1053, 737, 1030, 1387, 914, 964, 1110, 1239, 726, 269, 1284, 1066, 1021, 1222, 525, 1135, 1471, 1039, 627, 1154, 1016, 315, 303, 325, 1382, 537, 171, 956, 985, 1073, 193, 1213, 1052, 377, 1086, 1132, 1327, 882, 553, 1251, 451, 997, 887, 995, 1218, 765, 196, 576, 1500, 1411, 468, 843, 844, 556, 814, 417, 1484, 272, 1510, 1357, 1291, 369, 453, 501, 693, 996, 1464, 436, 727, 837, 1285, 507, 81, 554, 634, 662, 278, 1430, 55, 595, 388, 1310, 1141, 1157, 1189, 1370, 296, 1151, 127, 415, 832, 1265, 959, 252, 763, 1270, 705, 674, 320, 924, 626, 138, 710, 862, 786, 545, 442, 1155, 830, 605, 973, 1378, 1474, 1096, 1505, 519, 68, 1319, 427, 941, 1122, 347, 599, 1507, 611, 431, 630, 564, 1211, 71, 540, 866, 456, 888, 408, 1221, 238, 79, 971, 536, 817, 212, 226, 287, 666, 1121, 236, 979, 245, 61, 1124, 401, 267, 48, 1146, 256, 877, 217, 323, 474, 1352, 1094, 496, 1313, 1359, 606, 184, 133, 49, 847, 1354, 804, 987, 1435, 1273, 775, 1001, 533, 293, 1231, 567, 1184, 115, 1434, 1351, 905, 811, 410, 713, 873, 511, 87, 175, 558, 1015, 1176, 1197, 1305, 54, 1008, 99, 84, 1147, 965, 897, 247, 1393, 590, 812, 142, 691, 1044, 335, 1065, 874, 1257, 1205, 1112, 1263, 112, 1298, 1225, 461, 354, 1160, 430, 749, 1356, 840, 603, 1145, 1400, 7, 704, 88, 588, 111, 392, 479, 950, 992, 1106, 800, 355, 683, 1024, 15, 271, 513, 1404, 1416, 802, 1255, 1167, 1253, 179, 648, 22, 1201, 1076, 577, 711, 1331, 601, 810, 506, 994, 454, 823, 1200, 1343, 1085, 1440, 1441, 777, 119, 772, 53, 334, 1495, 1013, 930, 65, 1373, 25, 1111, 939, 4, 381, 422, 466, 952, 429, 685, 1185, 986, 934, 284, 1358, 1315, 946, 582, 613, 1230, 1456, 898, 531, 583, 1061, 1534, 318, 574, 505, 518, 682, 1196, 475, 35, 167, 274, 640, 1236, 689, 444, 747, 1514, 889, 676, 488, 1105, 1004, 1529, 1321, 170, 1349, 694, 426, 1497, 1136, 600, 945, 1057, 497, 1459, 295, 3, 1045, 1437, 746, 884, 1394, 517, 1390, 610, 638, 419, 910, 352, 261, 117, 200, 174, 1523, 773, 486, 484, 137, 593, 1040, 915, 1303, 1068, 759, 1214, 1279, 27, 1059, 482, 1506, 858, 1139, 928, 492, 366, 1530, 423, 856, 524, 393, 151, 47, 470, 314, 182, 433, 963, 165, 180, 94, 1248, 183, 741, 658, 1398, 855, 1028, 1194, 1060, 592, 736, 649, 701, 645, 1334]}
final_sum len:  96
delta_weights INFO: 
NUM OF TRACKED COV LAYERS:  1
NUM OF TRACKED WEIGHTS CHANGES PER LAYER:  20
avg_deltas INFO:  <class 'dict'>
avg_deltas keys:  ['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight']
avg_deltas size:  3
num of averages for 0 layer:  torch.Size([96])
################################################
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 1.42e-01	time: 00:00:35	Acc_train 0.00	Acc_test 0.00	convergence: 2.40e+01	R1: 0	Info MB:0.000e+00/SB:0.000e+00/MW:9.026e-03/SW:5.631e-01/MR:4.567e+00/SR:1.719e+00/MeD:1.320e+00/MaD:4.712e+00/MW:0.591/MAW:0.409
|       0 |       1 |      2 |       3 |       4 |       5 |       6 |       7 |       8 |       9 |      10 |      11 |      12 |      13 |      14 |      15 |         16 |     17 |      18 |      19 |      20 |      21 |      22 |      23 |      24 |         25 |      26 |       27 |         28 |      29 |
|---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+------------+--------+---------+---------+---------+---------+---------+---------+---------+------------+---------+----------+------------+---------|
|   0.144 |   0.132 |   0.11 |   0.155 |   0.161 |   0.105 |   0.141 |   0.174 |   0.177 |   0.189 |   0.165 |   0.172 |   0.162 |   0.121 |   0.134 |   0.178 |   0.000344 |   0.17 |   0.177 |   0.186 |   0.161 |   0.143 |   0.153 |   0.183 |   0.191 |   0.000322 |   0.204 |   0.0439 |   0.000562 |   0.177 |
|   4.24  |   3.72  |   2.9  |   4.75  |   5.07  |   2.72  |   4.11  |   5.71  |   5.88  |   6.57  |   5.23  |   5.63  |   5.1   |   3.28  |   3.79  |   5.93  |   1        |   5.52 |   5.89  |   6.41  |   5.05  |   4.2   |   4.66  |   6.24  |   6.73  |   1        |   7.53  |   1.3    |   1        |   5.88  |
|   0.48  |   0.48  |   0.53 |   0.55  |   0.49  |   0.54  |   0.32  |   0.5   |   0.49  |   0.47  |   0.53  |   0.54  |   0.33  |   0.54  |   0.46  |   0.45  |  21.6      |   0.53 |   0.48  |   0.46  |   0.34  |   0.43  |   0.47  |   0.54  |   0.5   |  26.13     |   0.31  |   0.43   |   1.26     |   0.39  |
| nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan        | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan        | nan     | nan      | nan        | nan     |
| nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan        | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan        | nan     | nan      | nan        | nan     |
| nan     | nan     | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan        | nan    | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan        | nan     | nan      | nan        | nan     |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_4C_CL/models
Current stored value:  [66, 25, 77, 43, 58] tensor([-0.1711,  0.1258, -0.5707,  0.0942, -0.1394], device='cuda:0')

 ********** Hebbian Unsupervised learning of blocks [1] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([8, 8, 9, 5, 9, 8, 5, 8, 9, 5, 2, 9, 5, 5, 9, 2, 9, 5, 5, 9])
[2, 5, 8, 9]
TARGETS AFTER CLEANER:  tensor([2, 2, 3, 1, 3, 2, 1, 2, 3, 1, 0, 3, 1, 1, 3, 0, 3, 1, 1, 3])
4000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 4000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([9, 9, 2, 8, 2, 9, 9, 9, 2, 2, 5, 9, 5, 2, 2, 2, 2, 9, 5, 9])
[2, 5, 8, 9]
TARGETS AFTER CLEANER:  tensor([3, 3, 0, 2, 0, 3, 3, 3, 0, 0, 1, 3, 1, 0, 0, 0, 0, 3, 1, 3])
20000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 20000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([9, 9, 2, 8, 2, 9, 9, 9, 2, 2, 5, 9, 5, 2, 2, 2, 2, 9, 5, 9])
[2, 5, 8, 9]
TARGETS AFTER CLEANER:  tensor([3, 3, 0, 2, 0, 3, 3, 3, 0, 0, 1, 3, 1, 0, 0, 0, 0, 3, 1, 3])
20000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 20000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: ToTensor()
------------------------
INDICES:  20000
IMAGE SIZE: torch.Size([10, 3, 32, 32])
SAVING FOLDER FOR UNSUP:  C10_4C_CL
DEPTH:  3
ACTIVATIONS SHAPE:  tensor([[-19.0913, -17.5778, -17.0418, -20.7014,  -5.7380,   1.7833, -18.9365,
         -27.1295, -22.5587, -28.8760, -30.9593, -24.8747,  -2.2874,  22.3807,
           2.3114,  20.8828],
        [ 27.6188,  20.2385,  26.2904,  18.3446,  33.7465,  33.5219,  -7.7264,
         -17.7288, -10.7568, -18.0720,  -4.1064,  -0.6973,  13.4525,  42.7137,
          23.3662,  42.5807],
        [ 39.9607,  25.9935,  28.0288,  19.9573,  26.7739,  40.4724,   5.0525,
           2.1109,   3.6564,   5.2410,   7.1666,   7.8946,  26.8531,  56.5774,
          36.1286,  43.6557],
        [ 34.3275,  27.1000,  31.4539,  29.2389,  41.6887,  48.3123,  18.3891,
          -0.2136,   3.2016,  12.9726,   7.9486,   1.7731,  16.4942,  44.4036,
          23.8666,  33.9507],
        [ 32.2427,  24.3444,  36.4296,  40.8423,  53.6000,  50.3071,  27.5633,
           6.6025,  -4.6533,   3.8952,   8.2017,   6.8393,  10.7121,  37.5985,
          22.5609,  37.2266],
        [ 36.9122,  27.0216,  33.4300,  38.3270,  42.8380,  47.9090,  26.9623,
          16.4811,  15.5668,  19.6120,  29.3438,  18.6636,  14.6003,  27.7704,
           6.2405,  30.5647],
        [ 45.7269,  37.7191,  33.7730,  29.5774,  26.7853,  26.4755,  26.8411,
          23.9007,  30.3030,  46.7426,  60.7583,  37.3546,  15.1950,  14.5909,
          -8.7511,  18.4801],
        [ 47.6863,  37.7439,  35.1479,  21.5623,  24.1208,  30.1824,  34.4059,
          33.2560,  34.4711,  54.1787,  65.6204,  54.6396,  23.4746,  -3.1455,
         -18.4969,   3.0765],
        [ 31.0615,  28.7846,  28.3588,  23.0857,  13.8503,  22.1367,  22.1652,
          37.4668,  28.7924,  36.1023,  38.0585,  40.7081,  27.8679,   4.4449,
         -10.5459,   3.6304],
        [ 38.9628,  42.0147,  28.8404,  13.4824,  15.5209,  17.5938,  10.5525,
          22.4671,  16.8862,  16.1423,  18.7621,  36.4846,  20.9209,  13.7391,
           3.1996,  14.9775],
        [ 54.9721,  55.7732,  32.5639,   5.1089,   8.2512,  17.3791,  41.8220,
          39.5062,  33.0297,  29.1945,  30.2029,  33.2777,  14.3146,  14.8055,
           6.3896,  35.0638],
        [ 49.7980,  40.1407,  26.6343,  -2.5710,  -2.7064,   9.5477,  36.1482,
          39.3823,  37.4989,  32.4770,  40.9089,  19.8536,   6.6339,  18.6470,
          17.4840,  39.1019],
        [ 18.8381,  16.3156,  13.2647, -11.9445, -12.2738,   4.8409,  22.7756,
          20.3119,  29.0452,  35.3527,  35.8813,  21.4290,   9.5836,  17.6705,
          17.5590,  28.4052],
        [  0.6660,   3.4690,   2.9645, -11.8311, -24.2615,  -1.4982,  27.7270,
          26.0989,  23.2478,  23.8225,  20.8263,   9.7968,   2.3850,  13.3220,
          14.8145,  28.2294],
        [-20.1902, -15.8049, -19.3179, -23.5015, -22.6265,  11.4580,  22.7956,
          28.0234,  20.8230,  10.0787,   8.2855,   6.4592,   3.3713,  10.5285,
           7.4427,  22.3784],
        [-31.1811, -31.3580, -31.2467, -35.9839, -34.0621,  -3.5596,  13.6466,
          13.6166,  10.8304,  -0.7603,  -6.5105, -10.2267, -10.1040,  13.0819,
          17.0452,  28.4060]], device='cuda:0')
LAYER_NUM:  1
FINAL SUM LENNNN  384
FINAL_SUM:  [77, 330, 351, 158, 59, 99, 174, 282, 243, 294]
acts len:  3
acts keys:  ['conv0', 'conv1', 'conv2']
acts:  {'conv0': [66, 25, 77, 43, 58, 74, 16, 28, 29, 55, 41, 49, 86, 67, 84, 87, 19, 11, 5, 10, 30, 46, 92, 59, 70, 24, 34, 91, 80, 18], 'conv1': [77, 330, 351, 158, 59, 99, 174, 282, 243, 294, 71, 260, 254, 13, 175, 321, 247, 46, 74, 67, 41, 64, 8, 66, 204, 270, 359, 205, 257, 19, 116, 78, 259, 251, 346, 370, 276, 147, 12, 92, 286, 324, 179, 300, 61, 333, 249, 110, 93, 197, 215, 164, 317, 310, 362, 119, 86, 322, 63, 40, 69, 70, 30, 265, 372, 49, 217, 184, 207, 171, 227, 118, 121, 153, 27, 113, 180, 355, 129, 361, 311, 80, 290, 58, 209, 25, 2, 177, 194, 277, 368, 376, 214, 54, 157, 382, 151, 349, 107, 280, 115, 101, 22, 141, 145, 285, 75, 172, 213, 352, 220, 87, 26, 125, 123, 369], 'conv2': [108, 128, 268, 870, 1174, 1458, 97, 660, 1490, 1261, 637, 205, 1371, 1383, 1082, 294, 1412, 927, 1483, 1005, 919, 1149, 1195, 1425, 168, 75, 350, 617, 301, 1409, 650, 931, 539, 1113, 503, 903, 332, 730, 557, 998, 1532, 1050, 1323, 1389, 191, 459, 607, 760, 1053, 737, 1030, 1387, 914, 964, 1110, 1239, 726, 269, 1284, 1066, 1021, 1222, 525, 1135, 1471, 1039, 627, 1154, 1016, 315, 303, 325, 1382, 537, 171, 956, 985, 1073, 193, 1213, 1052, 377, 1086, 1132, 1327, 882, 553, 1251, 451, 997, 887, 995, 1218, 765, 196, 576, 1500, 1411, 468, 843, 844, 556, 814, 417, 1484, 272, 1510, 1357, 1291, 369, 453, 501, 693, 996, 1464, 436, 727, 837, 1285, 507, 81, 554, 634, 662, 278, 1430, 55, 595, 388, 1310, 1141, 1157, 1189, 1370, 296, 1151, 127, 415, 832, 1265, 959, 252, 763, 1270, 705, 674, 320, 924, 626, 138, 710, 862, 786, 545, 442, 1155, 830, 605, 973, 1378, 1474, 1096, 1505, 519, 68, 1319, 427, 941, 1122, 347, 599, 1507, 611, 431, 630, 564, 1211, 71, 540, 866, 456, 888, 408, 1221, 238, 79, 971, 536, 817, 212, 226, 287, 666, 1121, 236, 979, 245, 61, 1124, 401, 267, 48, 1146, 256, 877, 217, 323, 474, 1352, 1094, 496, 1313, 1359, 606, 184, 133, 49, 847, 1354, 804, 987, 1435, 1273, 775, 1001, 533, 293, 1231, 567, 1184, 115, 1434, 1351, 905, 811, 410, 713, 873, 511, 87, 175, 558, 1015, 1176, 1197, 1305, 54, 1008, 99, 84, 1147, 965, 897, 247, 1393, 590, 812, 142, 691, 1044, 335, 1065, 874, 1257, 1205, 1112, 1263, 112, 1298, 1225, 461, 354, 1160, 430, 749, 1356, 840, 603, 1145, 1400, 7, 704, 88, 588, 111, 392, 479, 950, 992, 1106, 800, 355, 683, 1024, 15, 271, 513, 1404, 1416, 802, 1255, 1167, 1253, 179, 648, 22, 1201, 1076, 577, 711, 1331, 601, 810, 506, 994, 454, 823, 1200, 1343, 1085, 1440, 1441, 777, 119, 772, 53, 334, 1495, 1013, 930, 65, 1373, 25, 1111, 939, 4, 381, 422, 466, 952, 429, 685, 1185, 986, 934, 284, 1358, 1315, 946, 582, 613, 1230, 1456, 898, 531, 583, 1061, 1534, 318, 574, 505, 518, 682, 1196, 475, 35, 167, 274, 640, 1236, 689, 444, 747, 1514, 889, 676, 488, 1105, 1004, 1529, 1321, 170, 1349, 694, 426, 1497, 1136, 600, 945, 1057, 497, 1459, 295, 3, 1045, 1437, 746, 884, 1394, 517, 1390, 610, 638, 419, 910, 352, 261, 117, 200, 174, 1523, 773, 486, 484, 137, 593, 1040, 915, 1303, 1068, 759, 1214, 1279, 27, 1059, 482, 1506, 858, 1139, 928, 492, 366, 1530, 423, 856, 524, 393, 151, 47, 470, 314, 182, 433, 963, 165, 180, 94, 1248, 183, 741, 658, 1398, 855, 1028, 1194, 1060, 592, 736, 649, 701, 645, 1334]}
final_sum len:  384
delta_weights INFO: 
NUM OF TRACKED COV LAYERS:  1
NUM OF TRACKED WEIGHTS CHANGES PER LAYER:  20
avg_deltas INFO:  <class 'dict'>
avg_deltas keys:  ['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight']
avg_deltas size:  3
num of averages for 1 layer:  torch.Size([96])
################################################
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 1.42e-01	time: 00:01:09	Acc_train 0.00	Acc_test 0.00	convergence: 2.40e+01	R1: 0	Info MB:0.000e+00/SB:0.000e+00/MW:5.194e-03/SW:1.757e-01/MR:4.935e+00/SR:1.536e+00/MeD:1.224e+00/MaD:3.935e+00/MW:0.592/MAW:0.408
|       0 |       1 |        2 |        3 |         4 |        5 |         6 |        7 |         8 |         9 |       10 |        11 |       12 |       13 |       14 |     15 |        16 |        17 |        18 |       19 |       20 |      21 |        22 |       23 |         24 |       25 |       26 |        27 |       28 |       29 |
|---------+---------+----------+----------+-----------+----------+-----------+----------+-----------+-----------+----------+-----------+----------+----------+----------+--------+-----------+-----------+-----------+----------+----------+---------+-----------+----------+------------+----------+----------+-----------+----------+----------|
|   0.008 |   0.011 |   0.0105 |   0.0031 |   0.00152 |   0.0109 |   0.00595 |   0.0102 |   0.00963 |   0.00891 |   0.0125 |   0.00528 |   0.0115 |   0.0118 |   0.0063 |   0.01 |   0.00937 |   0.00945 |   0.00959 |   0.0113 |   0.0116 |   0.011 |   0.00973 |   0.0105 |   2.47e-05 |   0.0113 |   0.0093 |   0.00949 |   0.0127 |   0.0108 |
|   3.56  |   5.87  |   5.43   |   1.38   |   1.09    |   5.74   |   2.42    |   5.16   |   4.71    |   4.17    |   7.26   |   2.11    |   6.31   |   6.61   |   2.59   |   5    |   4.51    |   4.57    |   4.68    |   6.15   |   6.39   |   5.81  |   4.78    |   5.43   |   1        |   6.08   |   4.46   |   4.6     |   7.42   |   5.7    |
|   0.14  |   0.15  |   0.21   |   0.27   |   0.4     |   0.16   |   0.08    |   0.12   |   0.08    |   0.16    |   0.15   |   0.14    |   0.16   |   0.2    |   0.16   |   0.15 |   0.14    |   0.1     |   0.11    |   0.13   |   0.2    |   0.11  |   0.19    |   0.19   |  11.64     |   0.19   |   0.12   |   0.12    |   0.15   |   0.15   |
| nan     | nan     | nan      | nan      | nan       | nan      | nan       | nan      | nan       | nan       | nan      | nan       | nan      | nan      | nan      | nan    | nan       | nan       | nan       | nan      | nan      | nan     | nan       | nan      | nan        | nan      | nan      | nan       | nan      | nan      |
| nan     | nan     | nan      | nan      | nan       | nan      | nan       | nan      | nan       | nan       | nan      | nan       | nan      | nan      | nan      | nan    | nan       | nan       | nan       | nan      | nan      | nan     | nan       | nan      | nan        | nan      | nan      | nan       | nan      | nan      |
| nan     | nan     | nan      | nan      | nan       | nan      | nan       | nan      | nan       | nan       | nan      | nan       | nan      | nan      | nan      | nan    | nan       | nan       | nan       | nan      | nan      | nan     | nan       | nan      | nan        | nan      | nan      | nan       | nan      | nan      |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_4C_CL/models
Current stored value:  [66, 25, 77, 43, 58] tensor([-0.1711,  0.1258, -0.5707,  0.0942, -0.1394], device='cuda:0')

 ********** Hebbian Unsupervised learning of blocks [2] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([8, 8, 9, 5, 9, 8, 5, 8, 9, 5, 2, 9, 5, 5, 9, 2, 9, 5, 5, 9])
[2, 5, 8, 9]
TARGETS AFTER CLEANER:  tensor([2, 2, 3, 1, 3, 2, 1, 2, 3, 1, 0, 3, 1, 1, 3, 0, 3, 1, 1, 3])
4000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 4000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([9, 9, 2, 8, 2, 9, 9, 9, 2, 2, 5, 9, 5, 2, 2, 2, 2, 9, 5, 9])
[2, 5, 8, 9]
TARGETS AFTER CLEANER:  tensor([3, 3, 0, 2, 0, 3, 3, 3, 0, 0, 1, 3, 1, 0, 0, 0, 0, 3, 1, 3])
20000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 20000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([9, 9, 2, 8, 2, 9, 9, 9, 2, 2, 5, 9, 5, 2, 2, 2, 2, 9, 5, 9])
[2, 5, 8, 9]
TARGETS AFTER CLEANER:  tensor([3, 3, 0, 2, 0, 3, 3, 3, 0, 0, 1, 3, 1, 0, 0, 0, 0, 3, 1, 3])
20000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 20000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: ToTensor()
------------------------
INDICES:  20000
IMAGE SIZE: torch.Size([10, 3, 32, 32])
SAVING FOLDER FOR UNSUP:  C10_4C_CL
DEPTH:  3
ACTIVATIONS SHAPE:  tensor([[-12.5533, -12.4937,  -6.9092, -12.4866,  -8.0424,  -2.2308, -14.6431,
         -23.0207, -18.6152, -25.4857, -30.8595, -27.5383, -14.2015,   8.6185,
          -9.8695,  -7.1518],
        [ 17.4464,  11.5531,  20.6914,  13.7777,  18.8271,  17.7092,  -8.8243,
         -19.1595, -13.9489, -21.9513, -15.9194, -12.1592,  -5.9923,  19.0569,
           1.7689,   6.4656],
        [ 23.5311,  13.5169,  17.5331,   9.9381,   9.2762,  19.0883,  -1.6968,
          -5.6065,  -4.9040,  -8.1386,  -8.4887,  -5.7392,   2.0083,  26.5910,
           8.9255,   8.4597],
        [ 15.5404,   9.4008,  14.0300,   9.4183,  12.3837,  16.3312,  -0.9514,
         -13.1565,  -9.9281,  -6.6922, -10.4098, -10.8164,  -7.7614,  13.0315,
          -4.5641,   1.6454],
        [ 13.0781,   7.0419,  18.1167,  18.6191,  20.6504,  15.7105,   2.5632,
          -9.6430, -17.6603, -16.5896, -12.3502,  -9.1092, -12.6969,   7.7110,
          -4.4214,   3.9085],
        [ 13.5330,   6.2537,  14.6573,  18.6096,  14.7111,  15.1518,   2.6039,
           0.5701,  -0.1114,  -2.4438,   2.3394,  -4.5813, -11.4970,   2.2116,
         -14.3655,  -2.9128],
        [ 18.5964,  12.4316,  13.2814,  11.1462,   4.2480,   0.5877,   4.7705,
           8.3635,  13.3612,  18.4187,  24.5372,   4.9132, -10.9139,  -6.6313,
         -27.0375, -16.3070],
        [ 17.0102,   8.8886,  10.7023,   2.1730,   0.9434,   4.4097,  10.0580,
          15.5477,  17.0504,  25.6309,  27.9700,  15.3166,  -4.6495, -19.5802,
         -33.4409, -28.5790],
        [  7.0931,   3.5216,   6.7130,   2.3685,  -4.3271,   2.0469,   3.0476,
          17.8666,  12.6409,  15.2759,  11.7290,   8.4733,   1.7039, -14.8699,
         -28.4313, -26.4457],
        [ 14.1577,  13.2746,   7.0086,  -6.1248,  -1.6253,   0.1762,  -5.9519,
           5.2001,   1.9262,   0.6459,  -0.9776,   9.0516,  -1.8178,  -8.7939,
         -19.9322, -15.6110],
        [ 24.5940,  22.5955,   9.9076,  -9.9708,  -4.3439,   2.7665,  16.4496,
          17.4233,   9.7597,   5.0416,   3.8984,   5.4845,  -7.8988,  -8.9700,
         -19.3436,   0.5485],
        [ 20.8869,  11.7590,   5.0711, -13.7404, -10.5164,  -0.6367,  13.9372,
          16.4993,  12.0080,   5.8819,  12.2483,  -2.3731, -11.4443,  -2.5485,
          -7.6983,   7.3778],
        [  2.9465,  -1.1662,  -2.9823, -18.8781, -16.2735,  -4.9133,   7.7818,
           5.3456,   8.0628,   7.9053,   7.5202,  -0.7558,  -8.7561,  -1.6570,
          -5.6859,  -0.7436],
        [ -3.5129,  -3.3705,  -2.9098, -10.3726, -18.4919,  -4.4498,  14.2022,
          10.0205,   5.8985,   2.0834,  -2.2421,  -9.3599, -12.8985,  -2.3011,
          -4.9853,  -0.4029],
        [-14.8028, -13.3626, -12.8285, -14.0271, -12.7510,   8.2623,  13.2480,
          10.9677,   2.6900,  -7.1554,  -8.3382,  -9.6460,  -9.5172,  -3.4500,
         -10.0403,  -2.9046],
        [-25.9603, -26.5832, -22.3787, -22.6871, -21.0585,  -1.7442,   8.1809,
           1.0674,  -5.1584, -14.8178, -18.7688, -22.0695, -17.6290,   0.7566,
          -1.4642,   3.4502]], device='cuda:0')
LAYER_NUM:  2
FINAL SUM LENNNN  1536
FINAL_SUM:  [870, 128, 108, 903, 1261, 1483, 924, 1110, 287, 1149]
acts len:  3
acts keys:  ['conv0', 'conv1', 'conv2']
acts:  {'conv0': [66, 25, 77, 43, 58, 74, 16, 28, 29, 55, 41, 49, 86, 67, 84, 87, 19, 11, 5, 10, 30, 46, 92, 59, 70, 24, 34, 91, 80, 18], 'conv1': [77, 330, 351, 158, 59, 99, 174, 282, 243, 294, 71, 260, 254, 13, 175, 321, 247, 46, 74, 67, 41, 64, 8, 66, 204, 270, 359, 205, 257, 19, 116, 78, 259, 251, 346, 370, 276, 147, 12, 92, 286, 324, 179, 300, 61, 333, 249, 110, 93, 197, 215, 164, 317, 310, 362, 119, 86, 322, 63, 40, 69, 70, 30, 265, 372, 49, 217, 184, 207, 171, 227, 118, 121, 153, 27, 113, 180, 355, 129, 361, 311, 80, 290, 58, 209, 25, 2, 177, 194, 277, 368, 376, 214, 54, 157, 382, 151, 349, 107, 280, 115, 101, 22, 141, 145, 285, 75, 172, 213, 352, 220, 87, 26, 125, 123, 369], 'conv2': [870, 128, 108, 903, 1261, 1483, 924, 1110, 287, 1149, 607, 97, 168, 315, 205, 760, 459, 832, 660, 1005, 1352, 1284, 1412, 1082, 637, 1174, 997, 507, 1383, 350, 294, 525, 436, 191, 1425, 567, 905, 1458, 931, 1371, 726, 1222, 1409, 1113, 1016, 557, 927, 914, 545, 564, 1195, 1490, 1160, 456, 919, 1323, 127, 737, 599, 269, 1270, 539, 956, 193, 650, 501, 1382, 617, 79, 1357, 171, 1464, 22, 1533, 998, 837, 1218, 556, 996, 1257, 1428, 1030, 303, 442, 629, 415, 55, 1154, 479, 627, 1189, 1285, 674, 1086, 847, 1015, 537, 1112, 1310, 1532, 229, 369, 786, 325, 649, 112, 272, 296, 662, 1141, 1411, 93, 1351, 61, 882, 1053, 278, 855, 410, 554, 117, 868, 200, 1039, 880, 945, 800, 1471, 1484, 184, 503, 611, 536, 1239, 1434, 749, 268, 1052, 71, 897, 1155, 676, 605, 301, 468, 165, 887, 1050, 840, 648, 1291, 730, 877, 1184, 1437, 540, 1132, 427, 81, 765, 626, 87, 47, 1354, 1358, 1065, 354, 1135, 422, 727, 1124, 773, 553, 453, 1273, 1327, 606, 320, 814, 1213, 54, 417, 1249, 595, 149, 985, 196, 274, 939, 88, 1359, 137, 238, 1505, 232, 1251, 142, 1529, 1221, 946, 830, 812, 964, 866, 965, 1122, 75, 959, 53, 577, 527, 1157, 1298, 804, 1514, 461, 888, 666, 394, 1370, 49, 1121, 811, 451, 1040, 454, 1312, 808, 388, 1400, 1196, 1393, 843, 1313, 970, 1068, 634, 378, 401, 1389, 48, 1231, 496, 603, 1510, 763, 261, 484, 1024, 950, 1167, 236, 1028, 574, 180, 466, 1387, 1211, 252, 513, 444, 1021, 1013, 94, 1001, 1465, 1145, 941, 408, 682, 693, 267, 486, 381, 371, 994, 247, 392, 583, 630, 1343, 1061, 747, 1474, 259, 772, 711, 1440, 68, 1453, 69, 1520, 645, 713, 280, 286, 736, 590, 922, 973, 1305, 1147, 332, 138, 1495, 1404, 1268, 705, 1066, 987, 1045, 497, 1214, 610, 842, 506, 1019, 1523, 810, 1253, 387, 1146, 592, 1534, 377, 563, 715, 685, 1151, 1265, 1332, 519, 170, 1260, 314, 818, 35, 217, 1106, 1148, 819, 1096, 84, 463, 1200, 257, 1137, 119, 199, 293, 31, 430, 1500, 1236, 751, 1142, 1178, 1041, 1114, 904, 1255, 1073, 995, 160, 302, 782, 1208, 689, 1152, 250, 284, 335, 437, 15, 928, 1263, 722, 1345, 971, 1470, 1044, 1306, 844, 862, 505, 1136, 1373, 671, 34, 874, 283, 426, 520, 884, 183, 979, 738, 704, 182, 405, 474, 1375, 131, 202, 609, 60, 533, 489, 502, 162, 691, 652, 848, 817, 323, 368, 8, 218, 612, 746, 63, 579, 384, 1349, 1225, 431, 1099, 1356, 1507, 1501, 1193, 470, 1008, 1307, 423, 1435, 952, 228, 728, 1057, 1144, 1201, 1188, 1493, 524, 643, 1439, 326, 1098, 1497, 1530, 1043, 20, 1325, 873, 558, 602, 743, 133, 856, 1524, 1319]}
final_sum len:  1536
delta_weights INFO: 
NUM OF TRACKED COV LAYERS:  1
NUM OF TRACKED WEIGHTS CHANGES PER LAYER:  20
avg_deltas INFO:  <class 'dict'>
avg_deltas keys:  ['blocks.0.layer.weight', 'blocks.1.layer.weight', 'blocks.2.layer.weight']
avg_deltas size:  3
num of averages for 2 layer:  torch.Size([96])
################################################
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 4.08e-02	time: 00:01:46	Acc_train 0.00	Acc_test 0.00	convergence: 1.67e+01	R1: 1	Info MB:0.000e+00/SB:0.000e+00/MW:8.175e-03/SW:3.031e-01/MR:1.771e+01/SR:2.013e+00/MeD:1.581e+00/MaD:1.671e+01/MW:0.443/MAW:0.557
|        0 |        1 |        2 |       3 |        4 |        5 |        6 |        7 |        8 |        9 |       10 |       11 |       12 |       13 |       14 |       15 |       16 |       17 |      18 |      19 |       20 |       21 |       22 |       23 |       24 |       25 |       26 |       27 |       28 |       29 |
|----------+----------+----------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+---------+---------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------|
|   0.0437 |   0.0416 |   0.0431 |   0.042 |   0.0412 |   0.0419 |   0.0422 |   0.0384 |   0.0374 |   0.0426 |   0.0411 |   0.0381 |   0.0361 |   0.0404 |   0.0387 |   0.0409 |   0.0432 |   0.0445 |   0.041 |   0.043 |   0.0439 |   0.0368 |   0.0371 |   0.0427 |   0.0392 |   0.0393 |   0.0368 |   0.0412 |   0.0392 |   0.0439 |
|  20.1    |  18.27   |  19.55   |  18.63  |  17.99   |  18.54   |  18.79   |  15.73   |  15.01   |  19.16   |  17.85   |  15.51   |  14.03   |  17.32   |  16.01   |  17.69   |  19.69   |  20.8    |  17.84  |  19.45  |  20.26   |  14.53   |  14.76   |  19.25   |  16.34   |  16.46   |  14.54   |  17.96   |  16.37   |  20.3    |
|   0.05   |   0.02   |   0.06   |   0.05  |   0.04   |   0.04   |   0.07   |   0.02   |   0.03   |   0.08   |   0.07   |   0.06   |   0.08   |   0.04   |   0.08   |   0.04   |   0.07   |   0.08   |   0.06  |   0.04  |   0.08   |   0.06   |   0.04   |   0.06   |   0.06   |   0.05   |   0.07   |   0.06   |   0.06   |   0.05   |
| nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |
| nan      | nan      | nan      | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan     | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_4C_CL/models
Current stored value:  [66, 25, 77, 43, 58] tensor([-0.1711,  0.1258, -0.5707,  0.0942, -0.1394], device='cuda:0')

 ********** Supervised learning of blocks [3] **********
SAVING FOLDER FOR SUP:  C10_4C_CL
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([8, 8, 9, 5, 9, 8, 5, 8, 9, 5, 2, 9, 5, 5, 9, 2, 9, 5, 5, 9])
[2, 5, 8, 9]
TARGETS AFTER CLEANER:  tensor([2, 2, 3, 1, 3, 2, 1, 2, 3, 1, 0, 3, 1, 1, 3, 0, 3, 1, 1, 3])
4000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 4000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([9, 9, 2, 8, 2, 9, 9, 9, 2, 2, 5, 9, 5, 2, 2, 2, 2, 9, 5, 9])
[2, 5, 8, 9]
TARGETS AFTER CLEANER:  tensor([3, 3, 0, 2, 0, 3, 3, 3, 0, 0, 1, 3, 1, 0, 0, 0, 0, 3, 1, 3])
20000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 20000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([9, 9, 2, 8, 2, 9, 9, 9, 2, 2, 5, 9, 5, 2, 2, 2, 2, 9, 5, 9])
[2, 5, 8, 9]
TARGETS AFTER CLEANER:  tensor([3, 3, 0, 2, 0, 3, 3, 3, 0, 0, 1, 3, 1, 0, 0, 0, 0, 3, 1, 3])
20000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 20000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: ToTensor()
------------------------
INDICES:  20000
IMAGE SIZE: torch.Size([64, 3, 32, 32])
Epoch: [1/50]	lr: 1.00e-03	time: 00:02:17	Loss_train 0.57210	Acc_train 72.42	/	Loss_test 0.01796	Acc_test 81.05
BLOCKS IN SUP:  [3]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_4C_CL/models
Current stored value:  [66, 25, 77, 43, 58] tensor([-0.1711,  0.1258, -0.5707,  0.0942, -0.1394], device='cuda:0')
Epoch: [10/50]	lr: 1.00e-03	time: 00:02:27	Loss_train 0.17814	Acc_train 87.80	/	Loss_test 0.01244	Acc_test 88.57
BLOCKS IN SUP:  [3]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_4C_CL/models
Current stored value:  [66, 25, 77, 43, 58] tensor([-0.1711,  0.1258, -0.5707,  0.0942, -0.1394], device='cuda:0')
Epoch: [20/50]	lr: 2.50e-04	time: 00:02:37	Loss_train 0.08567	Acc_train 92.82	/	Loss_test 0.00986	Acc_test 90.82
BLOCKS IN SUP:  [3]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_4C_CL/models
Current stored value:  [66, 25, 77, 43, 58] tensor([-0.1711,  0.1258, -0.5707,  0.0942, -0.1394], device='cuda:0')
Epoch: [30/50]	lr: 1.25e-04	time: 00:02:47	Loss_train 0.06043	Acc_train 94.13	/	Loss_test 0.00820	Acc_test 91.38
BLOCKS IN SUP:  [3]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_4C_CL/models
Current stored value:  [66, 25, 77, 43, 58] tensor([-0.1711,  0.1258, -0.5707,  0.0942, -0.1394], device='cuda:0')
Epoch: [40/50]	lr: 3.13e-05	time: 00:02:57	Loss_train 0.04925	Acc_train 94.74	/	Loss_test 0.00761	Acc_test 91.90
BLOCKS IN SUP:  [3]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_4C_CL/models
Current stored value:  [66, 25, 77, 43, 58] tensor([-0.1711,  0.1258, -0.5707,  0.0942, -0.1394], device='cuda:0')
Epoch: [50/50]	lr: 7.81e-06	time: 00:03:07	Loss_train 0.04549	Acc_train 94.96	/	Loss_test 0.00752	Acc_test 91.70
BLOCKS IN SUP:  [3]
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_4C_CL/models
Current stored value:  [66, 25, 77, 43, 58] tensor([-0.1711,  0.1258, -0.5707,  0.0942, -0.1394], device='cuda:0')
RESULT:  {'train_loss': 0.045486729592084885, 'train_acc': 94.95999813079834, 'test_loss': 0.007523290812969208, 'test_acc': 91.69999694824219, 'convergence': 16.710285186767578, 'R1': 1, 'dataset_sup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 4, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 4, 'selected_classes': [2, 5, 8, 9]}, 'dataset_unsup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 4, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 4, 'selected_classes': [2, 5, 8, 9]}, 'train_config': {'t0': {'blocks': [0], 'mode': 'unsupervised', 'lr': 0.08, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't2': {'blocks': [1], 'mode': 'unsupervised', 'lr': 0.005, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't4': {'blocks': [2], 'mode': 'unsupervised', 'lr': 0.01, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't6': {'blocks': [3], 'mode': 'supervised', 'lr': 0.001, 'nb_epoch': 50, 'batch_size': 64, 'print_freq': 10}}}
IN R2:  {'R1': {'train_loss': 0.046364881098270416, 'train_acc': 94.15749907493591, 'test_loss': 0.007617195602506399, 'test_acc': 90.9749984741211, 'convergence': 18.001705169677734, 'R1': 1, 'dataset_sup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 4, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 4, 'selected_classes': [0, 1, 3, 4]}, 'dataset_unsup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 4, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 4, 'selected_classes': [0, 1, 3, 4]}, 'train_config': {'t0': {'blocks': [0], 'mode': 'unsupervised', 'lr': 0.08, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't2': {'blocks': [1], 'mode': 'unsupervised', 'lr': 0.005, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't4': {'blocks': [2], 'mode': 'unsupervised', 'lr': 0.01, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't6': {'blocks': [3], 'mode': 'supervised', 'lr': 0.001, 'nb_epoch': 50, 'batch_size': 64, 'print_freq': 10}}}, 'R2': {'train_loss': 0.045486729592084885, 'train_acc': 94.95999813079834, 'test_loss': 0.007523290812969208, 'test_acc': 91.69999694824219, 'convergence': 16.710285186767578, 'R1': 1, 'dataset_sup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 4, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 64, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 50, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 4, 'selected_classes': [2, 5, 8, 9]}, 'dataset_unsup': {'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 4, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': True, 'old_dataset_size': 32, 'n_classes': 4, 'selected_classes': [2, 5, 8, 9]}, 'train_config': {'t0': {'blocks': [0], 'mode': 'unsupervised', 'lr': 0.08, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't2': {'blocks': [1], 'mode': 'unsupervised', 'lr': 0.005, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't4': {'blocks': [2], 'mode': 'unsupervised', 'lr': 0.01, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't6': {'blocks': [3], 'mode': 'supervised', 'lr': 0.001, 'nb_epoch': 50, 'batch_size': 64, 'print_freq': 10}}}}
SEED:  0
block 0, size : 96 16 16
range = 2.886751345948129
block 1, size : 384 8 8
range = 0.8505172717997146
block 2, size : 1536 4 4
range = 0.4252586358998573
range = 0.11048543456039805
The device used will be: 
True
cuda:0
PARAMSSSS IN LOAD:  {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}}, 'b3': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 3, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.11048543456039805, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 4, 'in_channels': 24576, 'old_channels': 1536, 'lr_scheduler': {'decay': 'cste', 'lr': 0.1}}, 'pool': None, 'activation': None}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
Previously stored value:  [66, 25, 77, 43, 58] tensor([-0.1711,  0.1258, -0.5707,  0.0942, -0.1394], device='cuda:0')

 Model C10_4C_CL loaded successfuly with best perf



 ----- Architecture Block BatchNorm2dSK3962(5, 5)1.0reflect, number 0 -----
- BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK963842(3, 3)0.6499999761581421reflect, number 1 -----
- BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.4)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK38415362(3, 3)0.25reflect, number 2 -----
- BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 3 -----
- Flatten(start_dim=1, end_dim=-1)
- Dropout(p=0.5, inplace=False)
- Linear(in_features=24576, out_features=4, bias=True)
[Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=24576, out_features=4, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)]
LAYER NAME:  Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=24576, out_features=4, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)
LAYER CHILDREN:  [BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=0.7)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
  (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
  (activation): Triangle(power=1.4)
), BasicBlock(
  (operations): Sequential(
    (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
  (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (activation): Triangle(power=1.0)
), BasicBlock(
  (operations): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
  )
  (layer): Linear(in_features=24576, out_features=4, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)]
subsubl NAME:  Sequential(
  (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=0.7)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
subsubl NAME:  Triangle(power=1.4)
subsubl NAME:  Sequential(
  (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
)
subsubl NAME:  HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
subsubl NAME:  AvgPool2d(kernel_size=2, stride=2, padding=0)
subsubl NAME:  Triangle(power=1.0)
subsubl NAME:  Sequential(
  (0): Flatten(start_dim=1, end_dim=-1)
)
subsubl NAME:  Linear(in_features=24576, out_features=4, bias=True)
subsubl NAME:  Dropout(p=0.5, inplace=False)
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
TARGETS BEFORE SUB:  tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([3, 0, 1, 3, 1, 0, 0, 4, 4, 0, 4, 4, 1, 4, 0, 3, 0, 3, 4, 3])
[0, 1, 3, 4]
TARGETS AFTER CLEANER:  tensor([2, 0, 1, 2, 1, 0, 0, 3, 3, 0, 3, 3, 1, 3, 0, 2, 0, 2, 3, 2])
4000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 4000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([4, 1, 1, 3, 4, 3, 4, 3, 3, 4, 0, 0, 1, 3, 4, 0, 3, 3, 3, 1])
[0, 1, 3, 4]
TARGETS AFTER CLEANER:  tensor([3, 1, 1, 2, 3, 2, 3, 2, 2, 3, 0, 0, 1, 2, 3, 0, 2, 2, 2, 1])
20000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 20000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: Compose(
               ToTensor()
               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)
           )
------------------------
TARGETS BEFORE SUB:  tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6],
       device='cuda:0')
TARGETS AFTER SUB:  tensor([4, 1, 1, 3, 4, 3, 4, 3, 3, 4, 0, 0, 1, 3, 4, 0, 3, 3, 3, 1])
[0, 1, 3, 4]
TARGETS AFTER CLEANER:  tensor([3, 1, 1, 2, 3, 2, 3, 2, 2, 3, 0, 0, 1, 2, 3, 0, 2, 2, 2, 1])
20000
<class 'dataset.FastCIFAR10'>
Dataset FastCIFAR10
    Number of datapoints: 20000
    Root location: /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/data
    Split: Train
    StandardTransform
Transform: ToTensor()
------------------------
INDICES:  20000
IMAGE SIZE: torch.Size([64, 3, 32, 32])
Accuracy of the network on the 1st dataset: 13.050 %
Test loss on the 1st dataset: 0.298


---------------- /cineca/prod/opt/modulefiles/deeplrn/libraries ----------------
cineca-ai/2.0.2  
2024-09-12 18:12:52,042	INFO worker.py:1749 -- Started a local Ray instance.
2024-09-12 18:13:00,203	INFO tune.py:263 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run(...)`.
2024-09-12 18:13:00,222	INFO tune.py:633 -- [output] This will use the new output engine with verbosity 2. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949
2024-09-12 18:13:00,613	WARNING tune.py:917 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.
[36m(func pid=32353)[0m /g100_work/EIRI_E_POLIMI/rcasciot/neuromodAI/SoftHebb-main/dataset.py:696: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[36m(func pid=32353)[0m   self.data = torch.tensor(self.data, dtype=torch.float)
[36m(func pid=1057)[0m /g100_work/EIRI_E_POLIMI/rcasciot/neuromodAI/SoftHebb-main/dataset.py:696: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[36m(func pid=1057)[0m   self.data = torch.tensor(self.data, dtype=torch.float)
[36m(func pid=6817)[0m /g100_work/EIRI_E_POLIMI/rcasciot/neuromodAI/SoftHebb-main/dataset.py:696: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[36m(func pid=6817)[0m   self.data = torch.tensor(self.data, dtype=torch.float)
[36m(func pid=10499)[0m /g100_work/EIRI_E_POLIMI/rcasciot/neuromodAI/SoftHebb-main/dataset.py:696: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[36m(func pid=10499)[0m   self.data = torch.tensor(self.data, dtype=torch.float)
2024-09-12 18:31:14,219	INFO tune.py:1021 -- Wrote the latest version of all result files and experiment state to '/g100_work/EIRI_E_POLIMI/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/search/CIFAR100_SoftHebb4_Best' in 0.1122s.


--------------- /leonardo/prod/opt/modulefiles/deeplrn/libraries ---------------
cineca-ai/3.0.0  cineca-ai/4.0.0  cineca-ai/4.1.1(default)  
cineca-ai/3.0.1  cineca-ai/4.1.0  cineca-ai/4.3.0           

Key:
(symbolic-version)  
BLOCKS:  {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5}}, 'b1': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 1, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.25, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10}, 'pool': None, 'activation': None}}
CL:  False
{'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': False}
CL:  False
{'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': False}
SEED:  0
block 0, size : 96 16 16
range = 2.886751345948129
range = 0.11048543456039805
The device used will be: 
True
cuda:0
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}

 ----- Architecture Block BatchNorm2dSK3962(5, 5)1.0reflect, number 0 -----
- BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- AvgPool2d(kernel_size=4, stride=2, padding=1)

 ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 1 -----
- Flatten(start_dim=1, end_dim=-1)
- Dropout(p=0.5, inplace=False)
- Linear(in_features=24576, out_features=10, bias=True)
MODEL PARAMETERS: 
torch.Size([10, 24576]) tensor([[ 5.2488e-03, -1.6758e-03, -1.2735e-03,  ...,  2.9649e-03,
          1.6020e-03,  1.7173e-03],
        [ 4.6174e-03, -2.1946e-03,  4.1762e-03,  ..., -3.1416e-03,
          5.9952e-03, -6.1561e-03],
        [ 1.3748e-03,  3.8777e-04,  3.6674e-03,  ..., -4.9891e-03,
         -2.3961e-03,  1.4417e-03],
        ...,
        [ 1.6207e-03, -6.3756e-03, -2.1324e-04,  ..., -1.6907e-03,
          1.9245e-03, -1.4870e-03],
        [-4.3847e-03,  5.6864e-04, -5.0209e-03,  ..., -9.4906e-05,
          2.3491e-03,  3.5744e-03],
        [ 9.0313e-04,  4.7383e-03, -3.2581e-03,  ...,  5.5493e-03,
         -6.2033e-04, -5.9052e-03]], device='cuda:0')
torch.Size([10]) tensor([-0.0047,  0.0060, -0.0037, -0.0012, -0.0040, -0.0033,  0.0044,  0.0013,
        -0.0007, -0.0036], device='cuda:0')
############################################
BLOCKS:  [0]

 ********** Hebbian Unsupervised learning of blocks [0] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
INDICES:  50000
IMAGE SIZE: torch.Size([10, 3, 32, 32])
SAVING FOLDER FOR UNSUP:  CIFAR10_Test
OrderedDict([('blocks.0.operations.0.running_mean', tensor([0.0501, 0.0496, 0.0466], device='cuda:0')), ('blocks.0.operations.0.running_var', tensor([0.9051, 0.9054, 0.9061], device='cuda:0')), ('blocks.0.operations.0.num_batches_tracked', tensor(1, device='cuda:0')), ('blocks.0.layer.weight', tensor([[[[-3.2500e+00, -3.3266e+00, -7.2336e-01, -1.2525e+00,  2.4500e+00],
          [ 1.9977e+00, -9.1225e-01, -6.1061e+00,  9.3033e-01, -3.6469e+00],
          [ 1.0103e+00,  8.8951e-01,  3.4595e-01,  3.5728e+00,  3.2239e+00],
          [-7.1383e-01, -3.9048e+00, -4.8957e+00,  1.6358e+00,  2.2907e+00],
          [ 1.7287e+00, -4.4892e+00, -9.8542e-01,  5.3492e+00,  2.1656e+00]],

         [[-1.6902e+00, -5.0055e-01,  5.2966e-01,  4.0108e+00,  4.5794e+00],
          [ 2.7317e+00, -2.4355e+00, -1.7713e+00,  9.1200e-02, -1.4222e+00],
          [ 7.1711e-01,  1.2693e+00,  3.2450e-01,  1.8498e+00,  1.2735e+00],
          [-2.9534e-01,  2.2876e+00, -8.3620e-01,  1.5158e-01,  1.5094e+00],
          [ 6.6459e+00, -4.2403e+00, -4.5804e+00, -1.9430e+00,  2.5196e+00]],

         [[ 3.0466e+00,  5.1339e-01, -6.6492e-01, -1.1309e+00,  1.5684e+00],
          [-1.1407e+00, -1.2881e+00,  2.1478e+00,  4.3907e+00,  9.8453e+00],
          [-4.4201e+00, -3.5626e+00,  5.2531e+00, -1.5921e+00, -1.6433e+00],
          [ 2.6557e+00,  3.2067e+00,  3.7235e+00, -4.2671e+00,  7.4110e+00],
          [-1.3658e+00,  9.6865e-01, -4.7035e+00, -1.5870e+00, -1.3852e+00]]],


        [[[-1.4425e+00, -3.0801e+00,  3.2186e+00, -4.0608e-01,  2.3260e+00],
          [-2.6947e-01,  1.9833e+00, -2.4200e+00,  2.5745e-03,  2.4303e+00],
          [-1.1548e+00,  3.0007e+00,  1.0339e+00, -7.1014e-01,  6.6468e+00],
          [-5.4320e+00, -1.4355e-01, -3.0166e+00, -2.7612e+00,  9.6798e-02],
          [ 2.0498e+00,  4.7512e+00, -3.9265e+00,  9.9467e-01,  1.5007e+00]],

         [[-7.5440e+00, -4.8973e+00, -6.5888e-01,  8.0816e-01,  7.1282e-01],
          [ 2.2195e-01,  9.7574e-01,  1.3117e+00,  1.3191e+00, -2.4981e+00],
          [ 2.2554e+00, -2.6754e+00, -6.3172e-01, -7.0294e+00, -2.1049e-01],
          [-9.8110e-02,  2.7786e+00,  1.0080e+00, -2.6600e+00, -1.6222e-01],
          [-1.7976e+00, -1.3387e+00,  5.5477e+00, -1.1620e+00,  3.5768e-01]],

         [[ 3.3624e+00,  2.6655e+00,  4.0048e+00, -2.5501e+00, -1.2093e+00],
          [-2.3233e+00,  1.6328e+00,  1.7620e+00,  1.3478e+00,  5.6311e+00],
          [-3.0689e+00, -2.2322e-01,  3.3601e-01, -1.7147e+00, -3.5909e+00],
          [-2.9472e-01, -2.9836e+00, -9.0251e-01,  7.0952e-01, -7.4952e-01],
          [ 3.4161e-01,  7.0425e-01,  3.3619e+00,  8.3305e-01,  1.1160e+00]]],


        [[[-5.8042e-01, -3.4043e-01,  5.5483e-01, -2.2290e+00, -5.4858e+00],
          [ 3.7723e-01, -2.0331e+00,  9.0852e-01,  4.5435e-01,  1.1124e+00],
          [ 2.7919e+00, -2.8610e+00,  8.7066e-01, -3.0980e-01,  2.8823e+00],
          [-1.4397e+00,  2.1971e+00,  1.7849e+00,  9.0658e-01,  6.1584e-01],
          [-3.4656e-01,  1.0406e+00, -9.0654e-01, -3.1140e+00,  6.9516e-01]],

         [[-4.0306e+00, -1.9094e-01, -1.0345e+00, -4.5078e+00, -1.0238e+00],
          [ 3.1208e+00,  3.7954e-01,  4.5424e+00,  2.2558e+00, -3.1138e+00],
          [-2.0811e+00,  4.2458e+00,  7.9569e-01,  1.9248e+00, -2.8706e+00],
          [-3.4334e+00, -3.4524e+00, -1.6155e+00,  1.5400e+00,  1.1746e+00],
          [ 1.1391e+00,  4.9511e-01,  2.5289e+00, -8.2875e-01,  2.9492e+00]],

         [[-2.1476e-01, -3.1530e+00,  1.1317e+00,  1.7162e+00,  1.9118e+00],
          [-3.4823e+00,  1.7535e+00, -1.5795e+00,  3.3806e+00,  2.8145e-01],
          [ 2.7810e+00,  2.4258e+00, -3.6190e+00,  2.8488e+00, -1.4279e+00],
          [-3.7038e+00,  2.7575e+00,  3.7054e+00, -1.9222e+00,  1.6314e+00],
          [ 8.3053e-01, -9.6345e-02, -3.0654e+00, -3.3032e-01, -9.9113e-01]]],


        ...,


        [[[-2.6170e+00, -4.3993e+00, -8.2832e-01, -5.6149e+00,  5.5079e+00],
          [-1.7963e+00,  3.9392e+00, -3.3401e+00, -2.6430e+00,  3.2242e-01],
          [-1.4878e+00, -6.7647e+00, -1.1489e+00, -8.5198e-01, -2.5857e+00],
          [-1.9114e+00, -4.2260e+00,  3.0851e+00, -3.0022e+00,  1.4741e+00],
          [ 6.3607e+00, -9.2802e-01, -3.6033e+00,  2.0088e+00,  3.7585e+00]],

         [[ 5.2407e-01,  9.0599e-01,  1.9380e-01,  5.0260e+00, -1.8664e+00],
          [ 3.3045e+00, -3.4480e+00, -3.8538e+00,  2.3058e+00, -2.0954e+00],
          [-5.5768e+00, -7.9083e+00,  3.7008e+00,  2.8583e-02,  1.8184e+00],
          [-2.9956e+00,  2.5053e+00, -1.6817e+00, -4.1040e-01, -3.6844e-01],
          [-5.9520e-02, -1.1993e+00, -4.1557e+00, -7.8241e-02, -2.1880e+00]],

         [[-8.4151e-02,  1.4961e+00, -7.7380e+00,  1.1001e+00,  1.2679e-01],
          [-1.9600e+00,  4.7428e+00, -1.9385e+00, -4.4021e+00,  6.5195e-02],
          [ 1.1991e-01, -3.5432e+00, -7.6286e-01, -3.3768e-01,  1.6836e-01],
          [-4.8340e+00,  3.3286e+00,  2.6009e+00, -1.6435e-01,  4.6032e+00],
          [ 1.1766e+00, -5.3180e+00, -1.9259e+00, -4.7645e+00,  2.9242e+00]]],


        [[[-3.6115e+00,  5.1875e+00, -2.2945e+00,  2.8956e+00,  1.1696e+00],
          [ 5.1779e+00, -2.1312e-01,  1.9622e+00,  8.1397e-01, -2.1231e+00],
          [-5.5180e-01,  1.2042e-01, -2.0493e+00,  4.9753e+00,  2.0565e+00],
          [ 1.7180e+00,  5.3035e+00,  3.7053e+00,  3.1269e+00, -1.3086e+00],
          [ 7.7094e-02, -8.8609e-01,  1.2052e-01, -2.1445e+00,  2.8456e+00]],

         [[-5.1940e+00,  2.2430e+00, -3.3758e+00, -1.6652e+00, -1.6116e+00],
          [-8.1296e-01,  4.1532e-01,  3.1913e+00,  6.3426e-01, -8.8727e-01],
          [-2.9445e+00,  1.2108e+00,  1.0310e+00,  1.9050e+00, -5.0343e+00],
          [-1.8438e+00, -2.1456e+00,  1.2446e+00, -1.5825e+00,  1.2810e+00],
          [-1.6881e+00,  3.6318e-01,  4.7821e-01,  2.9035e+00,  8.6359e-02]],

         [[ 3.3164e+00, -3.9001e+00,  3.2608e+00,  2.7189e+00,  2.3775e-01],
          [-1.3001e+00,  2.5901e+00, -2.2209e+00,  1.9926e+00,  3.4865e+00],
          [-2.7693e+00,  2.0345e+00, -2.2785e+00, -1.7511e+00, -2.2545e+00],
          [-6.9658e-01, -1.2599e+00,  1.1193e+00, -3.6015e+00,  9.6255e-01],
          [-8.1112e-01, -2.3121e-01,  2.0678e+00,  6.5440e+00, -7.4462e+00]]],


        [[[-2.2857e+00,  5.0479e-01, -5.2758e-01,  4.4781e+00,  2.4672e+00],
          [ 2.0417e+00, -1.2858e-01, -1.4037e+00, -1.6120e+00,  1.9287e+00],
          [-4.9449e+00,  5.4960e-01, -1.2962e+00,  1.0990e+00, -4.5368e+00],
          [-3.0911e+00, -2.9439e+00,  5.9340e+00, -4.3339e+00, -3.6744e-01],
          [ 2.3716e+00, -2.8239e+00,  2.4021e+00,  7.1596e-01, -2.3387e+00]],

         [[-6.7180e-01, -5.5156e-02, -2.7407e+00,  2.1389e+00,  6.3414e-01],
          [-3.6621e+00, -2.3912e-01, -6.8944e-01,  3.6263e+00, -3.0526e+00],
          [ 1.5329e+00, -1.2135e+00, -1.7292e+00, -3.5185e+00,  2.8902e+00],
          [ 2.4977e+00, -1.5753e+00, -2.3138e+00, -7.9298e-01, -1.6291e+00],
          [ 2.8021e+00, -1.8249e+00,  4.6985e+00, -3.3096e+00,  2.4190e+00]],

         [[ 4.1463e+00,  3.9685e-01, -2.6065e+00,  2.8193e+00,  3.6697e+00],
          [ 4.5741e+00, -1.0689e+00,  1.9505e+00, -3.8503e+00, -6.0542e-02],
          [-8.4074e-01, -3.3604e+00,  2.2484e+00, -4.0604e-01, -1.7381e+00],
          [ 3.7258e+00, -3.4509e+00, -2.3307e+00,  1.4649e+00, -1.1378e+00],
          [-2.4855e-01,  1.8889e+00, -5.5221e+00, -2.0352e+00,  1.6206e+00]]]],
       device='cuda:0')), ('blocks.1.layer.weight', tensor([[ 5.2488e-03, -1.6758e-03, -1.2735e-03,  ...,  2.9649e-03,
          1.6020e-03,  1.7173e-03],
        [ 4.6174e-03, -2.1946e-03,  4.1762e-03,  ..., -3.1416e-03,
          5.9952e-03, -6.1561e-03],
        [ 1.3748e-03,  3.8777e-04,  3.6674e-03,  ..., -4.9891e-03,
         -2.3961e-03,  1.4417e-03],
        ...,
        [ 1.6207e-03, -6.3756e-03, -2.1324e-04,  ..., -1.6907e-03,
          1.9245e-03, -1.4870e-03],
        [-4.3847e-03,  5.6864e-04, -5.0209e-03,  ..., -9.4906e-05,
          2.3491e-03,  3.5744e-03],
        [ 9.0313e-04,  4.7383e-03, -3.2581e-03,  ...,  5.5493e-03,
         -6.2033e-04, -5.9052e-03]], device='cuda:0')), ('blocks.1.layer.bias', tensor([-0.0047,  0.0060, -0.0037, -0.0012, -0.0040, -0.0033,  0.0044,  0.0013,
        -0.0007, -0.0036], device='cuda:0'))])
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 1.46e-01	time: 00:00:14	Acc_train 0.00	Acc_test 0.00	convergence: 3.74e+00	R1: 8	Info MB:0.000e+00/SB:0.000e+00/MW:1.570e-02/SW:5.813e-01/MR:4.741e+00/SR:1.705e+00/MeD:1.297e+00/MaD:5.197e+00/MW:0.603/MAW:0.397
|       0 |       1 |       2 |       3 |       4 |        5 |       6 |       7 |       8 |       9 |        10 |      11 |      12 |      13 |      14 |      15 |         16 |      17 |      18 |      19 |      20 |      21 |      22 |      23 |      24 |         25 |      26 |      27 |         28 |      29 |
|---------+---------+---------+---------+---------+----------+---------+---------+---------+---------+-----------+---------+---------+---------+---------+---------+------------+---------+---------+---------+---------+---------+---------+---------+---------+------------+---------+---------+------------+---------|
|   0.191 |   0.129 |   0.121 |   0.158 |   0.171 |   0.0964 |   0.143 |   0.135 |   0.176 |   0.192 |   0.00115 |   0.177 |   0.133 |   0.133 |   0.134 |   0.183 |   0.000361 |   0.187 |   0.163 |   0.184 |   0.184 |   0.175 |   0.142 |   0.187 |   0.172 |   0.000321 |   0.198 |   0.175 |   0.000361 |   0.172 |
|   6.68  |   3.62  |   3.29  |   4.9   |   5.55  |   2.45   |   4.18  |   3.87  |   5.86  |   6.78  |   1       |   5.91  |   3.75  |   3.75  |   3.8   |   6.24  |   1        |   6.49  |   5.16  |   6.27  |   6.31  |   5.77  |   4.14  |   6.44  |   5.63  |   1        |   7.12  |   5.8   |   1        |   5.61  |
|   0.55  |   0.51  |   0.53  |   0.57  |   0.55  |   0.63   |   0.42  |   0.48  |   0.55  |   0.53  |   2.68    |   0.53  |   0.44  |   0.53  |   0.53  |   0.46  |  18.8      |   0.56  |   0.49  |   0.55  |   0.55  |   0.56  |   0.49  |   0.58  |   0.51  |  24.34     |   0.38  |   0.56  |   0.99     |   0.49  |
| nan     | nan     | nan     | nan     | nan     | nan      | nan     | nan     | nan     | nan     | nan       | nan     | nan     | nan     | nan     | nan     | nan        | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan        | nan     | nan     | nan        | nan     |
| nan     | nan     | nan     | nan     | nan     | nan      | nan     | nan     | nan     | nan     | nan       | nan     | nan     | nan     | nan     | nan     | nan        | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan        | nan     | nan     | nan        | nan     |
| nan     | nan     | nan     | nan     | nan     | nan      | nan     | nan     | nan     | nan     | nan       | nan     | nan     | nan     | nan     | nan     | nan        | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan        | nan     | nan     | nan        | nan     |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/CIFAR10_Test/models
BLOCKS:  [1]

 ********** Supervised learning of blocks [1] **********
SAVING FOLDER FOR SUP:  CIFAR10_Test
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
INDICES:  50000
IMAGE SIZE: torch.Size([10, 3, 32, 32])
Epoch: [1/1]	lr: 1.00e-03	time: 00:00:23	Loss_train 0.41533	Acc_train 48.29	/	Loss_test 0.00638	Acc_test 44.83
STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/CIFAR10_Test/models


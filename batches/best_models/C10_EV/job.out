--------------- /leonardo/prod/opt/modulefiles/deeplrn/libraries ---------------
cineca-ai/3.0.0  cineca-ai/4.0.0  cineca-ai/4.1.1(default)  
cineca-ai/3.0.1  cineca-ai/4.1.0  cineca-ai/4.3.0           

Key:
(symbolic-version)  
BLOCKS:  {'b0': {'arch': 'CNN', 'preset': 'softkrotov-c96-k5-p2-s1-d1-b0-t1-lr0.08-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 0, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 0.7}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5}}, 'b1': {'arch': 'CNN', 'preset': 'softkrotov-c384-k3-p1-s1-d1-b0-t0.65-lr0.005-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 1, 'batch_norm': False, 'pool': {'type': 'max', 'kernel_size': 4, 'stride': 2, 'padding': 1}, 'activation': {'function': 'triangle', 'param': 1.4}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3}}, 'b2': {'arch': 'CNN', 'preset': 'softkrotov-c1536-k3-p1-s1-d1-b0-t0.25-lr0.01-lp0.5-e0', 'operation': 'batchnorm2d', 'num': 2, 'batch_norm': False, 'pool': {'type': 'avg', 'kernel_size': 2, 'stride': 2, 'padding': 0}, 'activation': {'function': 'triangle', 'param': 1.0}, 'resume': None, 'layer': {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'positive', 'weight_init_range': 2, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3}}, 'b3': {'arch': 'MLP', 'preset': 'BP-c10', 'operation': 'flatten', 'num': 3, 'att_dropout': None, 'dropout': 0.5, 'layer': {'arch': 'MLP', 'lr': 0.05, 'adaptive': True, 'lr_sup': 0.001, 'speed': 0.4, 'lr_div': 100, 'lebesgue_p': 2, 't_invert': 10, 'beta': 0.01, 'power': 4.5, 'ranking_param': 3, 'delta': 0.1, 'hebbian': False, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'lr_bias': 600, 'softness': 'soft', 'soft_activation_fn': 'exp', 'plasticity': 'SoftHebb', 'metric_mode': 'unsupervised', 'weight_init': 'positive', 'weight_init_range': 0.25, 'weight_init_offset': 0, 'weight_decay': 0, 'radius': 10, 'power_lr': 0.2, 'out_channels': 10}, 'pool': None, 'activation': None}}
CL:  False
{'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': False}
CL:  False
{'name': 'CIFAR10', 'noise_std': 0, 'channels': 3, 'width': 32, 'height': 32, 'validation_split': 0.2, 'training_sample': 50000, 'testing_sample': 10000, 'out_channels': 10, 'num_workers': 0, 'seed': 0, 'shuffle': True, 'batch_size': 10, 'augmentation': False, 'zca_whitened': False, 'training_class': 'all', 'split': 'train', 'nb_epoch': 1, 'print_freq': 10, 'validation': False, 'continual_learning': False}
SEED:  0
block 0, size : 96 16 16
range = 2.886751345948129
block 1, size : 384 8 8
range = 0.8505172717997146
block 2, size : 1536 4 4
range = 0.4252586358998573
range = 0.11048543456039805
The device used will be: 
True
cuda:0
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.08, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 1.0, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 2, 'weight_init': 'normal', 'weight_init_range': 2.886751345948129, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 96, 'kernel_size': 5, 'in_channels': 3, 'lr_scheduler': {'lr': 0.08, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.005, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.65, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.8505172717997146, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 384, 'kernel_size': 3, 'in_channels': 96, 'lr_scheduler': {'lr': 0.005, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}
CNN LAYER CONFIG:  {'arch': 'CNN', 'nb_train': None, 'lr': 0.01, 'adaptive': True, 'lr_sup': 0.001, 'speed': 7, 'lr_div': 96, 'lebesgue_p': 2, 'padding_mode': 'reflect', 'pre_triangle': False, 'ranking_param': 3, 'delta': 2, 't_invert': 0.25, 'groups': 1, 'stride': 1, 'dilation': 1, 'beta': 1, 'power': 4.5, 'padding': 1, 'weight_init': 'normal', 'weight_init_range': 0.4252586358998573, 'weight_init_offset': 0, 'mask_thsd': 0, 'radius': 25, 'power_lr': 0.5, 'weight_decay': 0, 'soft_activation_fn': 'exp', 'hebbian': True, 'resume': None, 'add_bias': False, 'normalize_inp': False, 'lr_decay': 'linear', 'seed': 0, 'softness': 'softkrotov', 'out_channels': 1536, 'kernel_size': 3, 'in_channels': 384, 'lr_scheduler': {'lr': 0.01, 'adaptive': True, 'nb_epochs': 1, 'ratio': 0.0002, 'speed': 7, 'div': 96, 'decay': 'linear', 'power_lr': 0.5}}

 ----- Architecture Block BatchNorm2dSK3962(5, 5)1.0reflect, number 0 -----
- BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
- Triangle(power=0.7)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK963842(3, 3)0.6499999761581421reflect, number 1 -----
- BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.4)
- MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)

 ----- Architecture Block BatchNorm2dSK38415362(3, 3)0.25reflect, number 2 -----
- BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
- HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
- Triangle(power=1.0)
- AvgPool2d(kernel_size=2, stride=2, padding=0)

 ----- Architecture Block FlattenDropout(p=0.5, inplace=False)Linear(in_, number 3 -----
- Flatten(start_dim=1, end_dim=-1)
- Dropout(p=0.5, inplace=False)
- Linear(in_features=24576, out_features=10, bias=True)
[Sequential(
  (0): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(3, 96, lebesgue_p=2, pruning=0, kernel_size=(5, 5), bias=False, padding_mode=reflect, t_invert=1.0, bias=False, lr_bias=0.1, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=0.7)
  )
  (1): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(96, 384, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.6499999761581421, bias=False, lr_bias=0.1538, ranking_param=3, delta=2, activation=exp)
    (pool): MaxPool2d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
    (activation): Triangle(power=1.4)
  )
  (2): BasicBlock(
    (operations): Sequential(
      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
    (layer): HebbSoftKrotovConv2d(384, 1536, lebesgue_p=2, pruning=0, kernel_size=(3, 3), bias=False, padding_mode=reflect, t_invert=0.25, bias=False, lr_bias=0.4, ranking_param=3, delta=2, activation=exp)
    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (activation): Triangle(power=1.0)
  )
  (3): BasicBlock(
    (operations): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
    )
    (layer): Linear(in_features=24576, out_features=10, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
  )
)]
MODEL PARAMETERS: 
torch.Size([10, 24576]) tensor([[ 0.0047, -0.0050,  0.0012,  ...,  0.0017,  0.0033,  0.0019],
        [-0.0021, -0.0006,  0.0044,  ..., -0.0026, -0.0023, -0.0007],
        [-0.0041,  0.0025, -0.0033,  ...,  0.0022, -0.0049,  0.0021],
        ...,
        [ 0.0029,  0.0008,  0.0023,  ...,  0.0007, -0.0021, -0.0063],
        [-0.0057, -0.0047,  0.0036,  ..., -0.0025, -0.0011,  0.0029],
        [-0.0046,  0.0051, -0.0040,  ...,  0.0028,  0.0012,  0.0015]],
       device='cuda:0')
torch.Size([10]) tensor([ 0.0046, -0.0054, -0.0008, -0.0015, -0.0060, -0.0058,  0.0034,  0.0053,
         0.0048,  0.0006], device='cuda:0')
############################################
BLOCKS:  [0]

 ********** Hebbian Unsupervised learning of blocks [0] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
INDICES:  50000
IMAGE SIZE: torch.Size([10, 3, 32, 32])
SAVING FOLDER FOR UNSUP:  C10_EV
DEPTH:  3
LAYER_NUM:  0
delta_weights INFO:
NUM OF TRACKED COV LAYERS:  1
NUM OF TRACKED WEIGHTS CHANGES PER LAYER:  50
avg_deltas size:  3
num of averages for 0 layer:  torch.Size([96])
################################################
torch.Size([96, 3, 5, 5])
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 1.46e-01	time: 00:00:18	Acc_train 0.00	Acc_test 0.00	convergence: 2.40e+01	R1: 0	Info MB:0.000e+00/SB:0.000e+00/MW:1.570e-02/SW:5.813e-01/MR:4.741e+00/SR:1.705e+00/MeD:1.297e+00/MaD:5.197e+00/MW:0.603/MAW:0.397
|       0 |       1 |       2 |       3 |       4 |        5 |       6 |       7 |       8 |       9 |        10 |      11 |      12 |      13 |      14 |      15 |         16 |      17 |      18 |      19 |      20 |      21 |      22 |      23 |      24 |         25 |      26 |      27 |         28 |      29 |
|---------+---------+---------+---------+---------+----------+---------+---------+---------+---------+-----------+---------+---------+---------+---------+---------+------------+---------+---------+---------+---------+---------+---------+---------+---------+------------+---------+---------+------------+---------|
|   0.191 |   0.129 |   0.121 |   0.158 |   0.171 |   0.0964 |   0.143 |   0.135 |   0.176 |   0.192 |   0.00115 |   0.177 |   0.133 |   0.133 |   0.134 |   0.183 |   0.000361 |   0.187 |   0.163 |   0.184 |   0.184 |   0.175 |   0.142 |   0.187 |   0.172 |   0.000321 |   0.198 |   0.175 |   0.000361 |   0.172 |
|   6.68  |   3.62  |   3.29  |   4.9   |   5.55  |   2.45   |   4.18  |   3.87  |   5.86  |   6.78  |   1       |   5.91  |   3.75  |   3.75  |   3.8   |   6.24  |   1        |   6.49  |   5.16  |   6.27  |   6.31  |   5.77  |   4.14  |   6.44  |   5.63  |   1        |   7.12  |   5.8   |   1        |   5.61  |
|   0.55  |   0.51  |   0.53  |   0.57  |   0.55  |   0.63   |   0.42  |   0.48  |   0.55  |   0.53  |   2.68    |   0.53  |   0.44  |   0.53  |   0.53  |   0.46  |  18.8      |   0.56  |   0.49  |   0.55  |   0.55  |   0.56  |   0.49  |   0.58  |   0.51  |  24.34     |   0.38  |   0.56  |   0.99     |   0.49  |
| nan     | nan     | nan     | nan     | nan     | nan      | nan     | nan     | nan     | nan     | nan       | nan     | nan     | nan     | nan     | nan     | nan        | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan        | nan     | nan     | nan        | nan     |
| nan     | nan     | nan     | nan     | nan     | nan      | nan     | nan     | nan     | nan     | nan       | nan     | nan     | nan     | nan     | nan     | nan        | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan        | nan     | nan     | nan        | nan     |
| nan     | nan     | nan     | nan     | nan     | nan      | nan     | nan     | nan     | nan     | nan       | nan     | nan     | nan     | nan     | nan     | nan        | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan     | nan        | nan     | nan     | nan        | nan     |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_EV/models
BLOCKS:  [1]

 ********** Hebbian Unsupervised learning of blocks [1] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
INDICES:  50000
IMAGE SIZE: torch.Size([10, 3, 32, 32])
SAVING FOLDER FOR UNSUP:  C10_EV
DEPTH:  3
LAYER_NUM:  1
delta_weights INFO:
NUM OF TRACKED COV LAYERS:  1
NUM OF TRACKED WEIGHTS CHANGES PER LAYER:  50
avg_deltas size:  3
num of averages for 1 layer:  torch.Size([96])
################################################
torch.Size([96, 3, 5, 5])
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 1.46e-01	time: 00:00:34	Acc_train 0.00	Acc_test 0.00	convergence: 2.40e+01	R1: 0	Info MB:0.000e+00/SB:0.000e+00/MW:5.022e-03/SW:1.525e-01/MR:4.222e+00/SR:1.517e+00/MeD:1.219e+00/MaD:3.370e+00/MW:0.586/MAW:0.414
|         0 |        1 |         2 |         3 |         4 |         5 |         6 |         7 |         8 |         9 |        10 |         11 |       12 |      13 |       14 |        15 |        16 |        17 |        18 |        19 |        20 |       21 |       22 |        23 |         24 |       25 |        26 |        27 |       28 |        29 |
|-----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+----------+---------+----------+-----------+-----------+-----------+-----------+-----------+-----------+----------+----------+-----------+------------+----------+-----------+-----------+----------+-----------|
|   0.00467 |   0.0106 |   0.00378 |   0.00144 |   0.00561 |   0.00861 |   0.00436 |   0.00802 |   0.00192 |   0.00942 |   0.00974 |   0.000888 |   0.0098 |   0.011 |   0.0105 |   0.00607 |   0.00919 |   0.00847 |   0.00267 |   0.00363 |   0.00735 |   0.0112 |   0.0118 |   0.00965 |   1.61e-05 |   0.0104 |   0.00979 |   0.00893 |   0.0112 |   0.00572 |
|   1.87    |   5.53   |   1.57    |   1.08    |   2.26    |   3.96    |   1.76    |   3.57    |   1.15    |   4.55    |   4.79    |   1.03     |   4.84   |   5.84  |   5.44   |   2.47    |   4.38    |   3.87    |   1.29    |   1.53    |   3.16    |   6.05   |   6.59   |   4.73    |   1        |   5.35   |   4.83    |   4.19    |   6.04   |   2.31    |
|   0.2     |   0.18   |   0.2     |   0.29    |   0.37    |   0.15    |   0.13    |   0.17    |   0.15    |   0.18    |   0.18    |   0.35     |   0.22   |   0.25  |   0.17   |   0.12    |   0.13    |   0.22    |   0.17    |   0.23    |   0.15    |   0.23   |   0.29   |   0.21    |   8.71     |   0.25   |   0.16    |   0.16    |   0.17   |   0.2     |
| nan       | nan      | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan        | nan      | nan     | nan      | nan       | nan       | nan       | nan       | nan       | nan       | nan      | nan      | nan       | nan        | nan      | nan       | nan       | nan      | nan       |
| nan       | nan      | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan        | nan      | nan     | nan      | nan       | nan       | nan       | nan       | nan       | nan       | nan      | nan      | nan       | nan        | nan      | nan       | nan       | nan      | nan       |
| nan       | nan      | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan       | nan        | nan      | nan     | nan      | nan       | nan       | nan       | nan       | nan       | nan       | nan      | nan      | nan       | nan        | nan      | nan       | nan       | nan      | nan       |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_EV/models
BLOCKS:  [2]

 ********** Hebbian Unsupervised learning of blocks [2] **********
SEED:  0
BEFORE RESIZING
Files already downloaded and verified
AFTER RESIZING
Files already downloaded and verified
INDICES:  50000
IMAGE SIZE: torch.Size([10, 3, 32, 32])
SAVING FOLDER FOR UNSUP:  C10_EV
DEPTH:  3
LAYER_NUM:  2
delta_weights INFO:
NUM OF TRACKED COV LAYERS:  1
NUM OF TRACKED WEIGHTS CHANGES PER LAYER:  50
avg_deltas size:  3
num of averages for 2 layer:  torch.Size([96])
################################################
torch.Size([96, 3, 5, 5])
INSIDE EVALUATE UNSUP
INSIDE EVALUATE UNSUP RETURNED 0,0
Epoch: [1/1]	lr: 3.97e-02	time: 00:00:56	Acc_train 0.00	Acc_test 0.00	convergence: 1.59e+01	R1: 1	Info MB:0.000e+00/SB:0.000e+00/MW:1.024e-02/SW:2.888e-01/MR:1.686e+01/SR:2.088e+00/MeD:1.624e+00/MaD:1.586e+01/MW:0.432/MAW:0.568
|        0 |        1 |        2 |        3 |        4 |        5 |        6 |        7 |        8 |        9 |       10 |       11 |       12 |       13 |       14 |       15 |       16 |     17 |       18 |       19 |       20 |       21 |       22 |       23 |       24 |       25 |      26 |       27 |       28 |       29 |
|----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+--------+----------+----------+----------+----------+----------+----------+----------+----------+---------+----------+----------+----------|
|   0.0453 |   0.0431 |   0.0382 |   0.0413 |   0.0401 |   0.0376 |   0.0392 |   0.0386 |   0.0402 |   0.0421 |   0.0428 |   0.0372 |   0.0349 |   0.0395 |   0.0353 |   0.0381 |   0.0381 |   0.04 |   0.0419 |   0.0438 |   0.0407 |   0.0363 |   0.0419 |   0.0419 |   0.0396 |   0.0372 |   0.041 |   0.0367 |   0.0382 |   0.0399 |
|  21.5    |  19.6    |  15.61   |  18.1    |  17.11   |  15.14   |  16.37   |  15.91   |  17.17   |  18.69   |  19.36   |  14.87   |  13.18   |  16.6    |  13.47   |  15.5    |  15.51   |  16.98 |  18.59   |  20.16   |  17.59   |  14.2    |  18.57   |  18.52   |  16.72   |  14.86   |  17.83  |  14.46   |  15.56   |  16.93   |
|   0.05   |   0.04   |   0.05   |   0.06   |   0.04   |   0.05   |   0.07   |   0.04   |   0.06   |   0.05   |   0.08   |   0.06   |   0.11   |   0.04   |   0.08   |   0.04   |   0.04   |   0.08 |   0.05   |   0.05   |   0.08   |   0.05   |   0.06   |   0.05   |   0.06   |   0.05   |   0.07  |   0.07   |   0.04   |   0.06   |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan    | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan    | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      |
| nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan    | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan      | nan     | nan      | nan      | nan      |

STORING PATH IS NONEEEEEE
SAVING THE MODEL
/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/result/network/C10_EV/models
BLOCKS:  [3]

 ********** Supervised learning of blocks [3] **********
SAVING FOLDER FOR SUP:  C10_EV
SEED:  0
BEFORE RESIZING
Files already downloaded and verified


slurmstepd: error: *** JOB 8414101 ON lrdn3369 CANCELLED AT 2024-10-17T13:30:53 ***
2024-10-17 13:13:11,471	INFO worker.py:1786 -- Started a local Ray instance.
2024-10-17 13:13:26,494	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run(...)`.
2024-10-17 13:13:26,547	WARNING tune.py:902 -- AIR_VERBOSITY is set, ignoring passed-in ProgressReporter for now.
[36m(func pid=1716930)[0m /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/dataset.py:658: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[36m(func pid=1716930)[0m   self.data = torch.tensor(self.data, dtype=torch.float)
[36m(func pid=1716930)[0m /leonardo/home/userexternal/rcasciot/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[36m(func pid=1716930)[0m   warnings.warn(_create_warning_msg(
[36m(func pid=1790808)[0m /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/dataset.py:658: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[36m(func pid=1790808)[0m   self.data = torch.tensor(self.data, dtype=torch.float)
[36m(func pid=1790808)[0m /leonardo/home/userexternal/rcasciot/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[36m(func pid=1790808)[0m   warnings.warn(_create_warning_msg(
[36m(func pid=1864639)[0m /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/dataset.py:658: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[36m(func pid=1864639)[0m   self.data = torch.tensor(self.data, dtype=torch.float)
[36m(func pid=1864639)[0m /leonardo/home/userexternal/rcasciot/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[36m(func pid=1864639)[0m   warnings.warn(_create_warning_msg(
[36m(func pid=1938226)[0m /leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/dataset.py:658: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
[36m(func pid=1938226)[0m   self.data = torch.tensor(self.data, dtype=torch.float)
[36m(func pid=1938226)[0m /leonardo/home/userexternal/rcasciot/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[36m(func pid=1938226)[0m   warnings.warn(_create_warning_msg(
2024-10-17 15:58:48,226	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/leonardo_work/try24_antoniet/rcasciot/neuromodAI/SoftHebb-main/Training/results/hebb/search/STL10_SoftHebb5_Best' in 0.3575s.


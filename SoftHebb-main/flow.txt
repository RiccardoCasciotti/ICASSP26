
library to visualize the pytorch model !!!!!

I am confused on what the accuracy of the unsupervised model is calculated on...

lebesgue_p

So we have a network model: 
what is the input type
What are the layer types
How is the learning implemented
what is the output


ray_search.py : where the parameters are parsed and the main is executed.
    
    run_unsup(): performs unspervised learning and evaluates at the end of every epoch
        train_unsup(): 
            train_hebb(): The criterion can be something like ... ??? criterion seems to be none always, just like measures. 
                            So are they both to be defined??? 


        evaluate_unsup(): 
            evaluate_hebb(): does the unsupervised evaluation ---------------- we can't get in here (return 0.0 error)
                infer_dataset(): 

    run_sup (): runs the supervised learning if the mode is supervised (??? there are instances where the MLP is unsup???)
        train_sup():
            train_sup_hebb(): weigth change vector which is done only the last layer ?????? +  if loss_acc problem
            train_BP():
        evaluate_sup(): 


    run hybrid ()





# Alright so how to organize the slides:
- I would first give a general explanation of what the code does ( so the different types of training, how 
they differ from each other and what they aim to do ), then I would get into the code with the high level 
explanation of the functions implementing the corresponding learning. 
- I would do a very high level discussion of the flow of the functions and the general 
execution of the program. Just include the name of the functions and what they generally do.
Like Input types and output. 

Then do a main discussion on the methods which are used for training and how the update happens. 
So when trainign a model the main thign to consider is what kind of input is used, 
the update rule used ( in our case it would either be hebbian learning without any loss feedback and classical BP)

Ok so the training is mainly done in multi_layer.py while in ray_search we have the search for the optimal hyperparameters. 

-- FIRST READ
We have seen that the soft hebb network is compared with the classical BP fully supervised method. 
From my understanding the hebbian network is trained first in a fully unsupervised manner in order to obtain  a way of clusterizing the points, 
After which we take it and use a classifier to assign the clusters to the classes. 


HebbHardKrotovLinear get_wta needs to be finished commenting.
In the soft implementation we use the activation function, which creates a vector of activations effectively assigning a probability 
distribution to the activations since their value is between 0 and 1. 


What is the concept of activation?? 
So we have a set of neurons on one layer linked to the neurons in the next layer. Let's say we go from neuron i to neuron k, what is the activation of the 
neuron i? It is the pre synaptic value passed in the activation function? Or is the post synaptic?? 

The datasets: 

ImageNette: width: 160, Imagenette is a subset of 10 easily classified classes from Imagenet (tench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball, parachute).

STL10: 10 classes: (some similar to cifar10)  "width": 96,
- airplane, bird, car, cat, deer, dog, horse, monkey, ship, truck.
- Images are 96x96 pixels, color.
- 500 training images (10 pre-defined folds), 800 test images per class.
- 100000 unlabeled images for unsupervised learning. These examples are extracted from a similar but broader distribution of images. For instance, it contains other types of animals (bears, rabbits, etc.) and vehicles (trains, buses, etc.) in addition to the ones in the labeled set.
- Images were acquired from labeled examples on ImageNet.

CIFAR100: 32x32 color images classified into 100 classes

CIFAR10: 

airplane										
automobile										
bird										
cat										
deer										
dog										
frog										
horse								
		
ship										
truck

Ok so when doing continual learning with the best models we need to set the skip-1 flag to true.

Question
But we trained the best models through the use of ray search which optimizes the training given a certain dataset,
should we do the same in the continual learning setting?? Like we do a ray search on the the best model but using another dataset, 
makes sense. So how to do it? We add it to the continual learning? Or better we do another file which is ray search for continual learning.


We need to train on less classes now. so we start with 2, and then same dataset but two other classes and then we
do an evaluation on the first two. And so on increasing the number of classes per task.

Important: in ray search continual learning and continual learning we use the BLOCKS gatheres from the datasets, maybe you should 
load the preset for the second dataset... but then watch out for the number of layers and stuff...
See if you should use the reset parameter or not in the ray search CL on the model... 

To implement the training on two different classes of hte same dataset we need to consider: 
    - we implemented a method which allows to train on two different datasets. 
    - we need to implement a method which allows us to train on one dataset and everytime we change the classes. 

The problems which I have found during the development of the continual learning approach which uses only subclasses to 
implement a task are: 
[bypassed]- the number of samples specified in the preset corresponds to the total set of samples and not to 
        the dataset which has been deprived of some classes. A possible solution to this is that when we load the preset we count the number 
        of samples present... this solution unfortunately is very hard to implement because of the way the code flow is organized. Unfortunaly the preset is loaded 
        way earlier than the actual dataset, so modifying this information can be only done after the dataset is loaded and eventually modified so that we know the amount of samples present. 
[fixed] - the same assumption is done in the select_dataset() function where the indices variable is intialized by manually inserting 
        the number of indices to create in the list, and not by actually counting the number of samples present.
[fixed] - finally the other problem is caused by the fact that when we select the classes to include in the task they might not start 
        at 0, so we could have classes like [1, 2], problem is torch wants the indexes of the classes to start from 0. So a solution 
        would be to decrease all the indexes of all the classes in the dataset. This has to be done after the dataset has been resized and modified.
        Important is also to extract the labels ( in readable language that is ) associated to the selected classes.


[bypassed] !!!!!!! Problem detected: the ray search CL done per tasks seems only to save the eval run and not also R1 and R2.
[fixed] !!!!!!! Problem detected: continual_learning loads the best model (trained with ray search) but overwrites it, instead we should use
                        a copy so to keep the best model untouched. So we need to figure it out. Just manually create a copy of the model before executing the program.

Everything seems to be working fine. 
Now let's check if the models are saved/retrieved correctly.

To check if the models are saved correctly I would go through the corresponding function, 
same goes for the loading phase. Everything seems to be in check, but I want to clean their code because it is disgusting.
There is redundancy in load layers and a loop which gets executed but there is the chance it won't do anything at all.

So to execute the continual learning: 
   - we have the best models for multiple datasets testing
   - we need to copy them and watch to pass the name of the copied model to the continual learning and not the original one
   - we need to operate in skip_1 mode True and add in continual learning an evaluation phase 

   - reguarding the subset of classes approach we need to train with ray search and find the best models 
   - at this point we save the models and copy them with a different name. 
   - Use the new name in the continual learning using the classes flag.
For both scenarios we need to clean the already existing json files before doing anything because they contain dirty runs.

Everything seems to be working fine reguarding the scaling in continual learning with diff datasets. 
We still need to try the continual learning with subclasses. CL with subclasses works fine. 
[completed]

#################

Now we have to implement the continual leanring. Briefly I would meassure the different in the weights after every 
either epoch, or batch or single image ( though this could be too expensive if done from the get go, so I would do it like in the end to fine tune the meta learning approach). 
With this difference caluclated I am able to understand where the weights chamge and how, additionally I am able to see how much fo the previolsy elarned representation my 
model is able to use with a different dataset. Knowin gthis, I would implement a metalearning approach which modulates the plasticity of certain areas of the network based on the change in weights in that area
and on the loss of the classification. 

To visualize the weight change we need to access to the state_dict of the network. These contains different information, amongst those 
also the weights tensor. They are organized by blocks (which I am assuming are an alias for layer). So we have to store per each block
this tensor, and put them in a dict... no sense just use the state_dict dictionary you dumb fuck. Ok, so maybe we can ust clean it... we ll see I don't think there is an actual need. 
Anyway, the important thing is state we store the current state_dict and the previous state_dict. Simple, we just have to store it before it changes, and that would be the previous state. 
Now, we care to store the difference, should we consider the absolute value of this difference or not? If we just want to know if it was a big or 
small change but we don't care in which direction than we cold consider the absolute value, if we also want to know the direction of the change
then it is necessary to keep the sign. Hinton graphs are the choice to represent this weight change, I still have to find a kind of graph which works but has color, 
like a dense red for a positive change or deep blue for high negative change. The question that I have right know is how should we combine this change 
in a graph? Like we wouldn't really need to visualize it for the meta learning approach to work, it would be more for us. As an example to start we could also create it for just one image to image, 
or one epoch to epoch, or also batch to batch or why not from dataset to dataset. 

[fixed]
First issue, keep in mind we are doing just one epoch. Second it seems that the kernels in the hebbian layers are not 
getting modified during training. It is true that the modification of the weights in a kernel happen in relation
to the backpropagated error and since here there is no error to backpropagate we don't know how to change them in the first place. 
But still, aren't the neurons just an input passed through a function which gives an output? And the weight is related to that 
output right? Ok fixed, I need to execute a deepcopy rather than a copy on the state_dict of the model so the weights change correctly during training.


################### NEUROMODULATION approach
Now another problem arises: 
    - We want to measure the change of weights between two different tasks. This means that we have to store all the weights of the first task, very expensive man. 
    Let's say we don't care about resources usage. We store it, and every few updates, like every 100 images we decide to apply neuromodulation through the analysis of the weight change. 
    This approach could work, but I don't like it because it is very expensive spatially. 
    - We want to measure the change of weights between two different tasks. A different approach from the one used above could be to train only on the first task,
    without storing all the weights,  and what we do is actually analyze the weight update in that particular kernel / zone. This implies that we would 
    need to find a correlation rule between following kernels, so that we are able to apply the neuromodulation in the same style if we organized all the weights in 
    a 2D matrix but without actually doing it.

Out of the methods explained above the second is the one I like the most. So I am going to go with the second done.
So the solution we could utilize for the second approach is to catch is the weight change and scale it. But wouldn't this be as if I apply an upper 
bound to the learning rate /  plasticity of the network? Yes, it is pretty much the same thing because whenever there is a weight update bigger than I would like 
I just limit it. But hold on, I limit it in a certain part of the network, this could mean that I unlock in other parts of the network, but then 
It would mean that I go over threshold on the other part of the network and limit it there too. I think the way I apply neuromodulation cannot be decided only 
based on the weight change, but also what that weight change is changing: like am I modifying zones where task information was stored or am I modifying parts of the network
where no acctual information was stored (like they are almost as the initialized weights before training) ? So I need to know what I am modifying.
Ok, what I can do is during task 1 I can keep track of the average change per kernel, I put them in a 2d matrix where the row 1 contains the average kernel changes for layer 1, and cell 1 is for the 1st kernel.
I take this matrix, and zones where there is high change in the kernel value means that zone is important for task 1 and so if the change is big for task 2 in the same kernel I suppress it and increase the plasticity in other parts of the network.

How is the activation calculated?
DELTA w_ik = n * y_k * x_i -  n * y_k * u_k * w_ik

Like if I want to redistribute the learning in other parts of the network, what should I neuromodulate?
I should act on the learning rate probably, I increase it here and reduce it where I want to redirect the learning.

- Remember that there was a paper which did adaptive learning rate, cite it and be sure to find the similarities/differences with this approach.
- Remember to consider also recurrent connections, maybe they could work. So learn how to implement them and understand if they accept the hebbian learning 
    like the one in the paper or they work in backpropagation setting. Unsupervised RNNs????

The fact about modifying the learning rate is also to consider the adaptive learning rate of softhebb itself.

Ok let's start. I would ignore the adaptive learning rate for now. 
During training we have to consider the way the weights are initialized. In particular, when doing the vaerage weight change in a kernel it could happen that the weights are by chance initialized
the correct way and so they don't really get updated. 
So first let's understand how the weights are initialized. The weights are initialized following a normal distribution, so I guess we just ignore it? Mhhh, let's see, 
If the weights are nitialized correctly for task 1, so there is no big update, then there is no real way of understanding how much that is important for the task 1 during training 
of task 2. In that case how should we behave? Bare in mind we are ignoring the loss of the classificator to keep it unsupervised.
The learnt information is expressed through the loss of the classificator. We have to investigate if using the network's loss to decide 
what parts of the network contain which information makes it supervised learning or still unsupervised. 

To see what happens in the network we can use the UMAP technique. 

Weight visualization: 
    - https://distill.pub/2020/circuits/visualizing-weights/
    - https://cs231n.github.io/understanding-cnn/
    - https://distill.pub/2018/building-blocks/

CNNs: 
    - https://cs231n.github.io/convolutional-networks/

We need to  study ho the weights change in the network. 
The solutions proposed are about using activation maps and heat maps. The activation maps basically contain the activations relative to a certain layer or kernel? Like do they contain the activation of the single layer and sum all the channels together? Or do they contain the activations per channel, so all the kernels in the same layer organized per spatial zone of the input they worked on and all stacked together? Or maybe an activation map can be all of the three and they are just different kinds of activation maps. 
Ok so the feature map, or activation map, is the result of convoluting one filter among all the ones in the channels over the input image. So the feature map is done by filter in one layer.
Now on with the heat maps, what are they? 
So the heatmaps are basically maps which when overlayed to a certain input allows us to know which parts of an image are useful to recognize something. I don't think they are very useful for our objective. 

I would start with visualizing the activation maps of a certain layer. And then we can graph the average change per filter in the layer and average activation
(so we need to create a semantic dictionary). The semantic dictionary can include sorted key value pairs of the filter and the corresponding activation. In addition we can graph somehow the vaerage filter weight update. 


The semantic dictionary can allow us to kind of sort which kernels are more significant for a certain dataset???

Anyways, let's reorder th e thought real quick. We did msanage to graph bothe the kernels and the activation map of the fifrst convolutional layer. 
Now we need to figure out how to graph the change in the kernels relative to a certain dataset. We can use the idea of calculating the average change in kernel:
to find the mean if we sum up all the changes of the kernels and divide them byt  henumber of kernels we obtain the average change of kernel per layer, so
not much is known about the importance of the single kernel. What we can do is calculate the average change of kernel: 
per every batch of # images we calculate the average. So we find the difference between the first step and the step #, sum all the changes and this represents one of the dataset_len/# changes, so we sum all this changes and divide them by dataset_len/#. At this point we have all kernels which contain the average change. 
When training the network on a new task we need to analyze the change that we have in the kernels, and if the change surpasses the avg change corresponding to a particular kernel we reduce the plasticity and increment the plasticity of other kernels in the 
layer. But to apply the3 plasticity rteroute we need to know which kernels are important for a specific task so that we can avoid modifying those. 

Another idea couldl be to use a recurrent layer so to consider the kernels state before seeing the current new task and somehow incorporate that into thelearning of the new unseen task.
MMMhhh look at the differene with residual layers...

anyways, let's start and implement the function which calculates the difference of the kernels and then calculates the average on the single 
filter every # images. To do so we need to tap into the cnn training procedure, store the activations...

!!!!!! AHHH found huge dataset that we could use: LESUN letsgoooooo

Ok, I calculated the difference in kernels every some steps and now I get about 50 delta tensors of all the kernels in all the layers.
Everything seems to be working fine, the only doubt I have is about the very small weight change I obtain after the sample the network sees, maybe
it is reduced to a very small number right after?? Could be due to the adaptive learning rate I have... better dig deeper into that. 

Now I need to calculate the average change per kernel, how to do so? 
we sum all the kernel changes by summing up all the cells of one diven kernel, and that would give us the change of 1 kernel, we repeat this for all the 
kernels for every different measurement and divide the sum obtained of every kernel by the number of delta measures we have.

Ok, the part where we gather the weight updates, collapse the three channels of every kernel, 
and calculates the average weight change per every kernel and then finally sum all the cells of the averaged 
kernels to obtain one final value per kernel and then we normalize that value for the maxiumum registered average weight change in the kernels is completed. 

Now, I would like to create a semantic dictionary which allows us to say which kernel is mostly important for that particular task or which
top k kernels are most important for that task. This could be doable by sorting all the kernels activations per image, retrieving the top ones and registering
how many times a kernel has fallen in the k first positions, then after the whole dataset has been seen we sort the kernels with the associated value and retrieve the first top k, 
so the first being the most important and the last the least important. And so we combine this idea with the average weight changes registered so that
if the weight change of the  new task is over the average weight change of a particular kernel of the previous task and that kernel is in the top k kernels for task 1, we apply some kind of neuromodulation
like for example reduce plasticity on those kernels and augment it on the ones not in the top k.

The question arises... where should I store the tensor of the average weights change? When should I apply the neuromodulation? 
When  should I check if the threshold of the weight update is broken? When should I calculate the semantic dictionary for the activations of the kernels?

For the complexity of a network: https://medium.com/@marvelous_catawba_otter_200/a-brief-discussion-the-computational-cost-of-backward-propagation-is-approximately-twice-that-of-5dd0eac9b389
                                https://medium.com/@pashashaik/a-guide-to-hand-calculating-flops-and-macs-fa5221ce5ccc

I could store the tensor into a variable, which is accessed in every training and keeps track of the change of every kernel and checks if the average change of that kernel is respected or not. 
If it is then we change the plasticity of that part of the network (with the 2 adjacent kernels for example) and increase it somewhere else where the threshold is not broken.

!!!! Instead of using sum to calculate the value of each kernel I can use standard deviation.

[Problem -- fixed ]
When saving the vector of the average kernel weight change the size of the saved vector is more than 1gb which is impossible, considering that the model
itself is much smaller. 

Ok, so apparently we are saving the delta_weights vector, rather than the avg weights update per kernel.

#############################
The problem I have right now is that I don't know how to keep a link between the most important kernels I have learned during training
and the current kernel during the current training so to apply the threshold check. 
I could associate an index to every kernel, and to keep the top k I put the indexes in a list. 
I need to find a way to access the single kernel instance. 
So, I could technically keep track of the single kernel, but the problem is that when I reload the model to implement continual 
learning I am not able to use the same kernel references, this is because when the model is reloaded the references are reassigned. 
So, I need to have a reference system which is relative to the model and not to the memory system. I could keep an index, I think the 
positions of the tensors is not changed, meaning that a kernel will have a relative position in the layer which is constant, so I can use a dictionary, made up of 
the indexes associated to the kernels.
So what I do is: 
I look at the activations, every how many training steps though?
Associate every activation with an index, inside a hashmap (dict)
Sort them by value to obtain a semantic dictionary ( it means I need a sorted dictionary)
At this point take the first k and increment the value of the first k in the kernel dictionary.
Do this for every layer.
At the end of training we should have a dictionary which contains the most important kernels. 
This is scalable due to the fact that we can keep on updating the dictionary when we increase the tasks effectivly obtaining
an importance score for the kernels associated with all the tasks. 

[Problem-- GPU out of memory]
2 scenarios, one could be thata I don't have all the 64 gb available for myself. Meaning I either need to speccify somehow that I need more gigs or I don't know.
The other scenario is that I somehow fuck up and store too much stuff in the GPU. Now, I never had any problem regarding allocation when executing
the code to find the best models, this means that the problem is due to the continual learning implementation of the engine.py file. If we think about it, we store a lot of tensors in the GPU, 
but this tensors are not so big to use the whole 64 gbs available. I should try by executing the code without using the engine_cl but the normal engine. 2

Ok, I ran the code again but with engine, and it doesn't have any problem. So I allocate too many things in engine_cl. 
Meaning I am the root cause of the problem. 
Where could the problem be? I need to look at all the tensors I allocate.
Since the memory that I end up using is the GPU memory, I need to watch out for all the tensors I copy into it. 
I do it in: 
- get_delta_weights, in particulare I do it to calculate the delta of the weight update. It is a for loop which is iterated # kernels in layer times, so I copy a layer into the the GPU, 
how many times? I do it 50 times for examples, if done for 3 three layers I have 150 layers!!! copied in GPU. 
This could be a potential issue. 
Since everytime I do the delta I detach and clone the result I can remove the just copied layer from GPU in order 
to not use all the space. 
I can use: 

del t # removes the allocated gpu memory for tensor t
torch.cuda.empty_cache() # removes the reserved memory for tensor t

It still doesn't work. I don't understand if the cause of the problem is linked to the training process or not.
What else is saved on GPU?
it is not caused by get_delta_weights because I made it do just one cycle and problem persists. 
Something else is filling up the memory 

[-- FIXED, it was the vectore to create the images which was using up all the memory]


Now we continue to analyze the problem: 

So what I do is: 

1. I look at the activations, every how many training steps though?
    So I have the list of activations, let's say we start by doing it for every training step.
    The problem is how should I save the activations, the training is done per layer, so the first layer sees data, 
    then the whole 2nd layer does and so on. Which means I need to save the information in such way that I can retrieve the whole activations
    object at once. 
    [!!!] WATCH OUT because the whole activations object is loaded in gpu, so you must unload it sometime somehow !!!
        here we have to dive deeper on the sign of the weights... should we consider abs value once we summed all the cells in the kernel
        or at the beginning before doing the sum? Or maybe not consider abs values at all... ?
2. Associate every activation with an index, inside a hashmap (dict)
    I keep on summing the activations instead of limiting them by task. I should probably normalize them by the highest possible activation 
    value. 
3. Sort them by value to obtain a semantic dictionary ( it means I need a sorted dictionary)
4. At this point take the first k and increment the value of the first k in the kernel dictionary.
5. Do this for every layer.
6. At the end of training we should have a dictionary which contains the most important kernels. 
    I would update the dict containing the top k kernels only after  every task is learned other 
    wise we have to sort it too many times ( or maybe every few training steps we update it and re-sort it) the control to modify the kernel plasticity 
7. This is scalable due to the fact that we can keep on updating the dictionary when we increase the tasks effectivly obtaining
an importance score for the kernels associated with all the tasks. 


One problem we need to address is the fact that when we do a first training and when we do a 2nd training  using another task
the network must behave differently. To memorize the kernels I use their index and I start with an empty dictionary, so when I check if
the kernel is among the top k or not I am free becaue there are no kernels in the dict yet (talking about the first task to be learned).

The vector containig the firt top k indexes of the kernels is fully implemented. 

Next step, it is to use the averages of the kernel change and the top k activations to modulate how the network learns. 
We need to figure out a way to pick on the sigle kernel in order to apply a plasticity variation which should happen in the 
variable learning rate part: 
Currently I am looking at the hebbconv.py file which contains the class for the convolutional layer.
If the training happens by simply doing a multiplication between kernels, which is probably how it happens, what we do is modify the
tensor which contains the weights by multiplying them with a vector of learning rates which has one learning rate per kernel. 

After carefully analyziyng the functions, in particular the ones inside the hebbconv.py file: get_wta of the HebbSoftKrotov class, we don't really need to understand 
what happens inside the function since we really just care about the returned value which is a tensor containg the pre activatioins of the current
layer which have been fed through a softmax function to obtain a tensor of values which are between 0 and 1. This tensor is then 
used in:
- forward: it is used to calculate the wta relative to the all the batches, so in the end we have a list of wtas to pass to 
    the plasticity function. 

The next impportant function is the plasticity function which basically calculates the weight update for the given layer. 
the self.delta_w field is filled with the weight update. The weight update is then used in the update function which is called everytime we do model.update

The activations are passed forward because we use the forward function inside the single convolutional layer (HebbSoftKrotov) and then also use the 
forward in the layer.py BasicBlock which will return the activated input x to the next layer. 

I would insert myself in the update function of the particular layer. We need to do a couple of things: 
- extract the top k kernels
- check in the delta_w is above the threshold for those top k kernels: 
    - if it is not we don't do anything, meanign wwe just keep on training
    - if it is above the threshold we need to limit the learning on the top k kernels. 

There is one slight problem: first the learning rate, which we need to modify but it is a number which multiplies the vector of kernels. 
In our implementation we should have something which can be applied to every single kernel independently. 
We could work in two different ways: first we could just implement a function which is called eveytime we need to update the laerning rate and returns a vector 
which is the learning rate based on the above considerations. The second solution we could implement is to create a mask, meaning that we create a funciton that return a tensor containing
scaling factors for each and every kernel. This scaling factors will be all multiplied by the learning rate and then the whole vector will be multiplied by the kernels' vector. 
In my opinion the second solution is much cleaner than the first one because we don't need to deal with the calculation of the learning rate which will be calculated like explained in the paper. 
Ok, actually the learning rate is a tensor which has the same size as the layer of weights. This is good, meaning we don't make things too much heavier. Ok, now the flux of execution:

- start by creating a function that retrieves the top k kernels
- after this create a function which returns a tensor containing the indexes of the kernels which break the threshold limit
(should we consider an interval around the mean, meaning that values which are slightly lower or sligthly higher are fine, or values which are only lower?)
(or in this case we can return a tensor which is set to 1 where the kernels break the limit and to 0 where they don't, but the tensor is of all the kernels, not just the top k)
- We have a mask which has 0 if the threshold is not broken and 1 if it is.
- Now, out of this mask we need to consider only the indexes in the top k kernels. 
(for this I could consider two masks, 1 where I set indexes to 1 if they break the threshold and on e where I set them to 1 if they are of the top k kernels. And then I confront them and teturn a 1 hot encoded mask which
has all bits to zeros except the ones breaking the threshold and belonging to the top k). And this is to calculate where to reduce the lr,
Then we consider a one-hot tensor of the cells which are not the top k kernels, and this is where we should increase the learning rate.
- lastly create a function which given the tensor of the kernels which break the threshold returns a mask which contains coefficients lower than 1 in the 
kernels which break the threshold. 
How to reduce or increase the learning? 
I could work in a simmetric way, meaning that I increase it and decrease of the same quantity. To do so the lower lr should be made up of -1s instead of +1s...
Ok when we want to increase it or decrease it it is usually better to decrease it by a higher factor which is between 2 and 10, thus multiplying by 0.5 and 0.1.
When we want to increase it, it is better to do it in smaller steps like 1.1 to 2 times.

If I have a mask which has 1s and I need to obtain an increase of 10 percent I can simply multiply everything by 1.1 and then multiply it by the lr.
Then I can do the same when I reduce the lr, meaning I multiply everything by 0.9 and then multply by the learning rate.
At this point I sum this two, and I obtain the fantastic mask. Attention though, you need to replace all the 0s with ones, so then you can place it in the multiplication of the weight update. 


One problem I have is how am I going to load the objects containing the top k kernels and the average weight change?
I could include a field in every layer, but that wouldn't be so efficient because I would load the same file too many times, meaning I would have the copy of the same object too many 
times. One thing I should do is try to attach it to the model object (MultiLayer). Let's say I attach it to the multilayer object, then I need to find a way to let the single layers know where they are in the model. 
Or better when I create the model I can pass to the layer the corresponding object directly. So I load the file when I create the model, and every layer I pass on the corresponding layer in the 
saved object. I am able to do this when I generate the multilayer in the init in model.py.

Last element to consider is the fact that the objects don't have the same shape as the tensors of kernels. So when comparing the weight update to the average allowed kernel weight
update we need to calculate the average on delta_w and then compare it to the average delta weights tensor, store where the limit
has been broken and return the indexes of these kernels. 

Ok we have successfully retrieved the top k kernels and the average weight change per layer and we have successfully propagated them until the 
update, forward etc. functions. Now we need to implement the functions that we were discussing. 


[Done] Figure out a way to move all the learning rate calculations to GPU for speed up... I loaded all the tensors in GPU but no speed up is visible. 
[!!!!!] MOVE WHERE THE ACTIVATIONS AND THE AVG DELTA CHANGE are calculated, from engine_cl into hebbconv.py

[!!!] New Hyperparameters:

    - Top k, choosing k based on the number of neurons in the layer
    - How to modify the learning rate 

[!!!!]
The first thing I need to do is to make it so that everytime a new folder is created, meaning I train a model on a subset of classes and
then retrain it on a second set of classes, what I do is create a new folder everytime which contains the vectors of the average delta weights and the relative top k 
vectors [to have a unique id I can use the date]
I should modify the code so that it saves the results into a separate file if they are about continual learning with the topk active. 
I need to add the option to pass in the run that the program has to emulate. Meaning that I pass in T25 for example and so the program
reads off that the classes which need to be used for CL.
[Done] Check that the classes are not shuffled everytime, otherwise it doesn't work... this wouldn't make sense because how can you compare different runs
on the same dataset if the indices associated to the classes change everytime...

To modify the way the models are saved: 
we do 1 run which is classical continual learning baseline without any other solution implemented.
In this case we want to save the model..., do we though? like what do I need it for. So we run the model, we train it and then we save it in to a folder
that has the name CIFAR10_2C. 
I need to differentiate between two cases: 
- When I run the models without CL sol and when I repeat the run on the same subset of classes but with the CL sol actiive, in this case I need
to keep a link between the 2. So I would store them based on the name, like for example CIFAR10_2C + date( of the first run without CL sol active).
- When I simply run the CL case, with or without CL sol active. 

I can combine the two by creating CIFAR10_2C folder and plcing inside two subfolders: single and paired, and in paired I emulate the run with no CL sol active.
In each I save them using the date. 

In the json file, to keep track of parameters, I simply add a field which tells me if the CL solution is active, if it is a single run or if it is paired with some other run, and what run it is paired with
and also a filed which contains the name of the run with the date id so I can go back and take it. Or I can simply use the ids in the json file as a unique ID 
instead of the date. 

How do I communicate which run does it have to emulate? 
I look at the run number in the json file and pass that as a parameter.

The other scenario is if I want to run a model with more than 2 tasks. I need to implement something which given a run name, it loads the model, checks 
the parameters it was trained with and makes sure the classes in the continual learning tasks are not repeated. Then trains on a new set of classes and 
saves the copied model in a new task by copying the data of the Tx is started of and appending data to it.

[Done] Check if there is a way to incorporate the tensors of activations and average deltas into the model itself so you don't need to save them separately.
OK so far wew got the following: 
- MultiLayer.py incorporates the different blocks. It is also what is actually saved of the model, so when we want to save parameters we have to pass them in torch.save
- Each block is contained inside the MultiLayer Object and is generated through the generate block function. 
What we can do is: 
- move the calulations of the single vectors inside each block, at the end, when the training is finished we run though every layer and store the calculated tensors inside a list/dict.
- when we load a model we retrieve this object and then we pass the corresponding vector to the associated layer to update the corresponding field. 
- Where should we place the calculation of the tensors? we could place it before saving the model.  
Basically until now we have saved 2 params in the model object, the entire tensors.  We couldn't save them per every layer because they save
model using the dict and not torch.save(model) method. So we simply save them and then redistribute them between the various layers. 
To  retrieve them in the single layers we have already dones the work. Technically wwe are done, we need now to deal with how we save them and how we load them.
to save them we add two fields in the torch.save and use the same fields in the load of the model. 


[!!!] Try and train for more than 1 epoch on every dataset.
[Done] modify the way the vectors are stored such that each layer keep one vector only without creating an object that is a dictionary. 
We would need to move the computation of these vectors somewhere else in the model. I would do it in the hebbconv.py file but the problem is that when we save stuff
we save them in the MultiLayer parent object.

[!!!!!!!!!!!!!!!] ISSUE 
There is a huge problem, basically we implement a continual learning solution which works only for the CNN layers... the problem is that we modify also the last
to classify the current task. What should we do? We could use the same approach also for the last layer... and see what happens. 
How could we do it on the fully connected layer as well?
Ok, we need to treat the fully connected layer like the CNN one. Basically we do the same exact operations. 
Another solution could be to use a different head per different task, so we store the head for every different task and when we evaluate/train we
pass which task we are working on select the correct head. So the model will have a field which contains a different head for every different task.

First try: 
Implement the same solution we implemented in the CNN on the last layer. The shape of the weights on the last layer is: [2, 24576].
So we have to apply the solution to 24576 neurons which will then be combined in only 2 activation values. Every epoch I take the previous weights, 
update th eodel, take the current weights and calculate the average weight change. At the same time I take the activation values, 
and sort them to find the most important ones, remeber to choose k accordingly to the size of the layer. There is to see what is the size of
the activation vector... [64, 2], why the fuck is it  like that. We have 64 objects, each one of them containing the probablity for class1 and class 2. Now, why 64? is it becasue we have a batch which is made up 
of 64 images: yes, it is like that because the isze of the input is torch.Size([64, 3, 32, 32]). So what could the solution be? I could consider the flattened output of the last
convolutional layer to say which are the important neurons, I flattened it and then I consider the top k ones ... but I am confused... 
wouldn't the input of the last layer be the output of the convolutional layer? Thus have different shape then torch.Size([64, 3, 32, 32]).
It is strange because the last layer seems to be trained separate from the convolutions, it doesn't seem to change anything also if I use
the successive option or not because the hebbian and the last layer are trained though two diff. functions and those functions call 
make_data_loader everytime, which loads the dsataset. So I know the classifier is mounted on top, but it doesn't seem to do anything other than learning without the CNN.
How is it possible though? Like it wouldn't make sense, to have a proof of what the last layer is seeing, meaning the image 
from the dataset or the activation from the last layer, I should print it. In the demo it actually passes the out of the last 
conv layter to the input of the last linear layer. So what the hell...
In the demo it works like it should meaning that the input from the convolutional layers is passed to the last classifier layer. The weird
thing is that in the normal multilayer file, th ecnn layers and the classifier ar trained separetely, it is very weird... 
Let's see more in detail what is the hybrid functionality doing. The hybrid functionality works by training all the layers together. How does it do so? 
In the other cases it would iterate on the different layers and then call the functions accordingly. In this case it looks like it calls the training 
function once and it does so on all the layers. The weird thing I don't understand is how does it do so?
The for loop in the main still iterates over the layers I think, unless they are organizedin a different object and in which case we would go through only one iteration.

ok I am going to try and impolement the continual learning solution on the last layer as well. If we consdier the last convolutional layer: we take all the activations of the 
kernels and we flatten them, then we combine them all (with weights) to return two values. 
To modify the update of the weights we cannot directly access to the learning rate because it is managed by torch. We need to find a way to apply a mask before the weight update happens. 
Easy, we apply the mask directly to the current newly calculated weights... mmh that would rescale also the prev weights though:
ok, we simply do curr - prev (and this corresponds to the weight update), multiply by the mask and then resum it to the prev weights
to obtain the curr weights with the mask applied. 

Now, let's recap a bit:
- every 50 something iterations we keep track of the weight update by summing it to a certain tensor.
- at the same time we keep track of the activations, by summing every time the activations into a tensor.
- during training we check, right after the curr weights have been calculated, if the weight update is over the threshold, and if it is we calculate a mask
and apply it before recalculating the current weights. 
- in the end we sort the value in the activation tensor by value, and then consder only the indexes of these neurons. 
- in the end we take the tensor of the average weight change and we calculate the average weight change per every neuron.
Let's do soemthing very tidy and clear: 
- we need to include also the tensors of averages and of deltas of the linear layer. 
- right now we save the tensors as follows: 
    - whenever we save a model we include the two fields acts and avg_deltas in the torch.save()
    - whevever we load a model a model we retrieve these 2 fields from the checkpoint and assign the corresponding fields in 
    the model. 
    - whenever we create a model we have: the corresponding acts and avg_deltas for every layer are extracted and passed in generate_block.
    [FIXED] Problem encountered: if I reload the model after the training of the first task and then assign the fields the generate_block, 
    which is called upon creation of a model, will receive all empty fields for acts_layer and avg_deltas_layer... we need to pass the parameters
    when we load the model. Let's check if they are actually correctly set or not: the problem exists since after loading the model we get these fields: 
    self.avg_deltas_layer:  <class 'NoneType'>
    self.top_acts_layer:  <class 'NoneType'>
    so the fields are not populated correctly. Let's fix it by passing the avg_deltas_layer and top_acts_layer as parameters when creating the model.
    ok the problem has been fixed.
    - until now we have done such book keeping operations for the the convolutional layers. to keep track of the classifiers as well we need to implement the same 
    functions also in the supervised learning part: 
    DONE    - first we need to add a hook to record the activations on the linear layer as well, so this needs to be done in the continual_learning.py file. 
    DONE    - then we need to tap into the training procedures: 
                - we can for sure reuse the functions implemented for the unsupervised learning part. 
                - use the same schema used in the usupervised learning. 
    The following cannot be donebecause the linear layer used is the default on eof pytorch.
        - after we managed to keep track of everything we need to modify how the layers are passed the single activation and average vectors:
            - tap into the linear layer class used for the classifier 
            - if you cannot do that you can always keep track of it with a specific field in the model class itself. It is not really  problem 
            because we won't have more than one fully connected layer probably, at most we will have two. create a list to store the specific vectors, 
            so in case we want to use more than 1 fully connected layer everything is already set up. 
    #################################

    DONE    - then remember to modify the ifs where we look only for the convolutional layers, because at this point we also look for the fully connected 1.
    DONE    - very important: we also have to modify the unsupervised training, b yplacing an if and checking that what we are loading is actually the correct type of layer tensor. 

    [!!! Problem] Basically what happened is that the values utilized for the calculation of the delta weights vtensors and the values used fro the calulaion of the top 
    k important kernels are jsut 2 values... I wonder if the problem is localized only to the top k kernels, for which we look at the activation or also to the delta
    weights. If it is localized only for the top k, we could use the "most important" kernels of the last convolutional layer and consider the weight updates of the last one... I think the last layer has
    only two weights though ... Let's check these 2 facts:
        - The shape of the weights on the last layer is: [2, 24576]. Which means that we have two weights...no?
        Ok, so the last layer actually doesn't have any activation function but it only multiplies the flattened weights for the input and spits out 2 probabilities, 
        so to understand which weights are more important per task, we can just do a top k list of the highest values obtained by multiplying the weights with the correspondign 
        inputs. The Linear Applies an affine linear transformation to the incoming data: y = x*A^T + b
    
    [!!! PROBLEM ]
    Ok hold on, I am at a loss. We now need to clarify why the network doesn't train using the classical feed forward way... It is so weird that they don't feed it but they give each layer the input right away everytime. 
    No sense man.
    It is important to mention that the training is the only thing happening in htis weird way. Meaning that the evaluation phase is probably performed normally, with 
    the output of a previous layer being passed as input to the next. This means that the last layer will receive as input the flattened tensor deriving from the last convolutional 
    layer. Does it make sense? Not at all. Because the model has beemn trained in a different way and so if we do evaluation passin gdifferenmt inputs to every layer the result is that the network will work with data that has never 
    seen before and thus it canot yeld a sufficient enough result. I think it is important to write ann email to the people who made the paper. 
    I wrote the email asking my doubts but no answer wass given I think it is time to abort the op of implementing the continual learing solution on the head as welland directly try to implement the different heads. 


    [SECOND APPROACH --- FULLY IMPLEMENTED]

    The other approach involves utilizing another head for each new task encountered. We first need to understand where the head is created and mounted on the model: 
    - the model is created by calling load_layers: if the model is resumed or if we create a new one the flow is the same, we call MultiLayer()
    - inside MultiLayer we call generate_block() and return the actual block by reading its characteristics from the params. 
    Basicaslly it works that we save the specs of the layer and then wwe load the corresponding state_dict in the model. So we would need to savae a list of state_dicts inside
    the model and load the corresponding one. How to choose the corresponding one?
    There can be 2 main cases: 
    - the task we are currently training on has never been seen before, in which case we need to create a new head: 
    - the task we are currently training on / evaluating has been seen before by the model.
    In both cases we need to do an evaluation run of all the stored heads, pick the one which has the highest evaluation and based on a threshold decide if that is enough
    or if we need a new head. Like if performance is below 80% or something like that. We could technically make it so the threshold depends on the evaluation of the previously
    seen task, by doing an average and giving some sort of tolerance, so it would be kind of autonomous.
    Steps: 

        - include a list of state dicts inside the torch.save(). 
        - inside the load layers include a section where you evaluate the model on all the possible state dicts by loading them one by one in the model. 
        We need to understand how to load the convolutional layers seprately from the classifier layer: 
        - We could just remove the classifier layer from the state_dict of the overall model saved through torch.save() and add one of the saved state_dicts of the head into the state_dict 
        object before loading it into the model. Let's its shape: 
            - odict_keys(['blocks.0.operations.0.running_mean', 'blocks.0.operations.0.running_var', 'blocks.0.operations.0.num_batches_tracked', 'blocks.0.layer.weight', 
            'blocks.1.operations.0.running_mean', 'blocks.1.operations.0.running_var', 'blocks.1.operations.0.num_batches_tracked', 'blocks.1.layer.weight', 
            'blocks.2.operations.0.running_mean', 'blocks.2.operations.0.running_var', 'blocks.2.operations.0.num_batches_tracked', 'blocks.2.layer.weight', 
            'blocks.3.layer.weight', 'blocks.3.layer.bias'])
            - so we would need to swap out only the following: 'blocks.3.layer.weight', 'blocks.3.layer.bias' (which are of the head layer).
            - we know if we need to add a new head only after we completed all the evaluations, meaning that we need a field in the model
            - which allows us to know, right before saving it, if we need to store the current head or not.
            - We store the head regardless, if it is an old one bein gretrained on the same dataset we resave it, if it is a new one we save it again. 
            - So everytime we pick a head for the list, we need to remove it, and then we save it at the end of training, to avoid having duplicates.
        - if the performance is below threshold on all the state dicts then have a model from scratch by not loading anything inside the model itself. 

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

We need to try to make the model such that the layers are all linked with each other and the information is forwarded. I am very confused about the fact that using or not 
using the continual learning solution doesn't seem o bring any real advantage. Weird because we technically change the input of the head layer, so also its output should change. 
I want to try something very extreme. If I set the learning rate to all zeros in the feature extractor... what is going to change? 
It doesn't work, it gives me an error because it looks for the wrong layer... the layers are found based on their change, but we just nulled it... 
so they are the same.

[!!! PROBLEM]

Ok found a problem. We only keep track of layer at a time, the one that is training. This is wrong because we could have multiple layers training 
at the same time if we use the other modes like simultaneous. At this point we need to modify the train_hebb and train_sup_hebb, 
such that they keep track of all the layers, not just one which is the one that is changing. Because in other modes they could all change 
in the same epoch. So the pred_dict needs to keep track of everything. 

[!!! PROBLEM]

Additionally we need to find a way to calculate the current layer which is not the one that checks if one layer has changed or not. We need something 
that tells us all the layers changing. Tha could be achieved though the blocks variable inside train_unsup by passing it to train_hebb and train_sup_hebb.

Different trainingg specs: 

-train_layer_order: successive {'t0': {'blocks': [0], 'mode': 'unsupervised', 'lr': 0.08, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't2': {'blocks': [1], 'mode': 'unsupervised', 'lr': 0.005, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't4': {'blocks': [2], 'mode': 'unsupervised', 'lr': 0.01, 'nb_epoch': 1, 'batch_size': 10, 'print_freq': 10}, 't6': {'blocks': [3], 'mode': 'supervised', 'lr': 0.001, 'nb_epoch': 50, 'batch_size': 64, 'print_freq': 10}}
-train_layer_order: consecutive {'t0': {'blocks': [0, 1, 2], 'mode': 'unsupervised', 'batch_size': 10, 'nb_epoch': 1, 'print_freq': 10, 'lr': 0.005}, 't1': {'blocks': [3], 'mode': 'supervised', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10, 'lr': 0.001}}
-train_layer_order: simultaneous {'t1': {'blocks': [0, 1, 2, 3], 'lr': 0.001, 'mode': 'hybrid', 'batch_size': 64, 'nb_epoch': 50, 'print_freq': 10}}

[!!! SOLVED]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     
The first thing we need to modify is the fact that both train_sup and train_sup_hebb use an approach which only works
with the successive training mode. Why? Because we keep track of just one layer, meaning that w assume that at each 
training step only one layer is changing, which is only true in the case of successive mode, in consecutive all the hebbian 
layers are in training mode, and in simultaneous all the layers, both hebbian and supervised are training. 
So the first thing is to modify the get_layer function: it is used to find the current layer changing, in the case of the 2 other modes we can use the blocks variable
which contains the layers changing because in training mode. The layer_num variable populated trhough the call get_layer is used in 2 
instances: 
- when adding the layer delta and activations to the tensors in order to have a field in the dictionary:
    activations_sum.append(activations["conv" + str(layer_num)].cpu())
    delta_weights = get_delta_weights(model, device, layer_num, depth, prev_dict, delta_weights)
- then inside delta weights: 
    curr_dict = {k: v for k, v in curr_dict.items() if str(layer_num) + ".layer.weight" in k and k in prev_dict }

- and when calculating the prev_dict: 
    prev_dict = {k: v for k, v in prev_dict.items() if "layer.weight" in k and str(depth) not in k} 

If I am not mistaken the layer_num variable is only used to select the layers we are tracking, meaning that if we don't use it we keep in memory also things we don't care about.
The thing is that we care about them depending on the mode, so if we want to make things simple we can just not optimize this part and tracking everything all the time. 
The first stop we encounter is at line 332 with activations_sum.append(activations["conv" + str(layer_num)].cpu()), the issue lies in the fact that the 
structure activations_sum is a list, thought to contain only the activations of 1 layer, the solution is to transform it into a dictionary where the key is the layer
and the value is the list of partial sums. We need to pay attention to the fact that if in successive mode, the only layer activating is the the one in blocks, so to update the lists
and to then calculate the mean we need to be sure the name of the layer is in blocks list. 
Let's also solve it for train_sup_hebb whihc is used in hybrid mode: 
Now let's go through what the code does: 
- we obtain the avg_deltas and the acts from the model
- we caluclate the maximum depth of the model
- obtain the prev_dict bby isolating all the entries which are weghts and which are in the blocks list
- do the same for the activations_sum
- activations_sum dict we append to the right list the activations of the corresponding block, and
we do this for all the blocks in blocks, including the linear ones.
- then if the iteration reaches the interval set to gather the delta_weights we gather them 
- and then we calculate the prev_dict again
- then for all the layers I:
    - summ along dimension 1
    - resum along dimension 1
    - obtain a sorted semantic dictionary of the activations
    - select the top_k to be considered
- Then I calculate the average deltas 


[!!!]
We need to calculate the averages of all the tests. Now, we have to create an array of possible combinations, the thing is that for 10 binary tuples
there is a 100 tests, maybe we can do 50. For 10 classes assigned in 4 there are 10k possible combination, so here we need to do a subset. I would 100 for both cases,
we cannot send 100s of jobs though, we might need to find a way to exploit:
    - nodes 
    - cpus
    - gpus  


[RESULTS]

Using the following configuration: 
{
    "training_mode": "simultaneous",
    "cf_sol": true,
    "head_sol": true,
    "top_k": 0.5,
    "high_lr": 0.0,
    "low_lr": 1.0,
    "t_criteria": "mean",
    "delta_w_interval": 100,
    "heads_basis_t": 0.6,
    "selected_classes": [[decided randomly]]
}

There is no improvement if we activate the kernel solution or not, while if we use the head swap solution the performance go way up. 
Increasing the size of the model seems to yield better performance. 
If we retrain the network on the same set of tasks a second time the performance goes way more up. 
if we do 4 tasks the solution still works and the same behaviour icd backs found as just 2 tasks.

if we use  "high_lr": 2000.0, "low_lr": 1.0 lower performance on eval_1
if we use: 

4 TASKS

On CIFAR 100: 

"cl_hyper": {
        "training_mode": "consecutive",
        "cf_sol": true,
        "head_sol": true,
        "top_k": 0.7,
        "high_lr": 0.2,
        "low_lr": 0.8,
        "t_criteria": "mean",
        "delta_w_interval": 100.0,
        "heads_basis_t": 0.8700000000000001,
        "selected_classes": [
            [
                2,
                8
            ],
            [
                1,
                5
            ]
        ]
    }

performance is higher than when cf_sol is off.

Let's see what happens if we reduce: "delta_w_interval": if we reduce it to 20 the performance goes up to about 89.5%
 "cl_hyper": {
        "training_mode": "consecutive",
        "cf_sol": true,
        "head_sol": true,
        "top_k": 0.7,
        "high_lr": 0.2,
        "low_lr": 0.8,
        "t_criteria": "mean",
        "delta_w_interval": 20.0,
        "heads_basis_t": 0.8624999999999999,
        "selected_classes": [
            [
                2,
                8
            ],
            [
                1,
                5
            ]
        ]
    }

Increasing topks above 0.7 doesn't seem to have any positive effect on the performance.
Reducing topks seems to lower performance to 81%:
"cl_hyper": {
        "training_mode": "consecutive",
        "cf_sol": true,
        "head_sol": true,
        "top_k": 0.1,
        "high_lr": 2000.0,
        "low_lr": 1.0,
        "t_criteria": "mean",
        "delta_w_interval": 100.0,
        "heads_basis_t": 0.805,
        "selected_classes": [
            [
                2,
                8
            ],
            [
                1,
                5
            ]
        ]
    }

On CIFAR10 it doesn't seem there is any improvement between no cf_sol and cf_sol
On STL-10 it has a very small improvement. 
On IMG it has about 2% improvement. 

All the above tests where ran with battery of 4 different tasks and evaluated on the 1st.
On STL10 with just 2 tasks and evaluated on the 1st no improveent at all. 

6 TASKS

On CIFAR100 I tried with 6 tasks and evaluation on the first. The performance improvment with cf_sol on or off is of 1%. Maybe I can increase the number of 
topk kernels and see.
 "cl_hyper": {
        "training_mode": "consecutive",
        "cf_sol": true,
        "head_sol": true,
        "top_k": 0.9,
        "high_lr": 0.2,
        "low_lr": 0.8,
        "t_criteria": "mean",
        "delta_w_interval": 20.0,
        "heads_basis_t": 0.8883333333333333,
        "selected_classes": [
            [
                2,
                8
            ],
            [
                1,
                5
            ]
        ]
    }
Increasing topks to 0.9 seems to help keep good performance on the first task (about +5%)
For C10: 
"cl_hyper": {
        "training_mode": "consecutive",
        "cf_sol": true,
        "head_sol": true,
        "top_k": 0.9,
        "high_lr": 0.2,
        "low_lr": 0.8,
        "t_criteria": "mean",
        "delta_w_interval": 20.0,
        "heads_basis_t": 0.8708333333333332,
        "selected_classes": [
            [
                2,
                8
            ],
            [
                1,
                5
            ]
        ]
    }
there is an increase of about 4% with cf sol on.


ADD TASKS PARAMETER IN CL_HYPER

box plot per deviazione standard
leva linee che collegano i punti 
includi numeri esperimenti 
aggiungi eval after runs 
absolute val per delta_w
prova altri parametri di threshold oltre a mean
seleziona testa in base a quale ha la prob più alta più spesso tra tutte. 
modify epochs number for unsupervised learning, maybe more than 1. 
modify everything so that you don't reload the same dataset all the times but you use a validation set.
Try bigger Networks for different datasets.

[!!! DONE - - HEAD SOLUTION UPDATE]
We need to modify when we select the head. I would write a function which takes out the logic of the load function so we don't need to modify the load function 
too much. 
We need to: 
    - load the head on the model
    - store the activation values of the heads
    - pick the head with the highest activation value the highest number of times: 
    model.heads è una lista di dizionari e ogni dizionario è una testa.
        - we create a dictionary which has has key the index of the head in the model.heads list and as value the number of times that that head
        had the highest output probability among all the heads. So we record the probabilities, and based on the values of the highest among the outputs 
        of the same head, we create a score. How...? 
        - when selecting the head we calculate the score with the raw logits, not probabilities. 
        A higher logit corresponds to a higherr probability, but check with this solution or passing the ouputs through a softmax to obtain 
        proabilities and try again to see if something changes.




Continual learning
    - include fig.1 kudithipudi
Catastrophic forgetting
datasets
softhebb
Hebbian Learning rule y_k summarize and keep text in notes
experiments (metodo + baseline)
Baseline: include also the accuracy of the bar plots.
Objective integration: 
    - Nitric Oxyde
    - Neuro-modulation
solutions or methods: 
 - summarize slides with keywords and keep text in notes 
 - visualization of head layer in the soft
 - Where to apply it? add image of top k red and green
 - How to apply? add formulas of how the learning rate is modified. 
results



Look at entropy, information gain
The more changing neurons on the last layer are probably the most important. Look at those and travel back by putting in the top k the kernels obtained travelling
back. 

[ !!! DONE ]
We need to implement the model so that: 
- during training we are able to select the head to mount on the model:
    - we assume that everytime we train we are seeing another task
    - so we need to create a default approach where we simply create a new head and add it to the list
    - at this point we need to understand how to do the evaluation phase for inference: 
        - I would mount a different head everytime and record the one with the highest accuracy.
        - I need to understand the difference between multi-head and using a single head different.

#############################


[GRAPHS]
1 total graph + single graphs with more stats in them

[statistical test]
Kruskal-Wallis per più popolazioni
Post-Hoc per capire chi tra le 4 pop è differente da chi 
(tieni comunque 2 coppie separate)

Wilcoxson
test non parametrici per pochi punti.

head solution fixed
box plot implemented

-leva linee che collegano i punti 
-absolute val per delta_w
-prova altri parametri di threshold oltre a mean
-modify epochs number for unsupervised learning, maybe more than 1. 
-Try bigger Networks for different datasets.


To implement the statics tests we need to obtain the vectors of complete accuracies and of std_deviations. then we feed them into the function and 
print the p-value.
To obtain the full vectors: 
- 

- Fix std graphs 
- varia top k e learning con e senza top k lock 
- prova diversi numeri di task 
- entropia (paper from fra) 
- network più grandi 
- varia epochs per hebbian learning (def is 1)

- WARNING: misunderstnding on the mean and KSE approach. Mean is used to see which kernels break the threshold during training,
while KSE is used to organize the top k kernels compared to the activations solution. 

- we used bio inspired solutions, hebbian learning we choose, why choose it, hebb vs SoftHebb
- for kernel solution: generalized neuromodulation approach to solve Catastrophic forgetting ( KUDIPUTHI )
- head solutions: the brain uses different parts of itself to solve diff tasks, we use diff heads to solve diff tasks. 
- look for backprop results in literature fo continual learning 
- create paragraph which clearly categorizes bio and back prop methods
- bio inspired advantages wrt back prop solutions in terms of net size, training times and general computability advantages (no feedback)

- incremental testing: start with 2 tasks per experiment, 3, 4, 5, 10
- diff datasets = same dataset size (cut down C10)
- try to increment the dataset sample per task by starting with a cut down C10 with 500, 1000, 2000 samples per class.
- 2 tasks, 2 classes per task with C10: so we have to see if the kernel solutions helps in the case of more samples per class or just for many tasks(5 vs 2)



- aggiungi caratteristiche rete
- aggiungi num experimenti
- aggiungi accuracy come measure
- aggiungi grafico che confronta numero layer con tabelle usando solo le prime due colonne della tabella
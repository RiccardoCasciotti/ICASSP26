
library to visualize the pytorch model !!!!!

I am confused on what the accuracy of the unsupervised model is calculated on...

lebesgue_p

So we have a network model: 
what is the input type
What are the layer types
How is the learning implemented
what is the output


ray_search.py : where the parameters are parsed and the main is executed.
    
    run_unsup(): performs unspervised learning and evaluates at the end of every epoch
        train_unsup(): 
            train_hebb(): The criterion can be something like ... ??? criterion seems to be none always, just like measures. 
                            So are they both to be defined??? 


        evaluate_unsup(): 
            evaluate_hebb(): does the unsupervised evaluation ---------------- we can't get in here (return 0.0 error)
                infer_dataset(): 

    run_sup (): runs the supervised learning if the mode is supervised (??? there are instances where the MLP is unsup???)
        train_sup():
            train_sup_hebb(): weigth change vector which is done only the last layer ?????? +  if loss_acc problem
            train_BP():
        evaluate_sup(): 


    run hybrid ()





# Alright so how to organize the slides:
- I would first give a general explanation of what the code does ( so the different types of training, how 
they differ from each other and what they aim to do ), then I would get into the code with the high level 
explanation of the functions implementing the corresponding learning. 
- I would do a very high level discussion of the flow of the functions and the general 
execution of the program. Just include the name of the functions and what they generally do.
Like Input types and output. 

Then do a main discussion on the methods which are used for training and how the update happens. 
So when trainign a model the main thign to consider is what kind of input is used, 
the update rule used ( in our case it would either be hebbian learning without any loss feedback and classical BP)

Ok so the training is mainly done in multi_layer.py while in ray_search we have the search for the optimal hyperparameters. 

-- FIRST READ
We have seen that the soft hebb network is compared with the classical BP fully supervised method. 
From my understanding the hebbian network is trained first in a fully unsupervised manner in order to obtain  a way of clusterizing the points, 
After which we take it and use a classifier to assign the clusters to the classes. 


HebbHardKrotovLinear get_wta needs to be finished commenting.
In the soft implementation we use the activation function, which creates a vector of activations effectively assigning a probability 
distribution to the activations since their value is between 0 and 1. 


What is the concept of activation?? 
So we have a set of neurons on one layer linked to the neurons in the next layer. Let's say we go from neuron i to neuron k, what is the activation of the 
neuron i? It is the pre synaptic value passed in the activation function? Or is the post synaptic?? 

The datasets: 

ImageNette: width: 160, Imagenette is a subset of 10 easily classified classes from Imagenet (tench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball, parachute).

STL10: 10 classes: (some similar to cifar10)  "width": 96,
- airplane, bird, car, cat, deer, dog, horse, monkey, ship, truck.
- Images are 96x96 pixels, color.
- 500 training images (10 pre-defined folds), 800 test images per class.
- 100000 unlabeled images for unsupervised learning. These examples are extracted from a similar but broader distribution of images. For instance, it contains other types of animals (bears, rabbits, etc.) and vehicles (trains, buses, etc.) in addition to the ones in the labeled set.
- Images were acquired from labeled examples on ImageNet.

CIFAR100: 32x32 color images classified into 100 classes

CIFAR10: 

airplane										
automobile										
bird										
cat										
deer										
dog										
frog										
horse								
		
ship										
truck

Ok so when doing continual learning with the best models we need to set the skip-1 flag to true.

Question
But we trained the best models through the use of ray search which optimizes the training given a certain dataset,
should we do the same in the continual learning setting?? Like we do a ray search on the the best model but using another dataset, 
makes sense. So how to do it? We add it to the continual learning? Or better we do another file which is ray search for continual learning.


We need to train on less classes now. so we start with 2, and then same dataset but two other classes and then we
do an evaluation on the first two. And so on increasing the number of classes per task.

Important: in ray search continual learning and continual learning we use the BLOCKS gatheres from the datasets, maybe you should 
load the preset for the second dataset... but then watch out for the number of layers and stuff...
See if you should use the reset parameter or not in the ray search CL on the model... 

To implement the training on two different classes of hte same dataset we need to consider: 
    - we implemented a method which allows to train on two different datasets. 
    - we need to implement a method which allows us to train on one dataset and everytime we change the classes. 

The problems which I have found during the development of the continual learning approach which uses only subclasses to 
implement a task are: 
        - the number of samples specified in the preset corresponds to the total set of samples and not to 
        the dataset which has been deprived of some classes. A possible solution to this is that when we load the preset we count the number 
        of samples present... this solution unfortunately is very hard to implement because of the way the code flow is organized. Unfortunaly the preset is loaded 
        way earlier than the actual dataset, so modifying this information can be only done after the dataset is loaded and eventually modified so that we know the amount of samples present. 
        - the same assumption is done in the select_dataset() function where the indices variable is intialized by manually inserting 
        the number of indices to create in the list, and not by actually counting the number of samples present.
[fixed] - finally the other problem is caused by the fact that when we select the classes to include in the task they might not start 
        at 0, so we could have classes like [1, 2], problem is torch wants the indexes of the classes to start from 0. So a solution 
        would be to decrease all the indexes of all the classes in the dataset. This has to be done after the dataset has been resized and modified.
        Important is also to extract the labels ( in readable language that is ) associated to the selected classes.


!!!!!!! Problem detected: the ray search CL done per tasks seems only to save the eval run and not also R1 and R2.
!!!!!!! Problem detected: continual_learning loads the best model (trained with ray search) but overwrites it, instead we should use
                        a copy so to keep the best model untouched. So we need to figure out. 
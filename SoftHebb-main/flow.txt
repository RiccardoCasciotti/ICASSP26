
library to visualize the pytorch model !!!!!

I am confused on what the accuracy of the unsupervised model is calculated on...

lebesgue_p

So we have a network model: 
what is the input type
What are the layer types
How is the learning implemented
what is the output


ray_search.py : where the parameters are parsed and the main is executed.
    
    run_unsup(): performs unspervised learning and evaluates at the end of every epoch
        train_unsup(): 
            train_hebb(): The criterion can be something like ... ??? criterion seems to be none always, just like measures. 
                            So are they both to be defined??? 


        evaluate_unsup(): 
            evaluate_hebb(): does the unsupervised evaluation ---------------- we can't get in here (return 0.0 error)
                infer_dataset(): 

    run_sup (): runs the supervised learning if the mode is supervised (??? there are instances where the MLP is unsup???)
        train_sup():
            train_sup_hebb(): weigth change vector which is done only the last layer ?????? +  if loss_acc problem
            train_BP():
        evaluate_sup(): 


    run hybrid ()





# Alright so how to organize the slides:
- I would first give a general explanation of what the code does ( so the different types of training, how 
they differ from each other and what they aim to do ), then I would get into the code with the high level 
explanation of the functions implementing the corresponding learning. 
- I would do a very high level discussion of the flow of the functions and the general 
execution of the program. Just include the name of the functions and what they generally do.
Like Input types and output. 

Then do a main discussion on the methods which are used for training and how the update happens. 
So when trainign a model the main thign to consider is what kind of input is used, 
the update rule used ( in our case it would either be hebbian learning without any loss feedback and classical BP)

Ok so the training is mainly done in multi_layer.py while in ray_search we have the search for the optimal hyperparameters. 

-- FIRST READ
We have seen that the soft hebb network is compared with the classical BP fully supervised method. 
From my understanding the hebbian network is trained first in a fully unsupervised manner in order to obtain  a way of clusterizing the points, 
After which we take it and use a classifier to assign the clusters to the classes. 


HebbHardKrotovLinear get_wta needs to be finished commenting.
In the soft implementation we use the activation function, which creates a vector of activations effectively assigning a probability 
distribution to the activations since their value is between 0 and 1. 


What is the concept of activation?? 
So we have a set of neurons on one layer linked to the neurons in the next layer. Let's say we go from neuron i to neuron k, what is the activation of the 
neuron i? It is the pre synaptic value passed in the activation function? Or is the post synaptic?? 

The datasets: 

ImageNette: width: 160, Imagenette is a subset of 10 easily classified classes from Imagenet (tench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball, parachute).

STL10: 10 classes: (some similar to cifar10)  "width": 96,
- airplane, bird, car, cat, deer, dog, horse, monkey, ship, truck.
- Images are 96x96 pixels, color.
- 500 training images (10 pre-defined folds), 800 test images per class.
- 100000 unlabeled images for unsupervised learning. These examples are extracted from a similar but broader distribution of images. For instance, it contains other types of animals (bears, rabbits, etc.) and vehicles (trains, buses, etc.) in addition to the ones in the labeled set.
- Images were acquired from labeled examples on ImageNet.

CIFAR100: 32x32 color images classified into 100 classes

CIFAR10: 

airplane										
automobile										
bird										
cat										
deer										
dog										
frog										
horse								
		
ship										
truck

Ok so when doing continual learning with the best models we need to set the skip-1 flag to true.

Question
But we trained the best models through the use of ray search which optimizes the training given a certain dataset,
should we do the same in the continual learning setting?? Like we do a ray search on the the best model but using another dataset, 
makes sense. So how to do it? We add it to the continual learning? Or better we do another file which is ray search for continual learning.


We need to train on less classes now. so we start with 2, and then same dataset but two other classes and then we
do an evaluation on the first two. And so on increasing the number of classes per task.

Important: in ray search continual learning and continual learning we use the BLOCKS gatheres from the datasets, maybe you should 
load the preset for the second dataset... but then watch out for the number of layers and stuff...
See if you should use the reset parameter or not in the ray search CL on the model... 

To implement the training on two different classes of hte same dataset we need to consider: 
    - we implemented a method which allows to train on two different datasets. 
    - we need to implement a method which allows us to train on one dataset and everytime we change the classes. 

The problems which I have found during the development of the continual learning approach which uses only subclasses to 
implement a task are: 
[bypassed]- the number of samples specified in the preset corresponds to the total set of samples and not to 
        the dataset which has been deprived of some classes. A possible solution to this is that when we load the preset we count the number 
        of samples present... this solution unfortunately is very hard to implement because of the way the code flow is organized. Unfortunaly the preset is loaded 
        way earlier than the actual dataset, so modifying this information can be only done after the dataset is loaded and eventually modified so that we know the amount of samples present. 
[fixed] - the same assumption is done in the select_dataset() function where the indices variable is intialized by manually inserting 
        the number of indices to create in the list, and not by actually counting the number of samples present.
[fixed] - finally the other problem is caused by the fact that when we select the classes to include in the task they might not start 
        at 0, so we could have classes like [1, 2], problem is torch wants the indexes of the classes to start from 0. So a solution 
        would be to decrease all the indexes of all the classes in the dataset. This has to be done after the dataset has been resized and modified.
        Important is also to extract the labels ( in readable language that is ) associated to the selected classes.


[bypassed] !!!!!!! Problem detected: the ray search CL done per tasks seems only to save the eval run and not also R1 and R2.
[fixed] !!!!!!! Problem detected: continual_learning loads the best model (trained with ray search) but overwrites it, instead we should use
                        a copy so to keep the best model untouched. So we need to figure it out. Just manually create a copy of the model before executing the program.

Everything seems to be working fine. 
Now let's check if the models are saved/retrieved correctly.

To check if the models are saved correctly I would go through the corresponding function, 
same goes for the loading phase. Everything seems to be in check, but I want to clean their code because it is disgusting.
There is redundancy in load layers and a loop which gets executed but there is the chance it won't do anything at all.

So to execute the continual learning: 
   - we have the best models for multiple datasets testing
   - we need to copy them and watch to pass the name of the copied model to the continual learning and not the original one
   - we need to operate in skip_1 mode True and add in continual learning an evaluation phase 

   - reguarding the subset of classes approach we need to train with ray search and find the best models 
   - at this point we save the models and copy them with a different name. 
   - Use the new name in the continual learning using the classes flag.
For both scenarios we need to clean the already existing json files before doing anything because they contain dirty runs.

Everything seems to be working fine reguarding the scaling in continual learning with diff datasets. 
We still need to try the continual learning with subclasses. CL with subclasses works fine. 
[completed]

#################

Now we have to implement the continual leanring. Briefly I would meassure the different in the weights after every 
either epoch, or batch or single image ( though this could be too expensive if done from the get go, so I would do it like in the end to fine tune the meta learning approach). 
With this difference caluclated I am able to understand where the weights chamge and how, additionally I am able to see how much fo the previolsy elarned representation my 
model is able to use with a different dataset. Knowin gthis, I would implement a metalearning approach which modulates the plasticity of certain areas of the network based on the change in weights in that area
and on the loss of the classification. 

To visualize the weight change we need to access to the state_dict of the network. These contains different information, amongst those 
also the weights tensor. They are organized by blocks (which I am assuming are an alias for layer). So we have to store per each block
this tensor, and put them in a dict... no sense just use the state_dict dictionary you dumb fuck. Ok, so maybe we can ust clean it... we ll see I don't think there is an actual need. 
Anyway, the important thing is state we store the current state_dict and the previous state_dict. Simple, we just have to store it before it changes, and that would be the previous state. 
Now, we care to store the difference, should we consider the absolute value of this difference or not? If we just want to know if it was a big or 
small change but we don't care in which direction than we cold consider the absolute value, if we also want to know the direction of the change
then it is necessary to keep the sign. Hinton graphs are the choice to represent this weight change, I still have to find a kind of graph which works but has color, 
like a dense red for a positive change or deep blue for high negative change. The question that I have right know is how should we combine this change 
in a graph? Like we wouldn't really need to visualize it for the meta learning approach to work, it would be more for us. As an example to start we could also create it for just one image to image, 
or one epoch to epoch, or also batch to batch or why not from dataset to dataset. 

First issue, keep in mind we are doing just one epoch. Second it seems that the kernels in the hebbian layers are not 
getting modified during training. It is true that the modification of the weights in a kernel happen in relation
to the backpropagated error and since here there is no error to backpropagate we don't know how to change them in the first place. 
But still, aren't the neurons just an input passed through a function which gives an output? And the weight is related to that 
output right?

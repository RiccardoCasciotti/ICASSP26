
library to visualize the pytorch model !!!!!

I am confused on what the accuracy of the unsupervised model is calculated on...

lebesgue_p

So we have a network model: 
what is the input type
What are the layer types
How is the learning implemented
what is the output


ray_search.py : where the parameters are parsed and the main is executed.
    
    run_unsup(): performs unspervised learning and evaluates at the end of every epoch
        train_unsup(): 
            train_hebb(): The criterion can be something like ... ??? criterion seems to be none always, just like measures. 
                            So are they both to be defined??? 


        evaluate_unsup(): 
            evaluate_hebb(): does the unsupervised evaluation ---------------- we can't get in here (return 0.0 error)
                infer_dataset(): 

    run_sup (): runs the supervised learning if the mode is supervised (??? there are instances where the MLP is unsup???)
        train_sup():
            train_sup_hebb(): weigth change vector which is done only the last layer ?????? +  if loss_acc problem
            train_BP():
        evaluate_sup(): 


    run hybrid ()





# Alright so how to organize the slides:
- I would first give a general explanation of what the code does ( so the different types of training, how 
they differ from each other and what they aim to do ), then I would get into the code with the high level 
explanation of the functions implementing the corresponding learning. 
- I would do a very high level discussion of the flow of the functions and the general 
execution of the program. Just include the name of the functions and what they generally do.
Like Input types and output. 

Then do a main discussion on the methods which are used for training and how the update happens. 
So when trainign a model the main thign to consider is what kind of input is used, 
the update rule used ( in our case it would either be hebbian learning without any loss feedback and classical BP)

Ok so the training is mainly done in multi_layer.py while in ray_search we have the search for the optimal hyperparameters. 

-- FIRST READ
We have seen that the soft hebb network is compared with the classical BP fully supervised method. 
From my understanding the hebbian network is trained first in a fully unsupervised manner in order to obtain  a way of clusterizing the points, 
After which we take it and use a classifier to assign the clusters to the classes. 


HebbHardKrotovLinear get_wta needs to be finished commenting.
In the soft implementation we use the activation function, which creates a vector of activations effectively assigning a probability 
distribution to the activations since their value is between 0 and 1. 


What is the concept of activation?? 
So we have a set of neurons on one layer linked to the neurons in the next layer. Let's say we go from neuron i to neuron k, what is the activation of the 
neuron i? It is the pre synaptic value passed in the activation function? Or is the post synaptic?? 

The datasets: 

ImageNette: width: 160, Imagenette is a subset of 10 easily classified classes from Imagenet (tench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball, parachute).

STL10: 10 classes: (some similar to cifar10)  "width": 96,
- airplane, bird, car, cat, deer, dog, horse, monkey, ship, truck.
- Images are 96x96 pixels, color.
- 500 training images (10 pre-defined folds), 800 test images per class.
- 100000 unlabeled images for unsupervised learning. These examples are extracted from a similar but broader distribution of images. For instance, it contains other types of animals (bears, rabbits, etc.) and vehicles (trains, buses, etc.) in addition to the ones in the labeled set.
- Images were acquired from labeled examples on ImageNet.

CIFAR100: 32x32 color images classified into 100 classes

CIFAR10: 

airplane										
automobile										
bird										
cat										
deer										
dog										
frog										
horse								
		
ship										
truck

Ok so when doing continual learning with the best models we need to set the skip-1 flag to true.

Question
But we trained the best models through the use of ray search which optimizes the training given a certain dataset,
should we do the same in the continual learning setting?? Like we do a ray search on the the best model but using another dataset, 
makes sense. So how to do it? We add it to the continual learning? Or better we do another file which is ray search for continual learning.


We need to train on less classes now. so we start with 2, and then same dataset but two other classes and then we
do an evaluation on the first two. And so on increasing the number of classes per task.

Important: in ray search continual learning and continual learning we use the BLOCKS gatheres from the datasets, maybe you should 
load the preset for the second dataset... but then watch out for the number of layers and stuff...
See if you should use the reset parameter or not in the ray search CL on the model... 

To implement the training on two different classes of hte same dataset we need to consider: 
    - we implemented a method which allows to train on two different datasets. 
    - we need to implement a method which allows us to train on one dataset and everytime we change the classes. 

The problems which I have found during the development of the continual learning approach which uses only subclasses to 
implement a task are: 
[bypassed]- the number of samples specified in the preset corresponds to the total set of samples and not to 
        the dataset which has been deprived of some classes. A possible solution to this is that when we load the preset we count the number 
        of samples present... this solution unfortunately is very hard to implement because of the way the code flow is organized. Unfortunaly the preset is loaded 
        way earlier than the actual dataset, so modifying this information can be only done after the dataset is loaded and eventually modified so that we know the amount of samples present. 
[fixed] - the same assumption is done in the select_dataset() function where the indices variable is intialized by manually inserting 
        the number of indices to create in the list, and not by actually counting the number of samples present.
[fixed] - finally the other problem is caused by the fact that when we select the classes to include in the task they might not start 
        at 0, so we could have classes like [1, 2], problem is torch wants the indexes of the classes to start from 0. So a solution 
        would be to decrease all the indexes of all the classes in the dataset. This has to be done after the dataset has been resized and modified.
        Important is also to extract the labels ( in readable language that is ) associated to the selected classes.


[bypassed] !!!!!!! Problem detected: the ray search CL done per tasks seems only to save the eval run and not also R1 and R2.
[fixed] !!!!!!! Problem detected: continual_learning loads the best model (trained with ray search) but overwrites it, instead we should use
                        a copy so to keep the best model untouched. So we need to figure it out. Just manually create a copy of the model before executing the program.

Everything seems to be working fine. 
Now let's check if the models are saved/retrieved correctly.

To check if the models are saved correctly I would go through the corresponding function, 
same goes for the loading phase. Everything seems to be in check, but I want to clean their code because it is disgusting.
There is redundancy in load layers and a loop which gets executed but there is the chance it won't do anything at all.

So to execute the continual learning: 
   - we have the best models for multiple datasets testing
   - we need to copy them and watch to pass the name of the copied model to the continual learning and not the original one
   - we need to operate in skip_1 mode True and add in continual learning an evaluation phase 

   - reguarding the subset of classes approach we need to train with ray search and find the best models 
   - at this point we save the models and copy them with a different name. 
   - Use the new name in the continual learning using the classes flag.
For both scenarios we need to clean the already existing json files before doing anything because they contain dirty runs.

Everything seems to be working fine reguarding the scaling in continual learning with diff datasets. 
We still need to try the continual learning with subclasses. CL with subclasses works fine. 
[completed]

#################

Now we have to implement the continual leanring. Briefly I would meassure the different in the weights after every 
either epoch, or batch or single image ( though this could be too expensive if done from the get go, so I would do it like in the end to fine tune the meta learning approach). 
With this difference caluclated I am able to understand where the weights chamge and how, additionally I am able to see how much fo the previolsy elarned representation my 
model is able to use with a different dataset. Knowin gthis, I would implement a metalearning approach which modulates the plasticity of certain areas of the network based on the change in weights in that area
and on the loss of the classification. 

To visualize the weight change we need to access to the state_dict of the network. These contains different information, amongst those 
also the weights tensor. They are organized by blocks (which I am assuming are an alias for layer). So we have to store per each block
this tensor, and put them in a dict... no sense just use the state_dict dictionary you dumb fuck. Ok, so maybe we can ust clean it... we ll see I don't think there is an actual need. 
Anyway, the important thing is state we store the current state_dict and the previous state_dict. Simple, we just have to store it before it changes, and that would be the previous state. 
Now, we care to store the difference, should we consider the absolute value of this difference or not? If we just want to know if it was a big or 
small change but we don't care in which direction than we cold consider the absolute value, if we also want to know the direction of the change
then it is necessary to keep the sign. Hinton graphs are the choice to represent this weight change, I still have to find a kind of graph which works but has color, 
like a dense red for a positive change or deep blue for high negative change. The question that I have right know is how should we combine this change 
in a graph? Like we wouldn't really need to visualize it for the meta learning approach to work, it would be more for us. As an example to start we could also create it for just one image to image, 
or one epoch to epoch, or also batch to batch or why not from dataset to dataset. 

[fixed]
First issue, keep in mind we are doing just one epoch. Second it seems that the kernels in the hebbian layers are not 
getting modified during training. It is true that the modification of the weights in a kernel happen in relation
to the backpropagated error and since here there is no error to backpropagate we don't know how to change them in the first place. 
But still, aren't the neurons just an input passed through a function which gives an output? And the weight is related to that 
output right? Ok fixed, I need to execute a deepcopy rather than a copy on the state_dict of the model so the weights change correctly during training.


################### NEUROMODULATION approach
Now another problem arises: 
    - We want to measure the change of weights between two different tasks. This means that we have to store all the weights of the first task, very expensive man. 
    Let's say we don't care about resources usage. We store it, and every few updates, like every 100 images we decide to apply neuromodulation through the analysis of the weight change. 
    This approach could work, but I don't like it because it is very expensive spatially. 
    - We want to measure the change of weights between two different tasks. A different approach from the one used above could be to train only on the first task,
    without storing all the weights,  and what we do is actually analyze the weight update in that particular kernel / zone. This implies that we would 
    need to find a correlation rule between following kernels, so that we are able to apply the neuromodulation in the same style if we organized all the weights in 
    a 2D matrix but without actually doing it.

Out of the methods explained above the second is the one I like the most. So I am going to go with the second done.
So the solution we could utilize for the second approach is to catch is the weight change and scale it. But wouldn't this be as if I apply an upper 
bound to the learning rate /  plasticity of the network? Yes, it is pretty much the same thing because whenever there is a weight update bigger than I would like 
I just limit it. But hold on, I limit it in a certain part of the network, this could mean that I unlock in other parts of the network, but then 
It would mean that I go over threshold on the other part of the network and limit it there too. I think the way I apply neuromodulation cannot be decided only 
based on the weight change, but also what that weight change is changing: like am I modifying zones where task information was stored or am I modifying parts of the network
where no acctual information was stored (like they are almost as the initialized weights before training) ? So I need to know what I am modifying.
Ok, what I can do is during task 1 I can keep track of the average change per kernel, I put them in a 2d matrix where the row 1 contains the average kernel changes for layer 1, and cell 1 is for the 1st kernel.
I take this matrix, and zones where there is high change in the kernel value means that zone is important for task 1 and so if the change is big for task 2 in the same kernel I suppress it and increase the plasticity in other parts of the network.

How is the activation calculated?
DELTA w_ik = n * y_k * x_i -  n * y_k * u_k * w_ik

Like if I want to redistribute the learning in other parts of the network, what should I neuromodulate?
I should act on the learning rate probably, I increase it here and reduce it where I want to redirect the learning.

- Remember that there was a paper which did adaptive learning rate, cite it and be sure to find the similarities/differences with this approach.
- Remember to consider also recurrent connections, maybe they could work. So learn how to implement them and understand if they accept the hebbian learning 
    like the one in the paper or they work in backpropagation setting. Unsupervised RNNs????

The fact about modifying the learning rate is also to consider the adaptive learning rate of softhebb itself.

Ok let's start. I would ignore the adaptive learning rate for now. 
During training we have to consider the way the weights are initialized. In particular, when doing the vaerage weight change in a kernel it could happen that the weights are by chance initialized
the correct way and so they don't really get updated. 
So first let's understand how the weights are initialized. The weights are initialized following a normal distribution, so I guess we just ignore it? Mhhh, let's see, 
If the weights are nitialized correctly for task 1, so there is no big update, then there is no real way of understanding how much that is important for the task 1 during training 
of task 2. In that case how should we behave? Bare in mind we are ignoring the loss of the classificator to keep it unsupervised.
The learnt information is expressed through the loss of the classificator. We have to investigate if using the network's loss to decide 
what parts of the network contain which information makes it supervised learning or still unsupervised. 

To see what happens in the network we can use the UMAP technique. 

Weight visualization: 
    - https://distill.pub/2020/circuits/visualizing-weights/

We need to  study ho the weights change in the network. 
The solutions proposed are about using activation maps and heat maps. The activation maps basically contain the activations relative to a certain layer or kernel? Like do they contain the activation of the single layer and sum all the channels together? Or do they contain the activations per channel, so all the kernels in the same layer organized per spatial zone of the input they worked on and all stacked together? Or maybe an activation map can be all of the three and they are just different kinds of activation maps. 
Ok so the feature map, or activation map, is the result of convoluting one filter among all the ones in the channels over the input image. So the feature map is done by filter in one layer.
Now on with the heat maps, what are they? 
So the heatmaps are basically maps which when overlayed to a certain input allows us to know which parts of an image are useful to recognize something. I don't think they are very useful for our objective. 

I would start with visualizing the activation maps of a certain layer. And then we can graph the average change per filter in the layer and average activation
(so we need to create a semantic dictionary). The semantic dictionary can include sorted key value pairs of the filter and the corresponding activation. In addition we can graph somehow the vaerage filter weight update. 


The semantic dictionary can allow us to kind of sort which kernels are more significant for a certain dataset???

Anyways, let's reorder th e thought real quick. We did msanage to graph bothe the kernels and the activation map of the fifrst convolutional layer. 
Now we need to figure out how to graph the change in the kernels relative to a certain dataset. We can use the idea of calculating the average change in kernel:
to find the mean if we sum up all the changes of the kernels and divide them byt  henumber of kernels we obtain the average change of kernel per layer, so
not much is known about the importance of the single kernel. What we can do is calculate the average change of kernel: 
per every batch of # images we calculate the average. So we find the difference between the first step and the step #, sum all the changes and this represents one of the dataset_len/# changes, so we sum all this changes and divide them by dataset_len/#. At this point we have all kernels which contain the average change. 
When training the network on a new task we need to analyze the change that we have in the kernels, and if the change surpasses the avg change corresponding to a particular kernel we reduce the plasticity and increment the plasticity of other kernels in the 
layer. But to apply the3 plasticity rteroute we need to know which kernels are important for a specific task so that we can avoid modifying those. 

Another idea couldl be to use a recurrent layer so to consider the kernels state before seeing the current new task and somehow incorporate that into thelearning of the new unseen task.
MMMhhh look at the differene with residual layers...

anyways, let's start and implement the function which calculates the difference of the kernels and then calculates the average on the single 
filter every # images. To do so we need to tap into the cnn training procedure, store the activations...

!!!!!! AHHH found huge dataset that we could use: LESUN letsgoooooo

Ok, I calculated the difference in kernels every some steps and now I get about 50 delta tensors of all the kernels in all the layers.
Everything seems to be working fine, the only doubt I have is about the very small weight change I obtain after the sample the network sees, maybe
it is reduced to a very small number right after?? Could be due to the adaptive learning rate I have... better dig deeper into that. 

Now I need to calculate the average change per kernel, how to do so? 
we sum all the kernel changes by summing up all the cells of one diven kernel, and that would give us the change of 1 kernel, we repeat this for all the 
kernels for every different measurement and divide the sum obtained of every kernel by the number of delta measures we have.

Ok, the part where we gather the weight updates, collapse the three channels of every kernel, 
and calculates the average weight change per every kernel and then finally sum all the cells of the averaged 
kernels to obtain one final value per kernel and then we normalize that value for the maxiumum registered average weight change in the kernels is completed. 

Now, I would like to create a semantic dictionary which allows us to say which kernel is mostly important for that particular task or which
top k kernels are most important for that task. This could be doable by sorting all the kernels activations per image, retrieving the top ones and registering
how many times a kernel has fallen in the k first positions, then after the whole dataset has been seen we sort the kernels with the associated value and retrieve the first top k, 
so the first being the most important and the last the least important. And so we combine this idea with the average weight changes registered so that
if the weight change of the  new task is over the average weight change of a particular kernel of the previous task and that kernel is in the top k kernels for task 1, we apply some kind of neuromodulation
like for example reduce plasticity on those kernels and augment it on the ones not in the top k.

The question arises... where should I store the tensor of the average weights change? When should I apply the neuromodulation? 
When  should I check if the threshold of the weight update is broken? When should I calculate the semantic dictionary for the activations of the kernels?

For the complexity of a network: https://medium.com/@marvelous_catawba_otter_200/a-brief-discussion-the-computational-cost-of-backward-propagation-is-approximately-twice-that-of-5dd0eac9b389
                                https://medium.com/@pashashaik/a-guide-to-hand-calculating-flops-and-macs-fa5221ce5ccc

I could store the tensor into a variable, which is accessed in every training and keeps track of the change of every kernel and checks if the average change of that kernel is respected or not. 
If it is then we change the plasticity of that part of the network (with the 2 adjacent kernels for example) and increase it somewhere else where the threshold is not broken.

!!!! Instead of using sum to calculate the value of each kernel I can use standard deviation.

[Problem -- fixed ]
When saving the vector of the average kernel weight change the size of the saved vector is more than 1gb which is impossible, considering that the model
itself is much smaller. 

Ok, so apparently we are saving the delta_weights vector, rather than the avg weights update per kernel.

#############################
The problem I have right now is that I don't know how to keep a link between the most important kernels I have learned during training
and the current kernel during the current training so to apply the threshold check. 
I could associate an index to every kernel, and to keep the top k I put the indexes in a list. 
I need to find a way to access the single kernel instance. 
So, I could technically keep track of the single kernel, but the problem is that when I reload the model to implement continual 
learning I am not able to use the same kernel references, this is because when the model is reloaded the references are reassigned. 
So, I need to have a reference system which is relative to the model and not to the memory system. I could keep an index, I think the 
positions of the tensors is not changed, meaning that a kernel will have a relative position in the layer which is constant, so I can use a dictionary, made up of 
